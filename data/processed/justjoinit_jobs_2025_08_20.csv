Type of work,Experience,Employment Type,Operating mode,Job ID,Job Title,Employer Name,Job Description,Salaries,Category_manual,Category
Full-time,Senior,B2B,Remote,0,Senior Oracle Database Administrator,Hirexa,"Job Title: Oracle DBA Location: Remote Employment Type: B2B About Hirexa Solutions: Hirexa Solutions is a leading player in the recruitment ecosystem across the United States, United Kingdom, Europe, and India. As the fastest-growing next-generation provider of technology talent, we empower our clients to become resourceful, achieve higher productivity, adopt agile structures, and effectively execute project deliverables. Envisioned and co-founded by veterans of the Information Technology industry, our mission is to make recruitment efficient, flawless, and cost-effective. Our unwavering commitment to strategic investments in intelligent technology underscores our passion for people and our dedication to helping organizations realize their true potential. Job Description Key Responsibilities: Design, install, configure, and maintain database systems (e.g., SQL Server, Oracle, MySQL, PostgreSQL). Monitor database performance and tune complex queries for optimal performance. Manage database backup, recovery, high availability (HA), and disaster recovery (DR) strategies. Implement security measures to protect sensitive data and ensure compliance with data governance policies. Perform database upgrades, patching, and migration activities. Collaborate with development and DevOps teams to support application deployment and database changes. Troubleshoot database-related issues and provide 24/7 support as needed. Create and maintain documentation, standards, and procedures. Automate routine tasks using scripts or tools (e.g., PowerShell, Bash, Python). Mentor junior DBAs and support database-related aspects of software development lifecycle (SDLC). Required Skills: 8 to 10 years of experience in a DBA role with enterprise-level databases. Expertise in at least one major RDBMS (e.g., Microsoft SQL Server, Oracle, PostgreSQL). Strong knowledge of SQL, T-SQL/PL-SQL, indexing, and query optimization. Experience with database monitoring and performance tuning tools. Proven experience with backup and recovery strategies and tools. Knowledge of cloud database platforms (e.g., AWS RDS, Azure SQL Database, Google Cloud SQL). Familiarity with replication, clustering, and HA/DR configurations. Strong scripting skills (e.g., PowerShell, Shell, Python) Position Overview: For one of our partners, we are seeking a Oracle DBA who will be responsible for MSSQL,Oracle DBA. The ideal candidate will possess the necessary skills and experience to contribute to the success of our partner organization. How to Apply: If you are interested in this opportunity, please submit your resume. We look forward to hearing from you!","[{""min"": 900, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Database Administration,Database Administration
Full-time,Senior,Permanent or B2B,Remote,1,üëâ GenAI Lead,Xebia sp. z o.o.,"üü£ About project: GenAI Lead will take ownership of designing, developing, and deploying advanced AI solutions, specializing in machine learning, deep learning, and generative AI technologies. This role demands a strategic leader with deep technical expertise to architect scalable GenAI solutions, guide a team of AI professionals, and deliver transformative business outcomes through innovative AI applications. üü£ You will be: designing and implementing scalable Generative AI (GenAI) systems using Large Language Models (LLMs), vision models, and vector databases, leading the design, development, and deployment of AI-driven solutions, including machine learning models, deep learning frameworks, and generative AI applications, overseeing seamless integration of AI solutions with Azure AI Studio, SharePoint, and Power BI for deployment, reporting, and visualization, collaborating with cross-functional teams to shape AI strategies and deliver high-impact solutions, providing technical leadership in implemention of Agentic frameworks and tools like LlamaIndex to advance AI workflows, mentoring and empowering a team of AI developers, fostering a culture of innovation, collaboration, and technical excellence, staying at the forefront of AI advancements, proposing creative and practical solutions to address complex challenges, enforcing best practices in code quality, model optimization, and solution scalability to ensure robust production-ready systems. üü£ Your profile: Bachelor‚Äôs or Master‚Äôs degree in computer science, Data Science, Engineering, or a related field (PhD preferred), 8+ years of hands-on experience in AI/ML development, including at least 3 years in leadership or solution architect capacity, demonstrated success in delivering machine learning, deep learning, and generative AI projects in production environments, exceptional programming proficiency in Python and experience with frameworks such as TensorFlow, PyTorch, or equivalent. in-depth expertise in LLMs, vision models, vector databases, and RAG applications, strong command of Azure AI Studio, SharePoint, and basic Power BI for integration and reporting purposes, proven experience with Agentic frameworks and tools like LlamaIndex for intelligent system development, outstanding problem-solving abilities, with a knack for translating business needs into technical solutions, superior communication and leadership skills to manage teams, align stakeholders, and drive project success, excellent verbal and written communication skills in English (min. C1). üü£ Nice to have: experience with cloud platforms beyond Azure (e.g., AWS, GCP), knowledge of DevOps practices (e.g., CI/CD pipelines, Kubernetes), familiarity with AI ethics, governance, and compliance standards, exposure to advanced visualization tools or BI platforms. üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 40000, ""type"": ""Net per month - B2B""}, {""min"": 27000, ""max"": 32500, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,2,Senior Data Engineer (pharmaceutical industry),emagine Polska,"PROJECT INFORMATION: Industry: Pharmaceutical Remote work: remote + occassional visits to the Warsaw office FTE: full-time Project language: English Project length: 12 months contracts + prolongations (we‚Äôre looking for people open to long-term) Start: 01.09.2025 Assignment type: B2B Introduction & Summary: The position of Senior Data Engineer is crucial in facilitating and enhancing data and AI products aimed at improving patient lives globally. As an experienced Data Engineer, you will collaborate with cross-functional product teams to design scalable data solutions that are integral to our enterprise data platform. Main Responsibilities: Collaborate with cross-functional teams to design, develop and maintain data pipelines. Act as a liaison between data scientists and IT teams to improve data availability and validation. Ingest, integrate, and curate data from various internal and external sources. Utilize IaC and CI/CD methodologies to deliver infrastructure components to AWS environments. Develop automated tests to ensure data accuracy and quality. Implement best practices and standards for effective data management. Key Requirements: Strong proficiency in Python programming. Proficiency in developing ETL pipelines with Apache Spark, Glue, and Delta Lake. SQL knowledge and experience with Data Warehouse concepts. Experience in automated unit testing and code quality inspection. Working experience with Git, Azure pipelines, and Infrastructure-as-Code (e.g., CDK). Experience with AWS services (e.g., S3, Lambda functions, Athena). Experience in the pharma domain or other regulated areas is considered an advantage. Fluency in English, both written and spoken. Nice to Have: Teamwork skills and willingness to receive feedback. Ability to work independently and manage deadlines effectively. Other Details: This role is positioned within a collaborative team dedicated to innovation in patient care. The organizational culture focuses on personal contributions and career development. It offers a unique chance to impact projects and patient outcomes positively. We offer: Long-term cooperation. Transparently built relations based on trust and fair play. Co-financed benefits: Medicover card, Multisport card.",[],Data Engineering,Data Engineering
Full-time,Manager / C-level,Permanent,Hybrid,3,Data Assets Principal,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Data Assets Principal Are you ready to make a significant impact in the world of data governance and drive the strategic value of enterprise data assets? We are seeking a talented Data Assets Principal to become a vital part of our dynamic Data Assets, Analytics and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in developing and running our core data assets across market, consumer, customer, product, company, environmental and master data domains to enable cutting-edge AI solutions and data-driven decision-making. Our diverse, international team, spanning Poland, Germany and India, is dedicated to transforming data into strategic business assets ‚Äì from data discovery and quality assurance to the seamless delivery of fully governed data products. We pride ourselves on delivering innovative data solutions that empower data scientists, BI developers, and AI agents through robust data governance, quality frameworks, and modern data management practices powered by a cutting-edge tech stack featuring Azure and Databricks. If you're passionate about data management, and eager to lead cross-functional teams in building world-class data assets, we want to hear from you! Key Tasks & Responsibilities: Develop and implement a comprehensive data strategy to prepare data assets for easy consumption by business stakeholders, internal IT teams, autonomous AI systems, and data professionals, while drastically decreasing time to market for data science and business intelligence initiatives through improved data availability, quality, and semantics. Lead and coordinate core data asset teams (market, consumer, customer, product, company, environmental, and master data management), including data analysts, data stewards, data governors, data architects, and data engineers, to optimize data assets for maximum usability and business value, while maintaining a focus on cost-effectiveness. Establish and enforce data governance frameworks, policies, and standards to ensure data quality, completness, consistency, semantics, documentation, security, compliance, and regulatory adherence. Actively identify, evaluate, and prioritize new data sources and datasets, determining their strategic value and integration feasibility into the data asset portfolio. Foster collaboration across cross-functional teams to promote data governance best practices, driving a culture of data stewardship and accountability throughout the organization. Participate in strategic data initiatives and exploratory projects that investigate emerging data management technologies, keeping Bayer at the forefront of data asset optimization in the consumer health sector. Create and execute Data Assets, Analytics and AI Platform‚Äôs strategic vision as part of the Platform‚Äôs Leadership Team. Qualifications & Competencies (education, skills, experience): Master's or PhD degree with 8+ years of experience in data management, data analytics, data engineering, data architecture, data science or related fields. Experience with developing and delivering data products within modern tech stack, particularly Azure, Databricks, Unity Catalogue, Collibra and Power BI. Strong understanding of data modelling, ETL processes, data governance platforms, data quality tools & methodologies, metadata management, SQL, Python and Agentic AI concept. Extensive experience in leading cross-functional teams and managing complex data strategy, governance and quality initiatives in enterprise environments. Deep understanding of data governance frameworks, data stewardship principles, master & meta data management, data privacy regulations, licensed data contracts and regulatory compliance requirements. Ability to design and implement comprehensive data quality frameworks, data lineage tracking, and automated monitoring systems. Strong analytical and problem-solving skills with keen attention to detail and ability to translate business requirements into technical data solutions. Excellent interpersonal, communication and collaboration skills, with proven ability to engage and influence stakeholders at all organizational levels, from technical teams to C-suite executives. Experience with agile methodologies, project management, and change management in data transformation initiatives. Fluent in English, both written and spoken. Ability to travel. What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 28000, ""max"": 42000, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,4,Database Engineer - Technical Platform Services,Allegro,"Location: Company: Allegro sp. z o.o. Team: Technology Contract Type: Employee A hybrid work model requires 1 day a week in the office The Technical Platform Teams provide the foundations for the structure and operations of the entire Allegro platform. This is where we build tools, development and infrastructure stacks and automation systems used by more than a hundred Allegro product teams and by the customer support department. We develop the full stack, starting from our own server room and private cloud, through containerization, mobile apps and application software to tools which offer AI support to our customers. At the same time, we are responsible for the performance, availability and security of the entire platform. - You will eliminate traditional infrastructure in favor of database services managed in the public cloud and/or those based on Kubernetes, - This is a process that won't happen right away, so you will also have to take care of the existing database infrastructure so that it runs smoothly and is always available, - At the same time, you will create and maintain tools to improve, optimize and automate tasks in our infrastructure. - Have knowledge of MySQL, PostgreSQL, Oracle in the field of database administration, - Are familiar with as many as possible tools supporting the above technologies (PMM, percona-toolkit, pg_repack, Grid Control, RMAN, simpana, etc.), - Have knowledge of the GNU stack - bash, grep, awk, sed, as well as, of the Python language (as we create many services from simple scripts that automate backup / restore actions to complex data integrations based on the more advanced frameworks like flask or django) - Familiarity with Docker and Kubernetes, - Some knowledge of the database services in GCP (preferred) or Azure - A hybrid work model that you will agree on with your leader and the team. We have well-located offices (with fully equipped kitchens and bicycle parking facilities) and excellent working tools (height-adjustable desks, interactive conference rooms) - Annual bonus up to 10% of the annual salary gross (depending on your annual assessment and the company's results)- A wide selection of fringe benefits in a cafeteria plan ‚Äì you choose what you like (e.g. medical, sports or lunch packages, insurance, purchase vouchers)- English classes that we pay for related to the specific nature of your job- Laptop with m1 processor, 32GB RAM, SSD - a 16‚Äù or 14‚Äù MacBook Pro or corresponding Dell with Windows (if you don‚Äôt like Macs), two monitors and all other gadgets that you should need- Working in a team you can always count on ‚Äî we have on board top-class specialists and experts in their areas of expertise- A high degree of autonomy in terms of organizing your team‚Äôs work; we encourage you to develop continuously and try out new things- Hackathons, team tourism, training budget and an internal educational platform, MindUp (including training courses on work organization, means of communications, motivation to work and various technologies and subject-matter issues)- If you want to learn more, check it out Apply to Allegro and see why it is #dobrzetubyƒá (#goodtobehere)",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,5,Oracle SQL/APEX Developer,Pretius,"W Pretius poszukujemy Oracle SQL/APEX Developera w projekcie platformy w obszarze Healthcare na rynku UK. Lokalizacja: zdalnie lub Warszawa Wynagrodzenie: 90-150 pln netto/h O projekcie: Wsp√≥≈Çpraca z zespo≈Çem projektowym, szacowanie zmian, definiowanie API, wsparcie test√≥w Tworzenie formatek APEX, zapyta≈Ñ SQL i pakiet√≥w PL/SQL Wykorzystanie technologii frontendowych do budowy UI Udzia≈Ç w R&D i wprowadzaniu nowych rozwiƒÖza≈Ñ technicznych Stack: SQL, PL/SQL, Oracle APEX (wer. 18+), Oracle Cloud Oczekiwania: Dobra znajomo≈õƒá relacyjnych baz danych (struktury, SQL, PL/SQL - funkcje, procedury, pakiety) Do≈õwiadczenie w budowaniu aplikacji web komunikujƒÖcych siƒô z bazƒÖ danych Znajomo≈õƒá standard√≥w w obszarach test√≥w, CI/CD, jako≈õci kodu Podstawowa znajomo≈õƒá HTML/CSS/JS Jƒôzyk angielski - B2+ Co oferujemy w Pretius? Stawiamy na d≈Çugofalowe relacje oparte na uczciwych zasadach i rzetelno≈õci Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Mo≈ºliwo≈õƒá pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnƒôtrzne, konferencje, certyfikacje","[{""min"": 90, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,6,Senior Oracle Developer,P&P Solutions,"Poszukujemy dw√≥ch do≈õwiadczonych Programist√≥w Oracle (PL/SQL) do udzia≈Çu w rozbudowanym projekcie dla firmy z sektora energetycznego zlokalizowanej we Wroc≈Çawiu. Projekt zak≈Çada wieloetapowe dzia≈Çania rozwojowe nad systemem informatycznym bazujƒÖcym na technologii Oracle ‚Äì z silnym naciskiem na jako≈õƒá kodu, optymalizacjƒô wydajno≈õci oraz wsparcie architektoniczne. üìÖ Start projektu: wrzesie≈Ñ 2025 üìç Tryb pracy: Zdalnie + sporadyczne wizyty w biurze we Wroc≈Çawiu Min. 10 lat komercyjnego do≈õwiadczenia z PL/SQL i Oracle DB Zaawansowana znajomo≈õƒá projektowania struktur, optymalizacji zapyta≈Ñ, administracji Znajomo≈õƒá transformacji XML, JSON na poziomie bazy danych Praca z GIT, testy jednostkowe (np. utPLSQL) Do≈õwiadczenie w ≈õrodowisku SCRUM, znajomo≈õƒá JIRA üéØ Mile widziane: Znajomo≈õƒá Pythona Do≈õwiadczenie w systemach czasu rzeczywistego Projekty w bran≈ºy energetycznej lub elektroenergetycznej Projektowanie i implementacja struktur bazodanowych w Oracle Tworzenie z≈Ço≈ºonych procedur i optymalizacja zapyta≈Ñ PL/SQL Wsparcie architekta rozwiƒÖzania i architekta biznesowego Udzia≈Ç w projektowaniu aplikacji i procesie testowania (w tym testy automatyczne) Nadz√≥r technologiczny nad rozwiƒÖzaniami bazodanowymi Udzia≈Ç w projektowaniu standard√≥w dokumentacyjnych i rozwoju kompetencji zespo≈Çu ‚úÖ StabilnƒÖ i d≈ÇugoterminowƒÖ wsp√≥≈Çpracƒô ‚Äì projekt przewidziany na 12 miesiƒôcy z mo≈ºliwo≈õciƒÖ kontynuacji ‚úÖ Elastyczny model pracy ‚úÖ Atrakcyjne wynagrodzenie do 150 z≈Ç/h ‚úÖ Mo≈ºliwo≈õƒá wp≈Çywu na kluczowe decyzje technologiczne ‚úÖ Wsp√≥≈Çpraca przy projekcie o znaczeniu strategicznym ‚úÖ WspierajƒÖce i profesjonalne ≈õrodowisko ‚úÖ Dostƒôp do nowoczesnych narzƒôdzi i technologii","[{""min"": 120, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Mid,Any,Remote,7,üíª Delphi Developer üíª,LSI Software,"Nasze rozwiƒÖzania otaczajƒÖ Ciƒô w ≈ºyciu codziennym ‚Äì korzystajƒÖ z nich üõí sklepy, üçΩ restauracje, üè® hotele oraz üé• kina na ca≈Çym ≈õwiecie. Posiadamy 30 letnie do≈õwiadczenie w tworzeniu oprogramowania oraz dystrybucji innowacyjnego sprzƒôtu wspierajƒÖcego prowadzenie biznesu. MisjƒÖ LSI Software jest dostarczenie naszym Klientom tego co niezbƒôdne, aby mieli przestrze≈Ñ na zwinne dzia≈Çania i rozw√≥j. Do≈ÇƒÖcz do naszego zespo≈Çu, gdzie praca nie jest nudna! üìå Lokalizacja: Pracujemy w biurze w ≈Åodzi, ale je≈õli Ty nie mo≈ºesz, to do≈ÇƒÖcz do nas zdalnie : ) Zapraszamy Ciebie, je≈õli: Masz minimum 2 lata do≈õwiadczenia na stanowisku Programisty Delphi, Znasz bazƒô danych Microsoft SQL Server oraz jƒôzyk T-SQL, Posiadasz praktycznƒÖ znajomo≈õƒá ActiveX Data Objects jako dostƒôpu do baz danych, Masz do≈õwiadczenie w pracy w zespo≈Çach projektowych, Wyr√≥≈ºnia Ciƒô umiejƒôtno≈õƒá logicznego i analitycznego my≈õlenia, Znasz jƒôzyk angielski na poziomie umo≈ºliwiajƒÖcym czytanie dokumentacji technicznej. Bƒôdziesz odpowiadaƒá za: Utrzymanie oraz rozwijanie aplikacji desktopowych, Aktywny udzia≈Ç w planowaniu zada≈Ñ, prace programistyczne, codzienne spotkania, podsumowania, retrospekcje, Zapewnienie wysokiej jako≈õci dostarczanego kodu, Tworzenie odpowiedniej dokumentacji programistycznej, Wsparcie merytoryczne dla nowych cz≈Çonk√≥w zespo≈Çu. Benefity: dofinansowanie zajƒôƒá sportowych prywatna opieka medyczna dofinansowanie szkole≈Ñ i kurs√≥w ubezpieczenie na ≈ºycie elastyczny czas pracy mo≈ºemy zaczƒÖƒá miƒôdzy 7 a 10 owoce spotkania integracyjne brak dress code‚Äôu kawa / herbata parking dla pracownik√≥w strefa relaksu program rekomendacji pracownik√≥w pikniki rodzinne co drugi piƒÖtek kr√≥tszy dzie≈Ñ pracy w≈Çasna sala kinowa w biurze",[],Unclassified,Unclassified
Full-time,Mid,Mandate,Remote,8,Data Scientist (Healthcare/AI),SAVENTIC HEALTH sp. z o.o.,"About Us: Saventic Health is a mission-driven health-tech startup based in Warsaw, dedicated to transforming the diagnosis of rare diseases using cutting-edge Artificial Intelligence. We are developing innovative solutions that have the potential to significantly shorten diagnostic timelines and improve patient outcomes. As a startup, we thrive on innovation, agility, and a collaborative spirit where every team member makes a tangible impact. We work with complex medical data, requiring sophisticated approaches to extract meaningful insights. The Opportunity: We are looking for a motivated and skilled Data Scientist to join our team and contribute to the development of AI-powered diagnostic tools. This is an excellent opportunity to grow your career by applying your data science skills to challenging and meaningful problems in the healthcare domain. As a Data Scientist on our team, you will work alongside senior scientists and engineers to analyze complex medical data, develop and implement machine learning models, and contribute to the core algorithms that power our platform. You will be involved in various stages of the model development lifecycle, from data exploration and preprocessing to model training, evaluation, and supporting deployment efforts. Your Impact: As a key member of our data science team, you will: Contribute to Better Diagnoses: Play an active role in developing and refining machine learning models that aid in the faster and more accurate diagnosis of rare diseases. Help Uncover Insights: Analyze diverse datasets to identify patterns and generate insights that inform model development and product strategy. Apply ML Techniques: Implement and experiment with various machine learning algorithms to solve specific clinical and business problems. Support Innovation: Contribute to the team's efforts in exploring and utilizing effective data science methodologies. Key Responsibilities: Analyze complex medical datasets, including structured and potentially unstructured data, to identify trends and patterns. Develop, train, evaluate, and implement machine learning models using appropriate algorithms and techniques. Perform exploratory data analysis (EDA), feature engineering, and data preprocessing, often in collaboration with Data Engineers and senior team members. Validate model performance using robust metrics and methodologies. Communicate findings, results, and insights clearly to technical team members and potentially other stakeholders. Stay current with relevant advancements in Machine Learning and data science practices. Collaborate effectively within a team environment, working with Data Engineers, MLOps Engineers, other Data Scientists, and domain experts. Contribute to the documentation of models, experiments, and processes. Who You Are: Experienced Data Scientist: Proven practical experience working as a Data Scientist with demonstrated ability to contribute effectively to projects. ML Fundamentals: Solid understanding and hands-on experience with core Machine Learning algorithms and concepts (e.g., regression, classification, clustering, feature selection, validation). Python Proficient: Proficiency in Python is required, along with practical experience using essential data science libraries (e.g., Pandas, NumPy, Scikit-learn). Data Handling Skills: Experience working with real-world datasets, including cleaning, preprocessing, and feature engineering. Analytical Mindset: Good analytical and problem-solving skills with attention to detail. Clear Communicator: Ability to explain technical work and findings to team members. Educated: Bachelor's or Master's degree in Computer Science, Statistics, Mathematics, Physics, or a related quantitative field. Team Player & Learner: Collaborative attitude, eager to learn new techniques, and adaptable to a dynamic startup environment. Nice to Haves: Experience with Natural Language Processing (NLP) techniques and libraries (e.g., spaCy, NLTK, basic understanding of embeddings or text classification). Experience with Deep Learning frameworks (e.g., PyTorch, TensorFlow, Keras). Experience working within the healthcare, clinical research, or biomedical domain. Familiarity with cloud platforms (AWS, GCP, Azure) and their ML services. Experience with data visualization tools (e.g., Matplotlib, Seaborn, Plotly). Experience with SQL for data extraction. What We Offer: An opportunity to make a real-world impact by contributing to solutions that help diagnose rare diseases. A chance to work on challenging data science problems with complex medical data. Significant opportunities for learning and professional growth within a supportive team. Exposure to cutting-edge AI applications in healthcare. A dynamic, innovative, and collaborative startup culture. Ready to grow your data science career and make a difference? If you are a Data Scientist passionate about using your skills to solve meaningful problems in healthcare, we encourage you to apply!",[],Data Science,Data Science
Full-time,Manager / C-level,B2B,Hybrid,9,Chief of Data & AI,WealthArc,"Join a team of innovators building the data infrastructure and AI core for next-gen wealth and asset management. We‚Äôre not layering AI on top of legacy finance ‚Äî we‚Äôre rebuilding the system from the data up. We‚Äôre hiring a visionary, execution-driven Chief of Data & AI to architect our end-to-end data infrastructure and AI strategy. You‚Äôll lead with bold ideas and deep expertise, owning everything from scalable pipelines to proprietary models to applied machine intelligence. Your work won‚Äôt support the business ‚Äî it will be the business. The ideal candidate will combine bold innovation with deep expertise, driving transformative impact in a breakthrough, data-driven environment. This is a critical leadership role that will define and execute our data vision, helping to shape intelligent, data-driven capabilities across our entire platform. We are building modern, scalable data infrastructure specifically designed for the complex needs of wealth and asset management, enabling advanced analytics, real-time decision-making, and AI innovation across the business. Location: Hybrid (Poland, Switzerland, USA)",[],Data Science,Data Science
Full-time,Mid,B2B,Remote,10,Data Engineer (pharma),7N,"Work Mode : Remote from the territory of Poland Expectations 3+ years of experience in data engineering, 1+ year of experience in Python with data frameworks like Pandas Proficiency in SQL , Python , and Pandas for data manipulation Experience in dbt for transformations Strong understanding of data modelling , ETL processes , and data warehousing concepts Ability to set up and manage automated data pipelines using GitLab CI/CD Experience with Snowflake 's architecture, including warehouses, schemas, and security Ability to write efficient Python scripts for data processing, leveraging Pandas for data wrangling and analysis Experience documenting data workflows Ability to collaborate effectively in cross-functional, international teams Sc., B.Eng. (or higher) in Computer Science, Data Engineering or related fields Excellent verbal and written communication skills (in English and Polish) Ongoing support from a dedicated agent , taking care of your project continuity, client contact, necessary formalities, work comfort and development, Consultant Development Program ‚Äì advice on growth planning based on the latest trends and market needs in IT, including consultations with agents and growth mentors, Access to 7N Learning & Development ‚Äì a development and educational platform with webinars, a library of articles and industry reports, and regular invitations to one-time and recurring development events ‚Äì technical, business, and lifestyle, Spectacular integration events , both for you (e.g., annual Kick-Off trip , Christmas parties, or Summer Olympics sports events) and for your loved ones (e.g., family picnics, movie premieres), Professional development not only during the project ‚Äì you can get involved in knowledge transfer to others within the 7N Services offering directed at 7N clients, Relationships and access to the knowledge of the most experienced IT experts in the market ‚Äì the average professional tenure of our consultants in Poland is over 10 years, A complete benefits package , including funding for medical care, life insurance, sports cards for you and your loved ones, as well as discounts in stores in Poland and abroad.","[{""min"": 130, ""max"": 145, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,11,Database Administrator,PAYBACK,"üîµ PAYBACK is the world‚Äôs leading multi-partner loyalty program. As an international player, we operate in German, Italian, and Austrian markets. More than 10 million active customers already use the German PAYBACK app and mobile PAYBACK services via their smartphones. Your responsibilities: Daily operations, monitoring and patching Supporting the productive business in all upcoming questions Supporting developers Performance analysis and tuning Evaluation and implementation of new Oracle database features Develop ideas and methods to optimize the given infrastructure ‚Äì be innovative Your Profile: Distinct Service- and Customer focus Knowledge about Oracle databases (12c and above), preferred in High Availability Environments (RAC, DataGuard) Know How about Linux operating systems (preferred RedHat flavors) Basic PL/SQL development skills Scripting Know How on Linux environments (Shell, Python, Perl) would be an advantage Good Tuning-, Trace- and Performance-Know How for databases would be an advantage Experience with further database technologies (PostgreSQL, Redis, Cassandra) would be an advantage Experience with Cloud-Systems (GCP) would be an advantage How about? Employment contract? üìù Of course. With us you do not have to worry about stable employment. Benefits? üèãÔ∏è‚Äç‚ôÄÔ∏è We have them! Among other: corporate incentive program, sport card, private medical care. Working in a hybrid model? üè† Of course! You work with us 3 days a week from the office, 2 days a week from home. ‚ÄéFlexible working hours? ‚è∞ Sounds great! We start working between 8 to 10. Work wherever you want? üå¥ In PAYBACK you have the opportunity. Working 100% remotely, also from European countries for 10 days a year Trainings? üß† Of course. We provide training to develop hard and soft skills. Convenient location? üè† Sure! We invite you to our new office at Rondo Daszy≈Ñskiego, but we are currently also working remotely. Friendly atmosphere at work? ü§ùüèª Yes! In PAYBACK, people are the most important asset‚Äé. Dress code? üëï We definitely say no. There are no rigid dress code rules in our company, sneakers are more than welcome. ‚ÄéSomething is missing? üñê Open communication is our priority, so dare to ask!‚Äé",[],Database Administration,Database Administration
Full-time,Mid,B2B,Remote,12,Big Data Developer,B2Bnetwork,"KOMPETENCJE OBLIGATORYJNE: Do≈õwiadczenie zawodowe na stanowisku zwiƒÖzanym z przetwarzaniem du≈ºych zbior√≥w danych jako programista, minimum 2 lata Do≈õwiadczenie projektowe w przetwarzaniu du≈ºych zbior√≥w danych, minimum 1 projekt Do≈õwiadczenie projektowe w programowaniu w jƒôzyku Python, minimum 1 projekt Do≈õwiadczenie projektowe w ≈õrodowisku obliczeniowym on-premise, minimum 1 projekt Do≈õwiadczenie w programowaniu w ≈õrodowisku Apache Spark Do≈õwiadczenie w programowaniu w Python Do≈õwiadczenie w programowaniu w Apache Airflow Do≈õwiadczenie w programowaniu w SQL Znajomo≈õƒá zagadnie≈Ñ Hadoop Programowanie proces√≥w ELT/ETL Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z procesami CI/CD Umiejƒôtno≈õƒá korzystania z systemu kontroli wersji (Git) KOMPETENCJE DODATKOWE: Wykszta≈Çcenie wy≈ºsze Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Airflow (np. Airflow Fundamentals lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá tworzenia DAG√≥w Airflow (np. Dag Authoring lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL (np. W3Schools SQL Certificate lub r√≥wnowa≈ºny) WYMAGANIA TECHNICZNE: Apache Spark Python Apache Airflow SQL Hadoop Git","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,13,Big Data Developer,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 2-letnie do≈õwiadczenie na stanowisku zwiƒÖzanym z przetwarzaniem du≈ºych zbior√≥w danych jako programista Do≈õwiadczenie projektowe w przetwarzaniu du≈ºych zbior√≥w danych Do≈õwiadczenie projektowe w programowaniu w jƒôzyku Python Do≈õwiadczenie projektowe w ≈õrodowisku obliczeniowym on-premise Do≈õwiadczenie w programowaniu w ≈õrodowisku Apache Spark Do≈õwiadczenie w programowaniu w Apache Airflow Do≈õwiadczenie w programowaniu w SQL Znajomo≈õƒá zagadnie≈Ñ Hadoop Do≈õwiadczenie w programowaniu proces√≥w ELT/ETL Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z procesami CI/CD Umiejƒôtno≈õƒá korzystania z systemu kontroli wersji (Git) Dobra organizacja pracy w≈Çasnej, orientacja na realizacje cel√≥w Umiejƒôtno≈õci interpersonalne i organizacyjne, planowanie Komunikatywno≈õƒá, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista i dociekliwo≈õƒá Zdolno≈õƒá adaptacji i elastyczno≈õƒá, otwarto≈õƒá na sta≈Çy rozw√≥j i gotowo≈õƒá uczenia siƒô Mile widziane Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Do≈õwiadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Airflow (np. Airflow Fundamentals lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá tworzenia DAG√≥w Airflow (np. Dag Authoring lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL (np.. W3Schools SQL Certificate lub r√≥wnowa≈ºny) Kluczowe zadania Projektowanie, implementacja i utrzymanie rozwiƒÖza≈Ñ do przetwarzania du≈ºych zbior√≥w danych z wykorzystaniem jƒôzyka Python oraz SQL Realizacja projekt√≥w w ≈õrodowiskach obliczeniowych on-premise z wykorzystaniem Apache Spark i Apache Airflow Budowa i rozw√≥j proces√≥w integracji danych w modelu ETL/ELT Przetwarzanie danych w ≈õrodowiskach opartych o technologiƒô Hadoop Tworzenie i utrzymywanie wydajnych pipeline‚Äô√≥w danych oraz automatyzacja zada≈Ñ przetwarzania danych Wdra≈ºanie rozwiƒÖza≈Ñ zgodnych z praktykami CI/CD oraz praca z systemem kontroli wersji Git Wsp√≥≈Çpraca z zespo≈Çami projektowymi w celu realizacji cel√≥w biznesowych zwiƒÖzanych z analizƒÖ i przetwarzaniem danych Planowanie i organizacja w≈Çasnej pracy w spos√≥b umo≈ºliwiajƒÖcy realizacjƒô zada≈Ñ zgodnie z harmonogramem Aktywne rozwiƒÖzywanie problem√≥w, analiza danych oraz usprawnianie istniejƒÖcych proces√≥w CiƒÖg≈Çe poszerzanie wiedzy technicznej i gotowo≈õƒá do nauki nowych technologii oraz narzƒôdzi","[{""min"": 90, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,14,Data Engineer,Webellian Sp.z o o,"About the Webellian Webellian is a well-established Digital Transformation and IT consulting company committed to creating a positive impact for our clients. We strive to make a meaningful difference in diverse sectors such as insurance, banking, healthcare, retail, and manufacturing. Our passion for cutting-edge and disruptive technologies, as well as our shared values and strong principles, are what motivate us. We are a community of engineers and senior advisors who work with our clients across industries, playing a deep and meaningful role in accelerating and realizing their vision and strategy. About the position We are looking for Data Engineer , to work on a project for one of our key customers. You will work in hybrid mode with your teammates based in Poland and other stakeholders located worldwide and you will be in direct contact with business users of the solution. Objective: The primary goal for the Data Engineer is to oversee and drive the backend development Business Intelligence (BI) and analytics tool in the Finance stream. What you will do: Development in Snowflake and DBT (Senior). What you know: Technical Proficiency: Expertise in Snowflake and DBT is essential. A strong background in data engineering, data modeling and database management is required. Analytical Thinking: Enjoy complex problem-solving and have a keen interest in developing innovative, well-designed solutions to enhance data analysis and reporting capabilities. Experience in setting CI/CD pipelines for seamless code integration Good to have: Leadership and team-work: Experience in leading and working in large, diverse development teams and projects. Self leadership. Proactive work approach. Communication: Proactive communication skills and ability to communicate with different stakeholders Experience Migrating from Onprem to Cloud. What we offer Contract under Polish law: B2B or Umowa o Pracƒô Benefits such as private medical care, group insurance, Multisport card There are English classes available Hybrid work (at least 1 day/week on-site) in Warsaw (Mokot√≥w) Opportunity to work with excellent professionals High standards of work and focus on the quality of code New technologies in use Continuously learning and growth International team Pinball, PlayStation & much more (on-site) Interested? Please click the ‚ÄúApply for this job‚Äù button and send us your CV in English. Please include the following statement: I hereby authorize Webellian Poland Sp. z o.o. to the process personal data provided in this document for realising the recruitment process pursuant to the Personal Data Protection Act of 10 May 2018 (Journal of Laws 2018, item 1000) and in agreement with Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation).",[],Data Engineering,Data Engineering
Part-time,Mid,B2B,Remote,15,Regular Data Scientist,P&P Solutions,"üöÄ We are looking for a Data Scientist for our client from the insurance sector to carry out a pilot project in the area of predictive scoring . The project aims to assess the potential of predictive models within the client's environment. If the results of the pilot are positively evaluated, the client plans to extend the project to a production phase, which will include operationalizing machine learning models on the Microsoft Azure platform and integrating them with Microsoft Dynamics . üïê Start no later than the beginning of September , initially for 25 man-days , with the option to extend for an additional 3 months. 2+ years of experience in Data Science and/or Machine Learning Proficiency in Python or R (knowledge of one is sufficient) Familiarity with the Microsoft Azure platform Strong analytical mindset and practical experience with predictive modeling Good communication skills and ability to work independently as well as in a team English language proficiency at B1/B2 level Key Technologies: Azure, Machine Learning, Python Perform exploratory data analysis (EDA) on customer and insurance scoring data Build and evaluate 2‚Äì3 competitive machine learning models , including training and testing phases Prepare a summary of final results , including model performance metrics and key insights Validate the results with the client to support a go/no-go decision for production deployment Rate: up to 135 PLN/hour net (B2B contract) Flexible payment method Short 14-day invoice payment term Opportunity to work on interesting and growth-oriented projects","[{""min"": 100, ""max"": 135, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,16,Data Engineer (Hadoop),emagine Polska,"Industry: banking Location: fully remote (candidates must be based in Poland) Languages: fluent Polish and English Contract: B2B The Hadoop Data Engineer plays a critical role in enhancing the data processing capabilities within the organization, leveraging cloud technologies for efficient data handling and migration. The primary objective is to build and maintain robust data processing architectures that facilitate the flow of information and insights in a scalable manner. Main Responsibilities: Develop and maintain data processing systems using Hadoop, Apache Spark, and Scala. Design and implement data migration processes on the Google Cloud platform. Create solutions for data handling and transformation utilizing SQL and other relevant tools. Collaborate with stakeholders to ensure data architecture aligns with business needs. Engage in automated testing and integration to ensure smooth deployment processes. Debug code issues and communicate findings with the development team. Apply big data modeling techniques for effective data representation. Adapt to dynamic environments and embrace a proactive learning attitude. Key Requirements: 5+ years of experience in Hadoop, Hive, HDFS, and Apache Spark. Proficiency in Scala programming. Hands-on experience with Google Cloud Platform, especially Big Query and Cloud Dataflow. Strong understanding of SQL and relational database technologies. Experience with version control tools (Git, GitHub) and CI/CD processes. Ability to design large scale distributed data processing systems. Strong interpersonal skills and teamwork abilities. Experience in Enterprise Data Warehouse technologies. Exposure to Agile project methodologies (Scrum, Kanban). Google Cloud Certification - nice to have. Experience with customer-facing roles in enterprise settings. Exposure to Cloud design patterns.",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,17,"Senior Data Software Engineer (Python, GCP)",EPAM Systems,"We are looking for a Senior Data Software Engineer to join the project in the telecommunications sector, focused on creating network infrastructure for high-speed internet access using fiber optic communication. In this role, you will work with an American multinational technology company. The project offers many opportunities to learn as our client is considered one of the Big Five companies in the American information technology industry. This position offers a hybrid model, with 3 days per week working from the client‚Äôs office for candidates from Gdansk, Wroclaw, Warsaw, and Krakow. Responsibilities Provide technical leadership and oversight for the other developers working on the project Design and build ETL/ELT pipelines using Airflow and other technologies on the GCP Provide advice and recommendations (in writing when appropriate) to the Data Warehouse Technical Lead regarding technical options and best practices for the data warehouse and related ETL/ELT pipelines Analyze source data and work with internal data consumers to determine which data is needed and how it should be represented in the output table schemas Write and maintain related technical documentation Perform thorough design and code reviews for other developers Requirements 4+ years of relevant professional experience Solid experience in SQL scripting and Python programming At least 1 year of ETL/ELT to BigQuery experience Experience in developing solutions for the GCP Proficiency with ETL/ETL for data warehousing including data source investigation/analysis, target schema design and data pipeline design/implementation Ability to write clear, concise, and well-reasoned technical explanations and documentation for both engineer and analyst internal audiences (design docs, architecture views/diagrams, etc.) Solid interpersonal skills for working with both upstream teams providing data and downstream analysts consuming data B2+ English level proficiency Nice to have Experience with Apache Airflow Familiarity with tools like IntelliJ, Gradle, Google Cloud Storage, Google Cloud Datastream, Google Cloud Data Catalog, Google Looker, Google Cloud Cortex Google Cloud Platform (GCP) certification We offer We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Unclassified,Unclassified
Full-time,Manager / C-level,Permanent,Hybrid,18,Principal Data Science Engineer,Sabre,"Airline industry is going through a drastic transformation in the area of retailing and distribution that requires very advance data analytics support to optimize revenue performance and customer experience. Recently introduced concepts of Offer/Order Management and Continuous Dynamic Pricing significantly expand opportunities for engaging with travelers through multiple touch points and creating personalized offers accounting for individual preferences and market context. These practices can substantially benefit from a combination of statistical and machine learning techniques leveraging huge volumes and variety of consumer and competitive data available in airline industry. The Data Science Engineer applies expert level statistical analysis, data modeling, and predictive analysis on strategic and operational problems in airline industry. As a key member of the Sabre Labs team, you will leverage your optimization expertise to translate business questions into data analysis and models, define suitable KPIs, and present results to a wide range of audiences including internal and external clients, sales, and development team. In addition, you will engineer optimization software in C++, Java or Python, leverage commercial solvers like CPLEX or Gurobi and work on its continuous performance improvement and business enhancements. You will also utilize your strong communication skills to work with developers to support product development cycles. Responsibilities Work with subject matter experts from airlines to identify opportunities for leveraging data to deliver insights and actionable prediction of customer behavior and operations performance. Assess the effectiveness and accuracy of new data sources, data gathering and forecasting techniques. Develop custom data models and algorithms to apply to data sets and run proof of concept studies. Leverage existing Statistical and Machine Learning tools to enhance in-house algorithms. Collaborate with software engineers to implement and test production quality code for AI/ML models. Develop processes and tools to monitor and analyze data accuracy and models‚Äô performance. Demonstrate software to customers and perform value proving benchmarks. Calibrate software for customer needs and train customer for using and maintaining software. Resolve customer complaints with software and respond to suggestions for enhancements. Required Qualifications Advanced Degree in Statistics, Operations Research, Computer Science, Mathematics, or Machine Learning. Proven ability to apply modeling and analytical skills to real-world problems. Proficiency with solving large-scale optimization using decomposition algorithms and other advanced OR techniques for linear and non-linear optimization Software development experience with proven record of developing complex algorithm. Must have: Proficiency in optimization or any advanced mathematical algorithms Must have: Advanced knowledge of C++ or Java and associated development tools. Experience (minimum 4 out of 7) with deployment of machine learning on a cloud: MLOps within the enterprise CI/CD process for ML models ‚Äì 2 years Experience deploying ML APIs in production environments in GCP using GKE ‚Äì 2 years Experience in using GCP Vertex AI for ML and BigQuery ‚Äì 1 year Experience in optimization solvers like CPLEX/GUROBI -2 years Advanced knowledge in Python or Go ‚Äì 1 year Knowledge in Terraform and Containers technologies ‚Äì 2 years Experience writing data processing jobs using GCP Dataflow and Dataproc ‚Äì 2 years Desirable Qualifications Familiarity with airline, rail or supply-chain industries and decision support systems employed there. Experience developing network optimization, resource planning and task scheduling models. Understanding of airline network planning, scheduling, and disruption management concepts. Ôªø Work arrangements Hybrid working mode; 3 chosen days from the office in a week Flexible working hours: Maintain your work-life balance by adjusting your working hours to your needs Paid time off Year-End-Break: enjoy additional fully paid days off during the last week of the year Paid parental leave: Take up to 12 weeks off with pay after birth or adoption of a child. Sabre Global Paid Parental Leave runs concurrently with local leave policies. Paid volunteer time: take up to 4 days annually to give your time to a charitable organization of your choice Your money My Benefit platform/Multisport card: enjoy the benefit cafeteria system and use popular sport card Tax deduction: take the opportunity to claim deductible costs, reducing your income tax Employee Capital Plans: profit from long-term saving scheme co-financed by Sabre and the State Treasury Baby Bonus: benefit from one-time allowance on childbirth or adoption Say Thanks program: collect points on recognition program and transfer them to wide variety of gifts and services Health and wellness Luxmed VIP medical coverage: take care of yourself and your family with the extensive medical package with a broad range of additional services Foreign travel insurance: feel safe going abroad with free Allianz insurance offered as part of our Lux Med package Employee Assistance Program: find help in free, confidential program with a certified counselor Mindfulness & meditation apps: take care of your mental and physical health Life insurance: sign up for free, high coverage life insurance program Career development Professional development: access to e-learning platforms as well as join Sabre live learning sessions Certification and tuition reimbursement Our Communities: join one of our team member groups focused on sharing knowledge and best practices (Google Developers Group, Innovation Lab Community, Women in Technology, SOLVE!T and many more) And more Car and bike parking Fun & Relax zone in modern office: enjoy electronic tables to work, foosball, ping pong, pool table, swings, massage chairs and terraces to admire a panoramic view of Krak√≥w. We have parents‚Äô rooms as well No dress code Innovation Lab: access Augmented Reality & Virtual Reality equipment, Robot construction kit, 3D printers and many more Attractive Referral Bonus: earn $2500 USD for every hired referral",[],Data Science,Data Science
Full-time,Senior,Permanent or B2B,Remote,19,System Engineer ‚Äì data area,Sii,"We are seeking a highly skilled System Engineer to join our team. This role involves the maintenance, operation, monitoring, and administration of an IT system within the anti-money laundering domain. The successful candidate will work with various external data providers and manage the data collection, transformation, and distribution across different systems. Monitor, ensure, and improve the health and performance of the application and system Collaborate with reporting and data-providing teams to ensure system integration, data accuracy, and smooth operations Work with external vendors to report issues, request changes, and track bug resolutions Analyse system data to identify and resolve issues, using available tools and data integration processes Oversee data transformation using PySpark in Apache Spark, ensuring smooth and efficient data flows through the system Troubleshoot system and data pipeline issues as they arise Read and analyse system logs to diagnose issues, optimize performance, and set up alerts based on log data to monitor system health At least 5 years of experience in IT system maintenance, operation, and administration, with a focus on monitoring and troubleshooting Proficiency in SQL Familiarity with PySpark/Apache Spark for troubleshooting data processing tasks Expertise in data integration and ETL processes, ensuring smooth system operations and data flows Minimum 5 years of experience with APIs and system integrations, especially in the context of different data sources and systems Over 5 years of experience with Jira, Azure DevOps, and the ability to create change requests and report bugs Solid understanding of cloud computing platforms (AWS, Azure, Google Cloud) and cloud-based data storage solutions Experience with monitoring and alerting tools (e.g., Prometheus, Grafana, Datadog) Strong understanding of data security best practices, particularly concerning data integrity and confidentiality Scripting knowledge (e.g., Python, Bash) for task automation, such as data validation or system checks Knowledge of logs, ability to read and analyse them, and set up alerts based on log data to monitor system health Very good English skills Residing in Poland required Experience with financial crime compliance tools (analytic tools or case management tools from providers like LexisNexis or Oracle) is highly beneficial Previous work in the anti-financial crime domain is advantageous Strong problem-solving skills and the ability to propose system improvements Great Place to Work since 2015 - it‚Äôs thanks to feedback from our workers that we get this special title and constantly implement new ideas Employment stability - revenue of PLN 2.1BN, no debts, since 2006 on the market We share the profit with Workers - over PLN 60M has already been allocated for this aim since 2022 Attractive benefits package - private healthcare, benefits cafeteria platform, car discounts and more Comfortable workplace ‚Äì class A offices or remote work Dozens of fascinating projects for prestigious brands from all over the world ‚Äì you can change them thanks to Job Changer application PLN 1 000 000 per year for your ideas - with this amount, we support the passions and voluntary actions of our workers Investment in your growth ‚Äì meetups, webinars, training platform and technology blog ‚Äì you choose Fantastic atmosphere created by all Sii Power People",[],Unclassified,Unclassified
Full-time,Mid,B2B,Remote,20,Cloud Data Architect - GCP,in4ge sp. z o.o.,"Cloud Data Architect - GCP Warszawa Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. We are seeking a skilled Cloud Data Architect to support a UK-based enterprise client in designing and implementing modern data solutions on Google Cloud Platform. This is a remote position aligned with EMEA working hours. The role focuses on building scalable infrastructure, efficient data pipelines, and secure architectures aligned with best practices in data strategy, governance, and machine learning integration. Design end-to-end cloud data architecture using GCP services Build and optimize ETL/ELT workflows (batch and streaming) Apply data governance, IAM policies, and compliance frameworks Support machine learning integration into cloud data environments Implement Infrastructure as Code using Terraform or Deployment Manager Set up CI/CD processes for data pipelines (e.g., Cloud Build, GitOps) Collaborate with business and technical stakeholders on architecture and delivery 5+ years of experience in cloud data architecture or engineering Strong expertise in GCP services: BigQuery, Pub/Sub, Cloud Storage, Dataflow, Dataproc Practical experience with Apache Beam, Cloud Composer, Vertex AI or AutoML Solid understanding of data security, IAM, and compliance in cloud environments Hands-on with Terraform or similar IaC tools Experience with CI/CD tools and version control (e.g., GitOps, Cloud Build) Excellent communication skills in English (written and spoken) Comfortable working independently in a remote, distributed team Professional Cloud Architect (PCA) or equivalent GCP certification required Fully remote work with flexible working hours - EMEA Timezone. Long-term collaboration on B2B contract. Opportunity to work on complex cloud projects for international clients. Professional growth in a highly skilled and supportive team. Collaborative and open working culture. üí° Don‚Äôt miss out on tailored opportunities! We have many ongoing recruitments, and new projects are constantly coming in. By giving your consent to process your data for future recruitment processes , we‚Äôll be able to invite you to roles that match your experience and expectations! PS: We‚Äôll only reach out to you when we have projects that might genuinely interest you ‚Äî without your consent, we won‚Äôt be able to do that. Our recruitment process is transparent and focused on finding the right candidate for our clients. When you apply, you can count on our objectivity, respect, and full professionalism. We look forward to receiving your CV. We connect you with the right people.",[],Data Architecture,Data Architecture
Full-time,Mid,Any,Remote,21,Data Engineer,SAVENTIC HEALTH sp. z o.o.,"Saventic Health is a mission-driven health-tech startup based in Warsaw , dedicated to transforming the diagnosis of rare diseases using cutting-edge Artificial Intelligence. We are developing innovative solutions that have the potential to significantly shorten diagnostic timelines and improve patient outcomes. As a startup, we thrive on innovation, agility, and a collaborative spirit where every team member makes a tangible impact. We work with complex medical data, requiring robust and reliable data infrastructure. The Opportunity: We are looking for a skilled and enthusiastic Data Engineer to join our team and play a vital role in building and maintaining the data infrastructure that powers our AI platform. This is a fantastic opportunity to develop your expertise in data engineering within a meaningful healthcare context, working with complex data and modern tools. As a Data Engineer, you will collaborate with senior engineers, data scientists, and MLOps engineers to implement, manage, and optimize our data pipelines and storage solutions. You'll gain hands-on experience across the data lifecycle, ensuring our systems are efficient, reliable, and capable of handling sensitive medical data securely, primarily within our on- premise environment but also interacting with cloud services. Your Impact: As an important member of our technical team, you will: Contribute to AI Enablement: Build and maintain the essential data pipelines that provide clean, reliable data for our AI models. Help Ensure Data Reliability: Implement processes and monitoring to support data quality and system uptime. Support Our Data Foundation : Contribute to the development and maintenance of our data storage solutions and overall data infrastructure. Assist in Optimization: Help identify and implement improvements to make our data processes more efficient and scalable. Key Responsibilities: Build and Maintain Data Pipelines : Implement, automate, schedule, and maintain scalable ETL/ELT pipelines using tools like Apache Airflow. Implement Data Solutions: Work with senior team members to implement data models and database schemas. Manage Data Storage: Assist in managing and optimizing our data storage solutions, including our current PostgreSQL databases. Implement Data Quality Checks: Implement and monitor data quality rules and checks within pipelines. Performance Tuning: Assist in monitoring and tuning the performance of data pipelines and SQL queries under guidance. Utilize Tools : Work effectively with our data engineering stack, including Python, Airflow, SQL databases, and potentially cloud services. Collaboration: Work closely within the team, collaborating with Data Scientists, MLOps Engineers, and Senior Data Engineers. Follow Security Practices: Ensure data security best practices are followed in implemented solutions. Documentation: Document data pipelines, processes, and configurations clearly. Who You Are: Experienced Data Engineer: Proven practical experience working as a Data Engineer, demonstrating the ability to build and maintain data systems. Python Proficient: Proficiency in Python is required for scripting and pipeline development. SQL & Database Skills: Solid understanding of SQL and practical experience working with relational databases (including PostgreSQL). Pipeline Orchestration Experience: Hands-on experience building and maintaining data pipelines, with practical experience using Apache Airflow required. Data Concepts Aware: Understanding of core data engineering concepts, ETL/ELT processes, and data modeling principles. Cloud Familiarity: Basic familiarity with cloud platforms (AWS, GCP, Azure) and their data services is beneficial. Problem Solver: Good analytical and problem-solving skills, with an eagerness to tackle technical challenges. Team Player & Learner: Collaborative, communicative, and keen to learn and apply new data engineering techniques. Educated: Bachelor's degree in Computer Science, Engineering, or a related technical field often expected, or equivalent practical experience. Nice to Haves: Experience working in healthcare or another regulated industry. Experience with other data processing tools (e.g., Spark, Pandas for larger datasets). Experience with NoSQL databases. Familiarity with data warehousing concepts. Understanding of data governance principles. Experience with containerization (Docker). Familiarity with Infrastructure-as-Code (IaC) tools (e.g., Terraform, Ansible). What We Offer: An opportunity to make a real-world impact by contributing to solutions that help diagnose rare diseases. A chance to work on challenging data engineering problems with complex medical data using modern tools like Airflow. Significant opportunities for learning and professional growth within a supportive team. Exposure to the application of AI in healthcare. A dynamic, innovative, and collaborative startup culture. Competitive salary and benefits package. Ready to build the data backbone for healthcare AI? If you are a Data Engineer passionate about building reliable data systems and eager to apply your skills (including Python and Airflow) in a meaningful domain, we encourage you to apply!",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,22,Data Platform Engineer - available ASAP,ITDS,"As a Data Platform Engineer , you will be working for our client in the debt collection sector, helping to build and maintain a robust, cloud-native data platform. The role focuses heavily on data modelling, requiring an expert who can translate conceptual business needs into logical and physical data models, build data contracts, and implement scalable ELT pipelines using Azure Databricks. Your main responsibilities: Design and maintain logical and physical data models based on DDD (Domain-Driven Design) principles Translate conceptual models and business glossaries into technical data structures for the Data Warehouse Perform data mapping and create data contracts between the Data Platform and source systems Collaborate with source system owners to define data contract requirements Work on data ingestion processes from source systems using various methods: Direct database queries (bulk read/CDC), API communication, Event streaming Implement ELT processes across Bronze, Silver, and Gold layers in Azure Databricks Ensure alignment of data models with business and analytical requirements You're ideal for this role if you have: Strong experience in Data Modelling (logical & physical), preferably in DDD-based environments Proven ability to work with Data Governance inputs: glossaries, conceptual models, HLD/LLD documentation Experience preparing and maintaining data contracts Solid knowledge of data ingestion techniques and working with source systems Experience with Azure Databricks Ability to develop and maintain ELT pipelines in cloud-native environments Nice to have: Experience in writing clear technical documentation (e.g. data contracts, field definitions, extraction rules) Background in mapping source data to target DWH structures Ability to interpret and work with ERDs and relational models Knowledge of master data management practices Familiarity with dbdiagram.io Awareness of Data Quality, Data Lineage, and metadata management concepts Experience using tools like Azure Purview or other metadata management platforms We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #6787 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 25200, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,23,Senior Data Engineer Customer Data Asset,Bayer Sp. z o.o.,"Senior Data Engineer Customer Data Asset The Data Assets, Analytics & AI organization at Bayer Consumer Health focuses on driving digital transformation and innovation by creating best-in-class analytical solutions that enable data-driven decision making and performance optimization for Bayer Consumer Health. You will be part of the Data Assets, Analytics & AI organization and will be responsible for building data assets. You partner with business stakeholders, data analysts, data architects, data scientists, analytics leads as well as other engineers. You will build data pipelines, data models and provision data for Analytics products and data scientists. You will also make sure that proper development processes are followed within the team, enhance implementation frameworks and guide other team members in building scalable, secure and well performing data products. If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Key Tasks & Responsibilities: Implement efficient data pipelines that integrate data from different sources and business domains to develop globally harmonized data models and KPI calculations Contribute to define data management and data quality standards. Ensure that data is well-managed to build stable, reusable and quality assured data assets. Collaborate with other IT functions (enabling functions data asset teams, analytics teams, platform product managers & integration architects) to ensure the aforementioned activities are executed effectively. Ensure that data products adhere to the data protection and compliance standards. Implement effective data access management policies, data privacy policies and secure data provisioning based on corporate guidelines. Continuously enhance implementation frameworks based on the needs of the analytics products consuming the data assets in your responsibility. Guide other data engineers in your team (internal or external engineers) and ensure that all engineers apply same design principles and assure code quality. Break down bigger work packages into manageable tasks. Together with the assigned data architect, ensure that cost and time estimations are accurate, quality of delivery is assured, and deliverables are properly tested, documented and handed over to the operations team. Qualifications & Competencies (education, skills, experience): Bachelor/Master‚Äôs degree in Computer Science, Engineering, or a related field. 5+ years of working experience in the field of Data & Analytics, preferably in the CPG industry 5+ years of proficient coding experience with Python for data engineering, including SQL and PySpark (DataFrame API, Spark SQL, MLlib), with hands-on experience in various databases (SQL/NoSQL), key libraries (e.g., pandas, SQLAlchemy), parallel processing, and advanced data transformation and performance optimization techniques. Excellent data engineering & technology knowledge (Azure Data Lake Gen2, Azure Data Factory and Databricks as well as data management knowhow (data cataloguing, data quality management) Solid understanding of data modeling, ETL processing and lakehouse concepts. Profound knowledge of CI/CD processes and tools (GitHub VCS, GitHub Actions, Azure DevOps Pipelines)) Profound data content knowledge in the area of wholesaler data, retailer data (inventories, pos data) as well as CRM data (se.g. Salesforce CRM or derivates like IQVIA OCE-P, Customer Times) and eCommerce (e.g. Profitero, Amazon Vendor Central API) Experience in leading developer teams and breaking down work into smaller work packages, experience in Agile methodologies (Scrum, Kanban) in order to manage the work. Strong problem solving and analytical skills. Excellent interpersonal and communication skills, team collaboration, active listening, consulting, challenging in a constructive way. Fluent in English, both written & spoken, intercultural awareness and willingness to travel What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,24,Middle Data Engineer (Databricks),N-iX,"#3821 Join our team to work on enhancing a robust data pipeline that powers our SaaS product, ensuring seamless contextualization, validation, and ingestion of customer data. Collaborate with product teams to unlock new user experiences by leveraging data insights. Engage with domain experts to analyze real-world engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Client was established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client‚Äôs platform empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renowned Scandinavian business group , Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities: Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements: Programming: Minimum of 3-4 years as data engineer, or in a relevant field. Python Proficiency: Advanced experience in Python , particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights. Cloud: Familiarity with cloud platforms (preferably Azure ). Data Platforms: Experience with Databricks , Snowflake, or similar data platforms. Database Skills: Knowledge of relational databases, with proficiency in SQL. Big Data: Experience using Apache Spark. Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability. Version Control: Experience with Gitlab or equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have: Experience with Docker and Kubernetes. Experience with document and graph databases. Ability to travel abroad twice a year for an on-site workshops.","[{""min"": 17862, ""max"": 21252, ""type"": ""Net per month - B2B""}, {""min"": 14399, ""max"": 17315, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,25,Data Engineer,Antal Sp. z o.o.,"Position: Data Engineer (Middle to Senior level) Client: American Airlines Project: Supporting AWS Data Engineering & MLOps initiatives. Focused primarily on AWS CDK for infrastructure management. Work Mode: Remote Technical Requirements: Strong experience with AWS CDK for Infrastructure as Code (essential) Knowledge of CloudFormation (desirable) or Terraform (if CDK or CF are unavailable) Familiarity with AWS IAM and KMS Hands-on expertise with AWS Lambda and AWS Kinesis (streaming) Exposure to AWS Data Zone (experience not mandatory) Proficiency in Python , particularly PySpark and Lambdas Basic familiarity with Apache Spark Knowledge of GitLab CI/CD pipelines (YAML configurations) Soft Skills: Strong communication skills Ability to work flexible hours up to 6: 30 CET/CEST English Proficiency: B2-C2",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,26,Senior Administrator Baz Danych Oracle / Senior Oracle DBA,Simora,"Do≈ÇƒÖcz do naszego zespo≈Çu jako Senior Administrator Baz Danych Oracle! Poszukujemy osoby, kt√≥ra zajmie siƒô zarzƒÖdzaniem bazami danych naszych klient√≥w. Jeste≈õmy firmƒÖ, kt√≥ra rozwija nowoczesne rozwiƒÖzania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzƒôdzi wspierajƒÖcych naszych klient√≥w. Cenimy pozytywnƒÖ atmosferƒô, wzajemny szacunek i zaanga≈ºowanie ‚Äì to fundamenty naszego zespo≈Çu. Tw√≥j zakres obowiƒÖzk√≥w Tworzenie baz danych oraz ≈õrodowisk bazodanowych na ≈õrodowisku produkcyjnym i testowym Migrowanie baz danych na nowe ≈õrodowiska ZarzƒÖdzania i aktualizacja baz danych i ≈õrodowisk bazodanowych Konfigurowanie i optymalizacja ≈õrodowiska bazodanowego Automatyzacja zada≈Ñ za pomocƒÖ jƒôzyk√≥w skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jako≈õci i bezpiecze≈Ñstwa dla tworzonych rozwiƒÖza≈Ñ Wdra≈ºanie rozwiƒÖza≈Ñ opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Sta≈Çe doskonalenie sposobu Twojej pracy Nasze wymagania Min 5 letnie do≈õwiadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomo≈õƒá jƒôzyka SQL i PLSQL Umiejƒôtno≈õƒá dbania o szczeg√≥≈Çy i jako≈õƒá rozwiƒÖza≈Ñ Komunikatywno≈õƒá Znajomo≈õƒá jƒôzyka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykszta≈Çcenie wy≈ºsze (preferowany kierunek: informatyka) Znajomo≈õƒá jƒôzyk√≥w programowania: Python, Java itp. Znajomo≈õƒá system√≥w operacyjnych Linux / Windows Znajomo≈õƒá system√≥w wirtualizacyjnych np.: OLVM Oferujemy CiekawƒÖ pracƒô w firmie o wysokiej dynamice rozwoju Mo≈ºliwo≈õƒá podniesienia kwalifikacji w obszarach zwiƒÖzanych z bazami danych, bezpiecze≈Ñstwem danych oraz sztucznƒÖ inteligencjƒÖ Benefity Karta Multisport Prywatna opieka zdrowotna ‚Äì Medicover Praca zdalna Brak dress code‚Äôu Dofinansowanie szkole≈Ñ i kurs√≥w Elastyczny czas pracy","[{""min"": 10000, ""max"": 16000, ""type"": ""Net per month - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,27,Senior Data/ETL Engineer,Transition Technologies MS,"Your responsibilities: Support and maintain applications New feature development User and environment management and administration Designing and implementing database solutions Working in Agile Development Methodologies Ability to act as a Scrum Master Setting up pipelines and building solutions We are looking for you if you have: At least 6 years of Apex applications development experience Strong SQL and PL/SQL (Structured Query Language) and data extraction skills Good command of Oracle DB Basic command of UNIX Basic knowledge of Apex/Oracle/Middleware administration/configuration Taking the role of Problem Coordinator, Major Incident Manager if such problems occur in the Service 5+ years of experience designing, building and implementing db based solutions Experience with DB/ETL best practices Experience working in Agile Development Methodologies Expertise knowledge of ITIL standards and SCRUM processes) We offer: Participation in interesting and challenging projects Flexible working hours A great, non-corporate atmosphere Stable employment conditions (contract of employment or B2B contract) Opportunities for development and promotion Attractive package of benefits Remote work We reserve the right to contact the selected candidates",[],Data Engineering,Data Engineering
Full-time,Mid,Any,Hybrid,28,Technology Specialist Data Migration (SAP BODS),Heineken,"Digital & Technology Team (D&T) is an integral division of HEINEKEN Global Shared Services Center . We are committed to making Heineken the most connected brewery. That includes digitalizing and integrating our processes, ensuring best-in-class technology, and embedding a data-driven culture. By joining us you will work in one of the most dynamic and innovative teams and have a direct impact on building the future of Heineken ! Would you like to meet the Team, see our office and much more? Visit our website: Heineken ( heineken-dt.pl ) This role is part of the D&T (Digital and Technology) department of HEINEKEN International and is located in Heineken Global Shared Service in Cracow. D&T is proud to bring cutting-edge innovation, strong technology and advanced analytics to HEINEKEN globally. With speed and agility we ensure HEINEKEN has the technological competitive advantages it needs to deliver on its ambition. A Technology Specialist Data Migration helps the major Heineken‚Äôs global Programs and Projects by executing an end-to-end Data validation and Data migration service. It has a delivery responsibility and data migration members work together with the assigned programs or projects until the activities are completed. Your responsibilities would include: supporting data mapping and integration execution and end-to-end reconciliation analyzing source system‚Äôs data model, on object/field level, and matching that towards the target system‚Äôs data model, on object/field levelpreparing mapping files (design, how-to, governance) for each migrated entity identifying any mapping gaps and address them with appropriate project teams, follow it up ensuring it will be resolved supporting on designing of reconciliation reports ‚Äì design, checks, validations, documentation; acting as a point of contact for assigned OpCos on Data Migration matters, and be the point of contact for all applicable Heineken teams, as well as external Vendors in workshops and meetings developing, improving and running extraction, conversion and load solutions managing the data migration per phase of the project (unit testing, UAT, deployment etc.) aligning with Data Modelling and Data Governance teams on data migration ensuring adherence to HEINEKEN data standards and indicate gaps towards the target solution and processes providing technical solutions with regards to data migration (i.e. support testing in resolving data related test incidents). You are a good candidate if you: speak English fluently have at least one recent full cycle, hands on, Data Migration projects experience with S/4Hana or ECC as a target system (1+ years) have good problem solving skills and focus on details like following the agile principles should feel comfortable in a self-steering team have experience with complex programs and projects have experience of working in a deadline driven environment with rapid release cycles can demonstrate good active listening and communication skills, able to bring the message across to different audiences have a good understanding of main objects: Customer, Vendor, Items, transactional data and their dependencies can efficiently browse SAP S/4 or ECC data structures to understand and possibly debug issues have a general understanding of main modules: Finance, Sales and Procurement show flexibility, proactivity and be an energetic team player. Must have: Data Migration to S/4Hana or SAP ECC SAP Data Services expert (former Business Object Data Services-BODS) Good knowledge of Microsoft SQL (code) SAP LSMW (with Abap code knowledge or understanding) SAP Bapi/Idocs for data load into S4. You are perfect match if you also have: Project certification. At HEINEKEN Krak√≥w, we take integrity and ethical conduct seriously. If someone has concerns about a possible violation of legal regulations indicated in Polish Whistleblowing Act or our Code of Business Conduct, we encourage them to speak up . Cases can be reported to global team or locally (in line with the local HGSS Whistleblowing procedure) by selecting proper option in this tool or by communicating it on hotline. üè† Flexible Work from Home scheme üí∏ Attractive Performance Bonus üöó Parking Space for Employees ‚è∞ Flexible working hours üí≥ Sodexo Card ‚òÇ Life Insurance ‚ûï Employee Referral Programme üåê Job Opportunities within HEINEKEN ü©∫ Private Medical Healthcare ‚≠ê Social Events",[],Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Hybrid,29,Data Analyst,Remodevs,"Please note - hybrid from Warsaw (3 days from the office, 2 remotely). About us: We help businesses use AI and digital tools to work better and grow faster, especially in private capital markets. Our Core Platform improves workflows and gives useful insights with AI. Olympus Software is a fast, smart cloud system that grows with your needs. The Pantheon Suite offers flexible tools to manage and improve business performance. With over 10 years of experience, we know how to turn technology into real business value. About the Role We are seeking a Data Analyst to join our PaaS (Platform-as-a-Service) Customer Delivery team in Warsaw. In this role, you will be responsible for transforming data into actionable insights to support decision-making across product, operations, and strategy for customer-facing platform implementations. You will work closely with stakeholders across implementation projects to develop dashboards, conduct ad-hoc analyses, and ensure the integrity of platform usage and performance metrics. This role is ideal for someone who is curious, business-minded, and eager to make an impact through data. Key Responsibilities Partner with cross-functional teams to define key metrics and build dashboards and reports that provide visibility into business performance. Conduct deep-dive analyses to answer business questions, uncover trends, and identify opportunities for growth and optimization. Design and maintain scalable data models and SQL queries to support reporting and analytical needs. Collaborate with data engineers to ensure data availability, quality, and consistency across systems. Communicate findings and recommendations clearly to technical and non-technical audiences. Develop documentation and contribute to data literacy across the organization. Qualifications Required 2‚Äì4 years of experience in a data analyst or business intelligence role. Strong SQL skills and experience working with large datasets in a cloud data warehouse environment. Proficiency with BI tools such as Looker, Tableau, Power BI, or similar. Strong analytical thinking and attention to detail. Excellent communication and data storytelling skills. Preferred Experience working with dbt or similar modeling tools. Familiarity with A/B testing design and analysis. Some experience with Python, R, or another scripting language for data analysis. Exposure to product analytics platforms (e.g., Mixpanel, Amplitude).","[{""min"": 16404, ""max"": 18226, ""type"": ""Net per month - B2B""}, {""min"": 16404, ""max"": 18226, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,30,Senior Power BI Developer,Holisticon Insight,"Holisticon Insight is a division of http: //nexergroup.com focused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! https: //holisticon.pl/holisticon-insight/ üöÄ We are looking for a Senior Power BI Developer to lead the frontend development of a proprietary Business Intelligence (BI) and analytics tool within the Dealer Management System (DMS) stream to work on a project in a team of our client, (a Swedish-based leading provider of transport solutions.) In the role of Senior Power BI Developer, you will be responsible for building modern BI solutions using Power BI Premium (Cloud) . You will play a key role in enhancing our data visualization and reporting capabilities across complex data environments. Develop and maintain dashboards, cubes, and reports using Power BI Premium (Cloud) Collaborate with cross-functional teams to gather requirements and deliver data-driven insights Participate in the migration from on-premise solutions to cloud-based BI infrastructure Drive innovation and best practices in frontend data engineering and analytics Expert-level proficiency in Power BI and Power Automate Strong SQL skills and hands-on experience with data modeling and BI tools Proven track record working on complex projects with multiple reports Analytical thinker with a passion for solving challenges through data Experience collaborating in large, diverse development teams Excellent communication skills to engage with both technical and non-technical stakeholders Experience with Python and Snowflake Familiarity with cloud migration projects Life insurance Multisport card Fully remote job Private medical care Flexible working hours Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories) 1Ô∏è‚É£ Initial chat with Joanna, our recruiter ‚Äì 30 minutes 2Ô∏è‚É£ Technical interview with our expert ‚Äì 30-45 minutes 3Ô∏è‚É£ Final interview with the client ‚Äì 1 hour","[{""min"": 130, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Hybrid,31,Senior Data Engineer/DWH Consultant,SimCorp,"WHAT MAKES US, US Join some of the most innovative thinkers in FinTech as we lead the evolution of financial technology. If you are an innovative, curious, collaborative person who embraces challenges and wants to grow, learn and pursue outcomes with our prestigious financial clients, say Hello to SimCorp! At its foundation, SimCorp is guided by our values ‚Äì caring, customer success-driven, collaborative, curious, and courageous. Our people-centered organization focuses on skills development, relationship building, and client success. We take pride in cultivating an environment where all team members can grow, feel heard, valued, and empowered. If you like what we‚Äôre saying, keep reading! WHY THIS ROLE IS IMPORTANT TO US SimCorp Advanced Delivery Center (ADC) was strategically established, initially in 2016 in Warsaw, Poland, and subsequently extended to Manila, Philippines in 2021 and Mexico City in 2023. This deliberate expansion enables us to efficiently connect with clients spanning EMEA, APAC, and NA. Within the ADC, our Business Consultants collaborate closely with market units, focusing on business functionality and solution workflows. Their expertise ensures customized configuration and seamless deployment of SimCorp Dimension for our valued customers. We are now looking for a Senior Data Engineering/DWH Consultant - a highly skilled professional who will form part of our Professional Services team, working within Consulting Services and based within Advanced Delivery Center in Poland. The main purpose of the role is to provide expert advice and functional delivery in the successful deployment of SimCorp products and services in the data engineering and reporting areas. The consultant will be heavily involved in the data migration, data platform modernization (moving from on-premises solutions to Snowflake) and data warehouse implementation projects. WHAT YOU WILL BE RESPONSIBLE FOR Act as a trusted advisor supporting our Clients in Cloud Journey. Facilitate requirements gathering workshops in business and technical-related areas. Participate in data platform modernization initiatives to offer Clients best cloud experience for data warehouse and reporting solutions. Identify and plan proper data migration strategy to support business requirements. Use Python skills to implement tools automating data migration process. Design, implement and test changes to the Data Warehouse model and ETL workflows. Work with data ingestion and extraction pipelines. Test and optimize the process and tools to meet performance and data quality requirements. Leverage project experience to estimate and plan delivery of small team. Demonstrate growth as a SME for the SC products or technologies. Participate in company knowledge sharing culture, mentor junior consultants, contribute to content creation. WHAT WE VALUE Min. 5 years of IT professional experience. Hands on experience with Oracle and Snowflake databases. Understanding of database concepts, data modeling for analytics and reporting solutions. Understanding of Data Warehouse and ETL/ELT concepts. Hands on experience with ETL tools. Experience with migrating/processing high volumes of data. Familiarity with cloud environments (preferred Azure data stack). SQL and PL/SQL programming skills. Strong Python development skills focused on automation and data pipelines processing. Data manipulation frameworks, like Pandas. Experience with project work delivery (member of project teams organized as an agile or waterfall). Feeling comfortable with business requirements gathering and clarification including direct work with business users. Ability to coordinate small projects, workstreams or work groups as a leading consultant/coordinator. Experience working in international/multicultural environment. Nice to have Basic knowledge of financial instruments and markets or experience working with financial data/business processes related to investment management. Experience working as a consultant for external client (prior experience in Services). Familiarity with BI reporting tools. Familiarity with HTML / CSS / JS. You want to develop your career as a consultant in data engineering field for top brand in fintech sector. BENEFITS Flexible working hours Hybrid working model (2 days per week in the office, next to Wilanowska metro station) Private medical care (Medicover) Sharing the costs of sports activities (Multisport Card) Life insurance Possibility to develop your career in an international environment Professional training and courses (starting with our 3 weeks‚Äô SimCorp Dimension¬Æ Academy) German and English language classes Integration events and charity projects In addition, we provide a good work and life balance and opportunities for professional development in an international environment: there is never just only one route - we offer an individual approach to professional development to support the direction you want to take. Visit our career pages to learn more about working at SimCorp: www.simcorp.com/career NEXT STEPS Please send us your application in English via our career site. Please note that only applications sent through our system will be processed. At SimCorp, we recognize that bias can unintentionally occur in the recruitment process. To uphold fairness and equal opportunities for all applicants, we kindly ask you to exclude personal data such as photo, age, or any non-professional information from your application. Thank you for aiding us in our endeavor to mitigate biases in our recruitment process. For any questions you are welcome to contact Hanna Brz√≥zka, Lead Talent Acquisition Partner, at "" hanna.brzozka@simcorp.com "" . If you are interested in being a part of SimCorp but are not sure this role is suitable, submit your CV anyway. SimCorp is on an exciting growth journey, and our Talent Acquisition Team is ready to assist you discover the right role for you. The approximate time to consider your CV is three weeks. We are eager to continually improve our talent acquisition process and make everyone‚Äôs experience positive and valuable. Therefore, during the process we will ask you to provide your feedback, which is highly appreciated. WHO WE ARE For over 50 years, we have worked closely with investment and asset managers to become the world‚Äôs leading provider of integrated investment management solutions. We are 3,000+ colleagues with a broad range of nationalities, educations, professional experiences, ages, and backgrounds in general. SimCorp is an independent subsidiary of the Deutsche B√∂rse Group. Following the recent merger with Axioma, we leverage the combined strength of our brands to provide an industry-leading, full, front-to-back offering for our clients. SimCorp is an equal opportunity employer. We are committed to building a culture where diverse perspectives and expertise are integrated in our everyday work. We believe in the continual growth and development of our employees, so that we can provide best-in-class solutions to our clients.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,32,Technical Application Support Engineer,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. bran≈ºy Fintech. Wykorzystywany stos technologiczny: SQL, T-SQL, Neo4J, SOAP/XML, REST API, Tableau, SSRS, Power BI, Java, konfiguracja rozwiƒÖza≈Ñ i produkt√≥w dla klient√≥w, wsparciem drugiej linii (2nd level support) oraz modyfikacja skrypt√≥w, zapewnienie jako≈õci przed przekazaniem klientowi: planowanie, definiowanie i realizacja test√≥w, tworzenie dedykowanych raport√≥w i dashboard√≥w dla klient√≥w, wsp√≥≈Çpraca z zespo≈Çami developerskimi przy diagnozowaniu i rozwiƒÖzywaniu problem√≥w, tworzenie i utrzymanie funkcjonalno≈õci opartych o bazƒô danych Neo4J z wykorzystaniem jƒôzyka Cypher, komunikacja z klientami w celu diagnozowania problem√≥w i proponowania rozwiƒÖza≈Ñ, wsparcie po sprzeda≈ºy i po wdro≈ºeniu ‚Äì rozwiƒÖzywanie z≈Ço≈ºonych problem√≥w, analiza i modyfikacja skrypt√≥w w jƒôzyku Java (korekta b≈Çƒôd√≥w i tworzenie nowych), udzia≈Ç w rozwoju system√≥w w ramach zmian zg≈Çaszanych przez obecnych klient√≥w, stawka do 75 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz wykszta≈Çcenie wy≈ºsze w zakresie informatyki, ekonomii lub matematyki, posiadasz minimum 2 lata do≈õwiadczenia zawodowego na podobnym stanowisku, posiadasz do≈õwiadczenie z Microsoft SQL, w tym czytania istniejƒÖcego kodu (T-SQL), masz do≈õwiadczenie z bazami danych Neo4J, posiadasz podstawowƒÖ znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z sieciami komputerowymi, najlepiej z do≈õwiadczeniem w pracy z VPN, FTP i SFTP, znasz SOAP/XML i/lub REST/JSON, znasz narzƒôdzia, np. Tableau, SSRS, Power BI, biegle komunikujesz siƒô w jƒôzyku angielskim (min. B2), mile widziana podstawowa znajomo≈õƒá Java. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 11760, ""max"": 12600, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,33,Data Analyst (banking),emagine Polska,"PROJECT INFORMATION: Industry: banking Location: Cracow or remote (with one office visit per month) Rate: it depends on your expectations Project languages: English Summary: This position seeks Data Analysts who will contribute to data analysis, governance, and system enhancements within the organization, focusing on the risk data platform. Responsibilities: Collate, test, and check independently sourced data, assessing its robustness and fitness for purpose. Learn and understand source systems data, models, and standards. Understand data requirements from model development & monitoring teams, working on data quality issues, and liaising with project teams for new ingests or data assets. Develop ETL pipelines to make data assets available on the Risk Data Platform. Contribute to designing roadmaps for data, application, and business process architecture to support future state environments. Prepare effective material for dissemination to key business stakeholders at all levels of seniority. Manage relationships across key functional teams effectively. Work with management to prioritize business and information needs. Encourage and drive process improvement opportunities. Must Have: 5+ years of hands-on data analysis experience. Bachelor‚Äôs degree in IT or a numerate subject; Master‚Äôs degree preferred. Proficiency in programming tools such as SAS, Python, PySpark, SQL, and Hadoop Big Data. (Python and SQL is must) Business analysis experience for requirements gathering and specification. Strong understanding of complex data architecture. Strong organizational, analytical, problem-solving, and project management skills. Ability to multitask and work under pressure within tight timelines. Experience with JIRA and Confluence. Effective communication skills and flexibility to work in an international team. Nice to Have: Previous experience in IT/Analytics. Knowledge of banking risk data, systems, and risk models. Experience with Prophecy.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Remote,34,Specjalista ds. Integracji System√≥w IT (SQL / ETL),Power Media,"Do dynamicznie rozwijajƒÖcej siƒô firmy z obszaru analityki predykcyjnej, sztucznej inteligencji oraz integracji system√≥w IT, szukamy osoby, kt√≥ra wzmocni zesp√≥≈Ç wdro≈ºeniowy. Firma od ponad 20 lat wspiera organizacje w podejmowaniu trafniejszych decyzji biznesowych, dostarczajƒÖc narzƒôdzia analityczne, systemy predykcyjne oraz rozwiƒÖzania integrujƒÖce dane z r√≥≈ºnych ≈∫r√≥de≈Ç. Wsp√≥≈ÇpracujƒÖ z liderami rynku w sektorach finansowym, publicznym, telekomunikacyjnym i przemys≈Çowym. Co bƒôdziesz robiƒá Projektowaƒá i wdra≈ºaƒá rozwiƒÖzania integracyjne u klient√≥w. ≈ÅƒÖczyƒá systemy analityczne z bazami danych i systemami operacyjnymi. Tworzyƒá skrypty i procedury bazodanowe (PL/SQL, T‚ÄëSQL), skrypty w Pythonie i/lub shell. Wspieraƒá klient√≥w w efektywnym u≈ºytkowaniu dostarczonych rozwiƒÖza≈Ñ. Czego oczekujemy Min. 5 lata do≈õwiadczenia na podobnym stanowisku. Bieg≈Ço≈õƒá w SQL ‚Äì PL/SQL i T‚ÄëSQL. Do≈õwiadczenie z narzƒôdziami ETL i optymalizacjƒÖ zapyta≈Ñ. Znajomo≈õƒá administrowania Windows/Linux. Umiejƒôtno≈õci analityczne Mile widziane: Umiejƒôtno≈õƒá skryptowania w shellu i Pythonie. Do≈õwiadczenie z administrowaniem Oracle/SQL Server. Znajomo≈õƒá narzƒôdzi analitycznych (SAS, R) Oferujemy: umowƒô o pracƒô ≈õwiadczenia dodatkowe (pakiet medyczny itp.) atrakcyjne wynagrodzenie z systemem premiowania mo≈ºliwo≈õƒá rozwoju i podnoszenia kwalifikacji aktywny udzia≈Ç w projektowaniu rozwiƒÖza≈Ñ mo≈ºliwo≈õƒá pracy zdalnej w 80%",[],Database Administration,Database Administration
Freelance,Senior,B2B,Hybrid,35,Data Architect ‚Äì (Azure & MS Fabric),INFOPLUS TECHNOLOGIES,"Dutch is a preferred but not necessary Integration of Internal and External Data Responsible for onboarding and integrating internal and external data onto the data platform. Ensure interoperability with System Integrations. Data Modeling & Preparation According to Standards Structure data into standardized data models and datamarts to enable dashboard development. Ensure consistency, quality, and alignment with enterprise data standards. Access Management Standardization Define and implement a technical standard for access management , aligned with the organization‚Äôs IAM (Identity & Access Management) framework. Facilitate secure and controlled access to data assets. Reduction of Technical Impediments Proactively identify and resolve technical bottlenecks to enable TDA dashboard development. Aim to meet a 95% service level for business operations dashboards. DP-700: Microsoft Fabric Data Engineer certification DP-600: Microsoft Fabric Analytics Engineer certification At least 2 years of experience integrating diverse data sources for consolidated reporting/dashboarding At least 2 years of hands-on experience with Azure and Microsoft Fabric Knowledge of and experience with Azure DevOps , including CI/CD pipelines Familiarity with IAM and RBAC (Role-Based Access Control) Proven experience working in Agile teams","[{""min"": 1698, ""max"": 2335, ""type"": ""Net per day - B2B""}]",Data Architecture,Data Architecture
Full-time,Mid,B2B,Hybrid,36,"Data Engineer (Informatica Power Center, Azure Databricks)",Experis Manpower Group,"Start date: ASAP / 1 month / flexible Duration: Long term (36 months with further extensions) Work model: hybrid, min. 2 days per week work from the Wroclaw's office Type of cooperation: B2B Overview We are looking for a skilled Data Engineer to join a team focused on transforming industrial data into valuable insights. The team processes data from machines and factories to deliver customized data products to clients worldwide. The goal is to help organizations become fully data-driven by unlocking the potential of their data. You‚Äôll work with diverse data sources and technologies in a dynamic and collaborative environment. Responsibilities Design, develop, and maintain data pipelines using Azure Databricks and Informatica Power Center Work with large-scale datasets using PySpark and Spark SQL Collaborate with stakeholders from both IT and business to understand data needs and deliver solutions Implement and manage data workflows, jobs, and cataloging using Unity Catalog Ensure data quality, security, and governance using Azure Key Vault and DevOps practices Contribute to continuous improvement of data engineering practices and tools Requirements Proficiency in Informatica Power Center Strong experience with Azure Databricks (PySpark, Spark SQL, Unity Catalog, Jobs/Workflows) Advanced SQL skills Hands-on experience with at least one relational DBMS (SQL Server, Oracle, or PostgreSQL) Familiarity with Azure DevOps (Repos, Pipelines, YAML) Knowledge of Azure Key Vault Optional: experience with Azure Data Factory and DBT Soft skills: open-minded, engaged, flexible, proactive, and collaborative Bonus: experience working in a data mesh environment We offer B2B contract via Experis Access to Medicover healthcare Multisport card E-learning platform for continuous development Group insurance",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,37,Data Analyst - Master Data,Bayer Sp. z o.o.,"Data Analyst ‚Äì Master Data Are you ready to make a significant impact in the world of data analytics and data management? We are seeking a talented Data Analyst to become a vital part of our dynamic Data Assets, Analytics, and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in building and maintaining our core data assets across various domains, ensuring their completeness, semantics and quality. You will work closely with data owners, product managers, data engineers, data architects, data stewards, data governors and data scientist to enable our data analytics solutions, enhance the strategic value of our data assets and enable cutting-edge AI solutions and support data-driven decision-making. If you‚Äôre passionate about transforming data into actionable insights and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Collaborate with data owners, architects, engineers, stewards, governors, scientists and product managers to understand objectives and requirements for data assets. Serve as a liaison between technical teams and business stakeholders to ensure data assets meet both business requirements and technical standards. Proactively identify new datasets from across the organization and collaborate with data architects and data engineers to integrate them into the core data assets Define transformation logic and enrich data to create valuable KPIs and features in the consumption layer. Develop and maintain clear semantics and metadata for data assets, ensuring that all data is well-documented and easily understandable. Ensure data quality, availability, and completeness through implementation of quality checks, validation processes, and continuous monitoring in collaboration with data architects and data engineers. Serve as the primary point of contact for users of data assets, providing guidance and support to help them understand and utilize the data effectively. Analyze complex datasets to extract actionable insights that streamline the development of analytics and AI solutions. Become a go-to expert for master data. Qualifications & Competencies: Master's degree in Statistics, Computer Science, Data Management, Data Science or a related field. 3+ years of experience as a Data Analyst or Data Steward, preferably within the consumer-packaged goods, FMCG, pharmaceutical or healthcare industry. Strong knowledge of data management principles, data quality frameworks, and metadata management practices and tools. Understanding of data lineage and data cataloging concepts. Business acumen in the area of product master data. Familiarity with master data management tools (e.g. Reltio Cloud MDM, Syndigo Enterprise Data Suite), SAP modules related to master data management (e.g. Material Management, Product Lifecycle management) and master data aspects of the CRM systems (e.g. IQVIA OCE-P, Salesforce). Experience with data manipulation and analysis using Azure Databricks, SQL and Python. Familiarity with relational databases (PostgreSQL, MSSQL) and data modelling. Excellent analytical and problem-solving skills with a keen attention to detail. Understanding about data compliance & security standards such as data privacy regulations (GPDR, HIPAA), EU AI Act, management of confidential data, and experience with measures to mitigate data risks. Strong communication skills, with the ability to present complex data in a clear and understandable manner. Interest and experience with AI tools supporting data analysis and stewardship is a plus. Experience with preparing data for AI solutions (e.g. traditional machine learning models, AI Agents) is a plus. Ability to work collaboratively in a team-oriented environment. Fluent in English, both written and spoken. What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 14000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Remote,38,Senior Big Data Developer,EPAM Systems,"We are looking for a Senior Big Data Developer to join our growing Data Practice and make our team even stronger. Our projects and technologies are very diverse and cover all technologies currently on the market and represented by open-source communities. We are providing our service to Clients in different domains: Financial, Health Care, Insurance and many others, so you will have a chance to develop yourself in any direction you want. RESPONSIBILITIES Implement data ingestion from diverse sources utilizing technologies including RDBMS, REST HTTP API, flat files, Streams, Time series data, SAP Research and utilize Big Data technologies for effective data ingestion Process and transform data using tools such as Spark and various Cloud Services Understand and execute project-specific business logic through programming languages supported by the primary data platform Optimize data retrieval, develop dashboards, and perform data validation Collaborate with diverse teams including data scientists, analysts, and IT professionals Provide thought leadership on Big Data best practices and technology trends Maintain and ensure integrity and security of big data sources and platforms Perform troubleshooting, debugging, and upgrading of big data systems and applications REQUIREMENTS Bachelor‚Äôs degree in Computer Science, Mathematics or related technical field 5+ years experience in software development with Big Data technologies Advanced knowledge of Python, Scala, or Java Proficient with Spark and Databricks Experience with cloud platforms such as Azure, Google Cloud Platform, Amazon Web Services Demonstrated experience with big data models, databases, and tools Strong understanding of different data file formats like JSON, Parquet, Avro Hands-on experience with SQL and NoSQL databases, including Cassandra and MongoDB Proficiency in data warehousing solutions, ETL processes Proven ability to work within a fast-paced, team-oriented environment Detail-oriented with excellent analytical and problem-solving skills Knowledge of software development methodologies and best practices Exceptional oral and written communication skills in English (B2+) NICE TO HAVE Experience with Hadoop, Spark, Kafka, Oozie, Airflow, HDFS, YARN Experience with message queues, especially Kafka Production experience in search technologies such as Solr, OpenSearch, or Lucene WE OFFER We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Relocation within our 50+ offices We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, O‚ÄôReilly, Cloud Guru Language classes in English and Polish for foreigners We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Unclassified,Unclassified
Full-time,Senior,B2B,Remote,39,Data Engineer ETRM,INFOPLUS TECHNOLOGIES,"Join Our Team as an ETRM Data Engineer ‚Äì Poland (Remote) | B2B Contract ETRM Data Engineer to play a key role in transforming complex data into actionable insights. If you're eager to work on cutting-edge energy trading systems from the comfort of your home in Poland, this could be your next big opportunity! What You‚Äôll Do: Design and Build robust, scalable data pipelines and manage high-performance ETRM systems. Drive data integration projects within the dynamic Energy Trading and Risk Management (ETRM) landscape. Collaborate with cross-functional teams to seamlessly integrate data from leading ETRM trading platforms like Allegro, RightAngle, and Endur. Optimize data storage solutions using Data Lake and Snowflake for faster, more reliable access. Develop and maintain ETL workflows leveraging Azure Data Factory and Databricks. Write clean, efficient Python code for data processing, analysis, and automation. Uphold the highest standards of data quality, accuracy, and integrity across diverse sources and platforms. Partner with traders, analysts, and IT professionals to understand data needs and deliver innovative solutions. Continuously enhance data architecture for performance and scalability to support business growth. What We‚Äôre Looking For: Proven expertise with Azure Data Factory (ADF) Experience working with Data Lake and Snowflake/SQL databases Strong Python/PySpark programming skills Familiarity with FastAPI for API development Hands-on experience with Databricks platform Why Join Us? Remote flexibility from anywhere in Poland Engage with innovative energy trading solutions Be part of a forward-thinking team shaping the future of energy markets Competitive B2B contract terms","[{""min"": 28000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Office,40,Analytics Engineer,DOZ S.A.,"Zakres obowiƒÖzk√≥w: Projektowanie, budowanie i utrzymywanie warstwy danych analitycznych (np. w modelu star schema lub data vault) Tworzenie i rozw√≥j proces√≥w ELT w narzƒôdziach takich jak Data Factory, Databricks, Airflow itp. Wsp√≥≈Çpraca z analitykami, biznesem i in≈ºynierami danych w celu zrozumienia potrzeb analitycznych i wdra≈ºania nowych rozwiƒÖza≈Ñ Zapewnienie sp√≥jno≈õci i jako≈õci danych ‚Äì walidacja, testowanie danych, tworzenie kontroli jako≈õci (data quality checks) Optymalizacja zapyta≈Ñ SQL i modeli danych pod kƒÖtem wydajno≈õci i koszt√≥w Dokumentowanie ≈∫r√≥de≈Ç danych, modeli i proces√≥w Tworzenie i wspieranie ≈∫r√≥de≈Ç danych do Power BI Wsp√≥≈Çpraca z zespo≈Çami DevOps/DataOps nad monitoringiem i automatyzacjƒÖ proces√≥w Nasze wymagania: Wykszta≈Çcenie wy≈ºsze Min. 2 lata do≈õwiadczenia w pracy na podobnym stanowisku Bardzo dobra znajomo≈õƒá SQL i umiejƒôtno≈õƒá modelowania danych Zrozumienie potrzeb u≈ºytkownik√≥w biznesowych i umiejƒôtno≈õƒá przek≈Çadania ich na rozwiƒÖzania techniczne Umiejƒôtno≈õci tworzenia kompleksowych raport√≥w i analiz Umiejƒôtno≈õƒá pracy z narzƒôdziami do wersjonowania kodu (Git) Do≈õwiadczenie w pracy z hurtowniami danych i du≈ºymi zbiorami danych Dodatkowym atutem bƒôdzie znajomo≈õƒá jƒôzyka SPARK i platformy Databricks Entuzjastyczne nastawienie do pracy z danymi oraz chƒôci rozwoju w tym obszarze Komunikatywno≈õci i umiejƒôtno≈õci pracy w zespole Oferujemy: üë©‚Äçüíª AmbitnƒÖ pracƒô, w kt√≥rej nie spos√≥b siƒô nudziƒá üîèMo≈ºliwo≈õƒá do≈ÇƒÖczenia do dobrego ubezpieczenia grupowego na preferencyjnych warunkach, kt√≥re zapewni poczucie bezpiecze≈Ñstwa Tobie i Twoim bliskim ü©∫PrywatnƒÖ opiekƒô medycznƒÖ, dziƒôki kt√≥rej bƒôdziesz mia≈Ç mo≈ºliwo≈õƒá skonsultowania siƒô ze specjalistami na terenie ca≈Çego kraju üí™Karty sportowe, dziƒôki kt√≥rym zadbasz o swoje dobre samopoczucie üíâAkcje profilaktyczne, dziƒôki kt√≥rym chcemy zadbaƒá o Twoje zdrowie! üìâZni≈ºki oraz karty rabatowe na wybrane produkty üëºUdzia≈Ç w akcjach CSR, bo lubimy pomagaƒá ‚òïPysznƒÖ kawƒô w mi≈Çym towarzystwie wsp√≥≈Çpracownik√≥w üß° ü§ùRegularny feedback, kt√≥ry pomo≈ºe rozwijaƒá Twoje kompetencjeüß°",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,41,Programista SQL,Eyzee S.A.,"Poszukujemy Programist√≥w SQL, kt√≥rzy majƒÖ do≈õwiadczenie w projektowaniu i eksploatacji baz danych dla naszego klienta w bran≈ºy medycznej. Celem projektu jest opracowywanie i wdra≈ºanie innowacyjnych rozwiƒÖza≈Ñ, programowanie nowych funkcjonalno≈õci, rozwijanie, modyfikowanie i testowanie oprogramowania. Tworzymy przyjazne miejsce pracy i rozwoju dla specjalist√≥w w bran≈ºy IT, zapewniamy ciekawe wyzwania, dbajƒÖc o dobrƒÖ komunikacjƒô i atmosferƒô w zespole. Z nami przyspieszysz rozw√≥j swojej kariery! Praca zdalna, pe≈Çen etat. Zadania dla Ciebie: projektowanie, tworzenie i utrzymanie skrypt√≥w SQL oraz zaawansowanych zapyta≈Ñ do baz danych. tworzenie i rozwijanie procedur sk≈Çadowanych oraz funkcji w bazach danych PostgreSQL lub Oracle analiza i optymalizacja wydajno≈õci zapyta≈Ñ SQL w celu zapewnienia szybkiego dostƒôpu do danych. wsp√≥≈Çpraca z zespo≈Çem analityk√≥w i deweloper√≥w w celu implementacji rozwiƒÖza≈Ñ opartych na danych zapewnienie jako≈õci i sp√≥jno≈õci kodu oraz danych Wymagania: minimum 5 latat do≈õwiadczenia w programowaniu w jƒôzyku SQL i/lub PL/SQL praktyczne do≈õwiadczenie w optymalizacji zapyta≈Ñ SQL (udokumentowane przynajmniej w jednym projekcie) do≈õwiadczenie w tworzeniu procedur sk≈Çadowanych w bazie danych PostgreSQL lub Oracle do≈õwiadczenie w pracy z Hurtowniami Danych znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z przetwarzaniem danych do≈õwiadczenie w tworzeniu proces√≥w ETL praktyczna znajomo≈õƒá ≈õrodowisk CI/CD do≈õwiadczenie w pracy z systemem kontroli wersji GIT Mile widziane: do≈õwiadczenie w bran≈ºy medycznej Co oferujemy? stabilne zatrudnienie w oparciu o kontrakt B2B s≈Çu≈ºbowy laptop i monitor dofinansowanie prywatnej opieki medycznej sportowƒÖ kartƒô Multisport nauka jƒôzyka angielskiego omawianie postƒôp√≥w i rozwoju co p√≥≈Ç roku transparentna komunikacja z pracownikami mo≈ºliwo≈õƒá zaanga≈ºowania siƒô w rozw√≥j organizacji chƒôtnie dzielimy siƒô wiedzƒÖ - do≈ÇƒÖcz do Akademii Eyzee mocny kompetencyjnie zesp√≥≈Ç sk≈ÇadajƒÖcy siƒô w wiƒôkszo≈õci z senior√≥w praca z narzƒôdziami JIRA, Confluence, BitBucket dbamy o integracje i chƒôtnie wsp√≥lnie spƒôdzamy czas Kim jeste≈õmy? Jeste≈õmy polskƒÖ firmƒÖ specjalizujƒÖcƒÖ siƒô w realizacji z≈Ço≈ºonych projekt√≥w informatycznych oraz doradczych dla firm z sektora finansowego, telekomunikacyjnego i publicznego. Stanowimy zgrany zesp√≥≈Ç konsultant√≥w z wiedzƒÖ i wieloletnim do≈õwiadczeniem w tworzeniu i utrzymywaniu rozwiƒÖza≈Ñ. Wa≈ºne dla nas sƒÖ: doprecyzowanie wymaga≈Ñ przed napisaniem kodu, jako≈õƒá tworzonego kodu, testowanie oraz CI/CD. Nasze projekty to g≈Ç√≥wnie tworzenie nowych mikroserwis√≥w lub nowych funkcjonalno≈õci do istniejƒÖcych rozwiƒÖza≈Ñ. Dodatkowo rozwijamy w≈Çasne aplikacje i plugin‚Äôy, kt√≥re nie tylko usprawniajƒÖ pracƒô, ale te≈º pozwalajƒÖ rozwinƒÖƒá nasze do≈õwiadczenie. Jeden z nich mo≈ºesz pobraƒá tutaj (eZee Worklog). Jeste≈õmy partnerem Atlassian.","[{""min"": 15000, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Database Administration,Database Administration
Full-time,Senior,Permanent or B2B,Remote,42,Principal of Data Analytics Consulting,EPAM Systems,"We are looking for Principal of Data Analytics Consulting to join our team. You will work alongside a dynamic team to identify complex business problems and create solution-oriented strategies for some of the most recognized brands in the Financial Services industry. Responsibilities Build trusted partner relationships with senior client stakeholders and internal account managers Manage and drive collaborative, value-centered approaches to defining and solving data problems Coordinate with Senior Leadership to co-develop account plans and strategies for data-driven opportunities & business development Generate revenue within the client account portfolio by identifying data opportunities and enabling a wide range of data & analytics engagements in the financial sector Perform as principal consultant leading complex data transformational initiatives Establish & Contribute to the strategic deliverables such as mission statements, assessment results & roadmaps Participate in data-related pre-sales activities by assessing opportunities, responding to RFPs, creating proposals, and helping to close new deals Lead team development while supporting strategic financial accounts Impact client value and business growth by achieving target revenue goals Improve the business operations by achieving the target billable utilization Create & contribute to EPAM Data Practice people initiatives Requirements Minimum of 12 years of IT experience At least 8 years in Manager/Owner/Coordinator roles Hands-on experience influencing data solution design and technology selections for enterprise data platforms, warehouses, data lakes, and data science platforms Strong understanding of IT management consulting Expert in driving large data-driven programs and the revenue growth within a financial account portfolio Highly skilled in building data & analytics business development organizations Proven history of managing complex technology initiatives while communicating with senior business & IT stakeholders Familiarity with banking, trading, and data aggregation Knowledge of pre-sales activities Ability to develop and grow client relationships as well as facilitate and drive strategy definition Experience working with data analytics trends Exposure to interacting with customers, managing expectations, and explaining solutions & project deliverables to senior stakeholders Comfortable operating at the strategic level while being close enough to the details to add value to clients and the team C1 English level proficiency We offer International projects with top brands Work with global teams of highly skilled, diverse peers Healthcare benefits Employee financial programs Paid time off and sick leave Upskilling, reskilling and certification courses Unlimited access to the LinkedIn Learning library and 22,000+ courses Global career opportunities Volunteer and community involvement opportunities EPAM Employee Groups Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Hybrid,43,SAS/Oracle Data Warehouse Engineer,Onwelo Sp. z o.o.,"Poznaj Onwelo: Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Do≈ÇƒÖcz do projektu realizowanego dla klienta z bran≈ºy ubezpieczeniowej, gdzie bƒôdziesz czƒô≈õciƒÖ zespo≈Çu odpowiedzialnego za rozw√≥j narzƒôdzi wspierajƒÖcych Hurtowniƒô Danych. Projekt obejmuje budowƒô narzƒôdzi automatyzujƒÖcych, integracyjnych i raportowych, jak r√≥wnie≈º eksploracjƒô nowych technologii i podej≈õƒá architektonicznych. Tworzyƒá narzƒôdzia wspierajƒÖce pracƒô developer√≥w Hurtowni Danych (SAS, SAS Viya, Oracle i inne technologie) Budowaƒá API umo≈ºliwiajƒÖce integracjƒô r√≥≈ºnych system√≥w (REST, SAS, Office365) Projektowaƒá rozwiƒÖzania wspierajƒÖce wymianƒô danych pomiƒôdzy systemami (np. z wykorzystaniem Apache Kafka) Realizowaƒá POC nowych technologii i narzƒôdzi (np. SAS Viya, Exadata w chmurze Azure) Opracowywaƒá komponenty optymalizujƒÖce przetwarzanie danych i wydajno≈õƒá proces√≥w Projektowaƒá techniczne data mart-y i raporty BI (SAS Viya, Power BI) Monitorowaƒá aktywno≈õƒá u≈ºytkownik√≥w oraz tworzyƒá narzƒôdzia raportowe Wspieraƒá strategiczne projekty poprzez dostarczanie rozwiƒÖza≈Ñ technologicznych Masz do≈õwiadczenie w projektowaniu rozwiƒÖza≈Ñ dla Hurtowni Danych i system√≥w BI Znasz narzƒôdzia i ≈õrodowisko SAS oraz SAS Viya (DI Studio, Enterprise Guide, SAS Studio, Visual Analytics) Pracujesz z bazami danych Oracle oraz jƒôzykiem PL/SQL (mile widziane do≈õwiadczenie z Oracle Exadata) Potrafisz pracowaƒá z narzƒôdziami CI/CD (Git, Bitbucket, Bamboo, JIRA) Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 700, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Unclassified,Unclassified
Full-time,Manager / C-level,Permanent,Hybrid,44,"Engineering Manager, Data Infrastructure",Asana,"We are looking for an Engineering Manager who can provide technical leadership for our data platform and effectively support engineers with large-scope responsibilities. You will be responsible for the delivery and technical quality of our programs, taking on complex multi-year projects, and leading the work of several ICs. You‚Äôll also partner with cross-functional stakeholders in Infrastructure, Data Science, and Business teams to drive initiatives forward, and mentor team members in technical design, project leadership, and running team processes. The Data Infrastructure team builds the infrastructure responsible for moving product and business data in a secure, efficient and timely manner to Data consumers. We operate several systems ranging from data ingestion pipelines, Spark processors, data/ML tooling, experimentation platform, and logging pipelines for Asana‚Äôs Data Lake. This role is based in our Warsaw office with an office-centric hybrid schedule - in-office days are Monday, Tuesday, and Thursday. We offer a Contract of Employment (UoP) for our employees in Poland. What you‚Äôll achieve: Support and coach engineers in their technical and professional development. Partner with cross-functional stakeholders to guide the team‚Äôs roadmap and prioritization. Collaborate with data engineering and with other infrastructure teams on problems related to data infrastructure service models, operations, deployment and security. Evangelize good code and solid engineering and operability practices. Co-create and follow best practices of running data applications in the cloud (AWS) and all the state-of-the-art tooling around it. Bootstrap our Data Infra presence in Warsaw! Some upcoming technical challenges you may work on: Build cost tracking/observability systems for data use cases; Help improve the Databricks experience for Asana users; Improve deployment and release automation for data pipelines; and Implement best practices for security and privacy of our data infrastructure. About you: Prior experience building and operating cloud infrastructure (preferably involving analytical/real-time data or storage) and distributed systems at scale. Familiarity with distributed data processing systems (e.g. Spark, Databricks, Snowflake or similar systems). Expertise in AWS services (e.g. IAM, S3) and with infrastructure-as-code tooling (e.g. Terraform). 2-4 years building/leading engineering teams as a manager. Planning/execution as well as career and impact growth for your reports. Experience collaborating with other engineering teams, PMs, and cross-functional partners on alignment and project execution, and with teams in other timezones. Focus on maximizing impact, for yourself and your team. Ability to provide valuable input to any technical or product discussion. Oriented around the multi-year consequences of your decisions. What we offer: Generous, transparent and fair compensation system (base salary and generous Restricted Stock Unit for Asana Inc.). Contract of Employment (with 50% tax deductible costs for author‚Äôs rights usage for Engineers). Health insurance with dental and travel coverage (Lux Med). Lunch catering on the days that you work from the office. Career growth budget. Home office setup budget. Gym/Fitness card. Fertility healthcare and family-forming support with Carrot . Mental health support in Modern Health. Group life insurance. MacBooks with all necessary accessories. For this role, the estimated base salary range is between 39 500 - 50 400 PLN gross monthly on the contract of employment (UoP). The actual base salary will vary based on various factors and individual qualifications objectively assessed during the interview process. The listed range above is a guideline, and the base compensation range for this role may be modified. Our total compensation consists of base salary and equity (RSUs). About us Asana helps teams orchestrate their work, from small projects to strategic initiatives. Millions of teams around the world rely on Asana to achieve their most important goals, faster. Asana has been named a Top 10 Best Workplace for 5 years in a row, is Fortune's #1 Best Workplace in the Bay Area, and one of Glassdoor‚Äôs and Inc.‚Äôs Best Places to Work. After spending more than a year physically distanced, Team Asana is safely and mindfully returning to in-person collaboration, incorporating flexibility that adds hybrid elements to our office-centric culture . With 11+ offices all over the world, we are always looking for individuals who care about building technology that drives positive change in the world and a culture where everyone feels that they belong. We believe in supporting people to do their best work and thrive, and building a diverse, equitable, and inclusive company is core to our mission. Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We provide equal employment opportunities to all applicants without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by law. We also comply with the San Francisco Fair Chance Ordinance and similar laws in other locations. Our comprehensive compensation package plays a big part in how we recognize you for the impact you have on our path to achieving our mission. We believe that compensation should be reflective of the value you create relative to the market value of your role. To ensure pay is fair and not impacted by biases, we're committed to looking at market value which is why we check ourselves and conduct a yearly pay equity audit.","[{""min"": 35000, ""max"": 45000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Part-time,Manager / C-level,B2B,Remote,45,Tech Lead Data Scientist,Link Group,"We are looking for an experienced Tech Lead Data Scientist to support a team working on machine learning solutions in a cloud environment. The person in this role will be responsible for designing and implementing ML models, overseeing the technical quality of solutions, and collaborating with both business and technical stakeholders. Strong knowledge of Azure and Databricks Experience working with machine learning Ability to design and deploy ML models Previous experience as a Tech Lead or in a technical role with responsibility for providing technical support English proficiency at C1 level","[{""min"": 140, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Mid,B2B,Hybrid,46,Data Analyst,Optiveum,"Data Analyst ‚Äì PaaS Customer Delivery üìç Hybrid in Warsaw (3 days/week onsite required) | üíº Full-time | B2B contract up to $5,000/month About the Role Our client is a US-based technology company headquartered in New York City, delivering digital solutions and consulting services that transform businesses and drive measurable value. With offices in multiple countries, the company is now investing in a new engineering centre in Warsaw, recognizing the strong talent and culture of Polish software professionals. We are seeking a Data Analyst to join the Platform-as-a-Service (PaaS) Customer Delivery team. In this role, you'll help transform platform usage data into actionable insights that directly support product strategy, customer operations, and implementation success. This position is perfect for someone who is analytical, curious, and thrives at the intersection of data and business decision-making. This is a hybrid position ‚Äî you will be expected to work from the Warsaw office at least 3 days per week. Key Responsibilities Partner with implementation and strategy teams to define key performance metrics Build clear and impactful dashboards and reports to monitor product and platform health Conduct deep-dive analyses to surface trends, identify inefficiencies, and recommend improvements Develop and maintain scalable data models and robust SQL queries Collaborate with data engineers to ensure clean, consistent data pipelines Present findings clearly to technical and non-technical stakeholders alike Contribute to internal documentation and promote data literacy across teams Key Requirements Required 2‚Äì4 years of experience in data analytics or BI roles Advanced SQL skills and experience with cloud data warehouses Proficiency in BI tools such as Looker, Tableau, Power BI, or similar Strong analytical mindset and keen attention to detail Confident in communicating insights and telling stories with data Preferred Experience with dbt or other data modeling tools Familiarity with A/B testing methodologies Some coding experience in Python, R, or similar scripting languages Exposure to product analytics tools like Mixpanel or Amplitude What‚Äôs Offered B2B contract with monthly compensation up to $5,000 Strong career growth opportunities in a global fintech environment High-impact projects in a fast-growing sector Friendly, open, and ambitious team culture Hybrid model ‚Äì minimum 3 days/week in the Warsaw office","[{""min"": 14581, ""max"": 18226, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,47,Senior Data Engineer,Link Group,"Senior Data Engineer We are looking for a seasoned Senior Data Engineer to join a dynamic team that‚Äôs building a cutting-edge platform designed to enable automated decision-making and intelligent automation across the entire business lifecycle. This role offers a unique opportunity to help shape a transformative solution from the ground up. We're seeking individuals who are mission-driven, proactive, and passionate about data engineering and innovation. Key Responsibilities Partner with business stakeholders and product owners to gather data requirements and design scalable technical solutions. Build and maintain robust data models and schema structures to support analytics, business intelligence, and machine learning initiatives. Optimize data processing pipelines for speed, scalability, and cost-efficiency across cloud and on-premise environments. Ensure high data quality and consistency through validation frameworks, automated monitoring, and comprehensive error-handling processes. Collaborate closely with data analysts and data scientists to deliver reliable, well-structured, and easily accessible datasets. Stay informed of emerging trends, tools, and best practices in data engineering to help drive innovation and technical excellence. Maintain operational stability and system performance across data pipelines and platforms. Provide Level 3 production support when necessary, resolving critical data-related issues swiftly and effectively. Required Experience and Skills Minimum 8+ years of experience in data engineering, data architecture, or a similar technical role. Strong programming skills in SQL , Python , Java , or equivalent languages for data processing and pipeline development. Experience with both relational (e.g., PostgreSQL, SQL Server, Oracle) and NoSQL (e.g., MongoDB) databases, OLAP tools like Clickhouse , and vector databases (e.g., PGVector , FAISS , Chroma ). Expertise in distributed data processing frameworks such as Apache Spark , Flink , or Storm . Experience with cloud data solutions (e.g., Azure , AWS Redshift , BigQuery , Snowflake ) is highly desirable. Solid understanding of ETL/ELT pipelines , data transformation , and metadata management using tools such as Airflow , Kafka , NiFi , Airbyte , and Informatica . Proficiency in query performance tuning, profiling, and data pipeline optimization. Hands-on experience with data visualization platforms like Power BI , Tableau , Looker , or Apache Superset . Familiarity with DevOps principles, version control systems ( Git ), and CI/CD pipelines . Strong problem-solving skills, attention to detail, and the ability to work under pressure. Effective communicator with the ability to collaborate across multidisciplinary teams.","[{""min"": 200, ""max"": 250, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Practice / Internship,Junior,Internship,Office,48,Data Engineer & Data Analyst Internship Program (She/He/They),Accenture,"WHO WE ARE: Accenture is a leading global professional services company that helps the world‚Äôs leading businesses, governments, and other organizations build their digital core, optimize their operations, accelerate revenue growth, and enhance citizen services. We offer solutions and assets across Strategy & Consulting, Technology, Operations, Industry X, and Accenture Song. Internship at Accenture is a unique opportunity to work with qualified IT specialists who will willingly share their knowledge and possibilities that will be useful in the implementation of projects for the client. Taking internship at Accenture is a chance to be part of the projects we carry out for clients from Poland and international clients. We implement projects for many areas of the economy, such as: banking and finance, transport and infrastructure, energy, industry and many more. For more information about Accenture Technology Poland please visit our website: Poland | Let There Be Change | Accenture THE WORK: Work in database systems to manage data effectively. Analyze data and visualize results through comprehensive aggregation. Report on analysis outcomes to stakeholders effectively. Collaborate with experienced team members to gain knowledge and continuously improve your skills. WHAT‚ÄôS IN IT FOR YOU? Paid, several-month internship program - min. 3 months, 4-5 days a week, with flexible working hours. A chance for long-term cooperation - there is a high probability that at the end of the internship, we will offer you a full-time job. Ambitious projects for the most prestigious clients in the world. The opportunity to gain skills, knowledge and experience while working in international teams and with experts from the dynamically developing digital sector. Career development - based on a culture of feedback and constant support from a mentor, a clear path of development from the first day of work and Buddy's help in a new workplace. Qualifications HERE‚ÄôS WHAT YOU‚ÄôLL NEED: Work from the office in Katowice (100%). Active student status and availability min. 30h per week as of half of July. Excellent communication skills in English and Polish (written and oral) and good interpersonal skills. Proficiency in SQL. Desire to develop skills in Big Data/Data Analytics and interest in learning new tools and cloud technologies. Excellent analytical and communication skills, with the ability to manage tasks independently, proactively, and assertively, along with a commitment to teamwork and adaptability to new situations and deadlines. BONUS POINTS IF YOU HAVE: Basic knowledge of Power BI, including data modeling, DAX language syntax, and data visualization tailored for business users. Basic understanding of Microsoft Azure Knowledge of Amazon Web Services and Amazon QuickSight WHAT WE BELIEVE: Accenture does not discriminate employment candidates on the basis of race, religion, color, sex, age, disability, national origin, political beliefs, trade union membership, ethnicity, denomination, sexual orientation or any other basis impermissible under Polish law. All our leaders are committed to building a better, stronger and more durable company for future generations to create positive, long-lasting change. Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and creative, which helps us better serve our clients and our communities. Our position as partner to many of the world‚Äôs leading businesses, organizations and governments affords us both an extraordinary opportunity and a tremendous responsibility to make a difference. Sustainability is one of our greatest responsibilities, which we embed it into everything we do and for everyone we work with. Clicking apply I hereby express my consent to process my personal data included in my job offer by Accenture Sp. z o.o. or any other entity of the Accenture group for recruitment purposes, and that it is a data controller within the meaning of GDPR. More information about Accenture (and if necessary, also its representative) can be found here: https: //www.accenture.com/pl-pl/privacy-policy",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Any,Office,49,Analityk danych marketingowych,Centrum Medyczne Pu≈Çawska,"Do≈ÇƒÖcz do nas i razem tw√≥rzmy lepsze jutro! PracujƒÖc z nami, do≈ÇƒÖczysz do firmy, kt√≥ra nie tylko ceni sobie profesjonalizm i rozw√≥j osobisty, ale tak≈ºe aktywnie anga≈ºuje siƒô w dzia≈Çania na rzecz spo≈Çeczno≈õci. Jeste≈õmy dumni z naszego udzia≈Çu w kampaniach spo≈Çecznych, takich jak ‚ÄûR√≥≈ºowa Skrzyneczka‚Äù oraz innych licznych projekt√≥w. Wierzymy, ≈ºe praca to wiƒôcej ni≈º obowiƒÖzki ‚Äì to tak≈ºe szansa na bycie czƒô≈õciƒÖ czego≈õ wiƒôkszego i wp≈Çywanie na ≈õwiat w pozytywny spos√≥b. U nas mo≈ºesz rozwijaƒá swoje umiejƒôtno≈õci zawodowe, jednocze≈õnie dok≈ÇadajƒÖc swojƒÖ cegie≈Çkƒô do wa≈ºnych inicjatyw. Do≈ÇƒÖcz do nas! ‚ù§Ô∏è Miejsce pracy: Piaseczno, ul. Wi≈õniowa 6A Twoje zadania: Analiza ruchu marketingowego z r√≥≈ºnych kana≈Ç√≥w: Google Ads, YouTube, SEO, social media, e-mail marketing Ocena efektywno≈õci kampanii reklamowych w kontek≈õcie koszt√≥w, konwersji i ROI Modelowanie i ocena ≈õcie≈ºek atrybucji marketingowej (multi-touch attribution) Monitorowanie i optymalizacja lejk√≥w konwersji online (landing pages, formularze, klikniƒôcia) Wsp√≥≈Çpraca z zespo≈Çem marketingu oraz rekomendowanie dzia≈Ça≈Ñ na podstawie danych Tworzenie raport√≥w i dashboard√≥w dla dzia≈Çu marketingu i zarzƒÖdu Od Kandydat√≥w/Kandydatek oczekujemy: Min. 2-3 lata do≈õwiadczenia w analizie danych marketingowych Znajomo≈õci Google Analytics 4 (GA4), Google Tag Manager Do≈õwiadczenia z platformami reklamowymi: Google Ads, Meta Ads, YouTube Ads, inne Znajomo≈õci narzƒôdzi do modelowania atrybucji i analizy ≈õcie≈ºek konwersji Praktycznej znajomo≈õci SQL i/lub BigQuery (praca z du≈ºymi zbiorami danych marketingowych) Znajomo≈õci Power BI w praktyce Umiejƒôtno≈õci tworzenia lejk√≥w konwersji i raport√≥w efektywno≈õci kampanii Kompetencje miƒôkkie Analitycznego my≈õlenia i orientacji na dane Zrozumienia potrzeb marketingu i biznesu Umiejƒôtno≈õci pracy z wieloma ≈∫r√≥d≈Çami danych jednocze≈õnie Samodzielno≈õci i proaktywno≈õci w analizie i interpretacji wynik√≥w Umiejƒôtno≈õci formu≈Çowania rekomendacji opartych na danych Komunikatywno≈õci i wsp√≥≈Çpracy z zespo≈Çem marketingowym Do≈õwiadczenia w pracy z danymi dotyczƒÖcymi kampanii online, atrybucji i ≈õcie≈ºek u≈ºytkownika Mile widziane do≈õwiadczenie w bran≈ºy e-commerce, lead generation lub healthcare Znajomo≈õƒá specyfiki danych marketingowych B2C bƒôdzie dodatkowym atutem W zamian oferujemy: Umowƒô o pracƒô/umowƒô B2B Pakiet prywatnej opieki medycznej - dbamy o Twoje zdrowie, zapewniajƒÖc dostƒôp do profesjonalnej opieki medycznej. Dostƒôp do platformy benefitowej (w tym karta Multisport) ‚Äì ka≈ºdy znajdzie korzy≈õci dostosowane do swoich potrzeb. Mo≈ºliwo≈õƒá rozwoju zawodowego ‚Äì jeste≈õmy gotowi podzieliƒá siƒô naszƒÖ wiedzƒÖ i do≈õwiadczeniem, aby≈õ m√≥g≈Ç/≈Ça rozwijaƒá siƒô razem z nami. Szkolenia wewnƒôtrzne i zewnƒôtrzne ‚Äì stawiamy na ciƒÖg≈Çe doskonalenie w wykonywanej pracy. Program polece≈Ñ pracowniczych oraz inicjatyw - dajemy Ci szansƒô na dodatkowƒÖ gratyfikacjƒô za TwojƒÖ aktywno≈õƒá i inicjatywƒô.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,50,Data Engineer with GCP,STX Next,"Hi there! We are STX Next, a global IT consulting company specializing in customer-focused software services. Join a group of 500 professionals dedicated to helping customers build outstanding products. Leveraging the latest advancements in the field, and a passion for innovation, we're shaping the future of technology one project at a time. Are you the NEXT one? Type of contract A B2B contract for your flexibility and competitive compensation package Location A remote work model encouraging work-life balance. Salary Regular up to 18 480 PLN net + VAT Regular+ up to 23 520 PLN net + VAT Senior up to 26 880 PLN net + VAT Senior+ up to 30 240 PLN net + VAT Job description At STX Next, Data Engineers are creative problem-solvers who thrive at the intersection of data, innovation, and technology. Our approach to challenges is both collaborative and inventive, driven by the belief that every obstacle has a solution. You‚Äôll join a dedicated team that transforms data into valuable systems, addressing clients' needs and elevating the experience for end users. As a Data Engineer, you‚Äôll work closely with international clients, providing both consultancy and technical architecture to shape reliable, high-performance data solutions. You‚Äôll handle everything from large data sets to distributed systems, setting a high bar for data quality. If you‚Äôre excited by complex problems and want to contribute to a collaborative culture of craftsmanship, STX Next offers the platform to bring your ideas to life. Responsibilities Develop innovative solutions for complex data challenges, using creativity and technology. Design and build data systems that fulfill client requirements and optimize user experience. Collaborate across teams to integrate unique approaches and out-of-the-box thinking into problem-solving. Serve as a trusted consultant and architect for customized data solutions for global clients. Handle large-scale data processing and manage distributed systems to ensure reliability and performance. Uphold high standards of data quality and refine approaches to align with best practices. Foster a culture of collaboration, craftsmanship, and continuous improvement. Requirements Technical Expertise Strong experience in designing scalable data processing systems (pipelines, ETL, data warehouses, and lakes). Proficiency in GCP , especially BigQuery . Experience with dbt for data transformation. Strong SQL skills and experience with relational databases. Familiarity with Apache Airflow for workflow orchestration. Proficiency in Python for data engineering tasks. Soft Skills Good communication skills in English (minimum B2). Strong problem-solving and analytical thinking. Benefits Work-life Balance We are open to discussing individual needs. Set up working hours and limited remote work scheduled with your team and manager, in a way that works for both sides. Reimbursed private medical care (Medicover) and Multisport We care about the health and well-being of our colleagues. Choose a sports card and dedicated medical care for yourself and your relatives. Leader‚Äôs support Work with true enthusiasts and professionals who will support you along the way. You can count on leaders and experts who are willing to share their knowledge so that you too can join their ranks someday. Technology focus Python and JavaScript are not our only strengths, we are also very good at React Native, IoT, Machine Learning, .Net, DevOps and Blockchain. Growth review Junior, Regular or Senior? Every year we have a chance to discuss acquired skills and prepare a development plan for upcoming months. Events Attend exciting internal webinars, celebrate special days with us, and join us at conferences and meetups as a listener or speaker! Workation Team trips where you have a chance as a team not only to work together, but also integrate f2f.","[{""min"": 23520, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Office,51,"Senior Data Scientist, Ads Planning Metrics",Google,"Bachelor's degree in Statistics, Mathematics, Data Science, Engineering, Physics, Economics, or a related quantitative field or equivalent practical experience. 8 years of experience in using analytics to solve product or business problems, performing statistical analysis, and coding (e.g., Python, R, SQL) or 5 years of experience with a Master's degree. Experience with methods of statistical data analysis (e.g., linear models, hypothesis testing, multi-level models, Bayesian methods, etc.). Experience with A/B testing or other experimental design methodologies. Master's degree in Statistics, Mathematics, Data Science, Engineering, Physics, Economics, or a related quantitative field. Experience in Mathematical programming and modeling and statistics. Experience with Bayesian Statistics. Help serve Google's worldwide user base of more than a billion people. Data Scientists provide quantitative support, market understanding and a strategic perspective to our partners throughout the organization. As a data-loving member of the team, you serve as an analytics expert for your partners, using numbers to help them make better decisions. You will weave stories with meaningful insight from data. You'll make critical recommendations for your fellow Googlers in Engineering and Product Management. You relish tallying up the numbers one minute and communicating your findings to a team leader the next. Google Ads is helping power the open internet with the best technology that connects and creates value for people, publishers, advertisers, and Google. We‚Äôre made up of multiple teams, building Google‚Äôs Advertising products including search, display, shopping, travel and video advertising, as well as analytics. Our teams create trusted experiences between people and businesses with useful ads. We help grow businesses of all sizes from small businesses, to large brands, to YouTube creators, with effective advertiser tools that deliver measurable results. We also enable Google to engage with customers at scale. Design and analyze experiments, including randomized controlled trials, to measure and guide the impact of Ads Planning initiatives. Define key performance indicators and their associated value for advertisers and Google across various Ads Planning initiatives. Formulate metrics that capture the impact of Ads Planning tools on advertiser spend, reach, Google business growth, and agreement value. Develop methodologies for conducting impact studies for agencies and third-party tools. Collaborate with cross-functional teams to ensure the accuracy and feasibility of measurement strategies. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,52,"Senior Data Engineer (DBT, Snowflake)",Holisticon Connect,"Holisticon Connect is a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä We are looking for an experienced Senior Data Engineer with strong DBT and Snowflake skills to join our team at a leading Swedish manufacturing company . This role requires someone who is proactive, solution-oriented and not afraid to speak up, push for quality and champion their ideas in a dynamic, international setting involving close collaboration with external partners. You will thrive here if you combine technical expertise with a collaborative mindset and excellent communication skills. Although the project environment is fast-paced and engaging, it is firmly rooted in the Scandinavian work culture of trust, autonomy and transparency, where your initiative will be met with genuine support. Responsibilites: Drive backend development for a proprietary BI and analytics tool in the Finance stream , ensuring high-performance, scalable solutions. Take part in the transition from on-premises to the cloud , designing and implementing Snowflake data models and DBT transformations for the new environment. Architect and maintain CI/CD pipelines in GitLab for seamless code integration and deployment. Partner with stakeholders to translate reporting requirements into robust data solutions. Monitor and optimize the data warehouse in Snowflak e for query performance, storage efficiency, and cost control during and after migration. Implement secure, cloud-based data workflows on AWS or Azure , ensuring smooth cutover from legacy systems. Troubleshoot complex data issues and provide timely feedback to both technical and non-technical stakeholders throughout the migration. We offer a B2B Contract: 140 ‚Äì 190 PLN net/hour + VAT You might be the perfect match if you are/have: Bachelor‚Äôs or Master‚Äôs degree in Computer Science or a related field (or equivalent experience). 5+ years of professional experience in data engineering , with at least 2 years focused on Snowflake and DBT . Proven track record in complex Finance reporting projects . Experience in migration projects , specifically moving from on-prem to cloud environments. Experience driving on-prem to cloud migration initiatives. Experience setting up and maintaining CI/CD pipelines for data code in GitLab . Strong analytical thinking and passion for building innovative data models. Excellent communication skills , able to engage with diverse stakeholders and provide clear feedback. A self-starter mindset , able to work independently, prioritize tasks, and drive projects to completion. Fluent English for smooth collaboration in an international environment. Location in Poland to facilitate collaboration and attendance at occasional team meetings. Moreover, we appreciate skills in these areas: Experience in working in large, diverse, multinational development teams . Hands-on experience with AWS or Azure data services beyond basic usage. Demonstrated leadership abilities and teamwork in high-pressure projects. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private life so you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Perks and benefits: Fully remote work or in our office in Wroc≈Çaw; Free benefits such as Luxmed , Multisport , and life insurance in Nationale Nederlanden ; Attractive referral system (9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budget with additional paid hours; Passion Day - an extra day off for your hobby to spend as you please; Flexible working hours with no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment + 2 additional monitors and accessories. If you apply for this position and match our expectations, then: 1) You will be invited to an HR Screening with our IT Recruiter. 2) You will have a technical meeting with a Team Leader. 3) You will have a meeting with your team. Submit your application online in one easy step! Apply now!","[{""min"": 140, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,53,Data Scientist,Link Group,"Poszukujemy do≈õwiadczonego Data Scientista do zespo≈Çu in≈ºynieryjnego miƒôdzynarodowej firmy technologicznej specjalizujƒÖcej siƒô w rozwiƒÖzaniach automatyzujƒÖcych procesy finansowe i ksiƒôgowe. Firma od ponad 20 lat rozwija w≈ÇasnƒÖ platformƒô SaaS wykorzystywanƒÖ globalnie przez dzia≈Çy finansowe du≈ºych organizacji. Projekt skupia siƒô na budowaniu nowoczesnych, skalowalnych rozwiƒÖza≈Ñ AI/ML wspierajƒÖcych procesy ksiƒôgowe i finansowe, z wykorzystaniem Python, TensorFlow lub PyTorch, Google Cloud Platform oraz MLOps. Na co dzie≈Ñ Data Scientist w tym zespole odpowiada za projektowanie, trenowanie i wdra≈ºanie modeli uczenia maszynowego, budowanie pipelines danych, integracjƒô mikroserwis√≥w w ≈õrodowisku chmurowym oraz rozw√≥j us≈Çug AI w bliskiej wsp√≥≈Çpracy z product ownerami, innymi in≈ºynierami i zespo≈Çami cloudowymi. Wa≈ºny jest tu nie tylko mocny background techniczny, ale tak≈ºe umiejƒôtno≈õƒá szukania rozwiƒÖza≈Ñ w otwartych, nieoczywistych problemach i dzielenia siƒô wiedzƒÖ wewnƒÖtrz zespo≈Çu. Wymagania: Bardzo dobra znajomo≈õƒá Python, SQL, Spark Do≈õwiadczenie w budowie i wdra≈ºaniu modeli ML (klasyfikacja, klasteryzacja, prognozowanie) Znajomo≈õƒá TensorFlow i/lub PyTorch Praktyka w pracy z GCP lub innƒÖ du≈ºƒÖ chmurƒÖ (AWS, Azure) Do≈õwiadczenie z MLOps, pipelines, kontrolƒÖ wersji (Git) Wykszta≈Çcenie wy≈ºsze kierunkowe (Informatyka, Statystyka, Data Science) Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego ‚Äì komunikacja wewnƒôtrzna w firmie odbywa siƒô w tym jƒôzyku Mile widziane: Do≈õwiadczenie z GCP (BigQuery, Vertex AI) Znajomo≈õƒá specyfiki us≈Çug finansowych lub rozwiƒÖza≈Ñ dla ksiƒôgowo≈õci Wiedza z zakresu generative AI vs AI agents Informacje organizacyjne: üìç Lokalizacja: Krak√≥w ‚Äì model hybrydowy (2 dni w tygodniu z biura) üí¨ Wymagana bardzo dobra znajomo≈õƒá jƒôzyka angielskiego üìë Forma wsp√≥≈Çpracy: Umowa o pracƒô na czas nieokre≈õlony lub B2B (w zale≈ºno≈õci od preferencji) üóì Start: mo≈ºliwie jak najszybciej üíº Proces rekrutacyjny: rozmowy techniczne oraz spotkanie z przedstawicielem firmy","[{""min"": 21000, ""max"": 25000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Practice / Internship,Junior,Internship,Remote,54,Data Software Engineering Trainee,EPAM Systems,"Striving to gain market-oriented knowledge and skills to jumpstart your career in IT? Apply for this program and shape your professional path with EPAM experts. Details If you have strong Python programming skills and want to start a career in the Data Software Engineering (prev. Big Data) sphere, then this program is what you need. Our learning path includes two stages, offering you a step-by-step approach to gaining in-depth knowledge in this direction. By participating, you will have the opportunity to: Gain expertise in software development technologies and Git Dive into the capabilities of the main public cloud platforms Explore the design, development and maintenance of large data-volume software systems Learn large-scale data handling technologies (Hadoop, Apache Spark, Kafka, etc.) What do we offer? Education within an IT company. As a leading software engineering company, we will help you explore emerging technologies and best practices, ensuring you are equipped with the knowledge that the current market seeks. Top-notch learning materials. Our curriculum is designed by Data specialists with extensive project experience and validated in hundreds of training runs. Practice-oriented approach. This comprehensive program focuses on providing you with hands-on experience and practical application of the concepts learned. Deep dive into the specialization. Our graduates become highly skilled specialists ready to face complex technical challenges and work with the world's leading customers. Support from experienced mentors. We will guide you at advanced training stages, covering your questions and sharing feedback on assigned tasks. Training process The program consists of two stages: In the initial stage, you'll have the flexibility to explore self-study materials at your own pace. Through completing assigned tasks, you'll receive immediate automated feedback. Typically, this stage takes about 3 months to finish, requiring around 10-12 hours of weekly involvement. If you demonstrate promising results and pass a technical interview successfully, you'll advance to the next phase. This stage usually spans about 3 months and entails approximately 20 hours of weekly engagement. As part of this stage, you'll have the opportunity to join our mentoring program, featuring personalized one-on-one sessions with our production experts. Upon successfully completing the program, you'll have the chance to align your career path with your demonstrated skills and explore available opportunities within EPAM. What is required for training: English fluency at the Upper-Intermediate level (B2) or higher Strong Python programming skills Practical experience in one of the additional languages (Java or Scala) Solid knowledge of Relational Database Management System (DBMS) theory Good command of Structured Query Language (DML/DDL/DCL/TCL) Theoretical knowledge of DWH/Data Lake and one of the visualization tools Basic understanding of Linux, Docker and Kubernetes Please read this info before registration: This training is for citizens of Poland and specialists relocated to this country for a permanent stay. Please take into account that the training format is ongoing, however registration may close within a year. Active participants of trainings and EPAM Systems Company Employees are not allowed to register for the training. Please, contact your manager regarding the positions available. Please note that each stage typically takes around 3 months to complete. If, during this time, we don't observe progress, your application may be automatically withdrawn at any stage.",[],Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,55,Data Scientist,Procter & Gamble,"We are looking for a passionate Data Scientist who is eager to make an impact on their career and take on a new challenge! P&G Scaled Data Science Hub in Warsaw is dynamically growing and making impact across most key algorithmic investments of the Company. We work across problems and algorithmic products related to Retail, Media, Digital commerce, R&D, Supply Chain, Productivity, Knowledge management and others. In your role, you will take ownership of a particular business space with algorithmic model needs. We work all across traditional machine learning, deep learning, GenAI, optimization models, statistical models, adopting methods as required for the problem. To do your job well, you will need to build proven business understanding of your problem space. In Warsaw Hub we specialize in developing and bringing algorithms to scale, always keeping impact, value, resilience, and reliability in mind. Our work impacts P&G's 100k+ employees and billions of consumers around the world. You will join a world-class company improving lives of billions of consumers by applying data science. You‚Äôll be welcomed within a 200+ global team of Data Scientists across US, Panama, Switzerland, Belgium, Poland, Singapore, India and China. You will be able to network and learn from scientists with a diverse set of backgrounds and seniorities. On top, in our Warsaw Hub, we do plenty of knowledge sharing on a weekly basis and have a unique job setup which will allow you to experience data science beyond your own key scope of responsibilities. Our best-in-class P&G AI Factory platform you will use is an example taught about by Harvard Business School, and we continuously improve our developer experience, thanks to our strong AI Engineering team. With us you will: Partner with product teams and business leaders to fully understand the problem, and AI & data engineering teams to automate & deploy your models into applications, Develop your own insights to steer the algorithmic evolution roadmap in your area of ownership Analyze and model on big datasets ‚Äì e.g. translating 1.5TB daily consumer touchpoints and 500 million consumers' behaviors to actionable recommendations. Answer business questions and propose solution for business problems by applying machine learning techniques and algorithms Further develop understanding of machine learning, statistics, optimization, GenAI and other advanced analytical models ‚Äì and how to apply to real world problems Further learn what it takes to build and maintain a resilient algorithmic pipeline which passes the test of time, and what it takes for an algorithm to materialize it's impact and value Write production grade code applying best practices. Job Qualifications At least a Masters in a quantitative degree (Statistics, Operation Research, Systems Engineering, Computer Science, Applied Math, Economics), orBarchelor/Engineer degree with consecutive data science experience At least 2 years experience with production grade Data Science / algorithmically enabled applications Knowledge of Cloud Infrastructure : Microsoft Azure, Google Cloud Platform, Kubernetes Solid code writing capabilities - Python and Spark are preferable Strong technical & analytical skills (SQL, optimization, simulation, predictive modeling etc) Proven success in leadership, problem solving and prioritising Strong collaboration skills and working comfortably across teams Nice to have: Desired experience with Big Data ecosystem: Databricks, Spark, BigQuery Basic understanding of Business Intelligence Tools such as PowerBI, Tableau Experience with Agile DevOps, Github, Jira, Confluence - advantageous What we offer Work in an international Data Science team with global responsibilities (with large part of engineering and product teams located in Warsaw) Long-term career with development and growth opportunities Competitive salary and benefits program (private health care, life insurance, P&G stock options, saving plans, lunch subsidy, sport cards, in-office fitness center) Relevant trainings, certifications, conference participation Internal coaching programs & training Flexible working arrangements",[],Data Science,Data Science
Full-time,Senior,Any,Hybrid,56,Data Engineer,Antal Sp. z o.o.,"Role Description We are seeking a skilled Data Engineer to join our Client's team in Krak√≥w. As a Data Engineer, you will be responsible for designing, developing, and maintaining scalable data pipelines and systems to ensure efficient data processing and storage. Your daily duties will include collaborating with cross-functional teams, implementing data integration solutions, and ensuring data quality and security. You will be expected to develop and optimize ETL processes, manage large datasets, and support data analytics initiatives. Additionally, you will play a key role in troubleshooting data issues and contributing to architectural decisions surrounding data infrastructure. Company Description Our Client is a leading global financial institution, delivering a broad range of banking and financial services to millions of customers worldwide. They are committed to upholding the highest standards of integrity, innovation, and sustainability in all aspects of our operations. Their culture is built on inclusivity, respect, and continuous learning, fostering an environment where individuals can thrive and contribute to meaningful change in the financial sector. Requirements Proven experience as a Data Engineer or in a similar data-focused role Strong proficiency in SQL and experience with data modeling Hands-on experience with ETL processes and data pipeline development Familiarity with cloud platforms and big data technologies Ability to work effectively in a collaborative environment Excellent problem-solving and analytical skills Strong communication skills in English Bachelor‚Äôs degree in Computer Science, Engineering, or a related field What We Offer Competitive salary range commensurate with experience Comprehensive benefits package, including private healthcare and life insurance Opportunities for professional growth and skill development Access to advanced tools and technologies Supportive and inclusive working environment Flexible working arrangements Participation in global projects within the financial sector Recruitment Process Initial Application Review ‚Äì Your application will be assessed to ensure alignment with the position requirements. Preliminary Acceptance ‚Äì Selected candidates will be invited for further consideration. Telephone Interview ‚Äì An initial conversation to discuss your experience and suitability for the role. Client Verification ‚Äì Verification of your professional background and credentials. Qualification Interview ‚Äì A detailed interview with the hiring team to assess technical and interpersonal fit. Offer ‚Äì Successful candidates will receive a formal offer of employment. Screening ‚Äì Completion of required background checks prior to onboarding. To learn more about Antal, please visit www.antal.pl",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,57,Senior Azure Data Engineer,Link Group,"You will be responsible for: Working to take data throughout its lifecycle - acquisition, exploration, data cleaning, integration, analysis, interpretation, and visualization; Creating the pipeline for data processing, data visualization, and analytics products, including automated services, and APIs; Ingest data sources into data management platforms; Structure data into a scalable and easily understood architecture; Implement/build methodologies as well as (understand how to) scale them together with the businesses; Analyzing, maintaining and optimizing performance of existing solutions; Data Quality checks implementation; Designing, managing, and maintaining tools to automate operational processes; Work in a multi-disciplined team where you will turn data discoveries and ideas into models and insights. You will find how to leverage the data and the models to create and improve products for our customers, in lean development cycles; Advising clients and proposing improvements (direct contact with stakeholders). What you will need: 4+ years of experience in Azure and 5+ of industrial experience in the domain of large-scale data management, visualization and analytics; Hands-on knowledge of the following data services and technologies in Azure (for example Databricks, Data Lake, Synapse, Azure SQL, Azure Data Factory, Azure Data Explorer); Good understanding of Azure as a platform; Experience with analytics, databases, data systems and working directly with clients; Good knowledge of SQL and Python; Azure DevOps/GitHub at least intermediate level; Ability to work in small, highly technical groups; Very good knowledge of English (and Polish) with the ability to speak but also explain and simplify technical topics for the business; Experience working on projects for international clients; Proactive approach and goal-oriented, problem-solving skills. Nice to have: Knowledge of Infrastructure as a Code principles (Terraform or Azure ARM or Bicep); Willingness to show up at the Warsaw office as needed (customer visits / workshops / integrations / important project periods requiring f2f teamwork).","[{""min"": 130, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,58,QlikSense / Postgre SQL Developer,Rossmann,"Poszukujemy developera do zespo≈Çu odpowiedzialnego za rozw√≥j system√≥w wspierajƒÖcych procesy zakupowe w najwiƒôkszej sieci drogeryjnej w Polsce. Stanowisko obejmuje tworzenie i rozw√≥j aplikacji analitycznych w Qlik Sense oraz projektowanie rozwiƒÖza≈Ñ bazodanowych w PostgreSQL. Wsp√≥≈ÇpracujƒÖc z analitykami biznesowymi, bƒôdziesz tworzyƒá funkcjonalne i wydajne narzƒôdzia wspierajƒÖce kluczowe decyzje biznesowe obejmujƒÖce procesy zwiƒÖzane z pozyskiwaniem produkt√≥w do katalogu, zarzƒÖdzaniem informacjƒÖ produktowƒÖ oraz politykƒÖ cenowƒÖ. Tw√≥j zakres obowiƒÖzk√≥w: kompleksowe tworzenie aplikacji w Qlik Sense, obejmujƒÖce zar√≥wno projektowanie i optymalizacjƒô skrypt√≥w, jak i budowƒô zaawansowanych wizualizacji oraz interaktywnych dashboard√≥w, bezpo≈õrednie zaanga≈ºowanie w wytwarzanie oprogramowania bazodanowego na platformie PostgreSQL. Bƒôdziesz odpowiedzialny za projektowanie struktur baz danych, w tym procedur sk≈Çadowanych i trigger√≥w, wspierajƒÖcych z≈Ço≈ºonƒÖ logikƒô biznesowƒÖ, ≈õcis≈Ça wsp√≥≈Çpraca z analitykami biznesowymi w celu zrozumienia ich potrzeb i przekszta≈Çcania ich w funkcjonalne rozwiƒÖzania w Qlik Sense, Inphinity Forms oraz PostgreSQL, dbanie o wysokƒÖ jako≈õƒá i wydajno≈õƒá tworzonych i utrzymywanych produkt√≥w. Nasze wymagania: praktyczne do≈õwiadczenie w pracy z Qlik Sense, znajomo≈õƒá jƒôzyka SQL i do≈õwiadczenie w pracy z relacyjnymi bazami danych, umiejƒôtno≈õƒá projektowania i budowania skalowalnych i wydajnych system√≥w danych, rozwiniƒôte umiejƒôtno≈õci analitycznego i logicznego my≈õlenia, dobra komunikacja i umiejƒôtno≈õƒá efektywnej pracy zespo≈Çowej, znajomo≈õƒá jƒôzyka angielskiego w stopniu umo≈ºliwiajƒÖcym swobodne czytanie i tworzenie dokumentacji technicznej. Mile widziane: do≈õwiadczenie z Inphinity Forms, znajomo≈õƒá BigQuery, PostgreSQL, do≈õwiadczenie z Qlik Sense w wersji Cloud. To oferujemy: umowƒô o pracƒô, lub kontrakt B2B, mo≈ºliwo≈õƒá pracy 100% zdalnie, mo≈ºliwo≈õƒá uczestnictwa w konferencjach, przestrze≈Ñ do eksperymentowania, wsp√≥≈Çpracƒô opartƒÖ na warto≈õciach - jeste≈õmy zespo≈Çem, kt√≥ry kieruje siƒô okre≈õlonymi warto≈õciami. Wspieramy atmosferƒô pracy opartƒÖ na szacunku, zaufaniu i wsp√≥≈Çpracy. Promujemy innowacyjno≈õƒá, kreatywno≈õƒá i odpowiedzialno≈õƒá, mo≈ºliwo≈õci rozwoju - wierzymy w nieustanne doskonalenie i chcemy pomagaƒá naszym pracownikom w rozwoju potencja≈Çu (oferujemy szkolenia wewnƒôtrzne i zewnƒôtrzne, warsztaty, szkolenia e-learningowe, programy rozwojowe, oraz mo≈ºliwo≈õƒá uczestniczenia w rekrutacjach wewnƒôtrznych), przyjaznƒÖ atmosferƒô, zar√≥wno w pracy jak i w Game Room, catering w biurze, wsparcie merytoryczne od lider√≥w technologicznych, sprawd≈∫ co jeszcze czeka na Ciebie w Rossmannie: www.kariera.rossmann.pl/benefity.",[],Database Administration,Database Administration
Full-time,Senior,Permanent or B2B,Remote,59,üëâ Senior AWS Data Engineer (Future Opening),Xebia sp. z o.o.,"üü£ You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. üü£ Your profile: 3+ years‚Äô experience with AWS (Glue, Lambda, Redshift, RDS, S3), 5+ years‚Äô experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools ‚Äì Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, working knowledge of Git, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ üü£ Nice to have: experience with Amazon EMR and Apache Hadoop, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification, üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,60,Senior Data Engineer with AWS and Snowflake,Sii,"We are looking for a Data Engineer with expertise in Snowflake, AWS, and ETL processes, who will work closely with AI scientists and data analysts to design, develop, and maintain data pipelines and systems that support clinical and operational data use cases. Design and develop data architecture incorporating Snowflake, Immuta, Collibra, and cloud security Implement and optimize ETL/ELT processes, manage raw data, automate workflows, and ensure high performance and reliability of data processing systems Manage data quality and security, monitor quality, and implement metadata management strategies Collaborate with analytics and business teams to identify user needs and deliver comprehensive solutions supporting analytics and reporting Optimize and migrate data systems through data and process conversion from existing systems to Data Vault Define and enforce coding standards, data modeling, and ETL/ELT architecture, ensure compliance with Data Mesh and other modern approaches Coach and mentor the data engineering team, conduct code reviews, participate in architectural discussions, and initiate innovative technological solutions Strong expertise in cloud technologies and the Snowflake ecosystem, particularly AWS, Data One Platform, Immuta, Collibra, and cloud security aspects Minimum 5 years of experience in Python programming for building and maintaining data pipelines Advanced knowledge of databases, including query optimization, relational schema design, and MPP using SQL Expertise in data storage technologies, including files, relational databases, MPP, NoSQL, and various data types (structured, unstructured, metrics, logs) Deep familiarity with Data Vault, Data Mesh, dimensional modeling, and metadata management Understanding of business and analytical processes, as well as integration with analytics and reporting systems Experience working in Agile environments, coaching development teams with the use of fluent English and Polish Residing in Poland required Knowledge of Pharma data formats Great Place to Work since 2015 - it‚Äôs thanks to feedback from our workers that we get this special title and constantly implement new ideas Employment stability - revenue of PLN 2.1BN, no debts, since 2006 on the market We share the profit with Workers - over PLN 60M has already been allocated for this aim since 2022 Attractive benefits package - private healthcare, benefits cafeteria platform, car discounts and more Comfortable workplace ‚Äì class A offices or remote work Dozens of fascinating projects for prestigious brands from all over the world ‚Äì you can change them thanks to Job Changer application PLN 1 000 000 per year for your ideas - with this amount, we support the passions and voluntary actions of our workers Investment in your growth ‚Äì meetups, webinars, training platform and technology blog ‚Äì you choose Fantastic atmosphere created by all Sii Power People Send your CV Talk to us about your expectations Learn more about our projects and choose the best Start your adventure with Sii! Sii is the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We already employ more than 7 500 professionals and implement projects in a variety of industries for clients from many countries around the world. The Great Place to Work title, won 10 times in a row, proves that at Sii we create a friendly work environment. In a survey, as many as 90% of our employees responded that Sii is a great place to work, and 95% of them think we have a great atmosphere here.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,61,Software Engineer (Qlik),HSBC Service Delivery,"Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity Enterprise Technology/Compliance IT Department brings together all areas of financial crime risk management at HSBC including anti-money laundering: transaction monitoring, screening, risk assessment etc. and is dedicated to implement the most effective global standards to combat financial crime. The Risk & Compliance MI IT team provides MI and automated document generation applications to the Compliance business areas including Group Risk Assurance, Fraud, Financial Crime Risk, and Financial Crime Threat Mitigation. The user base is global covering differing needs of business customers from interactive management information, self-service platforms, improving efficiency by creating and distributing documents automatically, and providing technical and functional expertise on the use of the platform. Role Overview: The purpose of this role is to support existing Qlik Sense based applications, develop new applications as directed by the business users, and improve the quality controls and efficiency/automation across a delivery. There are a number of dashboards and reporting processes in the portfolio therefore the engineer needs to be able to multitask and self-manage their workload to a degree. The role requires ensuring that both server and their own workstation environments are operational in a non-development environment and meeting stringent information security requirements. In the production environment, the role requires the individual to provide support to the level 3 team. The engineer will be able to help shape the delivery process and expected to use their experience to meet the three key themes of projects: 1) Efficiency, 2) Effectiveness, 3) Constant Improvement. What you‚Äôll do ‚¶Å Review the business requirements that have been submitted via JIRAs. ‚¶Å Ensure that if business requirements are unclear then they are clarified with the appropriate stakeholders. ‚¶Å Design and implement the changes in QlikSense, NPrinting, GeoAnalytics. ‚¶Å Follow the team‚Äôs delivery methodology (unit testing, documentation, integration testing, package/deploy, release processes). ‚¶Å Support UAT processes, update documentation. ‚¶Å Support release management in deployments and provide production support. ‚¶Å Deliver improvement activities (such as better ways for the team to work). ‚¶Å Take ownership of the dashboards and be responsible for the quality of the deliverable. What you need to have to succeed in this role Essential Experience & Skills: ‚¶Å Minimum 3-years‚Äô experience in QlikSense, from a structured IT delivery methodology background. ‚¶Å Documentation skills (every developer has to create documentation covering design and testing). ‚¶Å Mature positive attitude to interact positively with senior stakeholders. Able to work alone but escalates when needed. ‚¶Å Pro-active and takes ownership, and responsible for the outcomes. ‚¶Å Strong communications skills I English (written & spoken). ‚¶Å Track record of working through the Agile project life cycle. ‚¶Å Experience of implementing CI/CD within Visualization solutions will be added advantage. ‚¶Å Experience of data warehousing techniques and SQL is required. Big Data and Hive along with data visualization will be added advantage. What we offer ‚¶Å Competitive salary ‚¶Å Annual performance-based bonus ‚¶Å Additional bonuses for recognition awards ‚¶Å Multisport card ‚¶Å Private medical care ‚¶Å Life insurance ‚¶Å One-time reimbursement of home office set-up (up to 800 PLN). ‚¶Å Corporate parties & events ‚¶Å CSR initiatives ‚¶Å Financial support with trainings and education ‚¶Å Nursery discounts ‚¶Å Social fund ‚¶Å Flexible working hours ‚¶Å Free parking If your CV meets our criteria, you should expect the following steps in the recruitment process: ‚¶Å Online behavioural test (for external candidates) ‚¶Å Telephone screen (for external candidates) ‚¶Å Job interview with the hiring manager We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Hybrid,62,Data Engineer,Gamefound Sp. z o.o.,"Gamefound.com to najszybciej rosnƒÖca platforma crowdfundingowa dla plansz√≥wek. Obecnie poszukujemy osoby na stanowisko Data Engineer , kt√≥ra do≈ÇƒÖczy do naszego zespo≈Çu technologicznego i wesprze nas w tworzeniu oraz rozwijaniu efektywnych rozwiƒÖza≈Ñ opartych na danych. Szukamy osoby samodzielnej, technicznie sprawnej i gotowej do przejmowania pe≈Çnej odpowiedzialno≈õci za rozwijane systemy. Lokalizacja: Wroc≈Çaw / hybrydowo Na czym bƒôdzie polegaƒá Twoja praca: Tworzenie i zarzƒÖdzanie zbiorami danych oraz projektowanie przep≈Çyw√≥w danych Budowa i wdra≈ºanie system√≥w ETL oraz rozwiƒÖza≈Ñ in≈ºynierii danych Projektowanie modeli danych i automatyzacja proces√≥w za pomocƒÖ skrypt√≥w Tworzenie raport√≥w i stosowanie wzorc√≥w projektowych w pracy z danymi Prowadzenie projekt√≥w od koncepcji po wdro≈ºenie ‚Äì pe≈Çna odpowiedzialno≈õƒá za rozwiƒÖzania Kogo szukamy: Masz minimum 3 lata do≈õwiadczenia jako Data Engineer lub w podobnej roli technicznej Programujesz w Pythonie (min. 3 lata do≈õwiadczenia) Znasz siƒô na projektowaniu i zarzƒÖdzaniu przep≈Çywami danych Masz do≈õwiadczenie w budowie hurtowni danych Swobodnie poruszasz siƒô w SQL oraz znasz relacyjne i nierelacyjne bazy danych (np. SQL Server, ClickHouse) Pracowa≈Çe≈õ/a≈õ z narzƒôdziami do przetwarzania danych (Dagster, Apache Airflow lub inne) Mile widziane: Do≈õwiadczenie w pracy ze zbiorami danych z kampanii marketingowych (np. Google Ads, Meta Ads) Co oferujemy: Elastyczne godziny pracy ‚Äì mo≈ºliwo≈õƒá rozpoczƒôcia miƒôdzy 7: 00 a 10: 00 Model hybrydowy ‚Äì do 5 dni pracy zdalnej w miesiƒÖcu Pracƒô nad rozwijanym d≈Çugoterminowo produktem ‚Äì nie jeste≈õmy software house‚Äôem Nowoczesne biuro w centrum Wroc≈Çawia, wspierajƒÖce work-life balance Atrakcyjne benefity ‚Äì prywatna opieka medyczna, system kafeteryjny, ≈õwie≈ºe owoce w biurze Brak dress code‚Äôu ‚Äì pracuj w tym, w czym czujesz siƒô komfortowo Spotkania integracyjne i strefa relaksu ‚Äì gry planszowe, Xbox, przestrze≈Ñ do odpoczynku Brzmi jak co≈õ dla Ciebie? Aplikuj i do≈ÇƒÖcz do Gamefound.","[{""min"": 17000, ""max"": 23000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 19100, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,63,Senior Administrator Baz Danych Oracle / Senior Oracle DBA,Simora,"Do≈ÇƒÖcz do naszego zespo≈Çu jako Senior Administrator Baz Danych Oracle! Poszukujemy osoby, kt√≥ra zajmie siƒô zarzƒÖdzaniem bazami danych naszych klient√≥w. Jeste≈õmy firmƒÖ, kt√≥ra rozwija nowoczesne rozwiƒÖzania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzƒôdzi wspierajƒÖcych naszych klient√≥w. Cenimy pozytywnƒÖ atmosferƒô, wzajemny szacunek i zaanga≈ºowanie ‚Äì to fundamenty naszego zespo≈Çu. Tw√≥j zakres obowiƒÖzk√≥w Tworzenie baz danych oraz ≈õrodowisk bazodanowych na ≈õrodowisku produkcyjnym i testowym Migrowanie baz danych na nowe ≈õrodowiska ZarzƒÖdzania i aktualizacja baz danych i ≈õrodowisk bazodanowych Konfigurowanie i optymalizacja ≈õrodowiska bazodanowego Automatyzacja zada≈Ñ za pomocƒÖ jƒôzyk√≥w skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jako≈õci i bezpiecze≈Ñstwa dla tworzonych rozwiƒÖza≈Ñ Wdra≈ºanie rozwiƒÖza≈Ñ opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Sta≈Çe doskonalenie sposobu Twojej pracy Nasze wymagania Min 5 letnie do≈õwiadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomo≈õƒá jƒôzyka SQL i PLSQL Umiejƒôtno≈õƒá dbania o szczeg√≥≈Çy i jako≈õƒá rozwiƒÖza≈Ñ Komunikatywno≈õƒá Znajomo≈õƒá jƒôzyka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykszta≈Çcenie wy≈ºsze (preferowany kierunek: informatyka) Znajomo≈õƒá jƒôzyk√≥w programowania: Python, Java itp. Znajomo≈õƒá system√≥w operacyjnych Linux / Windows Znajomo≈õƒá system√≥w wirtualizacyjnych np.: OLVM Oferujemy CiekawƒÖ pracƒô w firmie o wysokiej dynamice rozwoju Mo≈ºliwo≈õƒá podniesienia kwalifikacji w obszarach zwiƒÖzanych z bazami danych, bezpiecze≈Ñstwem danych oraz sztucznƒÖ inteligencjƒÖ Benefity Karta Multisport Prywatna opieka zdrowotna ‚Äì Medicover Praca zdalna Brak dress code‚Äôu Dofinansowanie szkole≈Ñ i kurs√≥w Elastyczny czas pracy","[{""min"": 10000, ""max"": 16000, ""type"": ""Net per month - B2B""}]",Database Administration,Database Administration
Full-time,Mid,B2B,Remote,64,Data Engineer AWS&Snowflake,Lingaro,"We are seeking an experienced Cloud Data Engineer proficient with Snowflake and AWS. The ideal candidate will have hands-on experience in designing and implementing data pipelines, data warehousing solutions, and ETL workflows. Tasks: Designing and implementing data processing systems using distributed frameworks like Snowflake. This involves writing efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data. Building data pipelines to ingest data from various sources such as databases, APIs, or streaming platforms. Integrating and transforming data to ensure its compatibility with the target data model or format. Designing and optimizing data storage architectures, including data lakes, data warehouses, or distributed file systems. Implementing techniques like partitioning, compression, or indexing to optimize data storage and retrieval. Identifying and resolving bottlenecks, tuning queries, and implementing caching strategies to enhance data retrieval speed and overall system efficiency. Designing and implementing data models that support efficient data storage, retrieval, and analysis. Collaborating with data scientists and analysts to understand their requirements and provide them with well-structured and optimized data for analysis and modeling purposes. Collaborating with cross-functional teams including data scientists, analysts, and business stakeholders to understand their requirements and provide technical solutions. Communicating complex technical concepts to non-technical stakeholders in a clear and concise manner. Independence and responsibility for delivering a solution. Ability to work under Agile and Scrum development methodologies. What We're Looking For: 4+ years of professional experience in the Data & Analytics area. Very good level knowledge of Snowflake. Good knowledge of AWS cloud services (S3, RDS, Redshift, etc.), knowledge of other clouds is an asset. Very good level of communication, the ability to clearly and specifically convey. information, experience in working with business users. Experience working in the Agile (Scrum) methodology. English at least at B2 level, ideally C1. Capable of working both independently and in a team-oriented, collaborative, cross-functional and cross-cultural environment. Passion for the Data & Analytics solutions in the Cloud environment. Missing one or two of these qualifications? We still want to hear from you! If you bring a positive mindset, we'll provide an environment where you feel valued and empowered to learn and grow. We offer: Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,65,Big Data Developer,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: üíªHybrid work (2 days from the office) - Warszawa/Gdansk or Gdynia Responsibilities: Responsible for a successful design and implementation of a high-performing, flexible, robust, scalable and easily maintainable global reporting solution for the Bank Actively cooperating with Product Owner, Solution Architects, Analysts, Testers and Developers Working in an Agile team Requirements: 3+ years of experience with functional programming in Scala Experience with writing Spark-based applications in Scala Experience in containerized technologies including Docker and Kubernetes Deep understanding of Hadoop Technology Stack: Hive, Oozie, Kafka Strong communication skills and fluency in spoken and written English Experience within Agile ways of working Nice to have: Deep understanding of the principles of distributed systems Experience in data engineering and building ETL/ELT pipelines Experience with performance tuning of Hadoop/Spark solutions Experience in developing RESTful services and web applications with Bootstrap and ReactJS Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Hybrid,66,Senior Integration Engineer,Antal Sp. z o.o.,"Senior Integration Engineer üìç Location: Krak√≥w, Poland (Hybrid ‚Äì 2 days/week in office)üíº Employment Type: Full-time, B2B Contractüí∞ Rate: 170‚Äì200 PLN/hour About the Role We‚Äôre looking for a Senior Integration Engineer to join our dynamic team supporting service maintenance and migration projects. You‚Äôll be part of a collaborative environment with colleagues in Krak√≥w and D√ºsseldorf, working across the full lifecycle‚Äîfrom requirements to development and production. Your Responsibilities Develop and implement data integration processes Analyze technical requirements related to IT integration Design technical architectures and implement integration solutions Tech Stack & Skills Must-Have: IBM Message Broker (IIB 10 or ACE12) IBM Sterling Transformation Extender (ITX) GitHub, Jenkins Basic Unix/Linux knowledge SQL (preferably DB2) WebSphere MQ Strong English communication skills Self-organized and reliable Open-minded and proactive Nice-to-Have: Experience with CI/CD in IBM Integration Bus V10 or ACE V12 JUnit automated testing German language skills IBM InfoSphere DataStage (ETL) What We Offer Work in a cross-functional team with ownership over the full development lifecycle Opportunities for internal and external training A supportive environment focused on growth and collaboration Sprawd≈∫ inne ciekawe oferty pracy na: https: //antal.pl/dla-kandydata","[{""min"": 170, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,67,Data Architect (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Data Architect , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company. This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. üöÄ Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Hadoop, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. üéØ What you'll need to succeed in this role: 5+ years of proven commercial experience in implementing, developing, or maintaining Big Data systems. Strong programming skills in Python or Java/Scala : writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity with Big Data technologies like Spark , Cloudera, Airflow , NiFi, Docker , Kubernetes , Iceberg , Trino or Hudi. Proven expertise in implementing and deploying solutions in cloud environments (with a preference for AWS ). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master‚Äôs or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. Fluent English (C1 level) is a must. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn , Instagram ).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Mid,B2B,Remote,68,"Data Engineer (PySpark, Palantir)",Nexio Management,"Nexio Management to zaufany partner biznesowy w drodze do cyfrowej przysz≈Ço≈õci. Posiadamy prawie 20-letnie do≈õwiadczenie na rynku IT w Polsce i poza jej granicami. Prowadzimy dzia≈Çania w oparciu o transparentne i szczere relacje. Tworzymy innowacyjne rozwiƒÖzania technologiczne, kreujƒÖc przy tym interesujƒÖce i rozwojowe ≈õrodowisko pracy dla naszych ekspert√≥w. Obecnie zatrudniamy 550 konsultant√≥w, kt√≥rzy ≈õwiadczƒÖ us≈Çugi IT dla Klient√≥w na ca≈Çym ≈õwiecie. Nasza g≈Ç√≥wna siedziba mie≈õci siƒô w Warszawie, poza tym mamy biura w Rumunii oraz Wielkiej Brytanii. Posiadamy r√≥wnie≈º w≈Çasne R&D Center, kt√≥re jest miejscem powstawania innowacyjnych projekt√≥w m.in .: w obszarach test√≥w, Big Data, Cloud czy AI.W ramach naszych us≈Çug tworzymy szyte na miarƒô rozwiƒÖzania, utrzymujemy i rozwijamy nawet najbardziej wymagajƒÖce systemy IT. Dzia≈Çamy w takich modelach biznesowych jak managed services, fixed prices oraz wspieramy zespo≈Çy naszych klient√≥w w modelach scale up the team. Naszymi klientami sƒÖ firmy z wielu zr√≥≈ºnicowanych bran≈º, szukajƒÖce wsparcia najwy≈ºszej klasy ekspert√≥w Tworzenie, rozwijanie i utrzymywanie pipelines danych w sÃÅrodowisku Palantir Foundry Przetwarzanie i transformacja duzÃáych zbioroÃÅw danych z wykorzystaniem PySpark Integracja danych z roÃÅzÃánych zÃÅroÃÅde≈Ç (API, bazy danych, pliki, itp.) WspoÃÅ≈Çpraca z zespo≈Çami analitycznymi, data science oraz biznesem przy projektowaniu rozwiaÃ®zanÃÅ opartych na danych Monitorowanie, testowanie i optymalizacja wydajnosÃÅci procesoÃÅw ETL Udzia≈Ç w projektach zwiaÃ®zanych z rozwojem platformy danych i modernizacjaÃ® istniejaÃ®cych rozwiaÃ®zanÃÅ Minimum 3 lata dosÃÅwiadczenia komercyjnego na stanowisku Data Engineer Minimum 1 rok dosÃÅwiadczenia w pracy z PySpark i platformaÃ® Palantir Foundry Dobra znajomosÃÅcÃÅ SQL i przetwarzania danych UmiejeÃ®tnosÃÅcÃÅ pracy z duzÃáymi zbiorami danych i rozwiaÃ®zywania problemoÃÅw wydajnosÃÅciowych ZnajomosÃÅcÃÅ koncepcji Data Lake / Data Warehouse UmiejeÃ®tnosÃÅcÃÅ pracy zespo≈Çowej i komunikatywnosÃÅcÃÅ ZnajomosÃÅcÃÅ jeÃ®zyka angielskiego na poziomie umozÃáliwiajaÃ®cym praceÃ® z dokumentacjaÃ® technicznaÃ® i komunikacjeÃ® w zespole (min. B2/C1) R√≥≈ºnorodne formy wsp√≥≈Çpracy ‚Äì umowa o pracƒô, kontrakt B2B Stabilno≈õƒá zatrudnienia ‚Äì d≈Çugofalowe projekty, wsp√≥≈Çpracƒô z wiodƒÖcymi firmami z bran≈ºy, mo≈ºliwo≈õci zmiany projektu Benefity: Medicover (rozszerzony o stomatologiƒô) i Fitprofit UnikalnƒÖ atmosferƒô pracy w zgranych zespo≈Çach oraz udzia≈Ç w cyklicznych integracjach Ciekawe projekty, kt√≥re realizujemy dla wielu bran≈º oraz w r√≥≈ºnych technologiach Udzia≈Ç w r√≥≈ºnorodnych inicjatywach charytatywnych, sportowych, eventach (np. biegi firmowe, turnieje szachowe, dzie≈Ñ programisty itp.) Darmowe lekcje z jƒôzyka angielskiego",[],Data Engineering,Data Engineering
Full-time,Manager / C-level,Permanent or B2B,Hybrid,69,IDMC Developer,Cognizant Technology Solutions,"IDMC Developer Permanent Employment ‚Äì Poland. About the role An IDMC (Informatica Data Management Cloud) developer manages and maintains an Informatica IDMC environment, ensuring high availability, performance, and security. This role requires expertise in IDMC platform administration, data integration, and data governance processes, along with the ability to provide technical support and troubleshoot issues. The position is crucial for ensuring the smooth operation of the Informatica platform and providing strategic solutions for future growth. About You: The Informatica Developer will work closely with data analysts, business analysts, and other IT professionals to create and maintain data solutions that ensure efficient and accurate data integration and management. The ideal candidate will have hands-on experience with Informatica tools and a strong understanding of ETL processes. Your Key Result Areas: Design, develop, and maintain ETL/ELT workflows using Informatica IDMC tools Integrate data from various sources into cloud data warehouses (e.g., Snowflake, Redshift, BigQuery) Configure and manage Secure Agents for job execution and monitoring Develop and deploy data quality rules and validation processes Implement data cataloging and metadata management for improved data discovery Build and consume REST/SOAP APIs for data exchange and automation Collaborate with data architects, analysts, and business users to understand data requirements Troubleshoot and optimize data pipelines for performance and reliability Ensure compliance with data governance and security policies Document technical designs, workflows, and best practices Required & nice to have Skills: Must Haves: Informatica IDMC Tools: Proficiency in Data Integration, Cloud Data Quality, and Cloud Application Integration ETL/ELT Development: Experience designing and building data pipelines and transformation logic Cloud Platforms: Familiarity Azure for cloud-native data workflows Secure Agent Configuration: Knowledge of deploying and managing Secure Agents Data Cataloging: Experience with metadata ingestion, data lineage, and classification API Integration: Working with REST/SOAP APIs for data exchange and automation Scripting Languages: Basic scripting in Python, Shell, or PowerShell for automation tasks SQL and Data Modeling: Strong SQL skills and understanding of relational and non-relational databases Version Control: Proficiency with Git for source code and workflow management Debugging and Optimization: Ability to troubleshoot data jobs and optimize performance Nice to Have/ Your chance to grow Strong knowledge of SQL and database management systems. Experience with data warehousing and data modeling. Proven problem-solving and analytical skills. Excellent communication and teamwork skills. The Cognizant community: We are a high caliber team who appreciate and support one another. Our people uphold an energetic, collaborative and inclusive workplace where everyone can thrive. Cognizant is a global community with more than 300,000 associates around the world. We don‚Äôt just dream of a better way ‚Äì we make it happen. We take care of our people, clients, company, communities and climate by doing what‚Äôs right. We foster an innovative environment where you can build the career path that‚Äôs right for you. About us: Cognizant is one of the world's leading professional services companies, transforming clients' business, operating, and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build, and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant (a member of the NASDAQ-100 and one of Forbes World‚Äôs Best Employers 2024) is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com Cognizant is an equal opportunity employer. Your application and candidacy will not be considered based on race, color, sex, religion, creed, sexual orientation, gender identity, national origin, disability, genetic information, pregnancy, veteran status or any other characteristic protected by federal, state or local laws. Disclaimer: Compensation information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law. Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.",[],Unclassified,Unclassified
Full-time,Mid,B2B,Remote,70,Data Engineer,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Poszukujemy do≈õwiadczonego Data Engineer do wsp√≥≈Çpracy przy tworzeniu i rozwijaniu rozwiƒÖza≈Ñ przetwarzania danych dla naszych klient√≥w. Twoim zadaniem bƒôdzie projektowanie i wdra≈ºanie skalowalnych system√≥w przetwarzania danych, a tak≈ºe wsp√≥≈Çpraca z zespo≈Çami Data Science, Analityki i IT. Twoje zadania Projektowanie, rozw√≥j i utrzymanie hurtowni danych, pipelines ETL/ELT i system√≥w przetwarzania du≈ºych zbior√≥w danych. Optymalizacja wydajno≈õci proces√≥w przetwarzania danych. Tworzenie i zarzƒÖdzanie infrastrukturƒÖ w chmurze (AWS, GCP, Azure) lub on-premise. Wsp√≥≈Çpraca z zespo≈Çami ds. analizy danych i biznesu w celu identyfikacji potrzeb i wymaga≈Ñ. Implementacja rozwiƒÖza≈Ñ monitorujƒÖcych jako≈õƒá danych i ich bezpiecze≈Ñstwo. Wymagania Min. 3 lata do≈õwiadczenia na stanowisku Data Engineer. Znajomo≈õƒá przynajmniej jednego narzƒôdzia chmurowego (AWS, GCP, Azure). Do≈õwiadczenie z systemami big data, np. Apache Spark, Kafka, Hadoop. Umiejƒôtno≈õƒá pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL) i bazami NoSQL (np. MongoDB, Cassandra). Praktyczna znajomo≈õƒá narzƒôdzi ETL/ELT (np. Airflow, dbt). Nice-to-have : Certyfikaty z zakresu technologii chmurowych. Do≈õwiadczenie z modelowaniem danych i optymalizacjƒÖ zapyta≈Ñ SQL. Znajomo≈õƒá narzƒôdzi monitorujƒÖcych (Prometheus, Grafana). Zrozumienie zasad CI/CD oraz praktyczne do≈õwiadczenie z narzƒôdziami DevOps. Oferujemy Mo≈ºliwo≈õƒá bycia czƒô≈õciƒÖ miƒôdzynarodowych projekt√≥w. Ambitne i rozwojowe projekty. Wsparcie merytoryczne na ka≈ºdym etapie wdro≈ºenia. Dostƒôp do najnowszych technologii. Praca w systemie zdalnym lub hybrydowym, w zale≈ºno≈õci od projektu. Nasza oferta Mo≈ºliwo≈õƒá bycia czƒô≈õciƒÖ miƒôdzynarodowych projekt√≥w. Ambitne i rozwojowe projekty. Wsparcie merytoryczne na ka≈ºdym etapie wdro≈ºenia. Dostƒôp do najnowszych technologii. Praca w systemie zdalnym lub hybrydowym, w zale≈ºno≈õci od projektu. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. We connect you with the right people","[{""min"": 18000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,71,Staff Software Engineer,dotLinkers,"Salary: up to 37 500 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Staff Software Engineer focused on Structured Data Storage, you will lead the design and implementation of scalable, resilient cloud-native storage solutions using technologies like CosmosDB, Azure Parquet, ADLS, CockroachDB, TiDB, and Snowflake. Your work will enhance platform scalability, performance, and reliability, helping teams deliver greater value with less operational overhead.You‚Äôll join the Infrastructure Services group, supporting core platform areas like compute, networking, and storage. The Structured Storage team provides reliable and cost-effective storage building blocks for product teams. Acting as a technical leader and mentor, you‚Äôll drive architectural decisions and ensure collaboration across engineering teams to modernize and simplify data infrastructure. Job Responsibilities Lead the full software lifecycle and strategic implementation of modern cloud-native structured data storage technologies (such as CosmosDB, ADLS, Parquet, Snowflake, CockroachDB), aiming to enhance scalability, performance, and resilience across the data platform. Take hands-on responsibility for architecting and developing core structured storage services, ensuring high standards of reliability, security, and operational efficiency. Promote modern data design principles, storage strategies, and governance practices via design reviews, workshops, documentation, and knowledge-sharing initiatives. Serve as a trusted advisor to senior technical and product leadership (Directors, VPs, CTO), providing guidance on architectural decisions, technology trade-offs, and long-term platform direction. Act as technical owner and mentor for structured storage architecture, supporting multiple engineering teams and fostering technical excellence and collaboration. Evaluate new data storage technologies and drive their adoption where they align with business objectives and architectural evolution. Maintain and prioritize a roadmap for improvements and innovations in storage infrastructure, ensuring timely delivery through leadership and coordination. Detect and address scalability challenges, reliability risks, and performance limitations, driving cross-team initiatives to resolve them. Stay informed on trends in distributed databases, cloud storage architectures, and serverless data patterns to guide ongoing strategy development. Minimum Qualifications: 7+ years of experience in backend engineering, distributed systems, or data platform development. At least 4 years of practical experience designing and implementing cloud-native storage solutions on Azure, AWS, or GCP. Proven expertise in at least one large-scale data storage technology (e.g., CosmosDB, Snowflake, ADLS, Parquet). Strong understanding of data modeling, consistency models, data partitioning, and query optimization within distributed environments. Proficiency in C#, Java, Python, or similar programming languages. Experience developing production-grade APIs and integrating data solutions into enterprise systems. Preferred Qualifications: Familiarity with various structured storage models (relational, columnar, key-value, time-series) and associated technologies (e.g., Cassandra, MongoDB, Redis). Experience with infrastructure-as-code, container orchestration (Kubernetes), and serverless architectures for data management. Background in leading architectural and strategic platform initiatives at scale. Experience working within regulated industries (e.g., healthcare, finance, legal tech). Advanced degree in Computer Science or related discipline. Leadership Expectations: Define and drive technical strategy for structured data storage systems, ensuring adoption across engineering teams. Mentor senior and mid-level engineers, fostering high technical standards and continuous learning. Lead cross-functional initiatives requiring coordination between architecture, security, platform, and product teams. Represent structured storage topics in technical planning sessions, design reviews, and executive discussions. Cultivate a culture of ownership, innovation, and technical curiosity within the broader engineering community. Competencies and Skills: Technical Mastery: Expertise in cloud-native databases, storage design, and distributed systems scalability. System-Level Thinking: Ability to design holistic, optimized systems that balance performance, cost, and maintainability. Leadership & Influence: Comfortable guiding diverse engineering teams and aligning them to technical vision. Communication: Strong skills in articulating architectural strategies and technical recommendations to both engineers and leaders. Problem Solving: Skilled at analyzing and resolving complex scalability and performance challenges in cloud environments. Collaboration: Track record of building partnerships across platform, infrastructure, and application teams. Adaptability: Comfortable working in dynamic environments with shifting priorities and emerging technologies. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 32500, ""max"": 37500, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Remote,72,ETL Developer (Integration Specialist),KRUK S.A,"We are leader in debt collection industry, operating in 8 countries in Europe. We like passion, constant improvement and team work. You will be working in Headquarter of our Group which means multicultural environment and support for all our European entities. Right now we are implementing in entire Capital Group data governance practices and processes. Being responsible for implementation and maintenance of a metadata schemas and repositories to catalog and managing data assets, facilitating easy discovery and understanding by data users, you will have strong impact on elevating company wide data maturity. Your range of responsibilities: Develop standards and best practices for ETL processes Design, implement and maintain ETL processes, ensuring they are scalable, reliable, and adhere to data governance standards Monitor and optimize ETL processes for performance and efficiency, making improvements as necessary Troubleshoot and resolve issues related to data integration, ensuring data quality and integrity throughout the process Document ETL processes, data mappings, and transformations for future reference and training purposes Stay up-to-date with industry trends and advancements in data integration technologies and best practices Our expectations: Previous experience as an integration specialist or in a similar role. Practical knowledge of relational database concepts, logical&physical data models designing in DDD approach and data contracts concept. Practical knowledge of ETL/ELT tools in MS Azure environment (first of all ADF for Ingest, Databricks for Processing, Airflow for Orchestration, especially). Good communication & interpersonal skills to work with both business and IT. Ability to quickly learn new technologies and stay updated with industry trends in integration and data governance . Nice to have: Experience in agile projects What we offer: Employment based on an employment contract or B2B Flexible working hours Option for remote or hybrid work, so you can work wherever you feel most comfortable Attractive benefits package for employees: private medical care: Luxmed, participation in Multisport card cost, additional benefits adapted to your needs within the cafeteria system, employee insurance on favourable terms Possibility to use a company car for private purposes. We are dedicated to creating an inclusive recruitment process that upholds the principles of equal opportunity. Our focus is on candidates' competencies and their willingness to grow, regardless of gender, age, disability, religion, sexual orientation, background, or any other factors unrelated to their qualifications. We go the extra mile to ensure the recruitment process is accessible and thoughtfully tailored to accommodate individual needs.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,73,Data Engineer_L4 with Databricks,Experis Manpower Group,"Tasks: Design and implement data processing solutions using Databricks for large-scale and diverse datasets Design, build, and enhance data pipelines with Python and cloud- native tools Work closely with solution architects to define and uphold best practices in data engineering Ensure data consistency, security, and scalability within cloud-based environments Requirements: Experience in data engineering , with hands-on Databricks expertise Strong experience in Python for automation and data transformation Experience working with at least one major cloud platform (AWS, Azure, or GCP) Strong communication skills and strong English language skills Nice to have: Solid understanding of SQL with experience in query optimization and data modeling Familiarity with DevOps methodologies, CI/CD pipelines, and Infrastructure as Code (Terraform, Bicep) Experience with real-time data streaming technologies such as Kafka or Spark Streaming Knowledge of cloud storage solutions like Data Lake, Snowflake, or Synapse Hands-on experience with PySpark for distributed data processing Relevant certifications such as Databricks Certified Data Engineer Associate or cloud-based data certifications Our offer: Employment based on B2B contract via Experis for a period of 12 months Compensation: 150-175 PLN per hour 100% remote work Multisport card Private healthcare system Life insurance","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,74,Senior Data Engineer,1dea,"Warunki zaanga≈ºowania: Profil firmy: Bran≈ºa rozrywkowa. Globalne ≈õrodowisko medi√≥w i technologii, firma posiadajƒÖca ikoniczne marki i produkty, osiƒÖgajƒÖca miliony u≈ºytkownik√≥w na ca≈Çym ≈õwiecie. Typ umowy: Umowa o pracƒô + AKUP, benefity Tryb pracy: bezpo≈õrednie, d≈Çugoterminowe zatrudnienie w firmie o sta≈Çej pozycji, bez po≈õrednik√≥w Wymagania: Wy≈ºsze techniczne wykszta≈Çcenie Minimum 4 lat do≈õwiadczenia jako Data Engineer z Pythonem w roli g≈Ç√≥wnej Znajomo≈õƒá projektowania architektury danych (relacyjne i nierelacyjne bazy danych, hurtownie danych, rozproszone systemy) Do≈õwiadczenie z narzƒôdziami typu Apache Spark, Beam (czyszczenie i przetwarzanie danych) Praktyka w budowaniu i utrzymywaniu pipeline‚Äô√≥w danych (Kafka, Airflow lub inne) Znajomo≈õƒá chmury (AWS, GCP, Azure), w tym S3, Redshift, BigQuery, Dataflow Do≈õwiadczenie z technologiami big data (Hadoop, Spark, Hive) Znajomo≈õƒá zasad bezpiecze≈Ñstwa i prywatno≈õci danych Zdolno≈õƒá komunikacji z osobami technicznymi i nietechnicznymi Znajomo≈õƒá angielskiego na poziomie min. B2 Nice to have: Do≈õwiadczenie w projektach marketingowych lub z obszaru TV/ reklam / system√≥w rekomendacji Jakie warunki pracy zapewniamy: Wp≈Çyw na decyzje techniczne projektu Sprzƒôt do pracy - MacBook Pro Zaanga≈ºowanie na ka≈ºdym etapie SDLC Ka≈ºdy projekt jest na zasadach Agile Sta≈Çe podƒÖ≈ºanie za nowymi technologiami Je≈õli jeste≈õ pasjonatem nowoczesnych technologii i chcesz do≈ÇƒÖczyƒá do zespo≈Çu, kt√≥ry kszta≈Çtuje przysz≈Ço≈õƒá rozrywki, czekamy na Twoje zg≈Çoszenie!",[],Data Engineering,Data Engineering
Full-time,Junior,Internship,Remote,75,System Integration (Mulesoft) Internship Program (He/She/They),Accenture,"Accenture Salesforce Business Group is seeking innovative and experienced professionals to join our team in the integration of Mulesoft technologies. Accenture is one of the largest Salesforce and Mulesoft partners worldwide, and together, we help shape the future of leading global enterprises. Join us and work with some of the biggest names in business, providing services used by millions of people around the world. We value work-life balance and flexibility. We offer various training programs and certification paths to help you grow in your career. You will work closely with industry experts, continuously learning and improving as you contribute to groundbreaking projects. During your onboarding, you‚Äôll receive hands-on guidance from a senior team member to further enhance your technical skills. For more information about Accenture Salesforce Business Group, visit our website. INTERNSHIP PROGRAM: Assist in the integration and development of systems using Mulesoft technology, enabling seamless data and application flows between multiple platforms. Work with teams to implement innovative integration solutions for industries such as finance, manufacturing, retail, telecommunications, and energy. Assist in the design, development, and implementation of integration solutions, including the management of API lifecycles and supporting data flow processes. Provide ongoing support for system integrations, troubleshoot issues, and help ensure the reliability of integration systems in diverse client environments. Work with programming languages like Java, Python, and SQL to implement system integrations, data processes, and automation tasks. WHAT‚ÄôS IN IT FOR YOU? A paid internship program lasting a minimum of 3 months, with flexible working hours and the possibility of remote work. Opportunities for long-term collaboration, with the potential for a full-time offer upon completion of the internship. Involvement in ambitious projects with prestigious clients. Growth opportunities supported by a feedback-driven culture, mentorship, and guidance from a dedicated Buddy in your new role. A clear path for professional development from day one, with continuous support and a focus on your growth. Qualifications HERE'S WHAT YOU'LL NEED: Proficiency in both English and Polish (written and spoken) with strong communication skills. Availability to work a minimum of 30 hours per week. Basic knowledge of at least one programming language (e.g., Java, Python, SQL, C#). Experience with Mulesoft or similar integration platforms. Knowledge of data formats such as XML, JSON, and CSV. Basic knowledge of data structures and their types (flat, relational, object-oriented). A passion for integration technologies and a desire to learn and grow in the field. BONUS POINTS IF YOU HAVE: Experience in programming web services (SOAP, REST). Experience in programming for data processing (processing data from files, database records). WHAT WE BELIEVE: Accenture does not discriminate based on race, religion, color, sex, age, disability, national origin, political beliefs, trade union membership, ethnicity, denomination, sexual orientation, or any other factor impermissible under Polish law. Our leadership is committed to creating positive, long-lasting change for future generations. Inclusion and diversity are core to our values. We believe that our rich diversity makes us more innovative and creative, enabling us to better serve our clients and communities. Our role as a partner to leading global businesses, organizations, and governments gives us both the opportunity and the responsibility to make a difference. Sustainability is one of our core responsibilities, and we embed it into everything we do. By clicking apply, you consent to Accenture Sp. z o.o. or any other entity of the Accenture group processing your personal data for recruitment purposes, as the data controller under GDPR. More information about Accenture‚Äôs privacy policy can be found here: Accenture Privacy Policy . This version aligns closely with the tone and structure of the original, reflecting Accenture's professional and innovative culture. Let me know if you'd like further adjustments!",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,76,Data Engineer with Blockchain,DCG,"Responsibilities: Design, implement, and maintain scalable data pipelines using Azure Databricks, Spark, and PySpark Work with Delta Lake to manage large-scale data storage and optimize performance Develop robust data integration solutions using Azure Data Factory and Azure Functions Build and maintain structured and semi-structured data models, leveraging formats such as Parquet, Avro, and JSON Ensure efficient and secure data processing through proper performance tuning and code optimization Collaborate with development and analytics teams to support business data needs Apply version control best practices using Git and follow coding standards in Python and SQL Requirements: Strong hands-on experience with Azure Databricks, Spark, and PySpark Proficiency in building and tuning data pipelines with Delta Lake Solid understanding of data modeling and performance optimization techniques Practical experience with Azure Data Factory, Azure Functions, and Git Competence in working with data formats such as Parquet, Avro, and JSON Strong programming skills in Python and SQL Ability to work effectively in a fast-paced, enterprise-level environment Strong communication skills and fluency in spoken and written English (C1) Nice to have: Understanding of blockchain-related concepts and data structures Offer: Private medical care Co-financing for the sports card Training & learning opportunities Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,77,Senior Data Engineer ‚Äì Data Privacy & Protection,Upvanta sp. z o.o.,"Wykrywanie danych wra≈ºliwych (PII/NPI) przy u≈ºyciu narzƒôdzi Informatica DPM i OneTrust AI & Data Discovery Tworzenie regu≈Ç metadanych oraz regu≈Ç danych z u≈ºyciem wyra≈ºe≈Ñ regularnych Manualna analiza i walidacja wynik√≥w skanowania Tworzenie i zarzƒÖdzanie pakietami maskowania danych w Informatica TDM Automatyzacja proces√≥w maskowania i harmonogramowanie zada≈Ñ (BMC Control-M) Rozw√≥j rozwiƒÖza≈Ñ obszaru danych z wykorzystaniem baz danych Oracle, SQL Udzia≈Ç w projektach zgodnych z globalnymi przepisami (np. GDPR, CCPA ) Mentoring dla m≈Çodszych cz≈Çonk√≥w zespo≈Çu Minimum 5 lat do≈õwiadczenia jako Data Engineer / Developer w obszarze prywatno≈õci danych Do≈õwiadczenie z narzƒôdziami: Informatica DPM, TDM, PowerCenter OneTrust AI & Data Discovery Oracle / SQL (on-prem i cloud) BMC Control-M Znajomo≈õƒá zasad data governance , jako≈õci danych i standard√≥w zgodno≈õci Znajomo≈õƒá zagadnie≈Ñ z zakresu maskowania danych , ETL , data discovery Umiejƒôtno≈õƒá samodzielnej pracy i zarzƒÖdzania zadaniami Bardzo dobre umiejƒôtno≈õci komunikacyjne (w tym wsp√≥≈Çpraca z interesariuszami) Znajomo≈õƒá przepis√≥w o ochronie danych osobowych: GDPR, CCPA Znajomo≈õƒá jƒôzyk√≥w skryptowych: Python (mile widziany r√≥wnie≈º R) Certyfikaty IAPP CIPP/E lub CIPP/US bƒôdƒÖ du≈ºym atutem Do≈õwiadczenie w integracji OneTrust z narzƒôdziami Informatica Do≈õwiadczenie z ServiceNow oraz narzƒôdziami kolaboracyjnymi Automatyzacja deployment√≥w Znajomo≈õƒá machine learningu do automatyzacji wykrywania danych (Python) Udzia≈Ç w projektach o wysokim znaczeniu dla bezpiecze≈Ñstwa i zgodno≈õci danych Pracƒô w miƒôdzynarodowym zespole ekspert√≥w 100% zdalnƒÖ wsp√≥≈Çpracƒô",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,78,Cloudera Full Stack Engineer,ITDS,"Join us, and transform how data drives global finance! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As a Cloudera Full Stack Engineer, you will be working for our client, a global financial services leader undergoing a large-scale data transformation program. You are joining a cross-functional Agile team that is designing and building enterprise-grade Big Data platforms using Cloudera technologies. You are contributing to the full development lifecycle‚Äîfrom system setup and configuration to deployment, monitoring, and continuous optimization‚Äîwhile ensuring 24/7 environment availability. You are also helping the client evolve its data ecosystem by integrating scalable storage solutions, enabling faster processing and analytics at a massive scale. Developing and testing Big Data components in a Cloudera-based environment Creating and configuring new Hadoop users and managing Kerberos authentication Setting up and maintaining Spark, HDFS, and MapReduce access Monitoring and tuning performance of Apache Spark jobs Executing end-to-end Cloudera cluster installation and capacity planning Automating deployments using Jenkins and Ansible Managing platform security using Apache Ranger, Knox, and Kerberos Troubleshooting and resolving system-level issues and incidents Collaborating with data engineers to optimize processing pipelines Incorporating centralized S3-based storage across the processing stack 5+ years of experience with Big Data solutions in on-prem or cloud environments Hands-on expertise with Cloudera tools including Hive, Spark, HDFS, and Kafka Proficiency in Linux system administration and shell scripting Strong understanding of Hadoop platform security and encryption practices Experience implementing CI/CD pipelines using Jenkins and Ansible Ability to troubleshoot performance and infrastructure issues Knowledge of Agile and DevOps principles Strong problem-solving skills and independent thinking Excellent communication and collaboration skills Familiarity with capacity planning and operational support in data platforms Exposure to centralized S3 data storage systems like VAST Experience optimizing Spark workloads for large-scale processing Background in working with Apache Ranger and Knox for advanced security Familiarity with Agile and Kanban project methodologies We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #7403 üìå You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here.","[{""min"": 1200, ""max"": 1500, ""type"": ""Net per day - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Hybrid,79,Oracle Database Engineer,ASTEK Polska,"ASTEK to miƒôdzynarodowa firma o francuskich korzeniach, kt√≥ra istnieje od ponad 30 lat. Astek Group jest globalnym graczem w obszarze konsultingu us≈Çug IT obecnym na 5 kontynentach. W Polsce na naszym pok≈Çadzie jest obecnie 1700 konsultant√≥w o profilach g≈Ç√≥wnie IT i non-IT. To co nas wyr√≥≈ºnia to du≈ºa ilo≈õƒá projekt√≥w informatycznych z wielu bran≈º takich jak: bankowo≈õƒá, telekomunikacja, ubezpieczenia, pharma, finance, hi-tech. Aktualnie poszukujemy osoby na stanowisko Senior Database Engineer Lokalizacja: Praca hybrydowa 2x w tygodniu z biura w Warszawie Stawka: B2B: do 1220 z≈Ç netto + VAT/dzie≈Ñ UOP: do 18 000z≈Ç brutto/msc Twoje codzienne zadania: Przygotowanie ≈õrodowiska prePROD do uruchomienia FSFO, Synchronizacja DataGuard w przypadku b≈Çƒôd√≥w/problem√≥w, Odtwarzanie Standby DB w razie potrzeby, Tworzenie us≈Çug bazodanowych (DB Service Creation), Walidacja us≈Çug na poziomie PDB, Tworzenie nowych baz danych, Migracja starych PDB do nowego kontenera, Aktywacja FSFO, RozwiƒÖzywanie problem√≥w z FSFO, Wykonywanie test√≥w w prePROD: Switchover i Failover, Patchowanie baz danych, Wprowadzanie zmian zwiƒÖzanych z OS (procesy Unix, zarzƒÖdzanie obciƒÖ≈ºeniem, demony) wed≈Çug potrzeb, Obs≈Çuga problem√≥w RAC z DR, Konfiguracja OGG Microservices i OEM, Wdra≈ºanie zmian na ≈õrodowisku produkcyjnym. Czego oczekujemy od Ciebie: Minimum 10 lat do≈õwiadczenia w zarzƒÖdzaniu bazami Oracle w wersjach od 12c do 19c (RAC, Data Guard), Do≈õwiadczenie w pracy z systemami operacyjnymi Linux , Znajomo≈õƒá narzƒôdzi Oracle (Data Guard, RMAN, Data Pump), Znajomo≈õƒá zasad projektowania architektury , Umiejƒôtno≈õƒá samodzielnego rozwiƒÖzywania problem√≥w, zaanga≈ºowanie i dok≈Çadno≈õƒá w pracy, Tworzenie i zarzƒÖdzanie us≈Çugami PDB, uruchamianie/zatrzymywanie us≈Çug, Do≈õwiadczenie w rozwiƒÖzywaniu problem√≥w z Data Guard, Do≈õwiadczenie w ≈õrodowisku produkcyjnym Oracle RAC (ASM) w wersji 19c, Samodzielno≈õƒá, dba≈Ço≈õƒá o szczeg√≥≈Çy, Dobra znajomo≈õƒá Oracle GoldenGate (procesy, bazy downstream, uruchamianie/zatrzymywanie OGG) , Dobra znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z pamiƒôciƒÖ masowƒÖ, Znajomo≈õƒá rozwiƒÖzywania problem√≥w FSFO i konfiguracji observer√≥w, Znajomo≈õƒá proces√≥w (zarzƒÖdzanie zmianƒÖ, zarzƒÖdzanie incydentami), Do≈õwiadczenie w obs≈Çudze zg≈Çosze≈Ñ SR do Oracle, Znajomo≈õƒá architektury OEM i zarzƒÖdzania metrykami OEM . Co Ci oferujemy? Atrakcyjny i przejrzysty system p≈Çatno≈õci Stabilne zatrudnienie w oparciu o umowƒô o pracƒô Szkolenia Mo≈ºliwo≈õƒá rozwoju zawodowego w miƒôdzynarodowym ≈õrodowisku - naszymi klientami sƒÖ znane polskie i ≈õwiatowe marki Wymiana wiedzy i do≈õwiadcze≈Ñ z innymi wsp√≥≈Çpracownikami Budowanie sieci zawodowych i nawiƒÖzywanie kontakt√≥w Tworzenie Centr√≥w Kompetencyjnych do gromadzenia wiedzy Organizacja wydarze≈Ñ wewnƒôtrznych i zewnƒôtrznych dla doskonalenia umiejƒôtno≈õci Dostƒôp do nowych mo≈ºliwo≈õci rozwoju zawodowego Wsparcie ze strony Manager√≥w Centr√≥w Kompetencyjnych i Zasob√≥w Uczestnictwo w presti≈ºowych szkoleniach prowadzonych przez Ekspert√≥w Analiza trend√≥w technologicznych i tworzenie podgrup kompetencyjnych Do≈ÇƒÖczajƒÖc do Nas stajesz siƒô czƒô≈õciƒÖ Centrum Kompetencyjnego po≈õwiƒôconego Twojej ekspertyzie! üôã To nie o Tobie? Poleƒá nam znajomego i zgarnij bonus do 7000 z≈Ç! Szczeg√≥≈Çy: astek.pl/system-rekomendacji/","[{""min"": 20000, ""max"": 22400, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,80,MLOps / Data Engineer ‚Äì Databricks & AI Pipelines,Craftware,"Key Tasks & Responsibilities Design & Develop: Build scalable data platforms on Azure Cloud using Azure Data Factory, Databricks, ADLS, and other Azure services. Operationalize AI Solutions: Support deployment and maintenance of Data Science solutions; manage the full lifecycle of AI products with a focus on industrialization and operations. Workflow Orchestration: Orchestrate data workflows and machine learning pipelines. Monitoring & Resilience: Implement monitoring and backup strategies to ensure solution robustness; respond to failures and prevent recurrence (e.g., Azure Monitor, Log Analytics). Data Management: Experience integrating data from multiple sources and managing access with Unity Catalog; familiarity with Key Vault and Purview is a plus. CI/CD & Code Management: Build and maintain production-grade pipelines in Spark/PySpark; ensure version control via Git and apply best practices for CI/CD automation. Experimentation & Reproducibility: Improve reproducibility and scalability of ML workflows; build frameworks for experimentation efficiency. Optimization: Optimize ML solutions for performance and cost efficiency in cloud environments. Collaboration: Work closely with data scientists and analysts to deliver efficient data and AI solutions. Solution Architecture: Actively contribute to architectural discussions by proposing improvements and identifying technical gaps. Continuous Improvement: Stay current with Azure and Databricks technologies to continuously improve workflows and automation. Qualifications & Competencies Education: Bachelor‚Äôs degree in Computer Science, Mathematics, Statistics, Engineering or related fields. Master‚Äôs is a plus. Experience: 3‚Äì4+ years of hands-on experience working with cloud-based data platforms and advanced analytics/AI products. Programming Skills: Proficiency in Python Big Data & ML Pipelines: Strong experience with Spark & PySpark Experience building ETL/ELT and ML pipelines Azure Cloud Expertise: Experience with Azure Data Factory, ADLS, SQL services Familiarity with Azure Apps, Containers, Storage Experience with security and governance (Purview, Unity Catalog, Key Vault) DevOps & CI/CD: Hands-on experience with CI/CD pipelines, Git-based workflows, and deployment automation Experience with Docker Tools like SonarQube, flake8 (nice to have) Soft Skills: Strong problem-solving and communication skills Proven ability to collaborate across technical teams Nice to Have Experience with generative AI, forecasting, optimization, or simulation Knowledge of SQL/NoSQL, data modeling, and warehousing principles We offer: B2B contract 100% remote work Wide range of projects (internal and international) Dedicated certification budget Annual evaluation meetings to define an individual development path Benefits package Integration trips","[{""min"": 150, ""max"": 210, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Hybrid,81,Data Engineer,Antal Sp. z o.o.,"Join Us as a Data Engineer Location: Krak√≥w, Poland (Hybrid ‚Äì 5 days/month in office) Industry: FinTech / Data / Cloud / AI Are you passionate about working with impactful data in a modern tech stack? Want to help build solutions that directly protect millions of people and institutions from financial crime? This is your opportunity. We're part of Risk & Compliance Technology , a global team designing and delivering innovative solutions to protect the bank and its customers from financial crime, sanctions risk, identity threats, unauthorized trading, and regulatory breaches. Our mission: empower global growth through intelligent, data-driven risk management. Right now, we‚Äôre building a secure, scalable platform powered by Generative AI to automatically generate concise, accurate insights‚Äîhelping investigators assess risk faster, spot suspicious patterns, and act quickly. You‚Äôll be at the heart of this transformation. Who We‚Äôre Looking For We‚Äôre seeking a Data Engineer to join our FinCrime IT team , working across regions and disciplines. You‚Äôll collaborate with developers, architects, and business stakeholders to shape a solution that makes a difference. This is a hands-on role in a forward-thinking, agile environment. We value curiosity, ownership, and a problem-solving mindset. What You‚Äôll Do Design, build, and maintain components of a cutting-edge investigation support platform Design, build, and maintain components of a cutting-edge investigation support platform Take part in the entire development lifecycle: design, implementation, testing, and deployment Take part in the entire development lifecycle: design, implementation, testing, and deployment Work closely with cross-functional and international teams‚Äîincluding architecture, security, and compliance Work closely with cross-functional and international teams‚Äîincluding architecture, security, and compliance Build scalable, cloud-native processes that empower intelligent decision-making Build scalable, cloud-native processes that empower intelligent decision-making What You Bring Experience: Proven experience with data analysis and data engineering (Python, SQL) Proven experience with data analysis and data engineering (Python, SQL) End-to-end development lifecycle experience in large-scale IT projects End-to-end development lifecycle experience in large-scale IT projects Background in financial services or compliance-related technology (preferred) Background in financial services or compliance-related technology (preferred) Experience building and deploying solutions in a cloud environment (ideally Google Cloud) Experience building and deploying solutions in a cloud environment (ideally Google Cloud) Technical Skills: Languages & Tools : Python, SQL, Bash Languages & Tools : Python, SQL, Bash Cloud : Google Cloud Platform (BigQuery, Dataproc, Airflow) Cloud : Google Cloud Platform (BigQuery, Dataproc, Airflow) Containers & Infra : Docker, Kubernetes, GKE, Cloud Run Containers & Infra : Docker, Kubernetes, GKE, Cloud Run Security : IAM, roles, service accounts, secure development practices Security : IAM, roles, service accounts, secure development practices DevOps : Terraform, Jenkins, Ansible, Nexus DevOps : Terraform, Jenkins, Ansible, Nexus OS : Linux / Unix OS : Linux / Unix Agile methodology knowledge (Scrum, Jira, Confluence) Agile methodology knowledge (Scrum, Jira, Confluence) Soft Skills: Strong communication skills, with the ability to explain complex ideas clearly Strong communication skills, with the ability to explain complex ideas clearly Collaborative spirit and openness to working across time zones Collaborative spirit and openness to working across time zones Eagerness to learn and adapt in a fast-changing environment Eagerness to learn and adapt in a fast-changing environment Accountability and drive for quality Accountability and drive for quality Why Join Us? Make a tangible impact by fighting financial crime at a global scale Make a tangible impact by fighting financial crime at a global scale Work with modern tech: AI, GCP, containers, and automation tools Work with modern tech: AI, GCP, containers, and automation tools Collaborate with a global team of experienced professionals Collaborate with a global team of experienced professionals Be part of a purpose-driven organization that invests in innovation and ethical tech Be part of a purpose-driven organization that invests in innovation and ethical tech Ready to Apply? Help us protect the financial world through smarter technology. Apply now and be part of something meaningful. To learn more about Antal, please visit www.antal.pl","[{""min"": 170, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,82,Data Scientist,1dea,"Profil firmy: Dla globalnej firmy z bran≈ºy rozrywki i technologii poszukujemy do≈õwiadczonych in≈ºynier√≥w. Firma zarzƒÖdza rozpoznawalnymi na ca≈Çym ≈õwiecie markami, kt√≥re docierajƒÖ do miliard√≥w u≈ºytkownik√≥w. Opis stanowiska: Poszukujemy Senior Data Scientist, kt√≥ry do≈ÇƒÖczy do zespo≈Çu rozwijajƒÖcego zaawansowane rozwiƒÖzania analityczne i technologiczne dla globalnych platform cyfrowych. Osoba na tym stanowisku bƒôdzie odpowiedzialna za projektowanie i implementacjƒô nowoczesnych modeli danych oraz algorytm√≥w wspierajƒÖcych optymalizacjƒô proces√≥w i analizy danych w z≈Ço≈ºonym ≈õrodowisku miƒôdzynarodowym. ObowiƒÖzki: Rozw√≥j i implementacja zaawansowanych modeli danych optymalizujƒÖcych i atrybucjƒô. Tworzenie modeli predykcyjnych, prognozujƒÖcych zachowania u≈ºytkownik√≥w i wyniki kampanii z wykorzystaniem technik machine learning. Analiza du≈ºych zbior√≥w danych w celu identyfikacji trend√≥w i wniosk√≥w dotyczƒÖcych efektywno≈õci kampanii. Projektowanie eksperyment√≥w w celu poprawy skuteczno≈õci modeli i ich ROI. Wsp√≥≈Çpraca z miƒôdzynarodowymi zespo≈Çami oraz wdra≈ºanie nowych technologii do proces√≥w. Prowadzenie test√≥w A/B na du≈ºƒÖ skalƒô w celu weryfikacji modeli i optymalizacji wynik√≥w. Wymagania: Minimum 5 lat do≈õwiadczenia w pracy jako Data Scientist. Zaawansowana znajomo≈õƒá jƒôzyka Python oraz narzƒôdzi takich jak Pandas, NumPy, scikit-learn. Praktyczna wiedza z zakresu baz danych i SQL. Do≈õwiadczenie z platformami big data (Hadoop, Spark, Hive) oraz chmurami (AWS, GCP lub OCI). Wiedza o algorytmach uczenia maszynowego i modelowaniu atrybucji. Do≈õwiadczenie z platformami TV/CTV bƒôdzie du≈ºym atutem. Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego.",[],Data Science,Data Science
Full-time,Senior,Any,Remote,83,Senior MLOps Engineer,Billennium,"More about us Billennium is a global technology company with over 20 years of experience, committed to innovation and empowering businesses. As an employer, we offer a supportive, growth-focused environment where collaboration and creativity thrive. Join us to shape the future of technology together! About the Client Project is based within our client who is a leader in the pharmaceutical and healthcare industry, driving digital innovation to transform global R&D capabilities. What you will do ‚Ä¢ Design, build, and optimize ML and data pipelines for LLM-driven applications. ‚Ä¢ Collaborate with cross-functional teams to deploy scalable, production-ready ML models. ‚Ä¢ Implement fine-tuning techniques for large language models. ‚Ä¢ Automate workflows and streamline processes with CI/CD tools and scripting. ‚Ä¢ Leverage cloud platforms and MLOps tools like AWS SageMaker, Kubeflow, or Vertex AI. What we are looking for ‚Ä¢ 5+ years of experience in Data Science, including 2+ years in ML Engineering, Data Engineering, or MLOps. ‚Ä¢ Strong programming skills in Python (R is a plus). ‚Ä¢ Hands-on experience with machine learning frameworks (e.g., TensorFlow, PyTorch). ‚Ä¢ Knowledge of LLMs, fine-tuning methods, and agentic/automation-oriented design. ‚Ä¢ Experience with cloud platforms (AWS, Azure, GCP) and DevOps best practices. Perks and benefits ‚Ä¢ Comprehensive benefits ‚Äì enjoy Udemy for Business, private medical care, Multisport card, veterinary package, language lessons, and shopping vouchers. ‚Ä¢ Flexibility ‚Äì adaptable working hours and remote/hybrid work options to suit your lifestyle & location. ‚Ä¢ Career growth ‚Äì access opportunities for professional development and learning, including perks related to our official partnerships with global IT giants: Microsoft, AWS, Snowflake, Salesforce & more. ‚Ä¢ Global collaboration ‚Äì work with a diverse, international team. ‚Ä¢ Innovative environment ‚Äì be part of a forward-thinking and growth-oriented workplace. ‚Ä¢ Engaging community ‚Äì work with passionate professionals and participate in team-building events, hackathons, and CSR initiatives to make an impact beyond work. ‚Ä¢ Team-building events including our company tradition (annual company event in Mazury). ‚Ä¢ A pleasant surprise to start your journey with us in the form of a welcome pack. Recruitment process: ‚Ä¢ HR call ‚Ä¢ Technical Interview ‚Ä¢ Interview with the dedicated Client ‚Ä¢ Final decision/Feedback Sounds interesting? Click "" Apply"" and have a chance to hear more! üìû",[],Data Science,Data Science
Full-time,Senior,B2B,Hybrid,84,KYC Tool Data Steward/Analyst,Ework Group,"üíª Ework Group - founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client from banking industry we are looking for KYC Tool Data Steward/Analyst üîπ ‚úîÔ∏èYour areas of responsibility will include but not be limited to: Analyse existing KYC data to identify gaps, inconsistencies, and areas for improvement Work collaboratively with business and IT partners across business units to develop and implement data standards to ensure data accuracy & completeness Be part of major projects for delivery of Group KYC Tool Partner with Data Architects and business SMEs to create and maintain a Business Information Model (BIM) for each data domain that is aligned across the enterprise Monitor data quality metrics and report on findings to management Investigate and resolve data quality issues, escalating complex problems as needed Provide consultative services to agile delivery teams and ensure data governance best practices Support the implementation of new KYC systems and processes, ensuring data migration and integration are seamless and accurate Utilize enterprise approved tools to develop reproducible data-flows that ultimately fulfill reporting requirements Implement testing and monitor quality of developed data-flows and reporting outputs, performing upgrades as identified and maintenance as required Work with multiple customers to analyze and visualize complex data in simple and accurate ways Document data definitions, data lineage, and data quality rules Participate in data governance/stewardship communities of practice Contribute to the development and maintenance of data governance policies and procedures Stay up-to-date with industry best practices and regulatory requirements related to KYC data management Proactively identify opportunities to improve data quality and efficiency ‚úîÔ∏èYour profile and background: Proven experience in data management, data quality, or a related role, preferably within the financial services industry Experience with advanced SQL queries and familiarity with another programming language, e.g. Python, R, Java and Scala Familiar with data taxonomy and capable to build small data models Experience with Power BI to produce reports Excellent stakeholder management is mandatory as you will be working closely with multiple business areas and key business leads across IT, Architecture, Financial Crime, Compliance, Legal - and the Business in order to implement changes Experience within financial crime (AML/CTF, KYC, Transaction Monitoring and Sanctions), working agile and experience with SAFe is an advantage Professional, organised, and structured in approach and work produced to a high quality of standard, and you always strive to find the solution that is best for the bank Excellent analytical and problem-solving skills, strong attention to detail and a commitment to data accuracy Ability to communicate effectively with both technical and non-technical audiences Experience with data governance frameworks and methodologies is a plus Experience with data migration and integration projects is a plus Ability to work independently and as part of a team Fluency in English is a requirement (speaking and writing) ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,85,Data Engineer (Databricks),Remodevs,"Role We are looking for an enthusiastic Senior Data Engineer to join our team. In this role, you will collaborate with skilled professionals dedicated to providing insights into user behavior. Your work will help the organization understand when, where, and how users engage with our digital platforms. As a Senior Data Engineer, you will take charge of developing and improving advanced data-tracking solutions, ensuring seamless platform integration and transforming data into valuable insights that drive key business decisions. We are a global manufacturing company based in Scandinavia, with offices and operations around the world. 4+ years of experience with Python Knowledge of Databricks Experience with Snowplow or other tracking solutions Skills in building and maintaining data pipelines with dbt Proficiency with cloud platforms (preferably Azure) Strong SQL skills for data processing and transformation Fluent English Collaborate with developers and product managers Design, build, and manage data pipelines using dbt to process bronze, silver, and gold data layers in Databricks Work with cloud infrastructure, particularly Azure Use SQL and Python to create reliable data solutions Ensure data quality, security, and compliance throughout its lifecycle","[{""min"": 20000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,86,Big Data Solutions Architect,Britenet,"About the project We are looking for an experienced Big Data Architect who will play a key role in the design, development, and oversight of data platforms. We seek someone who can combine a strategic approach to data architecture with hands-on expertise in technologies and tools‚Äîand who understands how these technologies support business decision-making. The ideal candidate should also have experience in pre-sales activities, including preparing cost estimates and collaborating with the sales team. Our expectations Minimum 6 years of professional experience in the Big Data domain, with a strong focus on designing and implementing large-scale solutions Advanced knowledge of Big Data technologies (eg., Hadoop, Apache Spark, Kafka, Flink, Hive, HBase) Proficiency in programming with Python or Java Familiarity with NoSQL databases (e.g., Cassandra, MongoDB) and relational databases Experience working with cloud platforms (AWS, Azure, Google Cloud) and containerization/orchestration tools (Docker, Kubernetes) Knowledge of data workflow orchestration tools (e.g., Apache Airflow, NiFi) Strong analytical skills and the ability to solve complex technical problems Experience in team leadership or mentoring junior team members Ability to create technical documentation and effectively communicate with both technical and business stakeholders Fluent English (C1 level) Welcome Skills Experience and practical knowledge in integrating AI and machine learning solutions with Big Data platforms Experience working with AI and ML frameworks, e.g., TensorFlow, PyTorch, scikit-learn, Spark MLlib Experience in selling BI products and solutions Key tasks Design, develop, and implement advanced Big Data architecture, including integration of AI and machine learning solutions Analyze business and technical requirements and translate them into scalable and efficient data processing solutions Select and integrate appropriate tools and technologies to support data processing and AI algorithms Optimize data pipelines for performance and reliability Monitor and maintain Big Data and AI infrastructure, ensuring high availability, scalability, and security Provide mentoring and technical support for junior team members Create technical documentation, presentations, and reports for business and technical stakeholders Actively participate in the strategic planning of the organization's data platform and AI solution development","[{""min"": 150, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Manager / C-level,B2B,Remote,87,Head of Data & Quant Engineering,RedStone,"RedStone is a fast-growing Polish blockchain startup revolutionizing oracle infrastructure . With a team of 40, over half being senior engineers and technical experts, we deliver scalable, secure, and low-latency off-chain data to smart contracts across multiple chains. We‚Äôve secured over $6B in TVS, raised $15M in Series A funding, and were recognized by Forbes as the top VC-backed startup in Poland . Backed by leading Web3 names like Arrington Capital, Stani Kulechov (Aave), and Gnosis, our team includes alumni from Google, OpenZeppelin, and major crypto projects. We work remotely across time zones, with a Warsaw HQ for deep work and collaboration. JOIN THE REDSTONE TEAM! As Head of Data, you'll own the strategy and execution of RedStone‚Äôs data architecture. You‚Äôll lead a high-performing technical team, define standards for data quality and reliability, and help deliver real-time intelligence for smart contracts across multiple chains and financial ecosystems. This is a high-impact leadership role at the intersection of DeFi, infrastructure, and big data. Lead the Data Engineering and Analytics function across the company Architect robust systems for collecting, validating, and aggregating off-chain and on-chain data Define and enforce data reliability, accuracy, and uptime standards Design monitoring systems to detect anomalies, manipulation attempts, and cross-source inconsistencies Work cross-functionally with Engineering, Product, and Business teams to turn raw data into mission-critical insights Grow and mentor a team of high-caliber data and backend engineers Research and prototype advanced data products (e.g., asset pricing models, latency-adjusted feeds, cross-DEX aggregation) Own the roadmap and delivery of the data infrastructure used by our oracle network Strong backend engineering background with Python, Go, or Rust Experience working with AWS (especially Lambda), event-driven architectures , and message queues like RabbitMQDeep knowledge of time-series databases (InfluxDB, TimescaleDB) and monitoring systems (Grafana, CloudWatch) Familiarity with on-chain data structures , smart contract logs, block timing, and decentralized data fetching Advanced understanding of financial or trading data , including anomaly detection, latency compensation, and pricing validation Quality-Driven Mindset ‚Äì extreme attention to detail; cares deeply about data integrity in high-stakes environments Leadership & Mentoring ‚Äì experience managing or mentoring technical teams, building culture, and scaling impact Ownership & Execution ‚Äì strong project management and execution skills, especially in unstructured and fast-moving environments Experience with DEX protocols (Uniswap, GMX, dYdX, Curve, etc.) or DeFi lending platforms (Aave, Compound, Euler) Knowledge of CEX infrastructure , order books, matching engines, and arbitrage mechanics Understanding of liquidations , slippage , and pricing risk in lending/AMM environments Contributions to Web3 projects , DAOs, or open-source blockchain tooling Exposure to quantitative finance or financial modeling Hands-on experience with smart contracts , oracles, and price feed architecture The role can be fully remote but we also have an office in the center of Warsaw at Zgoda 3. Competitive salary (based on skills and experience) + token allocation after 3 months Multisport card and private healthcare Long-term stability and growth (Series A closed, backed by top global investors) Flexible hours A fixed, pre-agreed number of paid service break days. Top-tier equipment (MacBooks, external displays) Regular team off-sites, hackathons, and Web3 events Full ownership over your work A rare opportunity to build foundational infrastructure for the future of finance üåê Website üß† Docs üíª GitHub üê¶ X Be part of a world-class team building mission-critical Web3 infrastructure. Apply now and redefine the future of decentralized data.","[{""min"": 25000, ""max"": 35000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,Permanent,Remote,88,Data Scientist (Credit),Revolut,"People deserve more from their money. More visibility, more control, and more freedom. Since 2015, Revolut has been on a mission to deliver just that. Our powerhouse of products ‚Äî including spending, saving, investing, exchanging, travelling, and more ‚Äî help our 60+ million customers get more from their money every day. As we continue our lightning-fast growth,‚Äå 2 things are essential to our success: our people and our culture. In recognition of our outstanding employee experience, we've been certified as a Great Place to Work‚Ñ¢. So far, we have 10,000+ people working around the world, from our offices and remotely, to help us achieve our mission. And we're looking for more brilliant people. People who love building great products, redefining success, and turning the complexity of a chaotic world into the simplicity of a beautiful solution. We approach Data Science at Revolut the same way that we approach everything else ‚Äì with class, logical thinking, and lots of style üòé Let‚Äôs break it down: we take the most complex problems and create tailor-made solutions for our customers üöÄ If you‚Äôre thinking the Data team is kept in some sort of a secret den, doomed to never see the impact of their work, don‚Äôt worry - that‚Äôs not how we do things. They‚Äôre some of our best and brightest problem-solvers, deployed to the front lines to work in product teams and deliver rockstar solutions ü§ò We start with deep data analysis to understand our customers, their objectives and any issues they might have. We then use various data points and advanced machine learning algorithms, to come up with the best possible option for each client. We experiment, iterate, and build fully automated solutions, based on algorithms which self-improve with time. We‚Äôre looking for next-level Data Scientists to board our FinTech rocket ship and shape the future of financial services apps. Phew! It‚Äôs a big task, but you won‚Äôt do it alone. üí™ You‚Äôll be working with the toughest and most gifted professionals in Product, Design, Data Science and Engineering, on impactful projects that‚Äôll make our company move forward ‚è© Building models for full Retail Credit lifecycle Improving existing algorithms and building new Proofs of Concept Delivering real impact to the product through rigorous data-driven solutions Researching and then delivering PoCs into data products Collaborating with product owners, engineers, and data scientists to continually solve complex data problems Experience working in a data science team of a credit institution A Bachelor's/Master's/PhD in STEM (Mathematics, Computer Science, Engineering) Excellent knowledge of data science tools, including, python coding, SQL and production tools A deep understanding of probability and statistics fundamentals A big-picture mindset to correctly diagnose problems, produce research, and discover solutions Excellent communication and collaboration skills to partner with Product Owners and business heads Building a global financial super app isn‚Äôt enough. Our Revoluters are a priority, and that‚Äôs why in 2021 we launched our inaugural D&I Framework, designed to help us thrive and grow everyday. We're not just doing this because it's the right thing to do. We‚Äôre doing it because we know that seeking out diverse talent and creating an inclusive workplace is the way to create exceptional, innovative products and services for our customers. That‚Äôs why we encourage applications from people with diverse backgrounds and experiences to join this multicultural, hard-working team.",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,89,Data Engineer,TechTorch,"TechTorch is a pioneer among service companies in the way it conducts projects in the area of digital transformation. Through an implementation process supported by an AI-powered platform and a global team of world-class managers and technology experts, TechTorch has enabled dozens of private equity sector companies to accelerate the realization of business benefits. Job Description We are seeking a skilled and motivated Data Engineer to join our growing team. The ideal candidate will have a strong background in data engineering, with specific experience in working with Snowflake. You will be responsible for designing, building, and maintaining scalable data pipelines that can handle large volumes of data, ensuring the reliability, security, and performance of our data infrastructure. Key Responsibilities Design and Implement Data Pipelines: Develop, test, and maintain scalable data pipelines to support business analytics and reporting needs. Work with Snowflake: Design and optimize data architecture using Snowflake, including data modeling, data warehousing, and performance tuning. Data Integration: Integrate data from various sources, including APIs, databases, and third-party platforms, into our centralized data warehouse. Collaborate Across Teams: Work closely with data scientists, analysts, and other engineers to ensure data availability, quality, and reliability. Maintain Data Infrastructure: Monitor and maintain the performance and health of the data infrastructure, addressing any issues proactively. Documentation: Document data processes, pipeline architecture, and procedures for ongoing maintenance and future enhancements. Qualifications Experience with Snowflake: Minimum of 3 years of hands-on experience with Snowflake, including data warehousing, schema design, and query optimization. Programming Skills: Proficiency in programming languages such as Python, SQL. Data Pipeline Tools: Experience with ETL/ELT tools such as Apache Airflow, or similar. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or Google Cloud, particularly in data storage and processing. Database Management: Strong knowledge of relational and non-relational databases. Problem-Solving: Excellent problem-solving skills and ability to troubleshoot data issues quickly and efficiently. Communication: Strong communication skills, with the ability to collaborate effectively with cross-functional teams. What We Offer: Work in an international team Work with the largest capital groups in the world Development and rapid promotion opportunities Autonomy in action Participation in building a new global company Team-building activities","[{""min"": 21000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,90,Data Engineer with Python,Sii,"We are looking for a Senior Data Engineer who is not merely familiar with Google Cloud Platform and data engineering concepts through adjacent development or integration tasks, but someone who truly lives and breathes data. You should feel fully at ease working in cloud environments and have a deep commitment to building reliable, scalable, and high-quality data infrastructure. This role is ideal for someone passionate about transforming raw data into meaningful insight, who understands the strategic value of data governance, and who thrives in dynamic, cross-functional teams. Designing, developing, and maintaining scalable data pipelines and ETL processes in GCP Optimizing data processing workflows for performance, reliability, and cost-efficiency Collaborating with data scientists, analysts, and business stakeholders to translate requirements into technical solutions Ensuring compliance with data quality standards and implementing governance best practices Driving and supporting the migration of on-premise data products to GCP Managing and evolving cloud-based infrastructure and deployment practices A minimum of 5 years of experience in data engineering or a similar role Proficiency in Python and SQL, with hands-on experience working in cloud environments (GCP preferred) Solid experience with Google Cloud services: BigQuery, Cloud Storage, Pub/Sub, Cloud Build, Cloud Composer, DataFlow Familiarity with infrastructure and deployment tools such as Terraform, OpenShift, Docker, Kubernetes, and CI/CD pipelines Working knowledge of Oracle, PL/SQL, Airflow, Git, Java, and Kafka Strong problem-solving skills, autonomy, and the ability to collaborate effectively in cross-functional teams Fluent Polish required Residing in Poland required Continuous Deployment Continuous Integration Great Place to Work since 2015 - it‚Äôs thanks to feedback from our workers that we get this special title and constantly implement new ideas Employment stability - revenue of PLN 2.1BN, no debts, since 2006 on the market We share the profit with Workers - over PLN 60M has already been allocated for this aim since 2022 Attractive benefits package - private healthcare, benefits cafeteria platform, car discounts and more Comfortable workplace ‚Äì class A offices or remote work Dozens of fascinating projects for prestigious brands from all over the world ‚Äì you can change them thanks to Job Changer application PLN 1 000 000 per year for your ideas - with this amount, we support the passions and voluntary actions of our workers Investment in your growth ‚Äì meetups, webinars, training platform and technology blog ‚Äì you choose Fantastic atmosphere created by all Sii Power People 1 Send your CV 2 Talk to us about your expectations 3 Learn more about our projects and choose the best 4 Start your adventure with Sii! Sii is the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We already employ more than 7 500 professionals and implement projects in a variety of industries for clients from many countries around the world. The Great Place to Work title, won 10 times in a row, proves that at Sii we create a friendly work environment. In a survey, as many as 90% of our employees responded that Sii is a great place to work, and 95% of them think we have a great atmosphere here.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,91,Big Data Engineer,ITDS,"Big Data Engineer Join us, and build data solutions that drive global innovation! Krak√≥w - based opportunity with hybrid work model ( 2 days month in the office ). As a Big Data Developer, you will be working for our client, a leading global financial institution, contributing to the design and development of cutting-edge data solutions for risk management and analytics. The client is undergoing a strategic digital transformation, focusing on scalable, cloud-based big data platforms that support advanced analytics and regulatory compliance. You will be part of a high-performing Agile team, collaborating closely with business stakeholders and technical teams to build and maintain robust distributed systems that process large volumes of data efficiently. Designing and developing distributed big data solutions using Spark Implementing microservices and APIs for data ingestion and analytics Managing cloud-native deployments primarily on GCP Writing and maintaining test automation frameworks using tools like JUnit, Cucumber, or Karate Collaborating with cross-functional teams to translate business requirements into technical specifications Developing and scheduling data workflows using Apache Airflow Maintaining and optimizing existing big data pipelines Utilizing DevOps tools such as Jenkins and Ansible for CI/CD automation Participating in Agile ceremonies and contributing to sprint planning and retrospectives Monitoring, troubleshooting, and improving data systems and services A degree in Computer Science, IT, or a related discipline Proven experience in designing and developing big data systems Hands-on experience with Spark and distributed computing Solid Java , Python , and Groovy development skills Strong knowledge of the Spring ecosystem (Boot, Batch, Cloud) Familiarity with REST APIs, Web Services, and API Gateway technologies Practical experience in DevOps tooling like Jenkins and Ansible Proficiency in using RDBMS, especially PostgreSQL Hands-on experience with public cloud platforms, particularly GCP Excellent communication in English Experience with streaming technologies like Apache Beam or Flink Knowledge of OLAP solutions and data modeling Background in financial risk management or the banking industry Exposure to container technologies such as Docker and Kubernetes Familiarity with Traded Risk domain concepts Experience with RPC frameworks like gRPC Knowledge of data lakehouse tools like Dremio or Trino Hands-on experience with BI or UI development We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7225 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 28000, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,92,üëâ Senior Azure Data Engineer,Xebia sp. z o.o.,"üü£ You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. üü£ Your profile: ready to start immediately , 3+ years‚Äô experience with Azure (Data Factory, SQL, Data Lake, Power BI, Devops, Delta Lake, CosmosDB), 5+ years‚Äô experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools ‚Äì Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, experience with CI/CD tooling (GitHub, Azure DevOps, Harness etc), working knowledge of Git, Databricks will be benefit, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ Please note that we are currently looking to expand our talent pool for future opportunities within the IT industry. While we may not have an immediate project for you at the moment, we are proactively recruiting to ensure that we have the right expertise when new projects arise. We will contact you when a potential project matching your skills and experience becomes available. Thank you for your interest in joining our team. üü£ Nice to have: Ôªø experience with Azure Event Hubs, Azure Blob Storage, Azure Synapse, Spark Streaming, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification. üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Technical Interview (with live-coding elements) ‚Äì Client Interview (live-coding)‚Äì Hiring Manager call ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,93,Data Architect,Jit Team,"Salary: 32 000 - 34 000 PLN gross on Contract of Employment (umowa o pracƒô) Place of work: Warsaw, hybrid (3 days from the office per week) Project The project is part of a large-scale data and analytics transformation program aimed at modernizing the organization‚Äôs data platforms and architecture. The focus is on building a unified, future-ready data environment that will serve as the foundation for analytics, reporting, and operational efficiency across multiple business areas. Its primary goal is to design and implement a long-term target architecture that enables secure, scalable, and high-quality data management. This includes modernizing existing platforms, aligning technology and business strategies, and ensuring consistent standards for data across the enterprise. The initiative is highly strategic, involving close collaboration with leadership, product management, development teams, and architecture communities. It will drive better decision-making, reduce risks, and create a robust framework that supports both internal operations and the delivery of value-added services to end users. Expected competences: Proven ability to shape and execute data platform architecture in complex, multi-platform environments Experience in connecting business goals with technology strategy and translating them into actionable roadmaps Strong background in data management, data modernization, and setting high standards for data quality Hands-on expertise with cloud-based data platforms, particularly Databricks and Microsoft Azure (broad experience with both technologies is a must-have) Track record of driving architectural alignment across enterprise and domain levels during large-scale transformations Skilled at engaging and influencing stakeholders, including leading challenging discussions when necessary Advanced knowledge of architectural methods: trade-off analysis, defining target states, and building scalable solutions Technologies you'll work with Azure Cloud Databricks Client ‚Äì why choose this particular client from the Jit portfolio? Our client is known for large-scale, forward-looking technology initiatives that offer significant opportunities for impact and professional growth. They provide a stable environment with clear long-term strategies and a strong focus on building high-quality, future-ready platforms. Collaboration with this client means working in a mature organization where innovation, structured processes, and cross-functional teamwork are highly valued. Benefits Adjustable working hours to better fit your schedule. Opportunities for continuous learning and career growth . Attractive employee referral program with generous rewards. Engaging team activities and hobby groups , from sports and games to social events. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 32000, ""max"": 34000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,94,"Senior BI Developer (Power BI, Power Automate)",Holisticon Connect,"Holisticon Connect is a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä We are looking for an experienced Business Intelligence Developer (Power BI) to join our team at a leading Swedish manufacturing company . In this role, you will take ownership of the frontend development of a proprietary Business Intelligence (BI) and analytics platform used across distribution and customer management services. You‚Äôll become part of a cross-functional team currently leading the migration of reporting systems from on-premise to the cloud , using modern Microsoft technologies. This is a great opportunity to contribute your expertise to a high-impact transformation initiative. The project environment is dynamic and international, and success in this role will require strong technical proficiency , proactive communication , and a solutions-driven mindset . While expectations are high, you‚Äôll be working in a company shaped by Scandinavian work culture , where trust, autonomy, and transparency are central values. Responsibilities: Lead frontend development in Power BI Premium (Cloud) for BI and analytics tools. Design, build, and maintain dashboards, reports, and data visualizations that serve business-critical needs. Develop and optimize solutions using Power BI and Power Automate, ensuring high usability and performance. Collaborate with backend engineers, frontend engineers, analysts, and business stakeholders to translate complex requirements into clear, actionable reports. Contribute to the continuous improvement of reporting frameworks and analytical tools across the organization. We offer a B2B Contract: 120-150 PLN net/hour + VAT (depending on experience) You might be the perfect match if you are/have: 4+ years of experience with Power BI (including Premium features) and Power Automate . Proficiency in SQL and a strong understanding of data modeling, cubes, and reporting tools. Experience working on complex BI projects (preferably in the finance sector) , involving multiple dashboards and data sources. Familiarity with l arge-scale BI projects or migration efforts . Strong problem-solving skills and a passion for building well-designed, user-friendly reporting solutions. Clear and proactive communication skills , especially in cross-functional, international teams. A self-starter mindset , able to work independently, prioritize tasks, and drive projects to completion. Fluent English for smooth collaboration. Located in Poland to support formalities and occasional team meetings. Moreover, we appreciate skills in these areas: Hands-on experience with Python and/or Snowflake . Experience in cross-functional development teams within global organizations. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private life so you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Perks and benefits: Fully remote work or in our office in Wroc≈Çaw; Free benefits such as Luxmed , Multisport , and life insurance in Nationale Nederlanden ; Attractive referral system (9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budget with additional paid hours; Passion Day - an extra day off for your hobby to spend as you please; Flexible working hours with no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment + 2 additional monitors and accessories. If you apply for this position and match our expectations, then: 1) You will be invited to an HR Screening with our IT Recruiter.2) You will have an interview with client. Submit your application online in one easy step! Apply now!","[{""min"": 120, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Remote,95,Senior Azure Data Engineer + Databricks,Lingaro,"We are seeking an experienced Cloud Data Engineer proficient with Azure and Databricks. The ideal candidate will have hands-on experience in designing and implementing data pipelines, data warehousing solutions, and ETL workflows. Be an expert in mentioned technologies. Tasks: Designing and implementing data processing systems on Azure platform. This involves writing efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data. Building data pipelines to ingest data from various sources such as storages, databases, APIs, or streaming platforms. Support team members with troubleshooting and resolving complex technical issues and challenges. Provide technical guidance in data engineering and with team in selecting appropriate tools, technologies, and methodologies. Be updated with the latest advancements in data engineering and ensuring the team follows best practices and industry standards. Collaborate with stakeholders to understand project requirements, define scope, and create project plans. Support project managers to ensure that projects are executed effectively, meeting timelines, and quality standards. Monitor progress, identify risks, and implement mitigation strategies. Act as a trusted advisor for the customer. Oversee the design and architecture of data solutions, collaborating with data architects and other stakeholders. Ensure data solutions are scalable, efficient, and aligned with business requirements. Provide guidance in areas such as data modeling, database design, and data integration. Align coding standards, conduct code reviews to ensure proper code quality level. Identify and introduce quality assurance processes for data pipelines and workflows. Optimize data processing and storage for performance, efficiency and cost savings. Evaluate and implement new technologies to improve data engineering processes on various aspects (CICD, Quality Assurance, Coding standards). Maintain technical documentation of the project, control validity and perform regular reviews of it. Ensure compliance with security standards and regulations. Requirements: At least 6 years of professional experience in the Data & Analytics area 1+ years of experience (or acting as) in the Senior Consultant or above role with a strong focus on data solutions build in Azure and Databricks/Synapse/(MS Fabric is nice to have). Proven experience in Azure cloud-based infrastructure, Databricks and one of SQL implementation (e.g., Oracle, T-SQL, MySQL, etc.) Proficiency in programming languages such as SQL, Python, PySpark is essential (R or Scala nice to have). Very good level of communication including ability to convey information clearly and specifically to co-workers and business stakeholders. Working experience in the agile methodologies ‚Äì supporting tools (JIRA, Azure DevOps). Experience in leading and managing a team of data engineers, providing guidance, mentorship, and technical support. Knowledge of data management principles and best practices, including data governance, data quality, and data integration. Good project management skills, with the ability to prioritize tasks, manage timelines, and deliver high-quality results within designated deadlines. Excellent problem-solving and analytical skills, with the ability to identify and resolve complex data engineering issues. Knowledge of data security and privacy regulations, and the ability to ensure compliance within data engineering projects. Knowledge of data orchestration tools. Experience in designing and creating integration and unit tests will be nice to have. Continuous learning mindset, staying updated with the latest advancements and trends in data engineering and related technologies. Experience or familiarity with other cloud technologies, data warehouses, data governance, and business analysis is a plus. Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,96,Data engineer (senior),BEC Financial Technologies,"This position is based at our office in Warsaw and you will join one of our development teams in a division called Data, Analytics & Reporting. The division maintains and develop all Data Warehouse and BI platforms and solutions in BEC. These platforms and tools are used widely inside BEC and in our customers BI departments. We are around 100 people in the division and 10 people in the team. We offer an unique chance to work with a very advanced architecture and tool stack to support hundreds of developers and thousands of end users. You will learn all the tricks and bells in an advanced Data Warehouse setup. We have just started a transformation to a Microsoft Azure/Databricks setup, and this will happen while we continue maintaining the present platform on premise in the coming years. The team is very experienced and have been working together for a long time. We are located in Denmark and Poland. At BEC, we prefer to collaborate often in the office, but we also keep the opportunity to work remotely up to 8 days per month. Your direct manager will be Peter Tougaard. We offer employment exclusively based on an employment contract (umowa o pracƒô). The teams primary task and responsibilities include: Responsible for our daily batch load GDPR data masking in our non-prod environments Development of supporting tools/code in our ETL workflow Data Warehouse development Primary tools IBM Netezza database, Informatica PowerCenter, Databricks Travel to Denmark 1-2 times a year We have our own product owner and team architect in the team To succeed you will have: At least 5+ years experience within Data Warehouse development Broad knowledge in areas like Database design, ETL design, Data Warehouse methodologies like Data Vault, Dimensional Modeling, GDPR, Governance General coding experience with SQL, Python, Powershell etc. A curious mind with an open nature and desire to learn and knowledge share with others. Fluent in English (written and spoken) Master‚Äôs degree in the area of IT or similar It‚Äôs nice-to-have: Experience with other ETL tools and databases on premise and in cloud Experience from the financial sector Agile way of working like SAFe Be your best self with BEC‚Äôs benefits! We offer a diverse range of benefits for our employees. Here are just a few of them. Flexible working hours Mental health support Free lunch at the office Professional development Referral bonus up to PLN 10,000 PLN 600 on a benefit platform a month Passion clubs and social events (Tennis, salsa dancing, board games, family picnics and more!) What does the recruitment process look like? Send us your CV: We want to get to know you Screening call: Let‚Äôs chat and see if we‚Äôre a match Technical test : Show off your knowledge! Meet our talent partner and the hiring manager : Learn more about the job, BEC Poland, and tell us more about your skills end experience. Time to sign the contract : We‚Äôre ready to welcome you to BEC! If you have any questions related to the position, please contact Anna Barci≈Ñska at Anna.Barcinska@BEC.dk (Talent Attraction Partner) or Peter.Tougaard@BEC.dk (Hiring Manager). Make us aware of your talent We are an equal opportunities employer. We hire top talent regardless of race, religion, color, national origin, sexual orientation, gender identity, and age. We encourage all qualified candidates to apply. See our full list of vacancies at https: //www.bec.dk/en/vacancies/ You can also learn more about BEC by browsing our company culture book: wearebec.pdf",[],Data Engineering,Data Engineering
Full-time,Senior,Any,Remote,97,Data Scientist Tech Lead,Lingaro,"Lead the technical design and end-to-end development of AI solutions that align with business goals and technical standards. Design and build high-performing, scalable, enterprise-grade LLM/AI applications in cloud environments. Mentor the development team, conduct code reviews, and implement engineering best practices. Collaborate closely with AI Engineering and Data Science teams to deliver effective and efficient AI solutions. Make architectural decisions regarding AI solutions and frameworks. Define and implement best practices in GenAI/ML models lifecycle and ML operations/LLM operations. Optimize AI models and systems, analyze performance, and scale solutions in cloud environments. Gather technical requirements and estimate planned work. Ensure code compliance with best practices in AI solutions development. Create technical documentation. Present solutions, concepts, and results to internal and external clients. 10+ years of experience in AI/software engineering, including a minimum of 4 years in a Tech Lead role. At least 6+ years of experience in production-ready Python AI-related code development. At least 2+ years of experience in production-ready LLM-related code development, preferably based on the Retrieval-Augmented Generation (RAG) concept. Advanced knowledge of AI frameworks and experience leading AI projects from design through to implementation. Strong analytical and problem-solving skills with the ability to optimize AI solutions for diverse applications. Strong knowledge and experience in Generative AI, including LLMs, chatbots, AI agents, and RAG mechanisms. Deep understanding of LLM evaluators, validators, and guardrails. Proficiency in LLMOps concepts such as GenAI operationalization and scaling (e.g., LLMs serving, performance & API Gateways, LLMs tracking & monitoring). Excellent knowledge of Python, SQL, and GenAI frameworks (e.g., LangChain, LangGraph, vector databases). Experience in building and deploying production-ready systems based on microservices. Solid understanding of ML/AI concepts: types of algorithms, machine learning frameworks, model efficiency metrics, model lifecycle, AI architectures. Proven ability to collaborate effectively across technical and non-technical teams. Familiarity with cloud environments such as Azure (preferred), GCP, or AWS, including AI-related managed services. Familiarity with CI/CD, testing, and containerized deployments. A collaborative approach to leadership, with experience mentoring and guiding diverse technical teams. Excellent communication skills in English, with the ability to convey complex technical concepts to various audiences. Nice-to-Have Skills & Knowledge: Experience in designing and programming ML algorithms and data processing pipelines using Python. Good understanding of CI/CD and DevOps concepts, with experience working with selected tools (preferably GitHub Actions, GitLab, or Azure DevOps). Experience in productizing ML solutions using technologies like Spark/Databricks or Docker/Kubernetes. We offer: Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Science,Data Science
Full-time,Manager / C-level,B2B,Hybrid,98,Lead Data Analyst,Experis Manpower Group,"We are looking for an experienced Data Analyst (Manager level) to lead analytical initiatives aimed at optimizing business performance and product growth. This role focuses on transforming raw data into actionable insights through advanced data analysis, visualization, and reporting. The ideal candidate combines strong technical proficiency with leadership capability to manage cross-functional collaboration and ensure the delivery of high-quality analytical outcomes. Lead data collection, cleaning, and organization efforts for structured data sets Optimize business and product growth scenarios through advanced data analysis Analyze large data sets to identify patterns, trends, and opportunities Build and maintain dashboards and reports to present key insights to technical and non-technical stakeholders Interpret data findings and provide actionable recommendations to leadership and product teams Collaborate closely with data scientists, data engineers, and cross-functional teams to ensure data integrity and alignment Oversee data-related initiatives, ensuring quality, timeliness, and business relevance Stay current with emerging tools, technologies, and methodologies in data analytics Mentor junior analysts and contribute to the development of data literacy across the organization 5+ years of experience in data analytics, with at least 2 years in a leadership or managerial role Proven ability to lead data-driven decision-making processes Strong proficiency in R, Python, SQL, SAS, and SAS Miner Experience working with structured data from relational databases, spreadsheets, and cloud sources Advanced skills in data visualization and reporting (e.g., Power BI, Tableau, or similar tools) Excellent communication skills with the ability to translate technical findings into business-friendly language Familiarity with data governance, data quality standards, and best practices Demonstrated ability to manage multiple projects and collaborate with cross-functional teams Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Data Science, Statistics, or a related field Our offer: Work from Warsaw, Katowice or Gdansk office MultiSport card Private Healthcare Life insurance","[{""min"": 170, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,99,Senior Data Engineer,Experis Manpower Group,"Join a dynamic and collaborative team working in a fast-paced Scrum environment, where agility and teamwork are key to delivering impactful solutions. Each day begins with a stand-up meeting alongside data & analytics engineers, the Scrum Master, and the Product Owner ‚Äî aligning on sprint goals, resolving blockers, and sharing progress. Your Responsibilities: Design and enhance scalable data pipelines and infrastructure. Monitor system performance and optimize for cost-efficiency. Model data using DBT based on existing pipelines. Collaborate with functional analysts to refine business requirements. Participate in sprint ceremonies including refinements and retrospectives. Stay connected with your team and stakeholders through Jira and Slack. What We‚Äôre Looking For: Strong command of SQL. Hands-on experience with DBT for data transformation and pipeline automation. Enthusiasm for AWS technologies. AWS Certified Associate accreditation. Proficiency in one or more of the following : Scala, Python, Go, Java, Shell scripting ‚Äî or deep database expertise with a willingness to learn Python and Scala. Practical DevOps experience (CI/CD, system setup, monitoring). Excellent communication and analytical skills. Team-oriented approach and interest in pair programming. Nice to Have: Experience with Terraform or other Infrastructure as Code tools. Understanding of large-scale distributed systems. Familiarity with Domain-Driven Design. Knowledge of monitoring, logging, and security automation. What We Offer: B2B contract via Experis. Sports card. Life insurance. Private medical care. Fully remote work.","[{""min"": 160, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,100,Analytics Engineer,Datumo,"We‚Äôre looking for a Analytics Engineer ready to push boundaries and grow with us. Datumo specializes in providing Data Engineering and Cloud Computing consulting services to clients from all over the world, primarily in Western Europe, Poland and the USA. Core industries we support include e-commerce üõí, telecommunications üì° and life sciences üß¨. Our team consists of exceptional people whose commitment allows us to conduct highly demanding projects. Our team members tend to stick around for more than 3 years, and when a project wraps up, we don't let them go - we embark on a journey to discover exciting new challenges for them. It's not just a workplace; it's a community that grows together! Must-have: ‚úÖ at least 3 years of commercial experience in programming ‚úÖ proven record with a selected cloud provider GCP (preferred), Azure or AWS ‚úÖ good knowledge of JVM languages (Scala or Java or Kotlin), Python, SQL ‚úÖ experience in one of data warehousing solutions: BigQuery/Snowflake/Databricks or similar ‚úÖ in-depth understanding of big data aspects like data storage, modeling, processing, scheduling etc. ‚úÖ data modeling and data storage experience ‚úÖ ensuring solution quality through automatic tests, CI/CD and code review ‚úÖ proven collaboration with businesses ‚úÖ English proficiency at B2 level, communicative in Polish Nice to have: üåü knowledge of dbt, Docker and Kubernetes, Apache Kafka üåü familiarity with Apache Airflow or similar pipeline orchestrator üåü another JVM (Java/Scala/Kotlin) programming language üåü experience in Machine Learning projects üåü understanding of Apache Spark or similar distributed data processing framework üåü familiarity with one of BI tools: Power BI/Looker/Tableau üåü willingness to share knowledge (conferences, articles, open-source projects) What‚Äôs on offer: üî• 100% remote work, with workation opportunity üî• 20 free days üî• onboarding with a dedicated mentor üî• project switching possible after a certain period üî• individual budget for training and conferences üî• benefits: Medicover Private Medical Care , co-financing of the Medicover Sport card üî• opportunity to learn English with a native speaker üî• regular company trips and informal get-togethers Development opportunities in Datumo: üöÄ participation in industry conferences üöÄ establishing Datumo's online brand presence üöÄ support in obtaining certifications (e.g. GCP, Azure, Snowflake) üöÄ involvement in internal initiatives, like building technological roadmaps üöÄ training budget üöÄ access to internal technological training repositories Discover our exemplary project: üîå IoT data ingestion to cloud The project integrates data from edge devices into the cloud using Azure services. The platform supports data streaming via either the IoT Edge environment with Java or Python modules, or direct connection using Kafka protocol to Event Hubs. It also facilitates batch data transmission to ADLS. Data transformation from raw telemetry to structured tables is done through Spark jobs in Databricks or data connections and update policies in Azure Data Explorer. ‚òÅÔ∏è Petabyte-scale data platform migration to Google Cloud The goal of the project is to improve scalability and performance of the data platform by transitioning over a thousand active pipelines to GCP. The main focus is on rearchitecting existing Spark applications to either Cloud Dataproc or Cloud BigQuery SQL, depending on the Client‚Äôs requirements and automate it using Cloud Composer. üìà Data analytics platform for investing company The project centers on developing and overseeing a data platform for an asset management company focused on ESG investing. Databricks is the central component. The platform, built on Azure cloud, integrates various Azure services for diverse functionalities. The primary task involves implementing and extending complex ETL processes that enrich investment data, using Spark jobs in Scala. Integrations with external data providers, as well as solutions for improving data quality and optimizing cloud resources, have been implemented. üõí Realtime Consumer Data Platform The initiative involves constructing a consumer data platform (CDP) for a major Polish retail company. Datumo actively participates from the project‚Äôs start, contributing to planning the platform‚Äôs architecture. The CDP is built on Google Cloud Platform (GCP), utilizing services like Pub/Sub, Dataflow and BigQuery. Open-source tools, including a Kubernetes cluster with Apache Kafka, Apache Airflow and Apache Flink, are used to meet specific requirements. This combination offers significant possibilities for the platform. Recruitment process: 1Ô∏è‚É£Quiz - 15 minutes 2Ô∏è‚É£ Soft skills interview - 30 minutes 3Ô∏è‚É£ Technical interview - 60 minutes Find out more by visiting our website - https: //www.datumo.io If you like what we do and you dream about creating this world with us - don‚Äôt wait, apply now!","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Any,Remote,101,Data Engineer,Billennium,"More about us Billennium is a global technology company with over 20 years of experience, committed to innovation and empowering businesses. As an employer, we offer a supportive, growth-focused environment where collaboration and creativity thrive. Join us to shape the future of technology together! About the Client Project is based within our client who is a leader in AI-powered data innovation, providing cutting-edge digital solutions for global industries including life sciences and healthcare. What you will do ‚Ä¢ Design, build, and maintain scalable data pipelines in Google Cloud Platform (GCP) ‚Ä¢ Collaborate with cross-functional teams to ensure seamless data flow and integration ‚Ä¢ Optimize and troubleshoot data pipelines for performance and reliability ‚Ä¢ Work with tools like BigQuery, Airflow, and Terraform to build robust solutions ‚Ä¢ Contribute to continuous improvement of data engineering practices and tooling What we are looking for ‚Ä¢ 4‚Äì7 years of experience in a data engineering or similar role ‚Ä¢ Proficiency in Python and SQL for data pipeline development and querying ‚Ä¢ Experience with ETL processes, data modeling, and cloud environments (preferably GCP) ‚Ä¢ Familiarity with BigQuery, Airflow, and Terraform ‚Ä¢ Strong communication skills and ability to work in a fast-paced, collaborative environment Technologies used: Python, SQL, GCP, BigQuery, Airflow, Terraform, Git, Docker, CI/CD Perks and benefits ‚Ä¢ Comprehensive benefits ‚Äì enjoy Udemy for Business, private medical care, Multisport card, veterinary package, language lessons, and shopping vouchers ‚Ä¢ Flexibility ‚Äì adaptable working hours and remote/hybrid work options to suit your lifestyle & location ‚Ä¢ Career growth ‚Äì access opportunities for professional development and learning, including perks related to our official partnerships with global IT giants: Microsoft, AWS, Snowflake, Salesforce & more ‚Ä¢ Global collaboration ‚Äì work with a diverse, international team ‚Ä¢ Innovative environment ‚Äì be part of a forward-thinking and growth-oriented workplace ‚Ä¢ Engaging community ‚Äì work with passionate professionals and participate in team-building events, hackathons, and CSR initiatives to make an impact beyond work ‚Ä¢ Team-building events , including our company tradition (annual company event in Mazury) ‚Ä¢ A pleasant surprise to start your journey with us in the form of a welcome pack Recruitment process ‚Ä¢ HR call ‚Ä¢ Technical Interview ‚Ä¢ Interview with the dedicated Client ‚Ä¢ Final decision / Feedback Sounds interesting? Click ""Apply"" and have a chance to hear more!",[],Data Engineering,Data Engineering
Full-time,Manager / C-level,B2B,Remote,102,Lead AI Data Engineer,dotLinkers,"Position: Lead AI Data Engineer Employment: B2B Working model: Remote Our client is a rapidly growing, innovation-driven real estate company with a strong focus on customer experience and sustainable asset design. Operating across the UK, Spain, Portugal, and expanding into Germany and the Netherlands, the company is committed to leveraging technology and data to enhance business decisions, asset performance, and customer service. Their dedicated data and innovation unit is building a cloud-based, AI-enabled data platform to support smarter investment strategies and scalable data infrastructure. Role Overview We are currently seeking a Lead AI Data Engineer to join the team on a B2B basis. This is a hands-on, full-stack data engineering role that combines leadership, architecture, and AI/ML deployment to scale a dynamic data platform. The successful candidate will lead a small, talented team and play a critical role in shaping the technological future of real estate investment. Responsibilities: Lead and mentor a growing team of data engineers and analysts. Design and implement scalable, cloud-native infrastructure (GCP) to support AI/ML-powered data solutions. Develop and optimize end-to-end data pipelines to extract real-time, actionable insights. Collaborate with domain experts to align data architecture with business needs. Maintain data SLAs and ensure accessibility and usability for investment analysts and decision-makers. Drive innovation through the deployment of GenAI and ML models to solve real business problems. Requirements: 5+ years of hands-on experience as a Senior or Lead Data Engineer with a focus on AI/ML solutions. Proven track record in designing, coding, and deploying scalable data architectures. Proficiency in Python (with experience in key libraries), JavaScript, and SQL. Strong understanding of CI/CD, cloud-native infrastructure, and back-end systems. Master‚Äôs degree in a STEM field, preferably Data Science, Data Engineering, or Computer Science. Demonstrated ability to coach and lead a team (3+ years of mentoring experience). GCP certification or experience with Google Cloud Platform and its AI/ML tools is a strong plus. Full-stack mindset with experience building robust, production-grade data pipelines. The offer: Full-time engagement on a B2B basis. Competitive compensation: ¬£6,500 ‚Äì ¬£7,500 per month. Work remotely with minimal travel (2 days/month in London). Work in a fast-paced, innovation-first environment with a collaborative team. Opportunity to shape a cutting-edge data platform at the intersection of AI and real estate. Direct collaboration with company founders and strategic stakeholders for high-impact outcomes.","[{""min"": 31851, ""max"": 36751, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,103,Senior Data Engineer,Cloudfide,"You are Passionate about cloud and data analytics, inspiring person that enjoy your day-to-day job. Curious and eager to learn new technologies. One that would like to work with a team of like-minded people. Opportunity overview You will work on a project involving modern cloud data lake implementation, leveraging Databricks, CI/CD and cloud services as your daily driver. Your impact zone Designing, implementing, and optimizing modern cloud-based solutions. Designing, implementing, and optimizing modern cloud-based solutions. Building and launching new data models and data pipelines. Building and launching new data models and data pipelines. Implementing best practices in data engineering including data integrity, quality, and documentation. Implementing best practices in data engineering including data integrity, quality, and documentation. Optimization of existing analytical solutions. Optimization of existing analytical solutions. Leading small size teams of engineers and being a role model. Leading small size teams of engineers and being a role model. Qualifications & tech toolbox 4+ years of experience delivering complex data warehouse / data lake / business intelligence solutions. 4+ years of experience delivering complex data warehouse / data lake / business intelligence solutions. 2+ years of experience working with cloud services (Azure / GCP / AWS). 2+ years of experience working with cloud services (Azure / GCP / AWS). Knowledgeable and experienced in Python. Knowledgeable and experienced in Python. Experience in SQL and data analysis, knowledge of relational databases (preferably SQL Server, PostgreSQL). Experience in SQL and data analysis, knowledge of relational databases (preferably SQL Server, PostgreSQL). Knowledge of public cloud architecture, security, networking concepts and best practices (MS Azure preferred). Knowledge of public cloud architecture, security, networking concepts and best practices (MS Azure preferred). Knowledge of DWH data modeling practices and ETL/ELT development. Knowledge of DWH data modeling practices and ETL/ELT development. Conceptual and analytical skills ‚Äì the ability to define, analyze and document complex business and technical requirements. Conceptual and analytical skills ‚Äì the ability to define, analyze and document complex business and technical requirements. Extra stardust for Experience with Apache Spark or Databricks platform. Experience with Apache Spark or Databricks platform. Experience with Azure DevOps environment. Experience with Azure DevOps environment. Experience with Apache Airflow. Experience with Apache Airflow. Here's why you'll love Cloudfide BENEFITS: Regardless of the form of employment - Budget for your professional development - training and certification. MyBenefit cafeteria (with Multisport). Medicover medical care. Team-building meetings and trips. BENEFITS: Regardless of the form of employment - Budget for your professional development - training and certification. MyBenefit cafeteria (with Multisport). Medicover medical care. Team-building meetings and trips. FLEXIBILITY: Enjoy the freedom of working from anywhere, and have a genuine say on our tools, tech, and solutions. FLEXIBILITY: Enjoy the freedom of working from anywhere, and have a genuine say on our tools, tech, and solutions. STABILITY: Stable and long-term employment (employment contract, B2B). STABILITY: Stable and long-term employment (employment contract, B2B). START-UP CULTURE: Open communication, creative problem solving and a flat hierarchy. START-UP CULTURE: Open communication, creative problem solving and a flat hierarchy. GROWTH: Skyrocket your career by exploring new territories ‚Äì you can work on various projects related to Big Data and Cloud. GROWTH: Skyrocket your career by exploring new territories ‚Äì you can work on various projects related to Big Data and Cloud. COLLABORATION: Be part of our diverse, passionate team, where every voice matters. Work in a company full of well-coordinated people who do their work with passion and commitment. COLLABORATION: Be part of our diverse, passionate team, where every voice matters. Work in a company full of well-coordinated people who do their work with passion and commitment. Equal opportunities CLOUDFIDE is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,104,Data Platform Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients‚Äô systems, departments and sites. We provide an open technology platform that‚Äôs shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience. Want to be part of it? Job Title: Data Platform Engineer Job Summary : We are seeking a highly skilled and experienced Data Engineer to join our team. The successful candidate will be responsible for developing and maintaining our Data Lake and existing Data Pipelines, as well as continually exploring, analyzing, and proposing improvements to existing processes and tooling. They will also be responsible for ensuring best practices are being adopted and staying up to date with the latest research and trends in Data Engineering. Skills Required: Strong background in Data Engineering, with experience in developing and maintaining Data Pipelines and Data Lakes Proven record of accomplishment of staying up to date with the latest research and best practices in Data Engineering Excellent technical skills in Data Engineering tools and technologies Advanced proficiency in Python and SQL Understanding of AWS cloud technologies, including infrastructure as code (CDK preferred) Effective communication and interpersonal skills, with the ability to work effectively with stakeholders at all levels Strong understanding of information security and data protection principles Experience in driving technical and career development, creating appropriate goals and seeking learning opportunities within the company and the wider software community Good understanding and prior experience of the Agile process (Scrum or Kanban) Fluency with software design patterns Experience working with automotive retail technology would be a distinct advantage Key Responsibilities: Maintain and develop the Data Lake and existing Data Pipelines to support the product and data teams‚Äô requirements Continuously explore, analyze, and propose improvements to existing processes and tooling Stay up to date with the latest research, trends and best practices in Data Engineering Support the Business Intelligence team and wider Company in querying centralized data stores, including the Data Lake Work within department to maintain an ongoing understanding of the company‚Äôs data strategy and roadmap Proactively report on issues and problems Work independently, manage day-to-day workload and priorities, and take accountability for direction and output Drive your own technical and career development, create appropriate goals, and seek learning opportunities within the company and the wider software community Support colleagues on calls or in meetings with clients, partners, and suppliers as required Maintain systems under the team‚Äôs control, including user and access management Support colleagues and HR with onboarding as well as offboarding processes Ensure information security, data protection and support the business in complying with any legal obligations imposed upon it through positive actions Technologies: Python SQL: Trino, Spark-SQL, Hive, TSQL AWS Cloud services (including: s3, step functions, glue, CDK) Terraform Linux Windows Why join us? We‚Äôre on a journey to become market leaders in our space ‚Äì and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We‚Äôre committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles ‚Äì not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn‚Äôt require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply .","[{""min"": 18000, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,105,Data Analyst,Haddad Brands USA,"Along with dynamic company development, we are looking for talented and experienced Data Analyst. Self-motivated and team player. Long term contract only. As a Data Analyst, you will be responsible for developing and maintaining Power BI dashboards, analyzing complex data sets, and collaborating with cross-functional teams to provide actionable insights. The ideal candidate should have a strong analytical mindset, advanced proficiency in Power BI, and a deep understanding of data modeling, visualization, and business intelligence reporting. Key Responsibilities: Develop, maintain, and enhance interactive Power BI dashboards and reports. Transform raw data into meaningful insights using Power BI to support business decisions. Work closely with business stakeholders to understand their requirements and translate them into analytical solutions. Collaborate with data engineering and IT teams to ensure data accuracy, integrity, and consistency. Perform data analysis using DAX, Power Query, and other advanced Power BI features. Optimize dashboard performance and recommend best practices for data visualization. Support ad-hoc reporting and analytical requests from various departments. Identify trends, patterns, and key insights through complex data analysis. Ensure the security and governance of Power BI reports according to company standards. Train and support users in the effective use of Power BI reports and analytics tools. Required Qualifications: Bachelor‚Äôs degree in Statistics, data science, Computer Science, or a related field. 3+ years of hands-on experience in data analysis and business intelligence using Power BI. Proficiency in Power BI Desktop, Power BI Service, DAX, and Power Query. Strong experience with data modeling, ETL processes, and SQL. Knowledge of database management systems (e.g., SQL Server, Azure, etc.). Ability to work with large datasets and perform data cleaning, transformation, and analysis. Excellent communication skills with the ability to present complex data in a simple, actionable format. Strong problem-solving skills and attention to detail. Ability to work in a fast-paced environment and manage multiple projects simultaneously. Preferred Qualifications: Knowledge of Python or R for data analysis. Power Automate experience Web scraping experience Familiarity with other BI tools (Tableau, QlikView, etc.). Experience with cloud platforms (Azure, AWS, Google Cloud) and Power BI integrations. Power BI certification is a plus. Job Profile: 40% New features/reports, 40% Maintenance / Bug Fixing, 20% Meetings (developers / business) Hire process: Get to know meeting in Polish followed by a second one, technical oriented within 2 weeks period. Brief English conversation is possible to estimate the skills. Benefits: private medical insurance, 21 paid vacations a year in B2B, hardware/software, training and career-oriented development path, freedom in tools, friendly environment based on trust.","[{""min"": 10000, ""max"": 14000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,106,Analityk Systemowy,Detable,"Do≈ÇƒÖcz do Detable Sp. Z o.o. - prƒô≈ºnie rozwijajƒÖcej siƒô firmy, kt√≥ra stawia na d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô z do≈õwiadczonymi profesjonalistami. Od ponad 3 lat jeste≈õmy partnerem dla instytucji z sektora publicznego, wsp√≥≈Çpracujemy m.in . z Centrum e-zdrowia, Aplikacjami Krytycznymi Ministerstwa Finans√≥w, G≈Ç√≥wnym Urzƒôdem Nadzoru Budowlanego, Urzƒôdem Do Spraw Cudzoziemc√≥w , Narodowym Funduszem Zdrowia i wieloma innymi. Nasi konsultanci pracujƒÖ nad rozwojem aplikacji i system√≥w, z kt√≥rych korzystajƒÖ miliony Polak√≥w! Posiadasz minimum 3 lata do≈õwiadczenia w pracy na stanowisku Analityka w projektach dla klienta masowego; Na co dzie≈Ñ pracujesz z SQL, Python, PostgresSQL, Oracle; Sk≈Çadnice danych, Mapowania, Raportowanie (BI) nie majƒÖ przed TobƒÖ tajemnic; Wykorzystujesz w codziennej procesy ETL/ELT; Mia≈Çe≈õ okazjƒô tworzyƒá modele logiczne i fizyczne z wykorzystniem narzƒôdzia Enterprise Architect; Mo≈ºesz pochwaliƒá siƒô znajomo≈õciƒÖ i do≈õwiadczeniem w obszarze ochrony zdrowia; Posiadasz do≈õwiadczenie w obszarze Hurtowni Danych; Dodatkowym atutem bƒôdzie je≈õli mo≈ºesz pochwaliƒá siƒô jednym z certyfikat√≥w: AgilePM, certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL/Python. Pozyskiwaniu wymaga≈Ñ systemowo- biznesowych dla projekt√≥w IT; Modelowaniem otoczenia i proces√≥w systemowo-biznesowych oraz wytwarzaniem modelu danych; Okre≈õlanie przypadk√≥w u≈ºycia system√≥w informatycznych; Modelowanie danych na poziomie logicznym; Projektowanie i dokumentowanie przep≈Çyw√≥w danych (ETL) oraz struktur raportowych; Okre≈õlanie architektury dla projektowanych system√≥w; Definiowanie interfejs√≥w oraz przep≈Çywu komunikacji miƒôdzy modu≈Çami; Modelowanie diagram√≥w aktywno≈õci, sekwencji oraz stan√≥w dla modu≈Ç√≥w projektowanych system√≥w; Proponowanie i konsultowanie rozwiƒÖza≈Ñ systemowych ze zleceniodawcami oraz realizatorami zdefiniowanych wymaga≈Ñ; Wsparcie analityczne na etapach projektowania, wytwarzania i testowania system√≥w informatycznych. Konkurencyjne wynagrodzenie w oparciu o kontrakt B2B ( do 140PLN netto/h); D≈ÇugofalowƒÖ wsp√≥≈Çpracƒô opartƒÖ o wzajemny szacunek i partnerstwo; Dedykowanego opiekuna kontraktu po stronie Detable; Mo≈ºliwo≈õƒá 100% pracy zdalnej lub z naszego biura w Bia≈Çymstoku; Mo≈ºliwo≈õƒá podnoszenia swoich kwalifikacji poprzez skorzystanie z bud≈ºetu szkoleniowego; Realny wp≈Çyw na rozw√≥j projektu; Atrakcyjny program polece≈Ñ pracowniczych; Zdalny proces rekrutacji. Rozmowa HR z naszƒÖ IT RekruterkƒÖ; Weryfikacja umiejƒôtno≈õci technicznych przez naszego Lidera technicznego; Spotkanie z klientem; Decyzja i rozpoczƒôcie wsp√≥≈Çpracy","[{""min"": 20160, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Manager / C-level,Permanent,Hybrid,107,Associate Director - Head of Data Intelligence,Arla,"We‚Äôre making a bold shift, because staying the same isn‚Äôt an option. Consumer Intelligence is Arla‚Äôs reimagined Insights & Analytics function built to shape the future of dairy by putting real consumer understanding at the heart of everything we do. As the world around us changes fast, we‚Äôre stepping up with new capabilities, deeper expertise, and a sharper focus on impact. We‚Äôre not just here to answer questions, we‚Äôre here to challenge, to inspire, and to make sure the voice of the consumer drives the decisions that matter most in our business. Role Overview As the Head of Data Intelligence, you will lead and shape a team of advanced analytics experts, responsible for turning complex data into clear strategic action. You‚Äôll oversee our Marketing Mix Modelling, Forecasting, Data Storytelling and AI and automation capabilities. Acting as the core engines that power smarter marketing investments, future-focused commercial planning, and impactful narratives. This is more than a technical role. It‚Äôs a leadership role at the crossroads of data, creativity, and commercial value. You‚Äôll shape how data is used at Arla, not just to explain what happened, but to predict what‚Äôs next and influence what we do. You are not only a data expert. You are a translator, a connector, and a strategic driver. Ensuring we can layer the right data to drive smarter decisions. Key Responsibilities Lead a High-Impact, Future-Focused Team Build, guide, and inspire a cross-functional team of data scientists, forecasters, and data storytellers. Foster a culture of innovation, experimentation, and practical application. Champion the development of data-driven narratives that resonate with both technical and non-technical audiences. Drive Smarter, Sharper Decisions with Data Oversee our global Marketing Mix Modelling approach, ensuring that we optimize spend and unlock efficiencies that fuel growth. Lead forecasting efforts across key categories and markets, building robust predictive capabilities aligned to business strategy. Elevate data storytelling as a strategic capability ‚Äî transforming analysis into influence, and dashboards into direction. Integrate Analytics into the Heart of Strategy Collaborate closely to embed data into brand-building, innovation, and category planning. Ensure that analytics are proactive, business-led, and decision-focused not siloed or reactive. Translate complexity into clarity. Make the numbers meaningful. Act as a Strategic Connector Build strong, trust-based partnerships across global and local teams. Promote a shared understanding of how data and analytics drive brand and business success. Ensure your team‚Äôs work lands with impact ‚Äî influencing investment choices, shaping consumer strategies, and driving tangible outcomes. What Will Make You Successful 10+ years of experience in advanced analytics, forecasting, or marketing mix modelling, preferably in FMCG or global CPG environments. Commercial Fluency : You understand how data connects to business, and can speak both languages. Modern Data Leadership : You‚Äôre as comfortable discussing machine learning as you are explaining what matters to a marketer. Impact-Led Storytelling : You know how to make data stick. You bring numbers to life through powerful storytelling. People Leadership : You‚Äôve built, led, and developed high-performing, multi-disciplinary teams. Collaboration & Influence : You work brilliantly across functions, building influence and alignment from the C-suite to the execution layer. Curious & Courageous : You challenge conventional thinking, experiment with new approaches, and always push for better. Why This Role Matters In today‚Äôs fast-moving world, the brands that win are those that can act on what‚Äôs next ‚Äî not just what‚Äôs now. This role ensures we don‚Äôt just have data ‚Äî we have foresight. It ensures that data doesn‚Äôt just live in silos ‚Äî it lives in every strategic decision. You‚Äôll be the one ensuring Arla turns intelligence into action, every single day.",[],Unclassified,Unclassified
Full-time,Senior,Mandate,Remote,108,Senior Data Scientist,SAVENTIC HEALTH sp. z o.o.,"About Us: Saventic Health is a mission-driven health-tech startup based in Warsaw, dedicated to transforming the diagnosis of rare diseases using cutting-edge Artificial Intelligence. We are developing innovative solutions that have the potential to significantly shorten diagnostic timelines and improve patient outcomes. As a startup, we thrive on innovation, agility, and a collaborative spirit where every team member makes a tangible impact. We work with complex medical data, requiring sophisticated approaches to extract meaningful insights. The Opportunity: We are looking for a motivated and skilled Data Scientist to join our team and contribute to the development of AI-powered diagnostic tools. This is an excellent opportunity to grow your career by applying your data science skills to challenging and meaningful problems in the healthcare domain. As a Data Scientist on our team, you will work alongside senior scientists and engineers to analyze complex medical data, develop and implement machine learning models, and contribute to the core algorithms that power our platform. You will be involved in various stages of the model development lifecycle, from data exploration and preprocessing to model training, evaluation, and supporting deployment efforts. Your Impact: As a key member of our data science team, you will: Contribute to Better Diagnoses: Play an active role in developing and refining machine learning models that aid in the faster and more accurate diagnosis of rare diseases. Help Uncover Insights: Analyze diverse datasets to identify patterns and generate insights that inform model development and product strategy. Apply ML Techniques: Implement and experiment with various machine learning algorithms to solve specific clinical and business problems. Support Innovation: Contribute to the team's efforts in exploring and utilizing effective data science methodologies. Key Responsibilities: Analyze complex medical datasets, including structured and potentially unstructured data, to identify trends and patterns. Develop, train, evaluate, and implement machine learning models using appropriate algorithms and techniques. Perform exploratory data analysis (EDA), feature engineering, and data preprocessing, often in collaboration with Data Engineers and senior team members. Validate model performance using robust metrics and methodologies. Communicate findings, results, and insights clearly to technical team members and potentially other stakeholders. Stay current with relevant advancements in Machine Learning and data science practices. Collaborate effectively within a team environment, working with Data Engineers, MLOps Engineers, other Data Scientists, and domain experts. Contribute to the documentation of models, experiments, and processes. Who You Are: Experienced Data Scientist: Proven practical experience working as a Data Scientist with demonstrated ability to contribute effectively to projects. ML Fundamentals: Solid understanding and hands-on experience with core Machine Learning algorithms and concepts (e.g., regression, classification, clustering, feature selection, validation). Python Proficient: Proficiency in Python is required, along with practical experience using essential data science libraries (e.g., Pandas, NumPy, Scikit-learn). Data Handling Skills: Experience working with real-world datasets, including cleaning, preprocessing, and feature engineering. Analytical Mindset: Good analytical and problem-solving skills with attention to detail. Clear Communicator: Ability to explain technical work and findings to team members. Educated: Bachelor's or Master's degree in Computer Science, Statistics, Mathematics, Physics, or a related quantitative field. Team Player & Learner: Collaborative attitude, eager to learn new techniques, and adaptable to a dynamic startup environment. Nice to Haves: Experience with Natural Language Processing (NLP) techniques and libraries (e.g., spaCy, NLTK, basic understanding of embeddings or text classification). Experience with Deep Learning frameworks (e.g., PyTorch, TensorFlow, Keras). Experience working within the healthcare, clinical research, or biomedical domain. Familiarity with cloud platforms (AWS, GCP, Azure) and their ML services. Experience with data visualization tools (e.g., Matplotlib, Seaborn, Plotly). Experience with SQL for data extraction. What We Offer: An opportunity to make a real-world impact by contributing to solutions that help diagnose rare diseases. A chance to work on challenging data science problems with complex medical data. Significant opportunities for learning and professional growth within a supportive team. Exposure to cutting-edge AI applications in healthcare. A dynamic, innovative, and collaborative startup culture. Ready to grow your data science career and make a difference? If you are a Data Scientist passionate about using your skills to solve meaningful problems in healthcare, we encourage you to apply!",[],Data Science,Data Science
Full-time,Mid,B2B,Remote,109,Business Intelligence Developer,EndySoft,"Position Overview: We are seeking a skilled Business Intelligence Developer to join our team. The ideal candidate will have a strong background in designing, developing, and maintaining BI solutions that help organizations make data-driven decisions. This role involves working with large datasets, building data pipelines, and creating insightful reports and dashboards to support business operations and strategy. MD rate: 16000 - 2 0000 PLN Roles and Responsibilities: Design, develop, and maintain BI solutions , including dashboards, reports, and data visualizations. Collaborate with stakeholders to gather requirements and translate them into technical BI solutions. Build and optimize ETL pipelines to ensure efficient data flow from various sources to data warehouses. Develop and maintain data models to support reporting and analytics needs. Create interactive dashboards and reports using tools like Power BI , Tableau , or QlikView . Perform data analysis to provide actionable insights and support decision-making processes. Ensure data accuracy and consistency through data validation and quality checks. Monitor and improve the performance of BI systems and applications. Stay updated with the latest BI tools, technologies, and best practices. Required Skills and Experience: Proficiency in SQL for querying and managing data in relational databases. Experience with ETL tools such as Informatica , Talend , or SSIS . Hands-on experience with BI tools like Power BI , Tableau , QlikView , or similar. Strong understanding of data modeling concepts, including star schema and snowflake schema . Familiarity with data warehousing technologies such as Snowflake , Redshift , or Azure Synapse . Experience with scripting languages like Python or R for data analysis and automation. Strong problem-solving skills and attention to detail. Good communication and collaboration abilities. Nice to Have: Experience with cloud platforms like AWS , Azure , or Google Cloud for BI solutions. Familiarity with big data technologies such as Spark , Hadoop , or Databricks . Knowledge of machine learning and predictive analytics . Experience with version control systems like Git . Exposure to Agile/Scrum development methodologies. Knowledge of performance tuning for BI applications and databases. Additional Information: This is an exciting opportunity to work on innovative BI solutions and contribute to data-driven decision-making. If you are passionate about transforming data into actionable insights and thrive in a fast-paced environment, we encourage you to apply.","[{""min"": 16000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,110,Greenplum Database Administrator,Calimala.ai,"Calimala.ai is seeking a seasoned Greenplum Database Administrator to join our dynamic team. With a focus on ensuring optimal performance, security, and resilience of our Greenplum clusters, you will take charge of database maintenance, backup & recovery, monitoring, and performance tuning. This role is perfect for a professional who is passionate about leveraging modern technologies in a fast-paced, innovative environment. Responsibilities: In this role, you will: Manage and perform daily administration tasks for Greenplum clusters while ensuring optimal system performance and data integrity. Conduct routine health checks, cleanup, vacuuming, and reindexing operations to maintain a robust database environment. Oversee backup and restore processes along with disaster recovery setups, ensuring data safety and business continuity. Monitor system performance and tune queries to achieve high responsiveness and efficiency. Collaborate with development teams to support schema changes, manage access controls, and enhance overall system functionality. Requirements: Applicants must have a minimum of 5 years of hands-on experience in managing and maintaining enterprise database systems, including proficiency in Greenplum DB and PostgreSQL. A deep understanding of Linux environments, SQL scripting, and performance tuning is essential, along with a proven track record in backup & recovery and disaster recovery implementations. Benefits: We offer a competitive compensation package, flexible work arrangements, and continuous learning opportunities in a collaborative, innovative work culture. Join us at Calimala.ai and contribute to cutting-edge projects in a company that values your expertise and drive for excellence. Why Calimala.ai ? Be part of a team that pushes the boundaries of technology and innovation. At Calimala.ai , your expertise will be valued, and your professional growth nurtured. We are committed to supporting our staff and providing a stimulating work environment where you can thrive.","[{""min"": 15000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Database Administration,Database Administration
Full-time,Mid,Permanent,Hybrid,111,Azure Technical Customer Success Senior Analyst with Turkish,Accenture,"Accenture is a leading global professional services company that helps the world‚Äôs leading businesses, governments and other organizations build their digital core, optimize their operations, accelerate revenue growth and enhance citizen services. We offer solutions and assets across Strategy & Consulting, Technology, Operations , Industry X and Accenture Song. Operations drives business value and outcomes to advance our clients on their journey to intelligent operations through our innovative operating engine ‚Äî SynOps. We transform business processes to achieve sustainable growth by optimizing people, technology, data and intelligence. Azure is the most comprehensive, innovative and flexible cloud platform today and talented professionals are needed to drive customer cloud adoption within the most important companies in the market. Key Responsibilities: Lead, drive and manage engagements for repeatable achievement of revenue and consumption targets. Leverage best practices to guide customer strategy and future growth by cultivating customer affinity with client programs/solutions that drive impact for the customer. Provide feedback on customer development needs, customer blockers, or mitigation strategies. Conduct analyses into what customers are using versus needs. Drive greater consumption (cloud and support) with customers based on analysis of both usage and needs. Leverage insights to provide guidance and recommendations to customers; drive, retain, and optimize customer consumption. Apply technical knowledge and customer insights to create a modernization roadmap. Architect solutions to meet business and IT needs, ensuring technical viability of new projects and successful deployments, while orchestrating key resources and infusing key Infrastructure technologies. Run Architectural Design Sessions to build plans for implementing solutions which align to customer business goals and technical environments. Collaborate and orchestrate with other Cloud Solution Architects and stakeholders, including FastTrack, Solution Assessments, etc., in developing cloud solutions on Azure. Basic Qualifications Proficiency in English and Turkish (C1 level required) 3+ years‚Äô experience in technical architect, technical consulting, design and implementation, and/or technical sales OR Bachelor's Degree in Computer Science, Information Technology, Engineering, or related field AND 4+ years‚Äô experience in technical architect, technical consulting, design and implementation, and/or technical sales OR Relevant certifications from Client or competitive platforms AND 3+ years‚Äô experience in technical architect, technical consulting, design and implementation, and/or technical sales OR equivalent experience Preferred Qualifications : Bachelor's Degree in Computer Science, Information Technology, Engineering, or related field AND 8+ years‚Äô experience in technical architect, consulting, design and implementation, and/or sales 3+ years‚Äô experience with cloud-based solution designs, migrations and management MSFT Ceritificates: AZ-305, AZ400, AI-102, DP-100 or DP-500. Research indicates that some candidates, especially the most diverse ones, may hesitate to apply for positions if they don't meet all requirements. If you believe you possess the necessary skills, even if not meeting every requirement, we wholeheartedly encourage you to submit your application. What we offer: 6-month employment contract with the possibility of extension for 1 year or an indefinite employment contract. Hybrid work model: 2 days remote, 3 days in-office.‚ÄãUsing foreign language and new technology solutions daily, cooperating with various global Clients. Individual support of a People Lead and a specific path of professional development, as well as the possibility of a session with a Coach. A wide training package (soft, technical, and language training offer, access to the e-learning platforms, Gallup test, GenAI training, possibility of co-financing courses, and certification). Employee Assistance Program - legal, financial, and psychological consultations. Accenture employees eligible for the Employee share purchase plan automatically become eligible for quarterly dividends if they own company shares. Paid employee referral program. Private medical care, life insurance. Access to the MyBenefit platform (possibility of using a wide range of products and services, including the Multisport card). What we believe: Accenture does not discriminate employment candidates on the basis of race, religion, color, sex, age, disability, national origin, political beliefs, trade union membership, ethnicity, denomination, sexual orientation or any other basis impermissible under Polish law. All our leaders are committed to building a better, stronger and more durable company for future generations to create positive, long-lasting change. Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and creative, which helps us better serve our clients and our communities. Our position as partner to many of the world‚Äôs leading businesses, organizations and governments affords us both an extraordinary opportunity and a tremendous responsibility to make a difference. Sustainability is one of our greatest responsibilities, which we embed it into everything we do and for everyone we work with.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Hybrid,112,Data Scientist ‚Äì Investment Management,ITDS,"Data Scientist ‚Äì Investment Management Join a world of innovation and data-driven excellence! This is a Wroc≈Çaw -based hybrid opportunity ‚Äì 3 days in the office per week (Relocation package available, including move and acocomodation) As a Data Scientist, you will be working for our client ‚Äì a global leader in quantitative and systematic investment management. You will be responsible for leveraging data to drive investment strategies and enhance trading decisions. Your main responsibilities: Collaborate with Quantitative Researchers and Traders to design impactful datasets. Prototype and design code for data extraction, cleaning, and aggregation. Work with Engineers to automate and optimize data processes. Manage the onboarding of new datasets from start to finish. Solve data-related challenges to expedite production timelines. Innovate with novel data extraction methods to enhance capabilities. You're ideal for the role if you have: 3+ years of experience as a Data Scientist; buy-side quantitative finance experience is a plus. A postgraduate degree in Mathematics, Physics, or Engineering. Advanced Python programming skills, with proficiency in Pandas and NumPy. A keen interest in financial markets and data analysis. Experience with traditional and alternative financial datasets. Excellent communication skills for effective stakeholder collaboration. Ability to thrive in a high-performance, fast-paced environment. We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7131 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 25000, ""max"": 30000, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,113,Data Modeller,ITDS,"Join Us and Build a Cutting-Edge Data Platform! As a Data Modeller, you will be working for our client in the debt collection sector, helping to build and maintain a robust, cloud-native data platform. The role focuses heavily on data modelling, requiring an expert who can translate conceptual business needs into logical and physical data models, build data contracts, and implement scalable ELT pipelines using Azure Databricks. Your main responsibilities: Design and maintain logical and physical data models based on DDD (Domain-Driven Design) principles Translate conceptual models and business glossaries into technical data structures for the Data Warehouse Perform data mapping and create data contracts between the Data Platform and source systems Collaborate with source system owners to define data contract requirements Work on data ingestion processes from source systems using various methods: Direct database queries (bulk read/CDC), API communication, Event streaming Implement ELT processes across Bronze, Silver, and Gold layers in Azure Databricks Ensure alignment of data models with business and analytical requirements You're ideal for this role if you have: Strong experience in Data Modelling (logical & physical), preferably in DDD-based environments Proven ability to work with Data Governance inputs: glossaries, conceptual models, HLD/LLD documentation Experience preparing and maintaining data contracts Solid knowledge of data ingestion techniques and working with source systems Experience with Azure Databricks (or similar cloud platforms like GCP) Ability to develop and maintain ELT pipelines in cloud-native environments Nice to have: Experience in writing clear technical documentation (e.g. data contracts, field definitions, extraction rules) Background in mapping source data to target DWH structures Ability to interpret and work with ERDs and relational models Knowledge of master data management practices Familiarity with dbdiagram.io Awareness of Data Quality, Data Lineage, and metadata management concepts Experience using tools like Azure Purview or other metadata management platforms","[{""min"": 1200, ""max"": 1500, ""type"": ""Net per day - B2B""}]",Data Science,Data Science
Full-time,Senior,Permanent or B2B,Remote,114,Senior Data Engineer with Python,Sii,"The Embedded Competency Center is an organizational unit of Sii that brings together almost 500 specialists. We are currently looking for a Data Engineer to support a strategic initiative within the Client‚Äôs division. The focus of the engagement is the integration of a trading platform into the European Power portfolio under the EGO (Energy Global Optimization) project. This is a high-impact role that requires strong technical capabilities and excellent communication skills, as the engineers will closely collaborate with cross-functional teams, including developers, testers, architects, and other stakeholders. Design and implement scalable data pipelines and integrations for the trading platform integration Develop and maintain services using Azure Functions and Databricks Work with streaming data using Kafka Collaborate closely with project stakeholders to understand requirements and ensure data solutions meet business needs Contribute to architecture discussions and propose efficient technical solutions Provide technical leadership and mentorship to the engineering team, ensuring best practices and high-quality deliverables Strong hands-on experience with: Kafka, Python, Databricks, Azure Functions Proven track record of working on complex, enterprise-level projects Excellent stakeholder engagement and communication skills Ability to collaborate effectively in a cross-functional, distributed team Fluent Polish required Residing in Poland required Experience in the energy or trading domain Familiarity with Shell‚Äôs EGO environment Great Place to Work since 2015 - it‚Äôs thanks to feedback from our workers that we get this special title and constantly implement new ideas Employment stability - revenue of PLN 2.1BN, no debts, since 2006 on the market We share the profit with Workers - over PLN 60M has already been allocated for this aim since 2022 Attractive benefits package - private healthcare, benefits cafeteria platform, car discounts and more Comfortable workplace ‚Äì class A offices or remote work Dozens of fascinating projects for prestigious brands from all over the world ‚Äì you can change them thanks to Job Changer application PLN 1 000 000 per year for your ideas - with this amount, we support the passions and voluntary actions of our workers Investment in your growth ‚Äì meetups, webinars, training platform and technology blog ‚Äì you choose Fantastic atmosphere created by all Sii Power People Send your CV Talk to us about your expectations Learn more about our projects and choose the best Start your adventure with Sii! Sii is the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We already employ more than 7 500 professionals and implement projects in a variety of industries for clients from many countries around the world. The Great Place to Work title, won 10 times in a row, proves that at Sii we create a friendly work environment. In a survey, as many as 90% of our employees responded that Sii is a great place to work, and 95% of them think we have a great atmosphere here.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,115,"Manager, Data",Allegro,"Manager, Data Location: Company: Allegro sp. z o.o. Team: Technology Contract Type: Employee We are looking for an experienced Manager, who will coordinate a team with various data roles (Data Analyst/Product Analyst/Data Scientist) that plays a key role in shaping products at Allegro. The perfect candidate should not only possess strong analytical skills and product intuition but also demonstrate business maturity and proactive initiative. Your responsibilities will include cooperation in the implementation of the Technology/Product strategy with Product Management and the creation and coordination of analytical roadmaps that are a response to multi-faceted challenges.. Minimum of 3 years of experience as a Manager of analytical team or a similar capacity Basic knowledge about E-Commerce business, UX and relevant technology (data processing, various algorithms (ie. recommendations systems, causal inference methods), software development process) The ability to communicate analysis results and recommendations clearly Exceptional collaboration skills, with an emphasis on clear and effective communication with various stakeholders The ability to identify problems and propose effective solutions Team management skills, with a focus on team development Self-organization and a commitment to meeting deadlines Proficiency in using SQL and statistical knowledge related with A/B test Proficiency in English at a minimum B2 level A hybrid work model that you will agree on with your leader and the team. We have well-located offices (with fully equipped kitchens and bicycle parking facilities) and excellent working tools (height-adjustable desks, interactive conference rooms) Annual bonus up to 20% of the annual salary gross (depending on your annual assessment and the company's results) A wide selection of fringe benefits in a cafeteria plan ‚Äì you choose what you like (e.g. medical, sports or lunch packages, insurance, purchase vouchers) English classes that we pay for related to the specific nature of your job Laptop with m1 processor, 32GB RAM, SSD - a 16‚Äù or 14‚Äù MacBook Pro or corresponding Dell with Windows (if you don‚Äôt like Macs) and other gadgets that you may need Working in a team you can always count on ‚Äî we have on board top-class specialists and experts in their areas of expertise A high degree of autonomy in terms of organizing your team‚Äôs work; we encourage you to develop continuously and try out new things Hackathons, team tourism, training budget and an internal educational platform, MindUp (including training courses on work organization, means of communications, motivation to work and various technologies and subject-matter issues) If you want to learn more, check it out This may also be of interest to you: Allegro Tech Podcast ‚Üí https: //podcast.allegro.tech/ Booklet ‚Üí https: //jobs.allegro.eu/pl/obszary-prac/tech-data/ Send in your CV and see why it is #goodtobehere!",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,116,Senior Data Engineer (Azure Cloud & Data Platforms),ERGO Technology & Services,"ERGO Technology & Services S.A. (ET&S S.A.) was established in January 2021 following the integration of ERGO Digital IT and Atena into one entity, leveraging both companies‚Äô strengths and best practices. As a part of ERGO Technology & Services Management AG, the technology holding of ERGO Group AG, we support millions of internal and external customers with state-of-the-art IT solutions to everyday problems. In October 2022, ET&S S.A. expanded its scope of operations by creating a Business Services unit to contribute in a new way to the growth of ERGO‚Äôs business. Acting as a co-partner and internal consultant, it adds non-IT value and supports the development of the entire ERGO Group, currently offering skills in reporting, analysis, actuarial, and input management. We are committed to fostering innovation and meeting the evolving needs of our clients worldwide. Discover how we implement AI, IoT, Voice Recognition, Big Data science, advanced mobile solutions, and business-related services to anticipate and address our customers‚Äô future needs. About the role We are seeking a highly skilled and motivated Senior Data & Analytics Engineer to join our IT Business Solutions ‚Äì Life & Health Reinsurance team. This role is pivotal in designing, developing, and supporting data-driven solutions that leverage modern cloud-native technologies. How you will get the job done designing, building, and maintaining scalable data pipelines and workflows using Azure Data Factory and Databricks developing and optimizing SQL-based transformations and Python scripts for ETL processes building and maintaining Power BI dashboards and reports, ensuring they are intuitive, performant, and aligned with business KPIs collaborating with data scientists and actuaries to support ML model development, including data preparation, feature engineering, and model deployment pipelines integrating data from various sources into centralized data platforms for analytics, reporting, and ML use cases ensuring that data solutions are secure, compliant, and aligned with enterprise architecture standards participating in code reviews, mentoring junior engineers, and contributing to continuous improvement initiatives supporting production data systems as part of a DevOps model (build, own, run) Skills and experience you will need fluency in English Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Data Engineering, or a related field 5+ years of experience in data engineering or analytics engineering roles proficiency in Databricks SQL, Azure Data Factory, data pipeline development, and Power BI reporting strong experience with Python scripting for ETL and data transformation tasks proven experience with Azure Cloud services, including Azure Data Lake, Azure Synapse, and Azure Functions strong understanding of data modeling, ETL/ELT processes, and data governance principles hands-on experience with CI/CD pipelines using Azure DevOps for data workflows strong experience with relational databases, including SQL Server and PostgreSQL experience designing and implementing event-driven architectures proven ability to collaborate effectively with actuaries and data scientists in a matrixed and Agile organization with geographically distributed teams solid understanding of DevOps principles, automated testing, and monitoring practices‚Äã Nice to have exposure to Azure Bicep, ARM templates, or other IaC tools experience with API security best practices and zero-trust architectures experience with additional cloud platforms (AWS, GCP) familiarity with monitoring and observability tools such as Azure Monitor, Prometheus, Grafana, and Datadog familiarity with NoSQL databases prior experience working in a product team setup, contributing to iterative delivery and long-term product ownership Medical package, sports card, and numerous sports sections ‚Äì these are some of the beneÔ¨Åts that help our employees stay in good shape. Work-life balance is a key aspect of a healthy workplace. We offer our employees flexible working hours, a confidential employee assistant program, as well as the possibility of remote working. However, staying at home with our in-office gaming room and dog-friendly office in Warsaw won‚Äôt be easy. We organize numerous workshops and training courses. Thanks to hackathons and meetups, our specialists share their expertise with others. Additionally, we have a wide range of digital learning platforms and language courses. Each year, we participate in several CSR activities, during which, together with our colleagues, we do our best to create a better future. Company-wide bike races and soccer matches, Ô¨Ålm marathons in our cinema room or other engaging team-building activities ‚Äì we got it covered! Every team member is valued, regardless of gender, nationality, religious beliefs, disability, age, and sexual orientation or identity. Your qualiÔ¨Åcations, experience, and mindset are our greatest beneÔ¨Åt!",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,117,Data Engineer,Ework Group,"üíª Ework Group - founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking for Data Engineer üîπ ‚úîÔ∏è Ideally, candidates should be available to work from Wroc≈Çaw office (hybrid) ; however, remote work is also possible. ‚úîÔ∏èYour responsibilities include: Develop and enhance application features that support internal users and business processes. Diagnose and resolve issues related to data quality and system maintenance. Ensure efficient and reliable operation of the organization‚Äôs data processing systems. Design and implement solutions using cloud platforms ‚Äì preferably Microsoft Azure. Work hands-on with Databricks (primarily PySpark) for big data processing and analysis. Manage and optimize databases such as SQL Server or Netezza. Implement and integrate solutions using services such as: Azure Data Factory, Databricks, PySpark, Python. Work in a diverse and rapidly changing landscape of data sources. Collaborate within an Agile/Scrum team environment, following modern software development practices. Take initiative and ownership of tasks while maintaining accuracy and accountability for results. Communicate and collaborate effectively in an international environment using English. Contribute to a knowledge-sharing and team-oriented culture. ‚úîÔ∏èYour experience and background: You are graduated with a Master's degree in Computer Science, Data Engineering or any related field Data & Engineering has been your world for at least 3-5 years Cloud service platform expertise; preferably Azure Hands-on experience working with Databricks (PySpark) Databases (like SQL Server or Netezza) don‚Äôt have any secret for you Knowledge of at least three of these services: Azure Data Factory, Azure Synapse, Databricks, Azure SQL Database, Power BI, Spark, Python, Mongo DB, Azure Functions You are comfortable working in a diverse, complex and fast-changing landscape of data sources You care about agile software processes, data-driven development, reliability, and responsible experimentation. Capacity to work in an international environment using English Team player - having a sharing and collaborative mindset Autonomous, Rigorous, takes ownership at work Agile - flexibility to execute available tasks in a rapidly changing environment ‚úîÔ∏èSoft Skills: Excellent communication skills Ability to understand business stakeholders and the business landscape Ability to challenge decisions made by the platform or other architects Strong analytical, organizational, problem-solving, and time-management skills Comfortable working in a diverse, complex, and fast-changing landscape of data sources Proactive problem solver with innovative thinking and a strong team player Fluent in English (C1) ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events Contact person: karolina.rosikiewicz@eworkgroup.com(","[{""min"": 115, ""max"": 128, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Manager / C-level,Permanent or B2B,Remote,118,Data Engineering Team Leader,Profitroom,"We are currently looking for an experienced Data Engineering Team Leader to join our Data Team and help us make our company even more data-driven. The results of your work will directly impact product development, the way we support our customers, and influence our high-level business strategy. If you are ready to take initiative and believe in data-driven decision-making ‚Äì this role is perfect for you! Serve as a technical and team leader in the Data Engineering team: Lead team development and foster soft people management Support career paths and mentor team members Act as a Delivery Manager for data-related projects: Define, prioritize, and ensure timely delivery of engineering tasks Take ownership of major technical decisions and engineering excellence: Establish and enforce standards for testing, code review, CI/CD, documentation, and monitoring Oversee the architecture of our data platform: Maintain and evolve our Lakehouse infrastructure (preferably Databricks-based) Introduce new tools and technologies aligned with business and technical goals Supervise the logical structure and modeling of data: Ensure semantic and architectural consistency of datasets Leverage approaches such as the Kimball model or Medallion architecture Contribute to coding: Develop and optimize data pipelines using Dagster, Python, and PySpark Collaborate cross-functionally with data analysts, PMs, and other business stakeholders Drive and mature data governance practices, including cataloguing and lineage Minimum of 3+ years of experience as a Data Engineer or in a similar role related to data Experience in leading technical teams or managing data projects = at least 1 year (a big advantage) or willingness to grow into it Strong knowledge of Python, PySpark (nice to have), orchestration tools like Dagster or Airflow and SQL Experience working with cloud data platforms (Databricks experience is a plus) and Lake/Lakehouse architectures Ability to define and uphold high engineering standards and processes Proficiency in data modeling (Kimball, Medallion) Excellent communication skills (English at B2+ level) Strong ownership and self-organization Nice to have: Experience with Databricks and Delta Lake Familiarity with tools such as DataHub, Terraform, Docker Background in implementing data governance, lineage, and quality frameworks Python, PySpark, SQL, Databricks, GCP (BigQuery), Dagster, Airflow, Delta Lake, Docker, Terraform, DataHub Enjoy Work-Life Balance: Embrace a fully remote and flexible work environment. Explore the World: Avail annual 'Work with Us, Travel with Us' vouchers. Grow Your Skills: Access to English language classes along with a dedicated team development fund. Stay Healthy: Benefit from co-financed life and medical insurance, access sports facilities and receive professional mental health support whenever needed. Take Time Off: Get 26 days off with a Contract of Employment and 24 days off break with B2B contracts. Share hospitality: Take 2 extra days off (annually) for CSR activities. Join Celebrations: Participate in company retreats, events, and wedding & baby packs, benefit from our employee referral program. Transparent Culture: Experience a flat hierarchy and open communication channels for transparency. Contract Enhancements: earn between 25 500 PLN to 30 000 PLN on a B2B contract or between 21 000 to 25 000 PLN gross for Contract of Employment. About Us: We're a global leader in hospitality software, founded in Pozna≈Ñ, Poland in 2008. We‚Äôve grown to serve over 3,500 customers across five continents, helping hotels and resorts maximize their revenue and guest satisfaction.","[{""min"": 25500, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,119,Data Engineer,HSBC Service Delivery,"Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity HSBC Markets and Securities Services (MSS) is an emerging markets-led and financing-focused business that provides tailored financial solutions to major government, corporate and institutional clients worldwide. The role is within MSS IT - a department of HSBC Technology Services (HTS) - providing IT application development and support services to Business and IT teams within HSBC MSS. Our businesses generate millions of transactions across a range of financial markets. Our teams design and develop modern systems covering all aspects - trading in the markets, sales and customer tools, handling transactions, data analytics and building financial and regulatory reporting. You will be working with one of the most cutting-edge languages and timeseries databases (KDB+/q), where speed is crucial, and volume is exceptionally large. You can grow to be a developer that can individually handle tasks from gathering requirements, through analysing data, to implementation and testing - having support from senior colleagues along the way. This eRisk Trading MI (Management Information) team designs and develops a fast real-time, time series database (KDB+/q) and application processing Terabytes of data per day - translating to billions records per day - providing services for performing quantitative analysis on vast amounts of financial markets order, trade and pricing data. The team is part of the wider eRisk IT team, which develops pricing, trading, hedging and electronic algorithmic order systems for Global Foreign Exchange (GFX). The application‚Äôs usage has a broad scope: it is used by front office business quantitative analysts for research and back-testing of trading algorithms, employed for the testing and operation of algorithmic trading systems and used for a wide range of data analytics as well as client, regulatory and internal management information. There are increasing demands for services built within the application. The successful candidate will join a team of 5 created in our Krakow office, growing our existing global team, based in Hong Kong and London. What you‚Äôll do Develop software and engineer services which handle billions of records, analyse large datasets and find value in the data. Work on the prominent timeseries technology in financial markets. Learn from scratch/build expertise in a functional programming and query language (KDB+/q) and how this is applied to data engineering. Collaborate with and be part of a diverse and global team who focuses on fast delivery and user experience. Understand the business context of what we do and engage directly with Traders and Quantitative Analysts as well as other Technologists. What you need to have to succeed in this role An analytical mind and be excited about problem-solving complex data engineering problems. Strong computer science fundamentals or have worked with functional programming languages (e.g. Clojure, Haskell, Scala) or enjoy vectorisation in Python, R, Matlab/Octave, Wolfram Mathematica. A demonstrable passion for technology and automation, optimising and improving solutions. Demonstrable interest in understanding Data and how to combine and present it in various ways, dealing with large datasets and some basic statistical concepts. Knowledge of or interest in learning about Linux, Server-Client technology, Cloud infrastructure, GCP. Aptitude for understanding requirements and solving problems through proactive and collaborative engagement with teammates and owning a change from inception, to testing and releasing it to the users. What we offer Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery and kindergarten discounts Financial support with trainings and education Social fund Flexible working hours Free parking If your CV meets our criteria, you should expect the following steps in the recruitment process: Online behavioural test Telephone screen Job Interviews with the hiring manager We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Remote,120,Mid/Senior Data Engineer,Prosoma,"Location: Remote w/ periodic meetings in Poland (Development Headquarters in Poland) Seniority: Mid/Senior Company Overview: Prosoma is a medtech startup delivering digital healthcare solutions for psycho-oncology. Our products provide accessible, supportive care to patients to improve their overall treatment effectiveness while generating savings for the stakeholders across the healthcare system. Role Summary: As a Data Engineer, you will play a crucial role in designing, implementing, and maintaining robust data solutions within our Azure-based ecosystem. You'll leverage your expertise in Python, SQL, and Azure services to build scalable data pipelines, optimize ETL processes, and ensure data integrity. Working closely with cross-functional teams in Central Europe and the United States, you'll contribute to data architecture decisions, implement best practices in data security and performance, and support business insights through advanced analytics. Your strong technical skills, coupled with your ability to translate business needs into effective data solutions, will be key to driving our data-driven initiatives forward. Responsibilities: Work with cross-functional teams in Central Europe and the United States to support data-related projects and translate business needs into data architecture. Implement data architecture for large-scale systems using Azure services. Design and maintain scalable data pipelines, integrating diverse, distributed global data sources in multiple regulatory environments into a unified format. Develop and optimize ETL processes using Python and SQL for efficient data migration and transformation. Implement automated data quality checks to ensure data integrity throughout its lifecycle. Implement and maintain data security best practices within the Azure cloud ecosystem. Maintain comprehensive documentation for data processes and contribute to the data migration playbook. Expand the billing system based on analytics data. Manage CI/CD pipelines for streamlined code deployment. Requirements: Minimum of 3 years of experience as a Data Engineer. Strong proficiency in Python. Hands-on experience in data gathering, cleaning, and processing; data visualization skills are a plus. Strong understanding of data warehouse concepts and experience in data modeling within a data lake pattern. Proficiency in SQL and data analysis, with knowledge of relational databases (preferably SQL Server and PostgreSQL). Experience with automated unit testing and code quality inspection. Extensive experience with Cloud Vendors, for example Azure, including in-depth knowledge of Azure architecture and services. Familiarity with CI/CD pipelines, preferably using Github Actions or Azure DevOps. Knowledge of public cloud architecture, security, networking concepts, and best practices (Microsoft Azure preferred). Nice to Have: Experience with AI tools and deploying AI based applications will be a big plus. Experience with Apache Spark, Azure Data Factory or Databricks platform. Experience with Apache Airflow, or similar. Previous experience in the medical devices domain, or another regulated area. Languages: Fluency in Polish is required. Communicative in English. What We Offer : Work (UoP) contract. Opportunity to make an impact within the company (regular knowledge sharing, collaborative environment that encourages forward-thinking and initiative). Competitive salary. Potential to delve into the details of healthcare technology, including regulatory requirements, certification processes, and unique market characteristics. Fully remote work with flexible hours. A chance to be part of a growing international presence and contribute to our global expansion. How to Apply: If you are passionate about using your technical skills to make a positive impact in the healthcare sector, we would love to hear from you. Please submit your resume outlining your relevant experience and let us know why you are the perfect fit for this role. Join us in our mission to revolutionize healthcare with technology!",[],Data Engineering,Data Engineering
Full-time,Senior,Any,Remote,121,Data Engineering Tech Lead (Oracle+Azure),Lingaro,"Growth through diversity, equity, and inclusion. As an ethical business, we do what is right ‚Äî including ensuring equal opportunities and fostering a safe, respectful workplace for each of us. We believe diversity fuels both personal and business growth. We're committed to building an inclusive community where all our people thrive regardless of their backgrounds, identities, or other personal characteristics. Tasks: Lead the design, development, and maintenance of scalable, reliable data pipelines and infrastructure components to support product features and analytics. Collaborate closely with product managers, data scientists, and software engineers to translate product requirements into robust data solutions. Architect and implement data exchange processes, ensuring data quality, governance, and compliance with best practices. Mentor and guide data engineering team members, fostering technical growth and maintaining high coding and design standards. Drive adoption of modern data technologies and cloud-based platforms to enhance performance, scalability, and cost-efficiency. Own the end-to-end data lifecycle, from ingestion, processing, transformation to storage and accessibility. Establish monitoring, alerting, and performance tuning strategies to ensure data pipeline reliability and efficiency. Participate in Kanban planning meetings, code reviews, and continuous integration/continuous deployment (CI/CD) workflows within the product team. Communicate technical decisions and progress effectively to both technical and non-technical stakeholders. Stay current with industry trends and emerging technologies to continuously improve the data engineering capabilities of the product team. Requirements: Technical Skills: Strong experience in SQL, PL/SQL and Data Modeling . Hands-on experience with Oracle databases. Practical knowledge of Apache Spark ecosystem, including Delta , Parquet data formats and Spark SQL . Experience with CI/CD automation processes and tools such as Azure DevOps . Familiarity with Azure Cloud Services . Understanding of Security & Compliance best practices, including Security by Design . Soft Skills: Excellent attention to detail , strong prioritization and time management skills. Strong problem-solving and communication abilities. Ability to take initiative and provide mentorship to team members. Good presentation skills for internal and external stakeholders. Missing one or two of these qualifications? We still want to hear from you! If you bring a positive mindset, we'll provide an environment where you feel valued and empowered to learn and grow. We offer: Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,122,Programista / Programistka BI,Leroy Merlin Polska,"Jakie zadania na Ciebie czekajƒÖ? Zaawansowana analiza i wizualizacja danych w najnowszych technologiach oraz tworzenie raport√≥w BI; Projektowanie i implementacja proces√≥w zasilania hurtowni danych; Wsp√≥≈Çpraca z u≈ºytkownikami biznesowymi w definiowaniu potrzeb oraz przek≈Çadaniu je na procesy analityczne Udzia≈Ç w innowacyjnych projektach biznesowych prowadzonych w ≈õrodowisku miƒôdzynarodowym. Kogo szukamy? Posiadasz minimum 2 lata do≈õwiadczenia w tworzeniu raport√≥w BI oraz optymalizacji zapyta≈Ñ w SQL; Potrafisz tworzyƒá przep≈Çywy integracyjne w Microsoft SSIS, Google Data Flow lub Cloud Composer / Air Flow; Masz do≈õwiadczenie w analizie danych, budowie Data Marts oraz definiowaniu metryk na podstawie danych (KPI); Swobodnie komunikujesz siƒô w jƒôzyku angielskim min. B2; Potrafisz w czytelny spos√≥b komunikowaƒá kwestie z≈Ço≈ºonych danych oraz zale≈ºno≈õci Masz wykszta≈Çcenie wy≈ºsze. Mile widziane: Znasz architekturƒô chmury Google i BigQuery; Potrafisz tworzyƒá raporty w Power BI, Data Studio lub Looker. Oferujemy: Hybrydowy model pracy; Samodzielno≈õƒá dzia≈Çania, realny wp≈Çyw na projekty, r√≥wnie≈º te miƒôdzynarodowe - nuda i rutyna sƒÖ nam obce; Bezpo≈õredni udzia≈Ç w technologicznej transformacji naszej firmy; NieformalnƒÖ atmosferƒô pracy - mamy w sobie luz, skracamy dystans Udzia≈Ç w programach rozwojowych - bo dla nas Tw√≥j rozw√≥j jest wa≈ºny (zar√≥wno ekspercki, jak i liderski); Dostƒôp do specjalistycznych szkole≈Ñ, kurs√≥w Google / Coursera i nie tylko; Akcjonariat pracowniczy, udzia≈Ç w zyskach przedsiƒôbiorstwa oraz premie kwartalne uzale≈ºnione od wynik√≥w sieci; Bogaty pakiet benefit√≥w pozap≈Çacowych: kafeteria benefit√≥w MyBenefit, prywatna opieka medyczna, ubezpieczenie na ≈ºycie, wsparcie finansowe przy zakupie pierwszego mieszkania, mo≈ºliwo≈õƒá nauki jƒôzyk√≥w obcych poprzez Platformƒô goFLUENT dla Ciebie i Twoich bliskich, dostƒôp do platformy psychologicznej, edukacyjnej i wellbeingowej Mindgram ( z nielimitowanym dostƒôpem do konsultacji ze specjalistami - psychologowie, terapeuci, trenerzy profilaktyki zdrowia); Zni≈ºkƒô pracowniczƒÖ na zakupy w sklepach LMPL.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Office,123,Business Intelligence Analyst (Data Quality),ROCKWOOL GLOBAL BUSINESS SERVICE CENTER,"Ready to help build a better future for generations to come? In an ever-changing, fast paced world, we owe it to ourselves and our future generations to live life responsibly. At ROCKWOOL, we work relentlessly to enrich modern living through our innovative stone wool solutions. Join us and make a difference! Your future team: We are looking for two data specialists for positions as a Customer Business Intelligence Analyst and a Product Business Intelligence Analyst based in our Pozna≈Ñ location to join a newly founded GMC Master Data Management Team. You will be reporting to the Director of Customer and Product Data Management. What you will be doing: In this position, you'll ensure that sales and product processes are driven by accurate, high-quality data, leveraging analysis and AI to elevate ROCKWOOL's data management. Key Responsibilities: Data Governance & Quality: Develop and enforce policies to uphold data integrity, collaborating with Business Process Owners (BPOs). Provide regular reports and oversee data quality improvements alongside data stewards. Dashboarding & Monitoring: Design and implement dashboards to track data quality progress during the CEP design and rollout phases. Strategic Analysis: Analyze customer and product data to enhance customer journeys and sales efficiencies, supporting ROCKWOOL‚Äôs growth objectives. Process Management: Collaborate with process framework teams to ensure standardized documentation and workflows. Industry Insights: Utilize knowledge of digital marketing, sales processes, and the construction industry to inform strategic decisions and optimize data management. Change Management: Support change initiatives by coaching stakeholders and developing training materials for new policies and processes. Stakeholder Engagement: Partner with BPOs and organizational stakeholders to understand data requirements and secure their buy-in. Collaboration: You‚Äôll work closely with GMC's Digital Marketing & Sales, Product Management teams, and ROCKWOOL's local markets, engaging cross-functionally with master data teams within the ROCKWOOL Group . What you bring: University degree within computer science, data science, information technology or related field. 5+ years of experience within a global company with implementing data validation and cleansing processes to maintain data quality. Strong analytical, data modelling and problem-solving skills. Strong communication and facilitation skills. Excellent verbal and written English skills. Experience or familiarity with the following areas will be an asset: business process frameworks product management, marketing and/or CRM tools the construction industry programming languages, database management and big data technologies What we offer: By joining our team, you become a part of the people-centric work environment of a Danish company. We offer you a competitive salary, permanent contract after the probation period, development package, team building events, activity-based office in Poznan‚Äôs city centre in the new prestigious office building ‚Äì Nowy Rynek. The building is recognized as a building without barriers, which means that it is fully adapted to the needs of people with disabilities. Our compensation package on employment contracts includes: An office-first approach: home office is available up to 2 days per week Adaptable Hours: start your workday anytime between 7: 00 AM and 9: 00 AM Home office subsidy Private Medical Care Multikafeteria MyBenefit Wellbeing program Extra Day Off for voluntary activities ‚Ä¶ and while in the office you can also use modern office space with beautiful view and high standard furniture, bicycle parking facilities & showers, chill-out rooms with PlayStation, football table, pool table, board games, subsidized canteen with delicious food & fruit. Interested? If you recognize yourself in this profile and challenge, we kindly invite you to apply with CV written in English. Who we are We are the world leader in stone wool solutions. Founded in 1937 in Denmark, we transform volcanic rock into safe, sustainable products that help people and communities thrive. We are a global company with more than 12,200 employees, located in 40+ countries with 51 manufacturing facilities‚Ä¶ all focused on one common purpose ‚Äì to release the natural power of stone to enrich modern living. Sustainability is central to our business strategy. ROCKWOOL was one of the first companies to commit to actively contributing to the United Nations Sustainable Development Goals (SDGs) framework and are actively committed to 11 SDGs, including SDG 14, Life Below Water. Through our partnership with the One Ocean Foundation and in connection with our sponsorship of the ROCKWOOL Denmark SailGP team, we will help raise awareness around ocean health challenges in an effort to accelerate solutions to protect it. Diverse and Inclusive Culture We want all our people to feel valued, respected, included and heard. We employ 79 different nationalities worldwide and are committed to providing equal opportunities to all employees, promote diversity, and work against all forms of discrimination among ROCKWOOL employees. At ROCKWOOL, you will experience a friendly team environment. Our culture is very important to us. In fact, we refer to our culture as ‚ÄúThe ROCKWOOL Way‚Äù. This is the foundation in which we operate and is based upon our values of ambition, responsibility, integrity and efficiency.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,124,In≈ºynier Baz Danych,SCS Expert Sp. zo.o.,"Dla naszego klienta, firmy SAS Institute, poszukujemy In≈ºyniera Baz Danych Twoje codzienne obowiƒÖzki bƒôdƒÖ skupiaƒá siƒô na: Zapewnianiu niezawodno≈õci i stabilno≈õci system√≥w bazodanowych: Proaktywne monitorowanie i analiza wydajno≈õci baz danych, identyfikacja wƒÖskich garde≈Ç i wprowadzanie optymalizacji. Wdra≈ºanie strategii Disaster Recovery (DR) oraz zarzƒÖdzanie kopiami zapasowymi i procedurami odtwarzania danych, aby minimalizowaƒá ryzyko utraty danych i przestoj√≥w. Aktywne diagnozowanie i rozwiƒÖzywanie z≈Ço≈ºonych problem√≥w, przeprowadzanie analiz przyczyn ≈∫r√≥d≈Çowych (Root Cause Analysis ‚Äì RCA) po incydentach, aby zapobiegaƒá ich ponownemu wystƒÖpieniu. Definiowanie i monitorowanie Service Level Objectives (SLO) oraz Service Level Indicators (SLI) dla baz danych. Automatyzacji i Infrastructure as Code (IaC): Projektowanie i implementacja rozwiƒÖza≈Ñ automatyzujƒÖcych procesy zarzƒÖdzania bazami danych, wdra≈ºania zmian oraz reagowania na incydenty (np. przy u≈ºyciu skrypt√≥w w Python/Shell, narzƒôdzi do automatyzacji). Rozwijanie i utrzymywanie infrastruktury baz danych jako kodu (Infrastructure as Code) z wykorzystaniem narzƒôdzi takich jak Ansible, Terraform. Integracja proces√≥w bazodanowych z potokami CI/CD w celu przyspieszenia i usprawnienia wdro≈ºe≈Ñ. Wsp√≥≈Çpracy i optymalizacji: ≈öcis≈Ça wsp√≥≈Çpraca z zespo≈Çami deweloperskimi w celu projektowania i wdra≈ºania rozwiƒÖza≈Ñ bazodanowych z my≈õlƒÖ o skalowalno≈õci, wydajno≈õci i niezawodno≈õci ju≈º na etapie architektury. Doradztwo techniczne w zakresie najlepszych praktyk zwiƒÖzanych z bazami danych, optymalizacjƒÖ zapyta≈Ñ i schemat√≥w. Tworzenie i utrzymywanie szczeg√≥≈Çowej dokumentacji technicznej, kluczowej dla zarzƒÖdzania wiedzƒÖ i operacjami. ZarzƒÖdzaniu bezpiecze≈Ñstwem: Wdra≈ºanie i nadzorowanie polityk bezpiecze≈Ñstwa baz danych oraz mechanizm√≥w uwierzytelniania i autoryzacji. Czego oczekujemy? Do≈õwiadczenia na podobnym stanowisku, w tym znajomo≈õci bezpiecze≈Ñstwa baz danych i mechanizm√≥w uwierzytelniania Praktycznego do≈õwiadczenia w: administracji systemami Linux/Unix i sieci administracji i orkiestracji Kubernetes instalacji, konfiguracji i zarzƒÖdzania bazami danych pracy z platformami chmurowymi Azure i AWS, w tym us≈Çugami bazodanowymi Znajomo≈õci architektury baz danych, wzorc√≥w projektowych i najlepszych praktyk Znajomo≈õci SQL (np. PL/SQL lub podobnych jƒôzyk√≥w proceduralnych) Umiejƒôtno≈õci tworzenia skrypt√≥w i automatyzacji w Shell, Python lub innych jƒôzykach Znajomo≈õci mechanizm√≥w monitoringu i alertowania dla baz danych (np. Prometheus, DataDog, OEM) Wykszta≈Çcenia wy≈ºszego w obszarze informatyki, technologii informacyjnej lub pokrewne Doskona≈Çych umiejƒôtno≈õci interpersonalnych, komunikacyjnych i rozwiƒÖzywania problem√≥w Proaktywnego podej≈õcia do identyfikowania i eliminowania potencjalnych awarii Zdolno≈õci do efektywnej wsp√≥≈Çpracy z zespo≈Çami deweloperskimi i u≈ºytkownikami ko≈Ñcowymi Dodatkowe atuty: Do≈õwiadczenie z systemami RDBMS (Oracle, MS SQL Server, MySQL, PostgreSQL) Praktyczna znajomo≈õƒá SingleStore, Redis lub Kafka Praca z narzƒôdziami takimi jak SAS, Toad, Erwin czy innymi do projektowania i dostƒôpu do baz danych Do≈ÇƒÖcz do nas i rozwijaj siƒô w jednej z najbardziej innowacyjnych firm na rynku! Oferujemy: Mo≈ºliwo≈õƒá rozwoju zawodowego i podnoszenia kwalifikacji w miƒôdzynarodowym ≈õrodowisku KluczowƒÖ rolƒô w zespole kszta≈ÇtujƒÖcym niezawodno≈õƒá system√≥w IT w renomowanej firmie Atrakcyjne wynagrodzenie i zatrudnienie w oparciu o elastyczne formy wsp√≥≈Çpracy Dostƒôp do nowoczesnych technologii i narzƒôdzi Dostƒôp do opieki medycznej, ubezpieczenia grupowego i karty sportowej po atrakcyjnych cenach",[],Database Administration,Database Administration
Full-time,Mid,B2B,Remote,125,Data Engineer with Palantir,Link Group,"About the Role We are looking for a Data Engineer experienced with Palantir Foundry to join a cross-functional team working on large-scale data integration, modeling, and analytics platforms. The ideal candidate is hands-on, proactive, and capable of navigating complex data ecosystems in an enterprise environment. Design and build data pipelines and models using Palantir Foundry Integrate multiple data sources (structured and unstructured) into usable, high-quality data assets Collaborate with data scientists, analysts, and business stakeholders to support advanced analytics initiatives Apply data governance, lineage, and cataloging principles within Foundry Develop and maintain Foundry ‚ÄúObjects‚Äù, Code Workbooks, and other tooling Ensure quality, performance, and scalability of the implemented data solutions Support and document platform usage and development best practices 3+ years of experience in Data Engineering Hands-on experience with Palantir Foundry in a commercial or enterprise setting Proficiency in SQL , Python , and data transformation techniques Good understanding of data modeling (dimensional, relational, and graph-based) Familiarity with data governance and metadata management Experience working in cloud-based environments (AWS, GCP, or Azure) Excellent communication skills and ability to work with cross-functional teams Previous experience in highly regulated industries (finance, pharma, defense, etc.) Experience integrating Foundry with external tools and systems via APIs Knowledge of CI/CD , Git , and software engineering best practices Exposure to tools like Airflow , dbt , Databricks , Snowflake , etc. Experience with data privacy regulations (GDPR, HIPAA, etc.)","[{""min"": 90, ""max"": 105, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,126,Senior Data Engineer,SoftBlue,"As a Data Engineer, you will support the development of ETL processes, collaborate with BI developers and data scientists on data & analytics solutions, and manage data ingestion and processing within a data lake. You'll be responsible for ensuring data quality, implementing best practices, and leveraging ways of working like version control and CI/CD. With a strong foundation in data engineering and familiarity with the Cloud and Databricks platforms, you'll play a key role in enhancing our data infrastructure and optimizing cloud costs. If you have a passion for data engineering and want to accelerate your development curve, we would love to hear from you. Your responsibilites: Supporting the development of data infrastructure to extract, transform, and load (ETL/ELT) data, Supporting BI developers and data scientists to build data & analytics solutions, Responsible for the ingestion and processing of data in the data lake, Responsible for periodic data refreshes through data pipelines (scheduled jobs), Implementing data management practices to improve data quality and meta data in the data lake, Leveraging software engineering best practices such as version control (Git) and CI/CD (DevOps), Continuously strengthening the data foundation through updating the data infrastructure, Monitoring the cost associated with the cloud environment, data processing, and data computation. Our requirements: 5+ years of experience in data engineering or a similar role, At minimum a bachelor‚Äôs degree in computer science, information technology or a related field, Familiarity with the Cloud Platforms (e.g. Azure, GCP, AWS), Working knowledge of Databricks and Delta Lake , Experience with relational databases (e.g., SQL Server, PostgreSQL), Proficiency in coding (e.g., Python, PySpark and SQL), Experience with version control (Git) and CI/CD, Very good knowledge of English language - minimum B2 level, Ability to translate business requirements to code. Nice to have: Experience in building solution using Data Mesh Approach, Working knowledge of dbt, Experience with snowflake database, Familiarity with Airflow. We offer: Challenging role within the company that creates innovative solutions, Work in international environment on demanding projects, Remote work model, Subsidized private medical care, life insurance, multisport card, Integration meetings, Employee referral program. Sound like a good fit? Don‚Äôt wait ‚Äì send in your CV and let‚Äôs talk!","[{""min"": 140, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,127,Senior Data Engineer,HSBC Service Delivery,"Leadership and Team Management: Lead, mentor, and develop a team of data engineers, establish best practices for data engineering and ensure team adherence, coordinate with other IT teams, business units, and stakeholders. Data Pipelines Integration and Management. Design and implement scalable data architectures to support the bank's data needs. Develop and maintain ETL (Extract, Transform, Load) processes. Oversee the integration of diverse data sources into a cohesive data platform. Ensure data quality, data governance, and compliance with regulatory requirements, ensure the data infrastructure is reliable, scalable, and secure, develop and enforce data security policies and procedures. Monitor and optimize data pipeline performance. Troubleshoot and resolve data-related issues promptly. Implement monitoring and alerting systems for data processes. What you need to have to succeed in this role Strong experience with database technologies (SQL, NoSQL), data warehousing solutions, and big data technologies (Hadoop, Spark). Proficiency in programming languages such as Python, Java, or Scala. Experience with cloud platforms (AWS, Azure, Google Cloud) and their data services. Certifications in cloud platforms (AWS Certified Data Analytics, Google Professional Data Engineer, etc.) as a plus. Deep understanding of ETL processes and data pipeline orchestration tools (Airflow, Apache NiFi). Knowledge of data modelling, data warehousing concepts, and data integration techniques. Strong problem-solving skills and ability to work under pressure and excellent communication and interpersonal skills. Experience in the banking or financial services industry and familiarity with regulatory requirements related to data security and privacy in the banking sector. Experience with machine learning and data science frameworks. What we offer Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery discounts Financial support with trainings and education Social fund Flexible working hours Free parking If your CV meets our criteria, you should expect the following steps in the recruitment process: Online behavioural test Telephone screen Job interview with the hiring manager We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,128,Senior Data Analyst - Master Data,Bayer Sp. z o.o.,"Senior Data Analyst ‚Äì Master Data Are you ready to make a significant impact in the world of data analytics and data management? We are seeking a talented Data Analyst to become a vital part of our dynamic Data Assets, Analytics, and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in building and maintaining our core data assets across various domains, ensuring their completeness, semantics and quality. You will work closely with data owners, product managers, data engineers, data architects, data stewards, data governors and data scientist to enable our data analytics solutions, enhance the strategic value of our data assets and enable cutting-edge AI solutions and support data-driven decision-making. If you‚Äôre passionate about transforming data into actionable insights and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Collaborate with data owners, architects, engineers, stewards, governors, scientists and product managers to understand objectives and requirements for data assets. Serve as a liaison between technical teams and business stakeholders to ensure data assets meet both business requirements and technical standards. Take ownership of a data asset roadmap. Co-develop data strategy and governance frameworks. Proactively identify new datasets from across the organization and collaborate with data architects and data engineers to integrate them into the core data assets Define transformation logic and enrich data to create valuable KPIs and features in the consumption layer. Develop and maintain clear semantics and metadata for data assets, ensuring that all data is well-documented and easily understandable. Ensure data quality, availability, and completeness through implementation of quality checks, validation processes, and continuous monitoring in collaboration with data architects and data engineers. Serve as the primary point of contact for users of data assets, providing guidance and support to help them understand and utilize the data effectively. Analyze complex datasets to extract actionable insights that streamline the development of analytics and AI solutions. Forge the way Bayer Consumer Health manages its master data. Become a go-to expert for master data. Qualifications & Competencies: Master's degree in Statistics, Computer Science, Data Management, Data Science or a related field. 5+ years of experience as a Data Analyst or Data Steward, preferably within the consumer-packaged goods, FMCG, pharmaceutical or healthcare industry. Strong knowledge of data management principles, data quality frameworks, and metadata management practices and tools. Understanding of data lineage and data cataloging concepts. Business acumen in the area of product master data. Familiarity with master data management tools (e.g. Reltio Cloud MDM, Syndigo Enterprise Data Suite), SAP modules related to master data management (e.g. Material Management, Product Lifecycle management) and master data aspects of the CRM systems (e.g. IQVIA OCE-P, Salesforce). Experience with data manipulation and analysis using Azure Databricks, SQL and Python. Familiarity with relational databases (PostgreSQL, MSSQL) and data modelling. Excellent analytical and problem-solving skills with a keen attention to detail. Strong understanding about data compliance & security standards such as data privacy regulations (GPDR, HIPAA), EU AI Act, management of confidential data, and experience with measures to mitigate data risks. Strong communication skills, with the ability to present complex data in a clear and understandable manner. Interest and experience with AI tools supporting data analysis and stewardship is a plus. Experience with preparing data for AI solutions (e.g. traditional machine learning models, AI Agents) is a plus. Experience in IT product management is a plus. Ability to work collaboratively in a team-oriented environment. Fluent in English, both written and spoken. What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 14000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Hybrid,129,Data Modeler - banking üí•,ITDS,"Join us, and turn complex data into powerful business insights! Krak√≥w-based opportunity with a hybrid work model (2 days/week in the office). As a Data Modeller, you will be working for our client, a global financial services institution undergoing a large-scale digital transformation to modernise and enhance its data infrastructure. The project focuses on building scalable, high-quality data models that support operational efficiency and strategic decision-making. You will be collaborating with cross-functional teams to design, standardise, and govern data solutions that drive business value across regulatory, analytical, and customer-facing initiatives. This is a key role where your expertise in data modelling will shape the foundation of a forward-looking data strategy. Your main responsibilities Designing and developing conceptual, logical, and physical data models Designing and developing conceptual, logical, and physical data models Collaborating with stakeholders to gather and translate data requirements Collaborating with stakeholders to gather and translate data requirements Establishing and maintaining data modelling standards and governance policies Establishing and maintaining data modelling standards and governance policies Participating in architecture and design discussions to align data models Participating in architecture and design discussions to align data models Working closely with data engineers and architects on data integration strategies Working closely with data engineers and architects on data integration strategies Creating documentation to support metadata management and data lineage Creating documentation to support metadata management and data lineage Reviewing and optimising existing data models for scalability and performance Reviewing and optimising existing data models for scalability and performance Advising teams on best practices for data modelling and data quality Advising teams on best practices for data modelling and data quality Supporting regulatory and compliance requirements through accurate modelling Supporting regulatory and compliance requirements through accurate modelling Staying informed on emerging data technologies and recommending improvements Staying informed on emerging data technologies and recommending improvements You're ideal for this role if you have: Demonstrated expertise in data modelling across operational and analytical systems Demonstrated expertise in data modelling across operational and analytical systems Proven experience eliciting, documenting, and verifying data requirements Proven experience eliciting, documenting, and verifying data requirements Strong communication skills for engaging both technical and non-technical stakeholders Strong communication skills for engaging both technical and non-technical stakeholders Excellent consulting skills to advise and guide teams on data architecture Excellent consulting skills to advise and guide teams on data architecture Solid relationship management abilities across complex organisational structures Solid relationship management abilities across complex organisational structures Deep knowledge of metadata management and data cataloguing Deep knowledge of metadata management and data cataloguing Strong problem-solving skills for analysing and resolving data challenges Strong problem-solving skills for analysing and resolving data challenges Experience applying data governance principles and standards Experience applying data governance principles and standards Ability to develop scalable and efficient data models for enterprise systems Ability to develop scalable and efficient data models for enterprise systems Proficiency in using data modelling tools such as Erwin, PowerDesigner, or similar Proficiency in using data modelling tools such as Erwin, PowerDesigner, or similar It is a strong plus if you have: Familiarity with financial services data domains and regulatory reporting needs Familiarity with financial services data domains and regulatory reporting needs Experience working in Agile or DevOps environments Experience working in Agile or DevOps environments Understanding of cloud-based data platforms (e.g., AWS, Azure, GCP) Understanding of cloud-based data platforms (e.g., AWS, Azure, GCP) Knowledge of data warehouse and data lake design principles Knowledge of data warehouse and data lake design principles Exposure to master data management and reference data modelling Exposure to master data management and reference data modelling We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to an attractive Medical Package Access to Multisport Program Access to Multisport Program #GETREADY Internal job ID #7435 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 1200, ""max"": 1400, ""type"": ""Net per day - B2B""}]",Data Science,Data Science
Full-time,Senior,Permanent,Hybrid,130,Senior Big Data Engineer,Relativity,"Posting Type Hybrid Job Overview Here at Relativity we prioritize flexibility and work-life harmony. Our Hybrid work environment provides options tailored to your role and location, aiming to enhance engagement, connectivity, and productivity. Join us to experience a culture of collaboration and innovation, where connecting in-person adds value to our collective growth. Let's work together! Join our team as we innovate the future of data platform architecture, enabling massive scaling and data processing for ML and Gen AI projects. You'll be at the forefront of processing vast unstructured data, building high-throughput APIs, and supporting distributed compute frameworks for seamless model deployment. Ready to dive into the heart of cutting-edge tech? Job Description and Requirements Your role in action: Build our next-generation data platform tooling and services to support the ingestion and processing of billions of documents at scale. Improve and extend our Spark based distributed data processing pipeline. Improve and extend our Rust based distributed query engine used to request large amounts of document data. Create tools to automate and optimize processes across disciplines Actively participate in the on-call schedule to investigate and fix production issues related to our data processing pipeline or query engine. Participate in code reviews for projects written by your team Focus on quality through comprehensive unit and integration testing Your Skills: 4+ years of software development experience in writing performant, commercial-grade systems and applications Experience with monitoring and troubleshooting production environments ‚Ä¢ Proficiency in programming languages used in high volume data processing and applications like Java or Scala and Python Experience building data pipelines with distributed compute frameworks like Hadoop. Spark, or Dask Knowledge of Linux/Unix systems, Docker/Kubernetes and CI/CD including scripting in Python or other scripting languages to automate build and deployment processes Knowledge of professional software engineering practices & software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations Leverages best practices and past experiences to mentor and improve the productivity of the team We‚Äôd particularly love it if you have: Deep experience building and debugging distributed data pipelines Experience with columnar databases and storage formats like Delta Lake and Parquet Experience deploying and managing services on Kubernetes Experience building with Rust. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law. #LI-MM5 Relativity is committed to competitive, fair, and equitable compensation practices. This position is eligible for total compensation which includes a competitive base salary, an annual performance bonus, and long-term incentives. The expected salary range for this role is between following values: 181 000 and 271 000PLNThe final offered salary will be based on several factors, including but not limited to the candidate's depth of experience, skill set, qualifications, and internal pay equity. Hiring at the top end of the range would not be typical, to allow for future meaningful salary growth in this position.","[{""min"": 181000, ""max"": 271000, ""type"": ""Gross per year - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,131,Senior Product Data Scientist,Asana,"Asana's Data Science team helps us fulfill our mission by informing strategy, defining success metrics, and identifying new ways to deliver user value. Data scientists are at the crux of deepening our understanding of the customers and driving more business outcomes by leveraging experimentation, causal inference, statistical and machine learning techniques, and data storytelling. The Data Science team at Asana is instrumental in enabling Asana's mission by instilling a data-influenced approach in building the product or business strategy, defining and measuring success metrics, learning and iterating to deliver value to our users. Data Scientists closely partner with Product, Business, Design, Engineering, and also other members of the Data team. We leverage experimentation, statistical modeling and machine learning techniques, causal inference, and data storytelling to deepen our understanding of Asana customers and make optimal decisions to drive more business value. This role is based in our Warsaw office with an office-centric hybrid schedule. The standard in-office days are Monday, Tuesday, and Thursday. Most Asanas have the option to work from home on Wednesdays. Working from home on Fridays depends on the type of work you do, and your recruiter can share more about the in-office requirements. Our employees in Poland are employed under a contract of employment. What you‚Äôll achieve Design and analyze experiments and causal analysis to ship impactful product changes that help customers quickly find what they need on Asana, specifically for our Mobile team. Proactively take on Analyses and develop compelling narratives that inform product strategy and influence business & roadmap decisions. Defining and adding new metrics to track success of our enterprise products and aggregate and publish them to our data warehouse to help answer all types of questions proactively. Building dashboards to highlight impact potential and other areas of opportunities. Communicating effectively and presenting to both technical and non-technical audiences. Establishing the Data Science presence in a new office and providing mentorship as the office grows. About you Bachelor Degree in Math, Statistics, Economics, Computer Science, Engineering a related quantitative field, or equivalent experience 4+ years of experience in applying data science techniques to drive technical product development and decision-making Strong technical background in statistics, math, computer science, information science, or another quantitative field Fluency in at least one data processing and programming language (e.g. Python, R), relational data modeling and SQL, familiarity with distributed data processing systems (e.g. Spark, Redshift) Expertise in statistical methods and experimental design and analysis Background in advanced statistical modeling (e.g. GLM, mixed effects) and/or machine learning Excellent written and verbal communication skills Ability to collaborate effectively with peers and leaders across data science, design, and engineering organizations At Asana, we're committed to building teams that include a variety of backgrounds, perspectives, and skills, as this is critical to helping us achieve our mission. If you're interested in this role and don't meet every listed requirement, we still encourage you to apply. What we‚Äôll offer Generous, transparent and fair compensation system Contract of Employment (and the option of 50% tax deductible costs for author‚Äôs rights usage in respect of applicable roles ) Health insurance with dental and travel coverage (Lux Med) Lunch catering on the days that you work from the office Career growth budget Home office setup budget Gym/Fitness reimbursement Fertility healthcare and family-forming support with Carrot Mental health support in Modern Health Group life insurance MacBooks with all necessary accessories For this role, the estimated base salary range is between 29,000 PLN - 36,833 PLN gross per month (subject to all taxes and necessary deductions) . The actual base salary will vary based on various factors, including market and individual qualifications objectively assessed during the interview process. The listed range above is a guideline, and the base salary range for this role may be modified. In addition to base salary, your compensation package may include additional components such as equity and sales incentive pay (for most sales roles), and benefits. If you're interviewing for this role, speak with your Talent Acquisition Partner to learn more about the total compensation and benefits for this role. About us Asana helps teams orchestrate their work, from small projects to strategic initiatives. Millions of teams around the world rely on Asana to achieve their most important goals, faster. Asana has been named a Top 10 Best Workplace for 5 years in a row, is Fortune's #1 Best Workplace in the Bay Area, and one of Glassdoor‚Äôs and Inc.‚Äôs Best Places to Work. After spending more than a year physically distanced, Team Asana is safely and mindfully returning to in-person collaboration, incorporating flexibility that adds hybrid elements to our office-centric culture . With 11+ offices all over the world, we are always looking for individuals who care about building technology that drives positive change in the world and a culture where everyone feels that they belong. We believe in supporting people to do their best work and thrive, and building a diverse, equitable, and inclusive company is core to our mission. Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We provide equal employment opportunities to all applicants without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by law. We also comply with the San Francisco Fair Chance Ordinance and similar laws in other locations. Our comprehensive compensation package plays a big part in how we recognize you for the impact you have on our path to achieving our mission. We believe that compensation should be reflective of the value you create relative to the market value of your role. To ensure pay is fair and not impacted by biases, we're committed to looking at market value which is why we check ourselves and conduct a yearly pay equity audit. #LI-Hybrid #LI-AZ","[{""min"": 29000, ""max"": 36833, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Senior,B2B,Hybrid,132,Machine Learning Engineer / Data Engineer,B2Bnetwork,"We are developing advanced Big Data and Machine Learning solutions that enable near real-time processing and analysis of large data volumes. The infrastructure supports various business areas by providing critical insights for decision-making. By joining our team, you will be responsible for designing, implementing, and deploying ML models and Data Engineering solutions in close collaboration with other company departments. Requirements Minimum 5 years of experience in Python development At least 4 years working with database technologies (preferably Big Data environments such as Hadoop and Spark) 4+ years of experience with Machine Learning projects (model training, testing, and implementation) Advanced knowledge of SQL (T-SQL, PL/SQL, Spark SQL) Experience with the Hadoop ecosystem (e.g., Hive) for large-scale data processing Familiarity with code versioning tools (GIT/Bitbucket) Nice to have: knowledge of GenAI (LangChain, llamaindex, multimodal setups), Scala, MLOps Familiarity with Agile/SAFe frameworks and experience with microservices, REST APIs, WebSocket Tech Stack: Python Hadoop Spark SQL Git Responsibilities Develop and maintain advanced Machine Learning models and data processing workflows Design and implement data pipelines (ETL/ELT) in Big Data environments Collaborate with DevOps and Data Science teams to optimize performance and scalability Analyze business requirements and translate them into engineering tasks Monitor and troubleshoot deployed solutions, implementing improvements as needed Participate in project meetings, share expertise, and support other team members Gdynia, Gda≈Ñsk, Warszawa, ≈Å√≥d≈∫",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,133,Workplace SCCM/Intune Specialist,emagine Polska,"We are looking for an experienced Workplace SCCM/Intune Specialist , specializing in Modern Workplace to support the carve-out project. The focus of the role is to support the transition team with modern end-user technology services, with a strong focus on SCCM and Intune. This role is pivotal in ensuring a seamless, secure, and efficient digital workplace experience for our users. Strong documentation and communication skills are essential. Start: 1st September Duration: first contract for 6 months + possible extension Location: Fully Remote (Poland) Salary: B2b contract- 150-170 z≈Ç/hour Skills and Experience: Strong experience with Microsoft Intune, SCCM, Endpoint Manager (policy management, app deployments, reporting). Knowledge of hybrid-joined and Azure AD-joined device scenarios. Familiarity with Windows Autopilot, co-management, compliance policies, and Conditional Access. Good understanding of security controls on endpoints (AV, encryption, firewall, patching). Strong knowledge of end user devices, printers, device inventory, policies. Previous experience with Microsoft Exchange ‚Äì important nice to have. Previous involvement in large-scale device migration or M&A carve-out projects is a plus. Key Responsibilities: Perform complete device inventory for in-scope users, validating ownership, management method, and compliance state. Plan and execute endpoint separation strategy (re-provisioning or re-enrollment into buyer‚Äôs management platform). Adjust Intune MDM and SCCM policies, profiles, and configurations for separation readiness. Manage application packaging and redeployment for devices transitioning to the buyer. Handle BitLocker, encryption keys, and endpoint security policies to ensure secure handover. Support co-management scenarios (SCCM and Intune) for hybrid-managed devices during migration. Provide technical guidance to user support teams for device wipe/rebuild/re-enrollment procedures. Document device management changes and handover processes for operational continuity.","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Mid,Permanent,Office,134,"Cloud Architect, Google Workspace",Google,"Minimum qualifications: Bachelor's degree in Computer Science or equivalent practical experience Experience in client-facing projects/troubleshooting with cross-functional stakeholders. Experience in deploying workspace or similar technologies for customers. Experience in deploying or managing Cloud productivity solutions at enterprise level, with experience on the typical mail, networking, security, IAM, DLP and integration concepts. Experience in working with and promoting SaaS, IaaS, or PaaS platforms. Preferred qualifications: Experience with IT security practices such as identity and access management, security and data protection, encryption, certificate and key management. Experience assessing and delivering solutions that meet regional compliance and regulatory requirements. Experience in scripting in low code and no code solutions. Experience managing and working with external partners/customers. Excellent communication, presentation, and problem-solving skills. About the job The Google Cloud Consulting Professional Services team guides customers through the moments that matter most in their cloud journey to help businesses thrive. We help customers transform and evolve their business through the use of Google‚Äôs global network, web-scale data centers, and software infrastructure. As part of an innovative team in this rapidly growing business, you will help shape the future of businesses of all sizes and use technology to connect with customers, employees, and partners. As a Cloud Architect, you will drive digital transformation for Google Cloud‚Äôs customers by designing and delivering virtual agent solutions. You will also innovate with Google Engineering and partners to enhance our products. Google Cloud accelerates every organization‚Äôs ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google‚Äôs cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems. Responsibilities Lead virtual agent development engagements with Google Cloud partners and customers by managing stakeholders, gathering requirements and addressing deployment needs. Design engaging conversational interfaces for Contact Center AI (CCAI). Collaborate with internal specialists, product and engineering teams to package approaches, best practices and lessons learned into thought leadership, methodologies and published assets. Engage with sales, partners, and customer technical stakeholders to manage project scope, priorities, deliverables, risks, issues and timelines for client outcomes. Travel 30% of the time for client engagements.",[],Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,135,Data Architect,InPost,"We are Europe‚Äôs leading out of home delivery partner, making deliveries secure and convenient for millions of customers every day. Our mission is to provide best-in-class user experience for merchants and consumers. ‚ÄòSimplify everything‚Äô ‚Äì redefining e-commerce logistics. We work by innovating the market with constant technological research and with meticulous attention to the customer. We are seeking a highly skilled and visionary Data Architect to lead the strategic design and implementation of our global data model at InPost . This role demands a passion for building robust, scalable, and future-proof data ecosystems. The ideal candidate will possess a deep understanding of modern data architecture principles, including medallion architecture, and experience with cutting-edge frameworks for processing large-scale datasets. This role encompasses performance optimization for vast data volumes, establishing clear naming conventions for consistency, and collaborating effectively with diverse stakeholders across the InPost Group to ensure alignment with business needs and technical feasibility. Creating a strategy for designing the architecture and implementation process of a global data model, taking into account the multitude of source systems within the InPost Group, and thus the specific business needs of all markets within the InPost Group. Defining and overseeing the data naming strategy, establishing conventions, harmonizing existing names, and ensuring consistent application across the InPost Group. This will improve data readability and understanding, facilitate inter-team communication, and support process automation and AI utilization throughout the entire organization. Collaboration at various levels, including senior stakeholders from all InPost Group markets, to identify and prioritize data initiatives that drive business value. Staying at the forefront of data architecture trends and technologies, recommending and implementing innovative solutions to enhance InPost's data capabilities. Training and mentoring of data engineering and data consultanting teams on data modeling approaches (theory, tools, strategies). Creating comprehensive, multi-layered technical documentation that specifies the technical aspects of the data architecture, providing a clear and concise description of the solutions at InPost Group. Minimum 6 years of experience in the field of data engineering / analytics engineering 2 years of experience as a Data Architect Expert in SQL Advanced knowledge of Python, Spark, Databricks, Azure, dbt + willingness to work with in-house frameworks for creating data objects Extensive experience in designing and implementing data warehouses (preferably modern big data warehouses, data lakehouse), (long-term) experience with data modeling in star schema, theoretical and practical knowledge of Kimball approach Deep understanding of medalion architecture Proven track record of successfully delivering Big Data & Analytics solutions Proficiency in modern Data & Analytics technology stacks and architectural patterns Knowledge and experience in optimization of: queries in the Big Data stack, ETL processes, data storage solutions and partitioning strategies for high-performance analytics and processing Familiarity with the Parquet file format and experience working with Delta Lake, including data versioning, storage optimization, ensuring data integrity, and handling ACID transactions Understanding of the end-to-end development lifecycle of analytical data products High level of communication and cross-team collaboration skills Familiarity with CI/CD and git (preferred GitLab) Fluent Polish and English We will appreciate also: Knowledge of BI tools (Power BI preferred / Tableau) Experienced in establishing and enforcing data governance policies using Unity Catalog in Databricks for centralized data access control, lineage, and auditing Capability in defining and managing data quality frameworks, metadata management, and data cataloging Expertise in implementing role-based access control (RBAC) and data encryption (at rest and in transit) to ensure data security and compliance with regulations like GDPR, CCPA, and HIPAA Ability to present technical concepts and solutions to diverse audiences Significant Impact : Directly influence strategic decision-making through the data architecture designs. See the tangible results of your work across various InPost services and initiatives. Cutting-Edge Technology : Work with the latest technologies, including Databricks, Azure, Spark, and more, pushing the boundaries of data processing and analysis. You'll have the chance to explore and implement innovative solutions. Professional Growth : Expand your skillset and expertise through ongoing learning, leading cross-functional projects or presenting your work to senior leadership. Train and mentor other team members, sharing your knowledge and shaping future data professionals. International Collaboration : Work with diverse stakeholders across various InPost markets, gaining invaluable experience in a global context. Data Driven Culture : Contribute to a company deeply committed to data-driven decision-making, where your expertise will be highly valued and actively sought. Competitive Compensation and Benefits : Enjoy a competitive salary and a comprehensive benefits package, including options like a cafeteria system allowing you to choose benefits that align with your preferences (e.g., Multisport, private healthcare, shopping vouchers). Flexible Work Arrangements: Enjoy the flexibility to choose your work environment. Work remotely, from our modern Krakow or Warsaw office ‚Äì the choice is yours to create a work setup that best suits your needs and preferences. Thrive at InPost : Be part of a vibrant and supportive culture. Join our active employee initiatives, from sports teams and running clubs to social events and volunteering opportunities. We foster a work environment that values well-being and connection. Step 1 : CV screening Step 2 : Devskiller test Step 3 : Technical Interview (60 min) Step 4 : Home task Step 5 : Home task presentation and discussion (60min)",[],Data Architecture,Data Architecture
Full-time,Mid,B2B,Hybrid,136,Data Analyst & Automation Specialist (Treasury),Experis Manpower Group,"Location: Warsaw, Poland (Hybrid ‚Äì 2 days/week in office)",[],Data Analysis & BI,Data Analysis & BI
Full-time,Manager / C-level,B2B,Remote,137,Tech Lead DWH,emagine Polska,"Rola: Tech Lead DWH Wsp√≥≈Çpraca: d≈Çugofalowa Stawka: do 250 zl/h netto+vat (B2B) Model pracy: B2B Lokalizacja: praca zdalna z Polski Jƒôzyk: polski, angielski Poszukujemy Tech-Leada, kt√≥ry obejmie zar√≥wno Platformƒô Danych, jak i czƒô≈õƒá Analytical Platform. Kluczowym celem stanowiska jest wsparcie technologiczne oraz architektoniczne w wydajnym wdro≈ºeniu i rozwoju nowoczesnej Platformy Danych, obejmujƒÖcej Hurtowniƒô Danych w koncepcji Lakehouse oraz Platformƒô MLOps. Kandydat powinien posiadaƒá do≈õwiadczenie w implementacji system√≥w chmurowych oraz w modelowaniu bazodanowym w podej≈õciu DDD. ObowiƒÖzki: Analiza istniejƒÖcych system√≥w ≈∫r√≥d≈Çowych i docelowych (tj. system√≥w odbierajƒÖcych dane z Platformy) w organizacji pod kƒÖtem wymaga≈Ñ integracyjnych z Data Platform. Projektowanie i dokumentowanie architektury integracyjnej (HLD i LLD) obejmujƒÖcej: mechanizmy zasilania platformy danymi z system√≥w ≈∫r√≥d≈Çowych (batch, streaming, API), mechanizmy udostƒôpniania danych z platformy do system√≥w docelowych (np. API, warstwa prezentacyjna read-modelu, eksporty). Wsp√≥≈Çpraca z w≈Ça≈õcicielami system√≥w ≈∫r√≥d≈Çowych i docelowych w celu uzgodnienia interfejs√≥w, czƒôstotliwo≈õci wymiany danych, SLA i wymaga≈Ñ bezpiecze≈Ñstwa. Doradztwo w zakresie doboru komponent√≥w i us≈Çug posadowionych na Platformie. Wyb√≥r i rekomendacja narzƒôdzi oraz wzorc√≥w integracyjnych (z naciskiem na ELT dla batch z customowƒÖ czƒôstotliwo≈õciƒÖ oraz event-driven architecture). Wyznaczanie standard√≥w dla procesu wersjonowania i CI/CD. Zapewnienie zgodno≈õci z politykami bezpiecze≈Ñstwa danych, zgodno≈õci regulacyjnej i standardami organizacyjnymi. Wsparcie w kreowaniu zasad zarzƒÖdzania danymi i nadz√≥r nad zgodno≈õciƒÖ z politykƒÖ Data Governance. Doradztwo w zakresie integracji rozwiƒÖza≈Ñ ML posadowionych na Platformie z systemami biznesowymi, w tym z BPMN. Wsp√≥≈Çpraca z Chapter Leads zar√≥wno dla Data jak i Analytical Platform. Inne zadania zwiƒÖzane bezpo≈õrednio ze standaryzacjƒÖ podej≈õƒá do wdra≈ºania Platformy Danych. Kluczowe Wymagania: Wieloletnie do≈õwiadczenie w implementacji Platformy Danych w technologii chmurowej (w szczeg√≥lno≈õci dla przypadku migracji z on-prem do cloud). Do≈õwiadczenie w modelowaniu struktur bazodanowych w podej≈õciu DDD. Umiejƒôtno≈õƒá dokumentowania architektury HLD i LLD. Znajomo≈õƒá mechanizm√≥w integracji danych (batch, streaming, API). Znajomo≈õƒá standard√≥w bezpiecze≈Ñstwa danych oraz Data Governance.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,138,Data Solution Architect  (Azure),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: projekt budowy nowoczesnej Data Platform w chmurze Azure, obejmujƒÖcej warstwƒô Lakehouse (Bronze / Silver / Gold) oraz Analytical Platform (MLOps), modelowanie struktur bazodanowych w podej≈õciu DDD (Data Domain Driven Design), opracowywanie modeli danych na podstawie dokumentacji z obszaru Data Governance (glosariusz danych, modele konceptualne/logiczne), projektowanie przep≈Çywu danych ze ≈∫r√≥de≈Ç do warstw platformy danych (Ingest: direct query, API, event streaming), tworzenie i dokumentowanie Data Contracts, wsp√≥≈Çpraca z w≈Ça≈õcicielami system√≥w ≈∫r√≥d≈Çowych i docelowych w zakresie integracji danych i SLA, implementacja struktur danych w architekturze medallion (Bronze / Silver / Gold) przy u≈ºyciu ETL na Azure Data Platform, doradztwo w zakresie doboru narzƒôdzi, architektury integracyjnej i praktyk CI/CD, mo≈ºliwo≈õƒá realnego wp≈Çywu na standardy technologiczne i projektowe, praca 100% zdalna, a dla chƒôtnych mo≈ºliwo≈õƒá pracy z biura we Wroc≈Çawiu, stawka do 200 z≈Ç/h przy B2B, w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz do≈õwiadczenie w modelowaniu danych (ERD), przygotowywaniu Data Contracts i implementacji struktur domenowych w ≈õrodowisku Data Warehouse znasz procesy Data Ingestion i architekturƒô nowoczesnych DWH w Azure (Azure Synapse, Data Lake, Databricks), masz do≈õwiadczenie z platformami MLOps / Analytical Platform, mile widziane do≈õwiadczenie w: tworzeniu dokumentacji mapowania danych ≈∫r√≥d≈Çowych, zarzƒÖdzaniu metadanymi i jako≈õciƒÖ danych (np. Azure Purview), komunikujesz siƒô p≈Çynnie w jƒôzyku angielskim (B2/C1). Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 31920, ""max"": 33600, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Mid,B2B,Remote,139,Data Scientist (Optimization),Lingaro,"The person we are looking for will become part of Data Science & AI Sub- CC Team working within DS&AI competency. Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,140,Senior Data Engineer,edrone,"We're a hard-working, fun-loving, get-things-done type of team dedicated to providing unique marketing automation solutions for clients. We understand the challenges of eCommerce and the importance of seamless customer service and satisfaction. We roll our sleeves up, act fast, and learn together. We're looking for a Senior Data Engineer who will do the same! üöÄ Who are we? Edrone is a SaaS-based product that helps thousands of small and medium-sized businesses compete with major brands. Our mission is to provide simple solutions to big challenges in eCommerce. We achieve results through a strong feedback culture and clearly defined, transparent expectations. Currently, we work with nearly 2,000 online stores ‚Äî primarily in Poland and Brazil. Brands such as Bielenda , Mosquito , 2005 , or Lilou have placed their trust in our product. If you want to learn more about our culture and what it‚Äôs like to work at edrone, check it out here. Our social media: LinkedIn , Instagram , YouTube . Sounds great? Keep on reading! üöÄ What‚Äôs in it for you: Be part of a small, fast-paced team that values innovation, efficiency, and a positive work culture. We thrive on challenges, embrace change, and keep things moving. We value initiative and ownership‚Äîif something makes sense, we act on it quickly and take full responsibility for delivering it. Direct responsibility for projects, regular 1: 1s with your leader with a blameless postmortems, code reviews B2B contract (20-25k) & covering all the costs of accounting services Hybrid or remote work or a modern, well-equipped office - whatever you prefer! 26 paid days off so you can relax properly! Benefits - MultiSport card, LuxMed medical package, group accident insurance, English and Portuguese classes, and Mindgram - a portal for mental health and development üöÄ How you will spend your time: Leadership, Innovation & Excellence Drive technical design discussions, lead critical implementations, and support decision-making across the team Mentor other engineers and raise the bar for code quality and system thinking Stay current with evolving technologies and proactively introduce improved tools, patterns, or approaches when beneficial. Champion engineering excellence by questioning the status quo and influencing product/technical direction Backend System Development Design, build, and maintain robust Python-based services and microservices Develop and optimize RESTful APIs and background services supporting core business logic and integrations Ensure code quality, reusability, and scalability through modular design and adherence to best practices Cloud-Native Application Engineering Develop serverless and containerized applications using AWS Lambda , ECS , and other cloud-native tools Leverage AWS services (S3, RDS, DynamoDB, Step Functions, etc.) to support backend operations and workflows Collaborate with DevOps to provision, deploy, and monitor cloud infrastructure Automation and Task Orchestration Automate recurring tasks, background jobs, and workflows using Python scripts and AWS orchestration tools Build and maintain task schedulers and asynchronous workers for time-sensitive operations Implement monitoring, logging, and alerting systems for observability and proactive issue resolution Data Access and Integration Build data access layers and connectors for interfacing with relational and NoSQL databases Develop data integration scripts or services to move and sync data between systems when needed Write efficient, production-grade SQL and Python code to support internal tools and services Performance and Reliability Optimize application performance, API response times, and system throughput Implement retries, fallbacks, and circuit breakers to increase fault tolerance Continuously assess and improve system design for scalability and maintainability üöÄWho you are: 5+ years of professional experience as a Data or Python Engineer Experience in Data Engineering ‚Äì including schema design, query optimization, and managing pipelines in production Experienced with AWS services (Redshift, Aurora, DynamoDB, S3, Glue, Lambda, Step Functions, etc.) to build data pipelines and scalable cloud-native applications. You‚Äôre familiar with dbt or eager to work with it Experience with data orchestration tools (e.g., Airflow, Step Functions) ‚Äî scheduling, monitoring, and troubleshooting data pipelines. Experience in building and maintaining RESTful APIs , microservices, and backend systems. Have acted as a technical/feature lead on multiple projects, owning solutions from design through production operation. Comfortable working closely with the Product team , aligning goals, making independent technical decisions, and challenging assumptions when needed. Experienced in leading design reviews , engaging in pair programming, and documenting key decisions for long-term clarity. üöÄ It‚Äôs nice if you have: Experience in Java üìù How does the recruitment process look like: A 30-minute phone interview with the People and Culture Partner - Milena Micor , where we aim to get to know you a little better! A technical online interview with the Data Team Lead - Krystian Andruszek and another panelist 30-minute conversation with our CTO Maciej Mendrela to align on vision, culture, and expectations Decision regarding the offer and welcome on board! After each stage, you will always receive feedback regarding your candidacy.","[{""min"": 20000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,141,Big Data Engineer,Grid Dynamics Poland,Description: ,"[{""min"": 17000, ""max"": 18300, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Remote,142,Data Engineer,Sopra Steria,"We are looking for Data Engineer to join Sopra Steria team for a client in the automotive sector. You will work in an industrial, stable environment in multinational team (Germany, Poland) for a major automotive manufacturer ¬∑ Hands-on experience with SQL and ability to write complex queries ¬∑ Basic knowledge of data related AWS Services like: S3, Glue, Athena","[{""min"": 7000, ""max"": 11000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,143,Data Vault Architect with Snowflake,Link Group,"Opis stanowiska: Poszukujemy do≈õwiadczonego Data Vault Architecta , kt√≥ry wesprze naszego klienta w strategicznym przeglƒÖdzie, planowaniu i transformacji istniejƒÖcej platformy danych. Osoba na tym stanowisku bƒôdzie odpowiedzialna nie tylko za ocenƒô obecnej architektury, ale r√≥wnie≈º za opracowanie wizji docelowej, zarzƒÖdzanie interesariuszami oraz nadz√≥r nad realizacjƒÖ technicznƒÖ i zespo≈Çem wdro≈ºeniowym. Projekt ma kluczowe znaczenie dla poprawy efektywno≈õci i skalowalno≈õci system√≥w danych opartych o Snowflake, DBT oraz AWS . PrzeglƒÖd i ocena istniejƒÖcej platformy danych klienta oraz zaproponowanie optymalnej ≈õcie≈ºki rozwoju. Tworzenie strategii i wizji docelowej dla nowoczesnej platformy danych z wykorzystaniem Data Vault 2.0. Koordynacja dostarczania rozwiƒÖza≈Ñ, planowanie dzia≈Ça≈Ñ oraz nadz√≥r nad ich realizacjƒÖ. ZarzƒÖdzanie zmianƒÖ oraz skuteczna wsp√≥≈Çpraca z interesariuszami biznesowymi i technicznymi. Mo≈ºliwo≈õƒá objƒôcia roli mened≈ºerskiej oraz budowania zespo≈Çu technicznego. Wsp√≥≈Çpraca z zespo≈Çami in≈ºynieryjnymi, analitycznymi oraz DevOps. Must-have: Do≈õwiadczenie w projektowaniu i wdra≈ºaniu architektury danych w modelu Data Vault 2.0 . Bardzo dobra znajomo≈õƒá platformy Snowflake . Praktyczne do≈õwiadczenie z narzƒôdziem DBT (Data Build Tool) . Znajomo≈õƒá i do≈õwiadczenie z infrastrukturƒÖ AWS . Umiejƒôtno≈õƒá zarzƒÖdzania interesariuszami oraz prowadzenia inicjatyw transformacyjnych. Zdolno≈õƒá do definiowania strategii technicznej i architektonicznej.","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Mid,Permanent or B2B,Remote,144,Data Engineer,Transition Technologies MS,"Technical Skills: Proficiency in SQL, Python, and Pandas for data manipulation, along with experience in dbt for transformations and GitLab for version control and CI/CD. Data Engineering Expertise: Strong understanding of data modelling, ETL processes, and data warehousing concepts. CI/CD Knowledge: Ability to set up and manage automated data pipelines using GitLab CI/CD. Cloud & Infrastructure: Experience with Snowflake's architecture, including warehouses, schemas, and security. Python & Pandas: Ability to write efficient Python scripts for data processing, leveraging Pandas for data wrangling and analysis. Collaboration & Documentation: Strong communication skills to work with analysts and engineers, plus experience documenting data workflows. 2+ years of working with programming language focused on data pipelines,eg. Python or R 1+ years of experience working on GCP, Cloud (AWS/Azure/Google) or other cloud platform (optional) 1+ years of experience in data pipelines maintenance 1+ years of experience with different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc. 1+ years of experience in working in data architecture concepts (in any of following areas data modeling, metadata mng., workflow management, ETL/ELT, real-time streaming, data quality, distributed systems) 2+ years of experience working with SQL Exposure to open source and proprietary cloud data pipeline tools such as Airflow, Glue and Dataflow (optional) Very good knowledge of relational databases (optional) Very good knowledge of Git, Gitflow and DevOps tools (e.g. Docker, Bamboo, Jenkins, Terraform Very good knowledge of Unix Good knowledge of Java and/or Scala Pharma data formats is a big plus (SDTM) Interesting and challenging projects Flexible working hours Friendly, non-corporate atmosphere Stable working conditions (CoE or B2B) Possibility for self-development and promotion in the company Rich benefits package Possibility to work remotely We reserve the right to contact the selected candidates.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,145,Cloud Database Engineer,Altimetrik Poland,"Altimetrik Poland is a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. For our Fintech client from the UK, we are seeking a Cloud Database Engineer to join our growing team. The ideal candidate for this role will designs, manages, and optimises cloud-based database systems, ensuring scalability, performance, and data security while supporting business-critical applications. Responsibilities: Customer / client centricity: Ensures that cloud database services meet client expectations regarding reliability, performance, and security Actively works to troubleshoot and resolve client issues related to cloud database environments, ensuring minimal downtime and optimal performance Collaborates with clients to ensure that cloud database solutions align with their needs and support their operational goals Data & reporting: Gathers and analyses data on database performance metrics, resource usage, and incidents in cloud environments Produces regular reports that highlight database performance, cost management, and optimization opportunities within the cloud infrastructure Ensures data accuracy in reporting to support strategic decisions regarding cloud database management Industry knowledge: Stays up to date with cloud database technologies, trends, and best practices, including platforms like AWS RDS, Azure SQL Database, or Google Cloud SQL Applies knowledge of cloud database advancements to drive improvements in data management and retrieval efficiency Ensures the organization‚Äôs cloud database strategies are aligned with modern industry practices and innovations Planning & prioritisation: Manages the prioritisation of cloud database tasks, such as updates, patches, and incident responses Plans cloud database resource allocation effectively, ensuring high-priority issues are addressed promptly and efficiently Works with other teams to coordinate database deployments and updates, ensuring seamless transitions and minimal disruption to service Product Knowledge: Requires a detailed understanding of cloud-based data products and database technologies that support the company‚Äôs offerings Familiarity with database architecture, cloud solutions, and how they support the scalability and reliability of products. Strategic impact: Ensures that cloud database strategies and solutions align with the organization‚Äôs broader business objectives Recognizes how cloud database performance impacts organizational scalability and agility, contributing to the company's success Identifies ways to optimize cloud database resources to improve cost-efficiency and performance, ensuring alignment with strategic goals If you possess... Previously used SQL monitoring and performance tools. Demonstrable experience installing, scaling and maintaining MSSQL in HA architecture Securing and safeguarding sensitive data and maintaining referential integrity Architecting backup and disaster recovery strategies Performance tuning T-SQL for high throughput environments Experience with Installation, Performance monitoring, Security, and high availability and backups. Experience working with development life cycle models including CI/CD pipelines. ‚Ä¶ then we are looking for you! We work 100% remotely or from our hub in Krak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,146,Clinical Integration Developer,Upvanta sp. z o.o.,"Design and implement integration solutions connecting Medidata systems with other internal or external systems in the clinical ecosystem. Collaborate with business stakeholders from clinical teams and IT to ensure end-to-end interoperability. Develop integration workflows, define data mappings, and ensure adherence to clinical trial requirements and standards. Support validation and documentation processes in accordance with GxP and other regulatory guidelines. Experience: Minimum 5 years in software development within the life sciences industry, preferably in clinical trial technology. Technical Skills: Proficiency in programming languages such as Java, Python, or C#. Familiarity with integration tools/middleware (e.g., Informatica, Apache NiFi, AWS services). Experience with APIs and scalable system design. Domain Knowledge: Understanding of clinical trial data standards (e.g., CDISC, SDTM, ADaM) and data management processes. Experience working in Agile teams and validated (GxP) environments. Soft Skills: Strong problem-solving mindset, attention to detail, and clear communication.","[{""min"": 1200, ""max"": 1500, ""type"": ""Net per day - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,147,Database Engineer ‚Äì GTM Infrastructure,Mozaiq,"Help us build the most advanced outbound growth database in healthcare. We‚Äôre hiring a TAM Database Engineer to power the core infrastructure behind our next-generation GTM engine. You‚Äôll be at the center of a strategic company initiative to build a centralized, real-time TAM database of independent healthcare practices and billing companies, powering both AI- and human-led outbound workflows. 1. Database Architecture & Ingestion Design and manage scalable data pipelines to ingest large-scale datasets from web scraping (e.g., Healthgrades, Vitals,) and APIs (e.g., NPPES, Google Places, BuiltWith) into Snowflake or similar. Implement deduplication, field-level validation, and audit logs to ensure high data quality and reliability. Develop change tracking and delta snapshot tables to support scoring evolution and longitudinal analytics. 2.API & Platform Integrations Build and maintain bi-directional integrations across our GTM tech stack: Inputs: Clay, ZoomInfo, CMS, public registriesOutputs: Outreach, SFDC, Marketo, Instantly, Slack Orchestrate near real-time updates (<15 minutes) from source to sequence using event-based syncs and monitoring dashboards. 3. Clay-Snowflake Sync Enablement Collaborate with GTM Engineering to enable dynamic enrichment and outbound activation logic in Clay using Snowflake-driven scoring and field logic. Own the technical execution of Clay re-enrichment loops, field mapping QA, and sync health automation. 4. Operational Automation & Monitoring Automate data maintenance workflows (re-enrichments, suppression logic, enrichment freshness SLA triggers). Build Slack alerts and Looker/Tableau dashboards for sync failures, campaign gaps, and enrichment completeness. A fully functional TAM database used as a system of record by Marketing, Sales, and RevOps.Faster and smarter outbound campaigns through reliable enrichment, scoring, and activation layers. Increased pipeline conversion by enabling precise segmentation, trigger-to-channel routing, and campaign personalization. Proven experience with Snowflake, data warehousing, and ETL tools. Comfort scraping structured data at scale using Apify, SerpAPI, or custom Puppeteer pipelines. Familiarity with GTM systems: Clay, Outreach, Salesforce, Marketo, ZoomInfo, Instantly. A product mindset: you value clean schemas, data governance, and proactive debugging. Experience supporting GTM, RevOps, or Marketing teams is a plus (even better if you‚Äôve done it in SaaS or healthcare). Some exposure or interest in AI/NLP applications and how they interact with GTM systems. Mozaiq is a fully remote team with roots in the US and Eastern Europe. We build powerful, cross-functional teams that support some of the most sophisticated tech and marketing organizations in the world. üåê Global Family: A tight-knit, multicultural, remote-first team üíº Meaningful Work: Support world-class brands and products ü§ù Inclusive Culture: We prioritize collaboration, clarity, and growth Flexible hours (with limited CET‚ÄìUS overlap) Competitive hourly pay Exposure to global clients and leading tech brands 100% remote work",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,148,Senior BI Developer (she/he/they),≈ªabka Future,"W ≈ªabce Future napƒôdzamy cyfrowƒÖ transformacjƒô Grupy ≈ªabka. Tworzymy nowoczesny, skalowalny ekosystem technologiczny, kt√≥ry ≈ÇƒÖczy dane, narzƒôdzia i procesy w sp√≥jnƒÖ ca≈Ço≈õƒá. Dzia≈Çamy szybko, z pomys≈Çem i w zgranych zespo≈Çach. Stawiamy na skalowalne technologie, realny wp≈Çyw i ciƒÖg≈Çe doskonalenie üíô",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Remote,149,Lead Data Software Engineer (Azure),EPAM Systems,"We are looking for a Lead Data Software Engineer to join one of the projects in our growing Data Practice. Our client is a UK-based global leader in consumer health, hygiene, and nutrition products, with a strong portfolio of well-known brands available in over 200 countries. The company focuses on innovation and sustainability to enhance everyday health and well-being. You can work remotely or join one of our offices, located in Krakow, Wroclaw, Warsaw, Lodz, Gdansk, and Katowice. Responsibilities Design ETL pipelines to ingest, integrate, curate, and refine data Develop and support Common Data Products Review the organization's data landscape, including all data sources and systems Document shared data estates and enforce structured, controlled access through data segregation Perform analysis of business problems and technical environments to design effective solutions Participate in code reviews and test solutions to ensure alignment with best practices Build a high-performance engineering culture, mentor team members, and provide motivation tools Requirements 5+ years of experience in software development with Big Data technologies Background in building and maintaining reliable data pipelines using Databricks Advanced knowledge of SQL, Python, Spark, PySpark Understanding of modern storage formats, including Delta and Iceberg Proficiency in extracting data from various source systems such as databases, APIs, files, and streaming platforms Knowledge of Azure Competency in key data governance and cost optimization principles Familiarity with data architecture concepts such as Data Lakes, Data Warehouses, and Data Lakehouses Nice to have Background in Azure Data Factory We offer We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,150,Senior Data Engineer with AWS and Snowflake,Sii,"Strong expertise in cloud technologies and the Snowflake ecosystem, particularly AWS, Data One Platform, Immuta, Collibra, and cloud security aspects Minimum 5 years of experience in Python programming for building and maintaining data pipelines Advanced knowledge of databases, including query optimization, relational schema design, and MPP using SQL Expertise in data storage technologies, including files, relational databases, MPP, NoSQL, and various data types (structured, unstructured, metrics, logs) Deep familiarity with Data Vault, Data Mesh, dimensional modeling, and metadata management Understanding of business and analytical processes, as well as integration with analytics and reporting systems Experience working in Agile environments, coaching development teams with the use of fluent English and Polish Residing in Poland required Nice-to-have requirements Knowledge of Pharma data formats is a big plus We are looking for a Data Engineer with expertise in Snowflake, AWS, and ETL processes, who will work closely with AI scientists and data analysts to design, develop, and maintain data pipelines and systems that support clinical and operational data use cases. Design and develop data architecture incorporating Snowflake, Immuta, Collibra, and cloud security Implement and optimize ETL/ELT processes, manage raw data, automate workflows, and ensure high performance and reliability of data processing systems Manage data quality and security, monitor quality, and implement metadata management strategies Collaborate with analytics and business teams to identify user needs and deliver comprehensive solutions supporting analytics and reporting Optimize and migrate data systems through data and process conversion from existing systems to Data Vault Define and enforce coding standards, data modeling, and ETL/ELT architecture, ensure compliance with Data Mesh and other modern approaches Coach and mentor the data engineering team, conduct code reviews, participate in architectural discussions, and initiate innovative technological solutions Recruitment language: Polish Start ASAP Permanent contract Fully remote Free coffee Free breakfast No dress code Modern office Sport subscription Training budget Private healthcare Small teams International projects Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title ‚Äì get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers ‚Äì Power People. Learn more at sii.pl .","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,151,Data Engineer,Unit8,"Who We Are Founded in 2017, Unit8 is a fast-growing Swiss AI and data analytics consulting and services company dedicated to solving complex problems of traditional industries like automotive, chemical, financial services, manufacturing and pharma. We work with some of the biggest organisations in Europe to solve the challenges that directly affect their business - be it operations, finance, manufacturing or R&D. Since our foundation, we have successfully delivered more than 180 projects and have grown to 140+ talented individuals across 6 office locations. Unit8's mission is to drive the adoption of AI and Data Science in the non-digital industries and to help accelerate their digital transformation. Among its activities, Unit8 dedicates a part of its resources and time on projects that deeply matter to us - that includes collaboration on pro-bono and ""engineering for good"" causes. You can learn more about what we are passionate about at Unit8 Talks and on our website you can find more useful information about our business and recruiting process. These are some examples of projects that we have been working on Building a real-time production line monitoring system for the pharmaceutical industry in order to improve the throughput of the production processes for the Pharmaceutical Industry. Technologies: AWS, EKS, CloudFormation, Docker, Python Building system for calculating and delivering global climate scores i.e. estimating the impact of climate change on the risk of natural disasters (i.e. floods, wildfires & droughts) for the Insurance Industry. Technologies: PySpark, Python, Proprietary Data Processing Platform Building a modern data science platform including a data lake for Chemical Industry. Technologies: Azure, ML Studio, Data Factory, Databricks, Spark About You As a member of agile project teams, your mission will be to build solutions and infrastructure aiming at solving the business problems of our clients. You are a proficient software engineer who knows the fundamentals of computer science and you master at least one widely adopted programming language (Python, Java, C#, C++). You know how to write distributed services and work with high-volume heterogeneous data, preferably with distributed systems such as Spark. You are knowledgeable about data governance, data access, and data storage techniques. You have strong client-facing skills: comfortable interacting with clients (business & technical audience), delivering presentations, problem-solving mindset. You are willing to travel to meet with our clients and the team (mainly in Europe - up to 10% of your time). You are eligible to register as a sole trader (self-employment) in Poland. Don't worry if you don't know how to do the registration, we can help with that. What You'll Do Design, build, maintain, and troubleshoot data pipelines and processing systems that are relied on for both production and analytics applications, using a variety of open-source and closed-source technologies. Help drive optimization, testing, and tooling to improve data quality. Collaborate with other software engineers, ML experts, and stakeholders, taking learning and leadership opportunities that will arise every single day. Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives. What we offer Opportunity to shape the expansion of one of the leading Swiss data & AI consultancies Compensation package including base salary, yearly bonus based on the both individual + company performance Above the norm flexibility regarding when and from where you work as long as both client and internal commitments are met Work on cutting-edge data, AI & analytics topics (e.g., Generative AI) that have real impact across industries Dedicated time and budget for training and pro-bono projects 30 paid days off Private health care and Multisport Cross offices/company-wide frequent events (off-site or online) as well as quarterly budget to spend with the team on after-work activities","[{""min"": 20000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,152,"Senior Data Software Engineer (Python, GCP)",EPAM Systems,"We are looking for a Senior Data Software Engineer to join the project in the telecommunications sector, focused on creating network infrastructure for high-speed internet access using fiber optic communication. In this role, you will work with an American multinational technology company. The project offers many opportunities to learn as our client is considered one of the Big Five companies in the American information technology industry. This position offers a hybrid model, with 3 days per week working from the client‚Äôs office for candidates from Gdansk, Wroclaw, Warsaw, and Krakow. Responsibilities Provide technical leadership and oversight for the other developers working on the project Design and build ETL/ELT pipelines using Airflow and other technologies on the GCP Provide advice and recommendations (in writing when appropriate) to the Data Warehouse Technical Lead regarding technical options and best practices for the data warehouse and related ETL/ELT pipelines Analyze source data and work with internal data consumers to determine which data is needed and how it should be represented in the output table schemas Write and maintain related technical documentation Perform thorough design and code reviews for other developers Requirements 4+ years of relevant professional experience Solid experience in SQL scripting and Python programming At least 1 year of ETL/ELT to BigQuery experience Experience in developing solutions for the GCP Proficiency with ETL/ETL for data warehousing including data source investigation/analysis, target schema design and data pipeline design/implementation Ability to write clear, concise, and well-reasoned technical explanations and documentation for both engineer and analyst internal audiences (design docs, architecture views/diagrams, etc.) Solid interpersonal skills for working with both upstream teams providing data and downstream analysts consuming data B2+ English level proficiency Nice to have Experience with Apache Airflow Familiarity with tools like IntelliJ, Gradle, Google Cloud Storage, Google Cloud Datastream, Google Cloud Data Catalog, Google Looker, Google Cloud Cortex Google Cloud Platform (GCP) certification We offer We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,153,Senior Risk and Control Specialist,HSBC Service Delivery,"Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity The Senior Risk & Control Specialist will be accountable and responsible to deliver activities as defined by the CTO Enablement Risk & Controls Lead (in line with Agile principles, ownerships within the team are expected to change to allow for a different focus; these changes would be discussed and agreed with the role holder with sufficient time for transition). The role is performed with a focus on excellence & attention to detail with an ability to lead & inspire other staff, but also to communicate effectively with all levels of technology & business management. What you‚Äôll do The role of Senior Risk & Control Specialist in CTO Enablement Risk and Controls will be expected to cover responsibilities covering: Technology Control Ownership and Oversee its operation which would mean running the mandated governance, managing associated risks and actions, control process design, ensure key artifacts are created and be accountable for any control associated activities. Lead Control related Remediation , effectively being responsible for the delivery of defined remediation for a specific technology control (e.g. Patch Management Control) Risk Management & Control governance which would manage risks and audit findings, run all mandated meetings to ensure key artifacts are created, management of risks for either a horizontal or vertical slice of CTO. Team governance for the Risk and Controls team which would be working with CCO Tech to manage risk, track/manage appropriate Audit points, route Audit/regulatory queries to correct owner, ensure control owners understand their obligations, run controls meetings as defined by the Head of Risk and Controls. Provide leadership to a team , specifically proving non-technical leadership to a part of the Risk and Controls team. What you need to have to succeed in this role General knowledge with relevant experience (minimum 8+ years) within Risk Management, Controls Governance & Monitoring or IT Service Continuity Management. Strong understanding of regulatory requirements and risk governance practices. Good knowledge on Patch Management Control, Vulnerability Management Control and Information Security Risk Management processes would be advantageous Knowledge of HSBC‚Äôs Risk Management Platform (e.g. Helios) would be advantageous but not essential Knowledge of Agile Scrum & Kanban as well as familiar with Agile tooling (e.g. Confluence and JIRA) would be advantageous but not essential Good knowledge of data analysis/reporting through BI tools like Qlik sense and PowerBI would be advantageous Experience working on complex projects across multiple domains. Ability to interpret complex data and add value to the business decision making process. Strong interpersonal and communication skills with a proven ability to communicate effectively and confidently at all levels across the Group and across different cultures. What we offer Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery discounts Financial support with trainings and education Social fund Flexible working hours Free parking If your CV meets our criteria, you should expect the following steps in the recruitment process: Online behavioural test Telephone screen Job interview with the hiring manager We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Unclassified,Unclassified
Full-time,Senior,B2B,Hybrid,154,Cloud Data Engineer,Optiveum,"üöÄ We're Hiring: Cloud Data Engineer (Python, Databricks, SQL) ‚Äì Warsaw, Poland üß© About the Company Our client is a US-based technology company headquartered in New York City, delivering innovative digital solutions and cloud-based platforms for private capital markets. With offices in multiple countries, the company is now investing in a new engineering centre in Warsaw, highly valuing the technical expertise and strong work ethic of software engineers in Poland. üìç Location & Setup While the work starts fully remote, within a few months, the role will require working from the Warsaw office 3 days per week (hybrid model). üß† The Opportunity We‚Äôre looking for a skilled and self-driven Cloud Data Engineer to join a growing data team. In this role, you'll design, build, and optimize robust data pipelines and ETL frameworks for enterprise-level systems, with a strong focus on performance, scalability, and data quality. üîß Responsibilities Develop scalable ETL pipelines using Python, DBT, SQL, and Databricks (PySpark) Design and maintain data warehouse solutions and dimensional models Work with structured, semi-structured, and unstructured data Optimize data workflows in ADF and manage job orchestration Ensure best practices in data management, security, and governance Support code reviews, Git workflows, CI/CD processes, and deployment pipelines üìå Requirements 3+ years Python coding experience 5+ years SQL Server development with large datasets 5+ years building and deploying ETL pipelines using Databricks (PySpark) Strong experience with cloud data warehouses (Synapse, ADF, Redshift, Snowflake) Solid understanding of OLTP, OLAP, data modelling, dimensions/facts Experience with enterprise-level cloud data platform migration Bonus: Experience with Airflow, AWS Lambda, Glue, Step Functions Cloud certifications are highly desirable üéÅ What‚Äôs Offered Competitive compensation up to $7,000 USD/month Flexible start with remote onboarding, transitioning to hybrid Generous Paid Time Off (PTO) Full benefits package Opportunity to build and shape a new engineering hub in Warsaw A dynamic, inclusive, and innovative work culture","[{""min"": 18226, ""max"": 25517, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,155,Cloud Data Engineer,in4ge sp. z o.o.,"We are currently seeking a highly skilled Cloud Data Engineer to join the Data & Analytics team. This role is ideal for an experienced professional with deep expertise in Google Cloud Platform (GCP), data engineering and cloud-based data solutions. The successful candidate will play a key role in designing and implementing scalable data architectures, optimizing data workflows, and driving best practices in cloud data engineering. Design, develop, and optimize data pipelines and architectures within the GCP ecosystem. Build efficient, high-performing data models and queries in BigQuery, including advanced partitioning, clustering, and performance tuning. Implement and maintain data transformation workflows using dbt and orchestration tools such as Apache Airflow (Composer). Develop data processing solutions leveraging Dataproc (Apache Spark). Apply best practices in data warehousing, data lakes and data governance. Collaborate with data analysts, architects, and business stakeholders to understand requirements and deliver robust data solutions. Ensure code quality and maintainability through version control tools such as Git. 4+ years of experience in data engineering, with at least 2+ years on GCP data services. Strong SQL skills and a track record in optimizing large datasets. Expertise in BigQuery, dbt, Spark, and Airflow, enabling the development and automation of efficient, scalable data pipelines and transformations to support data-driven decision making. Proficiency in Python, Java, or Scala for data engineering tasks. Solid understanding of data warehousing and data lake best practices. Experience with Git for version control. Certifications: Professional Cloud Architect or Professional Data Engineer. Language skills: Spanish ‚Äì fluent, English ‚Äì good (minimum B2). Experience with Dataflow (Apache Beam). Knowledge of CI/CD for data workflows. Familiarity with data security and governance in the cloud. Fully remote work with flexible working hours. Long-term collaboration on B2B contract. Opportunity to work on complex cloud projects for international clients. Professional growth in a highly skilled and supportive team. Collaborative and open working culture. üí° Don‚Äôt miss out on tailored opportunities! We have many ongoing recruitments, and new projects are constantly coming in. By giving your consent to process your data for future recruitment processes , we‚Äôll be able to invite you to roles that match your experience and expectations! PS: We‚Äôll only reach out to you when we have projects that might genuinely interest you ‚Äî without your consent, we won‚Äôt be able to do that. Our recruitment process is transparent and focused on finding the right candidate for our clients. When you apply, you can count on our objectivity, respect, and full professionalism. We look forward to receiving your CV. We connect you with the right people.",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,156,DATA Architect,Power Media,"Nasz klient to firma, kt√≥ra odwa≈ºnie podchodzi do wyzwa≈Ñ technologicznych i nie boi siƒô szukaƒá nieszablonowych rozwiƒÖza≈Ñ. Innowacyjno≈õƒá ≈ÇƒÖczy tu siƒô z solidnƒÖ wiedzƒÖ technicznƒÖ oraz doskona≈ÇƒÖ znajomo≈õciƒÖ reali√≥w bran≈ºy. SpecjalizujƒÖ siƒô w projektach z pogranicza eCommerce i Business Intelligence, oferujƒÖc kompleksowe rozwiƒÖzania oparte na sprawdzonych technologiach. Co wiƒôcej ‚Äì umiejƒôtnie ≈ÇƒÖczƒÖ te dwa ≈õwiaty, tworzƒÖc narzƒôdzia, kt√≥re zapewniajƒÖ pe≈Çen wglƒÖd w dane i realne wsparcie dla biznesu. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozw√≥j rozwiƒÖza≈Ñ Data Platform w ≈õrodowisku chmurowym. Kluczowym zadaniem bƒôdzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejs√≥w w z≈Ço≈ºonym krajobrazie systemowym klienta. Projekt obejmuje budowƒô hurtowni danych oraz platform danych od podstaw. Zesp√≥≈Ç sk≈Çada siƒô z 22 os√≥b: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiƒÖzk√≥w: Rozw√≥j i utrzymanie Data Warehouse oraz Data Platform przy u≈ºyciu narzƒôdzi ETL i SQL, Analiza wymaga≈Ñ biznesowych i proponowanie rozwiƒÖza≈Ñ technicznych, Wsp√≥≈Çpraca z zespo≈Çem w celu zapewnienia dostƒôpno≈õci rozwiƒÖza≈Ñ biznesowych, Wykorzystanie swojej wiedzy i do≈õwiadczenia do tworzenia innowacyjnych rozwiƒÖza≈Ñ. G≈Ç√≥wne wymagania: Bardzo dobra znajomo≈õƒá metod i technik projektowania oprogramowania, Ponad 10-letnie do≈õwiadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych , Znajomo≈õƒá ekosystem√≥w Big Data, Znajomo≈õƒá rozwiƒÖza≈Ñ chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejƒôtno≈õci analityczne (analiza i dokumentowanie wymaga≈Ñ biznesowych oraz specyfikacji technicznych) Do≈õwiadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomo≈õƒá SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomo≈õƒá rozwiƒÖza≈Ñ ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (min. B2+/C1) ‚Äì codzienna praca w miƒôdzynarodowym ≈õrodowisku, Gotowo≈õƒá do podr√≥≈ºy s≈Çu≈ºbowych. Mile widziane: Znajomo≈õƒá narzƒôdzi Informatica, Znajomo≈õƒá Machine Learning oraz framework√≥w ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomo≈õƒá jƒôzyk√≥w programowania (Java, Scala, C++, Python). Znajomo≈õƒá jƒôzyka niemieckiego. Firma oferuje: Stabilne, d≈Çugofalowe zatrudnienie w oparciu o B2B lub UoP, Mo≈ºliwo≈õƒá pracy w wiƒôkszo≈õci zdalnej (praca z biura 1 raz w tygodniu, we wtorek) P≈Çaska struktura, anty korporacyjne podej≈õcie do pracy i zespo≈Çu, Ciekawe, miƒôdzynarodowe projekty, Zgrany zesp√≥≈Ç chƒôtnie uczestniczƒÖcy w aktywno≈õciach sportowo ‚Äì rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team ‚Äì building), Dodatkowe zajƒôcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajƒôƒá sportowych, Nowoczesne biuro ze strefƒÖ relaksu, Co roczny 5 dniowy wyjazd firmowy (ca≈Ça firma), warsztaty kulinarne, wyj≈õcia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywno≈õciami ≈öwietna atmosfera, partnerskie podej≈õcie, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa ‚Äûmiƒôkko-techniczna‚Äù. GorƒÖca pro≈õba o CV w j. angielskim : )","[{""min"": 18000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,Data Architecture
Full-time,Senior,Permanent,Hybrid,157,üöÄ MLOps Engineer üöÄ,Team Up,"üíª Senior MLOps Engineer We're looking for a Senior MLOps Engineer to join a top-tier tech company building one of the world‚Äôs most advanced media & advertising platforms. You‚Äôll work at scale with proprietary data across smart devices, designing ML infrastructure that personalizes user experiences in real-time. Ready to engineer systems used by millions globally? üöÄ üéØ What‚Äôs in it for you: Real impact ‚Äì Your work will fuel ML models that connect global brands with their audiences in real time Modern tech stack ‚Äì Work hands-on with cutting-edge tools like Sagemaker, EMR, Kubernetes, Airflow, Spark, PyTorch, and more Growth & knowledge ‚Äì Collaborate with world-class ML researchers and engineers, access internal training, and attend top industry conferences üß† What you‚Äôll do: Design and scale ML infrastructure with high availability and low latency Build robust end-to-end pipelines for data processing, model training & deployment Serve and monitor ML models in production with observability and rollback mechanisms Automate workflows and optimize for efficiency, reliability, and speed Prototype new serving architectures, integrate A/B testing and model validation tools Collaborate with ML engineers to deliver production-grade models at scale Optionally mentor others and contribute to technical decision-making üß∞ What we‚Äôre looking for: 5+ years of experience in building production-ready microservices and infrastructure Proficiency with AWS (Sagemaker, EMR, Lambda, Step Functions), Terraform, Airflow Strong Python and Go skills Familiarity with Spark, Kafka, Docker, Kubernetes, Redis, Aerospike Experience with CI/CD (GitHub Actions, ArgoCD), observability tools (Prometheus, Grafana, QuickSight) Knowledge of gRPC, HTTP/2, and distributed systems testing strategies üéÅ Benefits: Private medical care (for you & your family), Multisport, lunch card, life insurance, flexible hours, monthly team budget, conference access, relocation support, and unlimited entry to Copernicus Science Centre üåå üìç Warszawa | üßë‚Äçüíª Hybrid (3x office) | üïê Full-time, UoP üì© Interested? Apply now and we‚Äôll get in touch with the details!",[],Data Science,Data Science
Full-time,Junior,Permanent,Hybrid,158,Junior Conversion Specialist,Macrobond Financial,"About Us Macrobond is a leading provider of global economic and financial data and technology for investment professionals. Our customers include over 900 firms spanning the buyside, sell side, corporate and academic sectors. Our platform, rich in intellectual property and supported by a rapidly expanding global team, ensures we remain at the forefront of our industry. With the backing of Francisco Partners, a prominent global tech investment firm, we operate as a truly international company. Our headquarters are in Malm√∂, Sweden, and we have key offices in Gothenburg, London, Poland, Lisbon, Hong Kong, and New York."" Job Overview We are looking for an individual to join the Conversion Team. You will be an integral part of a dynamic team responsible for supporting the sales process and assisting our customers with the onboarding process onto our platform. You will work both independently and collaboratively with the team to identify optimal solutions for our customers Job Responsibilities: Finding and identifying economic data in our database Comparing and verifying new data with old data Displacing competing products by migrating clients' research from their old data provider to Macrobond Providing post-conversion support Working with your colleagues, manager, and other departments to provide top notch customer service. Required Qualifications and experience: Very good knowledge of English Proficiency in Microsoft Office (incl. advanced Excel) Higher education is a plus An open mind and willingness to learn Good research and analytical skills Ability to work under time pressure High attention to detail and an analytical mind. Willingness to take part in occasional international business trips What do we offer: Stable position in a growing business with a ""people first"" atmosphere Private health care and sport benefits Cafeteria platform Language learning platform Internal, external or online (e.g. Udemy) training possibilities Hybrid work model A modern office in the city center with an amazing chillout space. Our commitment to Diversity Diversity, equity, and inclusion are core values at Macrobond. We believe that diverse backgrounds and perspectives strengthen our organization and foster innovation. By joining us, you‚Äôll be part of a team that values and celebrates individuality while driving success together. Apply today and become part of our exciting journey!",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Remote,159,Staff Data Engineer,Dropbox,"Dropbox is a special place where we are all seeking to fulfill our mission to design a more enlightened way of working. We‚Äôre looking for innovative talent to join us on our journey. The words shared by our founders at the start of Dropbox still ring true today. Wouldn‚Äôt it be great if our working environment‚Äîand the tools we use‚Äîwere designed with people‚Äôs actual needs in mind? Imagine if every minute at work were well spent‚Äîif we could focus and spend our time on the things that matter. This is possible, and Dropbox is connecting the dots. The nearly 3,000 Dropboxers around the world have helped make Dropbox a living workspace - the place where people come together and their ideas come to life. Our 700+ million global users have been some of our best salespeople, and they have helped us acquire customers with incredible efficiency. As a result, we reached a billion dollar revenue run rate faster than any software-as-a-service company in history. Dropbox is making the dream of a fulfilling and seamless work life a reality. We hope you‚Äôll join us on the journey. Dropbox is rebuilding the foundation of its monetization and financial data systems - and at the center of that is our revenue and growth data platform. We‚Äôre looking for a Staff Data Engineer to lead the design and delivery of this critical foundation, enabling insights and systems that drive ARR tracking, financial reporting, and product monetization. Architect the next-generation data platform for ARR, revenue attribution, and growth analytics - setting vision, driving alignment, and delivering at scale. Own and evolve core data models and systems used across Finance, Product, and Analytics teams, ensuring accuracy, trust, and accessibility. Lead platform modernization, including the adoption of scalable lakehouse architectures (Databricks, Spark, Delta), CI/CD for data, and observability frameworks. Drive adoption of scalable data practices across Dropbox through reusable tooling, process improvements, and cross-team collaboration. Partner with stakeholders (e.g., Finance, Product, Data Science, and Infrastructure) to understand data needs and deliver solutions that drive real business outcomes. Mentor and grow junior engineers, and cultivate a high-performing, innovation-driven team culture. BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience. 10+ years building large-scale data systems, with a demonstrated track record of technical leadership, including ownership of architectural direction and cross-team platform work. Proven ability to set architectural direction, lead platform evolution, and influence technical strategy across teams. Deep hands-on expertise with Spark, Spark SQL, and Databricks, along with experience orchestrating data pipelines using Apache Airflow, and writing performant, maintainable Python and SQL code. Track record of implementing data quality, testing, and observability systems at scale. Experience supporting monetization, financial, or product growth analytics with trusted and governed data models. Familiarity with cloud platforms (AWS, GCP, or Azure) and lakehouse paradigms (e.g., Delta Lake, Iceberg). Experience leading data migrations or platform transformations (e.g., from on-prem to cloud, Hadoop to Databricks). Familiarity with tools for data contracts, lineage, and governance (e.g., dbt, Monte Carlo, Great Expectations). Understanding of data privacy and compliance frameworks, including GDPR, SOX, and audit-readiness. Dropbox applies increased tax deductible costs to remuneration earned by certain qualifying employees (to the extent an employee will be involved in the creation of the software as an ‚Äúauthor‚Äù) for the transfer of copyrights, in accordance with the relevant provisions of the Personal Income Tax Act. Poland Annual Pay Range 249 900 z≈Ç‚Äî338 100 z≈Ç The range listed above is the expected annual base salary/OTE (On-Target Earnings) for this role, subject to change. Please note, OTE are for sales roles only. Salary/OTE is just one component of Dropbox‚Äôs total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs). Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to: Competitive medical, dental and vision coverage* Retirement savings through a defined contribution pension or savings plan** Flexible PTO/Paid Time Off policy in addition to statutory holidays, allowing you time to unplug, unwind, and refresh Income Protection Plans: Life and disability insurance* Business Travel Protection: Travel medical and accident insurance* Perks Allowance to be used on what matters most to you, whether that‚Äôs wellness, learning and development, food & groceries, and much more Parental benefits including: Parental Leave, Fertility Benefits, Adoptions and Surrogacy support, and Lactation support Mental health and wellness benefits Additional benefits details are available upon request. Where group plans are not available, allowances may be provided Benefit, amount, and type are dependent on geographical location, based upon applicable law or company policy Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. A big part of that effort is our support for members and allies of internal groups like Asians at Dropbox, BlackDropboxers, enABLE, TODOS (Latinx), Pridebox (LGBTQ), Vets at Dropbox, and Women at Dropbox.","[{""min"": 20825, ""max"": 28175, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,160,Senior Data Engineer,KUBO,"We are seeking an experienced Senior Data Engineer to join our Data & Analytics team and play a critical role in designing, building, and maintaining high-quality data products. As a member of a cross-functional and agile team, you will collaborate with data architects, data scientists, analytics leads, front-end developers, and business stakeholders to support data-driven decision-making across the organization. This role is ideal for someone passionate about data engineering excellence, digital transformation, and delivering impactful analytical solutions. Key responsibilities: Design and implement scalable data pipelines and globally harmonized data models by integrating data from various domains Ensure data products adhere to architectural standards, data protection regulations, and quality guidelines Provide technical leadership and guidance to other data engineers Enhance and maintain implementation frameworks to support analytical use cases and evolving product needs Ensure accurate estimations of time and cost, high-quality delivery, and smooth handover of solutions to operations Ideal candidate profile: 5+ years of experience in Data & Analytics Min. 3 years of experience and good knowledge of the Azure data ecosystem: Azure Data Lake, Azure Synapse, Databricks Knowledge of data cataloguing, data quality management, and modern data management practices Proficiency with CI/CD tools and processes Fluency in English Knowledge of Snowflake is a plus Conditions: Work model: Hybrid ‚Äì 1 day per week in the Warsaw office Salary: 20 000 - 25 000 PLN gross/month Employment type: Full-time employment contract (UoP) directly with the client Business trips to Germany 1-2 times a year Benefits: VIP Medical Care Package, Life & Travel Insurance, Company Bonus, Holiday allowance, Co-financed sport card *Relocation package and full support with relocation to Warsaw Recruitment steps: Phone call with a Recruiter (20 - 30 min.) First interview with a Manager (1h) Second interview with a technical team (1h) Feedback and decision","[{""min"": 20000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,161,Senior Data Integration Engineer,EPAM Systems,"We are looking for a Senior Data Integration Engineer to join the Client‚Äôs security team, help develop and support data products (data pipelines, data integration, data analytics dashboard). Our team is distributed between the US and the EU (Poland, Portugal). This role offers a hybrid model, with 3 days per week working from the client's office in Wroclaw, Gdansk or Krakow. RESPONSIBILITIES Assess Client‚Äôs internal infrastructure, get used to Client‚Äôs services and tools Work closely with the Client to understand their data needs, consult them and develop reporting and analytics solutions Use internal Client‚Äôs tools, build new data pipelines and ETL workflows for various use cases Implement PoCs, perform data integration and migration activities Implement data quality checks and monitor data pipelines to ensure they perform as expected Investigate and support released data products Own the code, plan and do sanity check before initiating code review Develop and maintain technical documentation Participate closely in the refinement and scope definition Present the results and outcomes REQUIREMENTS 2+ years of hands-on experience in SQL Proven expertise in dimensional data modeling, data warehousing Good communication, problem solving, and collaboration skills English: B2 Availability from 8 AM to 10 AM Pacific Time (17: 00 - 18: 00 Poland Time) is required. About once a week, the candidate should be available until 19: 00 Poland Time NICE TO HAVE Experience in programming and scripting languages such as Python, Bash, or Java Data-driven CI/CD development WE OFFER We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,162,Administrator Baz Danych Oracle / Oracle DBA,Simora,"Do≈ÇƒÖcz do naszego zespo≈Çu jako Administrator Baz Danych Oracle! Poszukujemy osoby, kt√≥ra zajmie siƒô zarzƒÖdzaniem bazami danych naszych klient√≥w. Jeste≈õmy firmƒÖ, kt√≥ra rozwija nowoczesne rozwiƒÖzania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzƒôdzi wspierajƒÖcych naszych klient√≥w. Cenimy pozytywnƒÖ atmosferƒô, wzajemny szacunek i zaanga≈ºowanie ‚Äì to fundamenty naszego zespo≈Çu. Tw√≥j zakres obowiƒÖzk√≥w Tworzenie baz danych oraz ≈õrodowisk bazodanowych na ≈õrodowisku produkcyjnym i testowym Migrowanie baz danych na nowe ≈õrodowiska ZarzƒÖdzania i aktualizacja baz danych i ≈õrodowisk bazodanowych Konfigurowanie i optymalizacja ≈õrodowiska bazodanowego Automatyzacja zada≈Ñ za pomocƒÖ jƒôzyk√≥w skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jako≈õci i bezpiecze≈Ñstwa dla tworzonych rozwiƒÖza≈Ñ Wdra≈ºanie rozwiƒÖza≈Ñ opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Sta≈Çe doskonalenie sposobu Twojej pracy Nasze wymagania Do≈õwiadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomo≈õƒá jƒôzyka SQL i PLSQL Umiejƒôtno≈õƒá dbania o szczeg√≥≈Çy i jako≈õƒá rozwiƒÖza≈Ñ Komunikatywno≈õƒá Znajomo≈õƒá jƒôzyka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykszta≈Çcenie wy≈ºsze (preferowany kierunek: informatyka) Znajomo≈õƒá jƒôzyk√≥w programowania: Python, Java itp. Znajomo≈õƒá system√≥w operacyjnych Linux / Windows Znajomo≈õƒá system√≥w wirtualizacyjnych np.: OLVM Oferujemy CiekawƒÖ pracƒô w firmie o wysokiej dynamice rozwoju Mo≈ºliwo≈õƒá podniesienia kwalifikacji w obszarach zwiƒÖzanych z bazami danych, bezpiecze≈Ñstwem danych oraz sztucznƒÖ inteligencjƒÖ Benefity Karta Multisport Prywatna opieka zdrowotna ‚Äì Medicover Praca zdalna Brak dress code‚Äôu Dofinansowanie szkole≈Ñ i kurs√≥w Elastyczny czas pracy","[{""min"": 7000, ""max"": 13000, ""type"": ""Net per month - B2B""}]",Database Administration,Database Administration
Full-time,Mid,B2B,Hybrid,163,Big Data Analyst,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: udzia≈Ç w projekcie gromadzƒÖcym dane dotyczƒÖce u≈ºytkowania urzƒÖdze≈Ñ domowych. Wykorzystywany stos technologiczny w projekcie: SQL, Python, R, Scala, Hadoop, Spark, Hive, Kafka, Power BI, DAX, Power Query, przetwarzanie, modelowanie i transformacja danych, tworzenie i wdra≈ºanie modeli danych reprezentujƒÖcych struktury i relacje danych, programowanie skrypt√≥w w Pythonie, R lub Scala na potrzeby analizy i wizualizacji danych, automatyzacja proces√≥w przetwarzania danych, wdra≈ºanie efektywnych metod przechowywania i pozyskiwania danych, integracja technologii Big Data z przep≈Çywami danych, projektowanie i tworzenie interaktywnych, profesjonalnych dashboard√≥w w Power BI, zapewnienie intuicyjno≈õci i dopasowania dashboard√≥w do potrzeb biznesowych, tworzenie trafnych wizualizacji skutecznie komunikujƒÖcych wnioski z danych, dostosowywanie wizualizacji do r√≥≈ºnych odbiorc√≥w biznesowych, optymalizacja przep≈Çyw√≥w pracy zwiƒÖzanych z przetwarzaniem i wizualizacjƒÖ danych, praca w modelu hybrydowym: min. 1x na miesiƒÖc w biurze w Warszawie, stawka do 120 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz min. 3 lata do≈õwiadczenia na stanowisku Data Analyst, masz bardzo dobrƒÖ znajomo≈õƒá SQL i technik zapyta≈Ñ do baz danych, masz praktyczne do≈õwiadczenie w przetwarzaniu, modelowaniu i transformacji danych, znasz przynajmniej jeden jƒôzyk programowania wykorzystywany w analizie danych (np. Python, R, Scala), masz do≈õwiadczenie w pracy z du≈ºymi, z≈Ço≈ºonymi zbiorami danych, potrafisz tworzyƒá profesjonalne, interaktywne dashboardy w Power BI, masz do≈õwiadczenie w pracy z DAX i Power Query przy modelowaniu i transformacji danych w Power BI, potrafisz przek≈Çadaƒá potrzeby biznesowe na rozwiƒÖzania techniczne oraz prezentowaƒá rekomendacje oparte na danych, znasz jƒôzyk angielski na poziomie min. C1, mile widziane do≈õwiadczenie z technologiami Big Data (np. Hadoop, Spark, Hive, Kafka). Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 16000, ""max"": 19200, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Hybrid,164,"AVP, Model Development (AI & Analytics)",HSBC Service Delivery,"Regulatory Compliance ensures adherence to applicable regulations and standards by setting policies which cover HSBC‚Äôs regulatory requirements and mitigate conduct and reputational risk issues. The team strives to remain ahead of the regulatory change agenda and ensures the business understands the implications and is prepared for change. It monitors how Global Businesses and Functions manage their responsibilities with regard to complying with the regulations and helps resolve compliance deficiencies. This role is Hybrid from Krakow and requires at least 4 years experience. Promote a culture of data driven decision making, aligning short term decisions and investments with longer term vision and objectives. Help the business to manage regulatory risk in a more effective, efficient, and commercial way through the adoption of data science (AI/ML and advanced analytics) Support communication and engagement with stakeholders and partners to increase understanding and adoption of data science products and services also research opportunities. Collaborate with other analytics teams across the banks to share insight and best practice. Foster a collaborative, open and agile delivery culture. Build positive momentum for change across the organization with the active support and buy-in of all stakeholders. Create and deliver analytical solutions that are fully documented and explained that meet the business aims/requirements. Identify, monitor and mitigate top and emerging risks and ensure Senior Management awareness. University degree in technology, data analytics or related discipline or relevant work experience in computer or Data Science Understanding of Regulatory Compliance, risks and direct experience of deployment of controls and analytics to manage those risks. Experience in Financial Services (experience within a tier one bank) or related industry Solid understanding of applied mathematics, statistics, data science principles and advanced computing (machine learning, modelling, NLP and Generative AI) Experience working within the Hadoop ecosystem in addition to strong technical skills in analytical languages such as Python and SQL. Specific knowledge of GCP, AWS, Azure, Spark and/or graph theory an advantage. Experience of visualization tools and techniques including Qlik and Tableau Solid understanding of data & architecture concepts including cloud, ETL, Ontology, Data Modelling. Experience of using JIRA, GIT, Confluence, Teams, Slack, Advanced Excel Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery discounts Financial support with trainings and education Social fund Flexible working hours Free parking",[],Data Science,Data Science
Full-time,Mid,Mandate,Remote,165,IT Support & Systems Administrator (Salesforce CRM),BookingHost Sp. z o.o.,"Do zespo≈Çu IT poszukujemy osoby skrupulatnej, bystrej, komunikatywnej i samodzielnej, kt√≥ra wesprze nas w codziennej obs≈Çudze i rozwoju system√≥w informatycznych, w tym rozwiƒÖzywaniu zg≈Çosze≈Ñ naszych pracownik√≥w (ticket√≥w). Administracji system√≥w informatycznych - Salesforce CRM i szeregiem innych aplikacji - zarzƒÖdzaniem u≈ºytkownikami, konfiguracjƒÖ kont, zakresem ich uprawnie≈Ñ itp.); Udziale w planowaniu proces√≥w biznesowych w firmie, przygotowaniem pod nie wymaga≈Ñ technicznych oraz ich realizacji; Wdra≈ºaniu nowych funkcjonalno≈õci m.in. automatyzacji z u≈ºyciem Salesforce Flows, Make; Integracji nowych narzƒôdzi; Sprawowaniu pieczy nad porzƒÖdkiem w modelu danych; Generowaniu raport√≥w i analizie danych; Bie≈ºƒÖcym wsparciu u≈ºytkownik√≥w. Dobra znajomo≈õƒá administracji CRM Salesforce (ewentualnie innego systemu CRM / ERP); Do≈õwiadczenie w administracji innych system√≥w i/lub wsparciu technicznym; Sprawne pos≈Çugiwanie siƒô arkuszami Google Sheets / Excel; Samodzielno≈õƒá i chƒôƒá uczenia siƒô, tak≈ºe z wykorzystaniem dostƒôpnych w sieci materia≈Ç√≥w; Umiejƒôtno≈õƒá pracy w dynamicznym ≈õrodowisku, wielozadaniowo≈õƒá, szybko≈õƒá adaptacji do zmian; Dobra organizacja pracy, umiejƒôtno≈õƒá trafnej oceny priorytet√≥w; Swoboda w komunikacji, wysoka kultura osobista; Dobra znajomo≈õƒá jƒôzyka angielskiego (czytanie dokumentacji, aktywne uczestnictwo w spotkaniach). architekta rozwiƒÖza≈Ñ Salesforce i/lub w programowaniu w APEX lub Java; z narzƒôdziami takimi jak Front, Calendly, Make, Zapier, Google Workspace, Slack, wirtualna centralka, systemy ticketowe (np. Jira, ServiceNow); w nadzorowaniu projekt√≥w prowadzonych z zewnƒôtrznymi podmiotami jak agencje deweloperskie; W integracji Salesforce z innymi systemami / aplikacjami. W pe≈Çni zdalnƒÖ pracƒô, swobodnƒÖ atmosfera w zespole, elastyczne godziny; P≈Çatny urlop przy umowie B2B; Wsparcie w rozwoju umiejƒôtno≈õci administrowania system√≥w; Realny wp≈Çyw na codziennƒÖ pracƒô firmy i rozw√≥j struktury IT; Pracƒô w szybko rosnƒÖcej i dynamicznej firmie, aspirujƒÖcej do pozycji lidera na rynku wynajmu mieszka≈Ñ; Dostƒôpne opcje Karty MultiSport i MultiLife. üìù Proces rekrutacji Proces rekrutacji sk≈Çada siƒô z dw√≥ch etap√≥w ‚Äì oba odbywajƒÖ siƒô zdalnie: Rozmowa telefoniczna (ok. 15‚Äì30 minut) ‚Äì kr√≥tkie poznanie siƒô, om√≥wienie do≈õwiadczenia i oczekiwa≈Ñ. Spotkanie online ( ok. 60 minut) ‚Äì rozmowa techniczna z cz≈Çonkiem zespo≈Çu IT, podczas kt√≥rej sprawdzimy wiedzƒô praktycznƒÖ i dopasowanie do roli.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 9000, ""type"": ""Gross per month - Mandate""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Hybrid,166,Senior Master Data Specialists,emagine Polska,"PROJECT INFORMATION: Start : September Contract : B2B, 3-6 months + extensions Work mode : Hybrid (3-4 days per week from the Poznan or Warsaw office) Project language: English Business trips: occasional Recruitment process: 2 interviews We are looking for Senior Master Data Specialists who will establish and maintain robust data governance strategies for master data objects within the Group Operations and Technology framework. The primary goal is to enhance data integrity, improve procurement processes, and ensure alignment with organizational objectives. Main Responsibilities: Building Data Governance for Master Data objects in the scope of Group Operations and Technology. Analyzing existing processes to identify bottlenecks and inefficiencies. Designing new workflows, procedures, and manuals. Collaborating with GSP members to improve procurement processes. Ensuring data accuracy, consistency, and completeness through validation and cleaning. Identifying and eliminating duplicate data entries to reduce errors and enhance reliability. Creating and building visual representations of data to effectively convey information. Facilitating a better understanding of procurement metrics through data analysis. Managing and maintaining procurement master data. Monitoring processes, collecting feedback, and tracking performance to ensure long-term improvements. Creating and updating process manuals, instructions, and documentation of procurement tools and methodologies. Generating regular reports and sharing key findings and performance metrics with stakeholders. Organizing training sessions and knowledge-sharing initiatives for the procurement team. Collaborating with stakeholders across departments to align procurement master data activities with organizational goals. Participating in cross-functional projects and providing procurement data insights. Building open and transparent communication with GSP and other departments. Ensuring effective information exchange and alignment of procurement activities with broader organizational objectives. Key Requirements: 5+ years' experience in analysis, tools, and process areas, or procurement in production companies or other relevant industries . Experience with ERP systems, preferably SAP. Knowledge of Master Data Objects, particularly Vendor and Material. Knowledge of setup of Vendors and Materials. Relevant business degree. Strong analytical and problem-solving skills. Proven track record for delivering sustainable results. High professional business level proficiency in written and spoken English. Nice to Have: International experience. Advanced analytics skills. Change Management skills. Data visualization skills.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,167,Experienced Azure Data Engineer,Future Mind,"Future Mind is a brilliant, inspiring team, one of the most awarded tech consulting companies in the region with a broad portfolio of clients, including ≈ªabka, Jeronimo Martins (Hebe, Biedronka), LPP (Reserved, Sinsay, Mohito), eObuwie, Modivo, and other well-recognized brands. We have received several industry awards for delivering some of the best eCommerce applications in Poland, listed among the most popular across app stores. Our expert engineers, designers, project managers, and analysts work on projects ranging from top mobile commerce apps used daily by millions of customers to IoT, and telematics platforms that produce vast amounts of data. In 2023, we joined forces with Solita, a Finnish tech powerhouse with a vibrant community of over 2000 specialists across Europe, that combines data, business, and technology skills to build and improve digital services for leading organizations in manufacturing, medical, shipping, and other major industries, including such clients as NATO, Nokian Tires, Pfizer and many others. Together we are dedicated to delivering cutting-edge, data-driven solutions. We value proactive professionals who take ownership, enjoy solving problems, sharing knowledge, and collaboration. Together, we create high-quality software solutions that fulfil our clients' business needs and impact their customers' lives every day. As part of the Solita Group, Future Mind provides digital advisory & delivery services with the support of our international partners and upholds equally high cultural standards. Role description: As an Azure Data Engineer, your role involves designing and implementing ingestion and transformation pipelines, and data models while collaborating closely with the business stakeholders. You have worked with the Azure data stack and cloud for several years and understand the value created for customers. This job is all about: Building end-to-end ELT data pipelines on top of an Azure Data Services ; Designing data models based on large, complex data sets that meet both functional and non-functional business requirements; Holistic platform view (source systems, networking, orchestration, etc.) with the ability to deliver business value; Collaborating within our project teams to meet client needs and deliver high-quality solutions. Here‚Äôs what we‚Äôre looking for: Vast experience with Azure Data Services; Deep understanding of dimensional data modeling techniques (data vault would be a plus); Proficiency with combining SQL and Python with a modern ELT toolset (Fabric, Synapse, Spark, or equivalent); Experience with ADF, Airflow, or other orchestration tools; Effective communication skills, proficient in Polish and English. You also must have the legal right to work in the EU to apply for this position. ‚Ä¶and here‚Äôs what we offer: A dynamic work environment where innovation and collaboration are valued. Access to cutting-edge projects and technologies in a variety of industries. A supportive community of experts to foster your professional growth and development. Competitive compensation, comprehensive benefits, and a focus on work-life balance. Opportunities for continuous learning and career advancement, including specialized training in big data technologies and Snowflake certifications. The ability to work fully remotely or check into one of our offices whenever you like, Fully paid private health insurance, subsidized sports membership, mental health support, and language courses.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,168,Data Analyst (mid level),CGI,"Do≈ÇƒÖcz do naszego zespo≈Çu i wesprzyj realizacjƒô kluczowego projektu dla sektora bankowego! Szukamy Analityka Danych, kt√≥ry bƒôdzie odpowiedzialny za dostarczanie rzetelnych danych do analiz ryzyka, tworzenie raport√≥w oraz budowanie dashboard√≥w monitorujƒÖcych kluczowe wska≈∫niki. Do Twoich obowiƒÖzk√≥w bƒôdzie nale≈ºa≈Ço: ‚Ä¢ Przygotowywanie i dostarczanie danych do analiz oraz oblicze≈Ñ finansowych ‚Ä¢ Tworzenie raport√≥w i dashboard√≥w wspierajƒÖcych procesy decyzyjne ‚Ä¢ Wyszukiwanie i pobieranie danych z r√≥≈ºnych ≈∫r√≥de≈Ç ‚Ä¢ Zapewnienie wysokiej jako≈õci danych i ich zgodno≈õci z wymaganiami biznesowymi ‚Ä¢ Wsp√≥≈Çpraca z zespo≈Çem projektowym przy u≈ºyciu narzƒôdzi takich jak JIRA. Idealnie aby≈õ posiada≈Ç/a ‚Ä¢ Minimum 4 letnie do≈õwiadczenie na stanowisku Analityka Danych, najlepiej w sektorze bankowym lub finansowym ‚Ä¢ Bardzo dobrƒÖ znajomo≈õƒá SQL i umiejƒôtno≈õƒá pracy z du≈ºymi i z≈Ço≈ºonymi zbiorami danych ‚Ä¢ Do≈õwiadczenie w tworzeniu raport√≥w i dashboard√≥w (np. Power BI, Tableau lub inne) ‚Ä¢ Umiejƒôtno≈õƒá samodzielnego wyszukiwania danych niezbƒôdnych do analiz ‚Ä¢ Znajomo≈õƒá narzƒôdzi do zarzƒÖdzania projektami (np. JIRA) ‚Ä¢ Wiedzƒô na temat danych i proces√≥w bankowych (np. ocena ryzyka, analiza wynik√≥w finansowych) ‚Ä¢ Znajomo≈õƒá proces√≥w ETL i budowy pipeline‚Äô√≥w danych Mile widziane ‚Ä¢ Znajomo≈õƒá Pythona i/lub Sparka w kontek≈õcie analizy danych ‚Ä¢ Do≈õwiadczenie w zapewnianiu jako≈õci danych (data quality) ‚Ä¢ Podstawowa znajomo≈õƒá chmurowych platform danych (AWS, GCP, Azure). Ze swej strony mo≈ºemy zaoferowaƒá: ‚Ä¢ Pracƒô hybrydowƒÖ w Warszawie ‚Ä¢ Benefity w ca≈Ço≈õci op≈Çacane przez CGI (przy UOP): ‚Ä¢ Opiekƒô zdrowotnƒÖ - rozbudowany plan Medicover Maxima obejmujƒÖcy opiekƒô stomatologicznƒÖ ‚Ä¢ Kartƒô Medicover Sport ‚Ä¢ Ubezpieczenie na ≈ºycie, od wypadku i powa≈ºnego zachorowania. ‚Ä¢ Program Wsparcia Pracownik√≥w, kt√≥ry zapewnia Tobie i Twojej rodzinie bezp≈Çatny i anonimowy dostƒôp do pomocy specjalist√≥w, m.in . psychologa, terapeuty oraz prawnika ‚Ä¢ Plan Zakupu Akcji CGI - je≈õli zdecydujesz siƒô na inwestycje w akcje ramach naszego programu, my do≈Ço≈ºymy drugie tyle. Swobodnie i bez kosztowo mo≈ºesz obs≈Çugiwaƒá sw√≥j rachunek maklerski ‚Ä¢ Sformalizowany program pracy tw√≥rczej - a≈º 90% swojego czasu mo≈ºesz zaraportowaƒá jako pracƒô podlegajƒÖcƒÖ autorskim kosztom uzyskania przychodu ‚Ä¢ PPP (Profit Participation Plan) - rocznƒÖ premiƒô zale≈ºnƒÖ od wypracowanych wynik√≥w Business Unitu ‚Ä¢ Referral Program ‚Äì otrzymasz ponad 6000 z≈Ç za ka≈ºdƒÖ osobƒô zatrudnionƒÖ z Twojego polecenia ‚Ä¢ Program Be Consultant , kt√≥ry daje mo≈ºliwo≈õƒá rozwijania kompetencji, rotacjƒô przy projektach, a nawet zmianƒô ≈õcie≈ºki zawodowej w ramach firmy ‚Ä¢ Mentoring Program , dziƒôki kt√≥remu mo≈ºesz rozwijaƒá swoje kompetencje poszerzaƒá wiedzƒô, czerpiƒÖc jƒÖ od swoich koleg√≥w z CGI. Je≈õli sam posiadasz du≈ºe do≈õwiadczenie, mo≈ºesz siƒô dzieliƒá wiedzƒÖ z innymi! ‚Ä¢ Atrakcyjny program szkole≈Ñ - masz dostƒôp do szerokiego pakietu szkole≈Ñ: partnerskich szkole≈Ñ, profesjonalnych certyfikat√≥w po szkolenia na platformie CGI Academia. Zapraszamy do rozm√≥w!",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,169,Senior Data Engineer,j-labs,"Do≈ÇƒÖcz do zespo≈Çu, kt√≥ry buduje platformƒô do zarzƒÖdzania majƒÖtkiem opartym na ETF-ach ‚Äì skalowalne, nowoczesne narzƒôdzie majƒÖce realny wp≈Çyw na spos√≥b inwestowania. Team jest odpowiedzialny za przetwarzanie du≈ºej ilo≈õci danych potrzebnych do generowania r√≥≈ºnego rodzaju raport√≥w.‚Äã JednƒÖ z domen, kt√≥rƒÖ zajmuje siƒô zesp√≥≈Ç jest zarzƒÖdzaniem ryzykiem.‚Äã Szukamy osoby samodzielnej i zaanga≈ºowanej, kt√≥ra bƒôdzie: Rozwijaƒá skalowalnƒÖ infrastrukturƒô danych w chmurze, kt√≥ra stanowi podstawƒô dzia≈Çalno≈õci firmy opartej na danych, z wykorzystaniem najnowszych technologii. Tworzyƒá rozwiƒÖzania do przetwarzania danych opartych na AWS, integrujƒÖcych dane z us≈Çug wewnƒôtrznych i zewnƒôtrznych. Budowaƒá jezioro danych finansowych, ≈ÇƒÖczƒÖcej nowoczesne technologie z funkcjami wymaganymi przez przepisy regulacyjne. ≈öci≈õle wsp√≥≈Çpracowaƒá z data scientistami, zespo≈Çami produktowymi i deweloperskimi przy wdra≈ºaniu inteligentnych funkcji do naszego produktu. Dzieliƒá siƒô wiedzƒÖ eksperckƒÖ na temat najlepszych praktyk w zakresie danych wewnƒÖtrz firmy. Wymagania: Do≈õwiadczenie w projektowaniu i obs≈Çudze potok√≥w danych w ≈õrodowisku AWS (min. 5 lat). Znajomo≈õƒá SQL . Do≈õwiadczenie w Pythonie, w tym znajomo≈õƒá framework√≥w takich jak DBT. Do≈õwiadczenie w pracy z us≈Çugami AWS , takimi jak S3, Athena i Glue. Znajomo≈õƒá narzƒôdzi Infrastructure-as-Code, takich jak Terraform . Pasja do podej≈õcia ""everything-as-code"" i pisanie dobrze zaprojektowanego, testowalnego i udokumentowanego kodu. Do≈õwiadczenie w pracy w metodykach zwinnych, np. Scrum. Zainteresowanie us≈Çugami finansowymi i rynkami. Bieg≈Ço≈õƒá w jƒôzyku angielskim w mowie i pi≈õmie- min. B2. Mo≈ºliwa praca 100% zdalna, ale ze wzglƒôdu na potrzebƒô okazjonalnych spotka≈Ñ zespo≈Çu w biurze, w pierwszej kolejno≈õci bierzemy pod uwagƒô osoby mieszkajƒÖce w Warszawie lub Krakowie, gdzie mamy biura.","[{""min"": 190, ""max"": 210, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,170,Remote Senior Quantitative Developer,Montrose Software,"Client description: Montrose Software has an ongoing relationship with one of the largest North American banks to enhance and maintain its production libraries and applications. The bank serves 17 million clients worldwide and provides personal and commercial banking, wealth management, insurance, investor services, and capital markets products and services globally. We work mainly with their capital markets teams, focusing on derivatives and fixed income systems. Project description: We work on maintaining and enhancing a Risk management system for a big bank. The system is a core internal instrument for the data analyst and economist to make risk calculations. It is split into 3 layers: Front end - Microsoft Excel with a fully custom set of function and menus; standalone Python app Business Logic Library - C++ and Python modules on the network Calculation grid - 50k computing grid running the simulations in house, AWS and Azure Qualifications: 5+ years of experience in software development, ideally in the quantitative finance area Good knowledge of C++ and basic knowledge of Python In-depth knowledge of various (vanilla and exotic) derivative products Familiarity with mathematical models for the dynamics of the financial market FpML knowledge is a plus Numeracy to be able to understand quantitative finance Commercial experience in financial or capital markets Ability to use a version control system, ideally Git Finding yourself in a large codebase Good communication skills Being open to code in other programming languages Fluent in spoken and written English Nice to have: Familiarity with AWS, Docker Experience in SQL Please note that the interview process is divided into four parts: Technical phone screening~30 minutes Technical interview~2 hours Non-technical call with hiring manager~45 minutes Optional: call with client for which you will be working We offer: Premier equipment to work Flexible working hours Remote work possibility Interesting, challenging and exciting work with international teams English lessons with native speaker Training Budget Multisport card Food : Lunches from Krak√≥w's restaurants that are delivered both to the office and homes( or a refund of the budget allocated for it) Kitchen full of food, drinks, fruit, and snacks Health Private medical insurance Air-conditioning Well-being: No dress code The chillout area incudes comfortable bean bags, therapy balls, PlayStation 4 or Nintendo Switch + games stretching area and pull-up bar Team events Shower Additional : Indoor parking place for bicycles","[{""min"": 22000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Hybrid,171,"Big Data Developer (Scala, Spark, Hadoop)",Link Group,"Big Data Developer (Scala, Spark, Hadoop) Hybryda: Warszawa, Gdynia, Gda≈Ñsk (2x w tygodniu) 100-120 z≈Ç/h net+vat B2B Regular Minimum 3 lata do≈õwiadczenia w programowaniu funkcyjnym w jƒôzyku Scala Do≈õwiadczenie w tworzeniu aplikacji opartych na Spark przy u≈ºyciu Scali Znajomo≈õƒá technologii Hadoop, Hive, Oozie, Kafka Znajomo≈õƒá rozwiƒÖza≈Ñ kontenerowych, w tym Docker i Kubernetes Bieg≈Ço≈õƒá w jƒôzyku angielskim oraz polskim Projektowanie, implementacja i optymalizacja aplikacji Big Data z wykorzystaniem Scali, Apache Spark i Hadoop Tworzenie oraz utrzymywanie pipeline‚Äô√≥w przetwarzania danych w ≈õrodowisku rozproszonym Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç z wykorzystaniem narzƒôdzi takich jak Hive, Oozie i Kafka Wsp√≥≈Çpraca z zespo≈Çem DevOps przy wdra≈ºaniu rozwiƒÖza≈Ñ w kontenerach Wsparcie w projektowaniu i wdra≈ºaniu skalowalnych, wydajnych rozwiƒÖza≈Ñ danych Udzia≈Ç w codziennej pracy zespo≈Çu w ≈õrodowisku Agile","[{""min"": 100, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,172,Senior Data Engineer - Advertising,Allegro,"About the team A hybrid work model requires 1 day a week in the office Allegro is among the largest and most advanced Tech companies in Poland. The Allegro Advertising business has been growing approximately twice as fast as the Allegro platform and about three times faster than the advertising market in Poland over the past few years. At Allegro, we are shaping the future of e-commerce by offering not only cutting-edge technologies but also an exceptional environment for professional growth - spanning both technical and soft skills. The Brand-First Analytics team, operating within Advertising Analytics, sits at the intersection of technology and business. It supports strategic advertising initiatives by providing data-driven insights, analyses, tools, and unique analytical solutions for brands advertising on Allegro. Your main responsibilities: In this role, you will be responsible for developing and optimizing large-scale data processing workflows that support strategic Brand-First Advertising initiatives at Allegro. Your tasks will include: Building, managing, and improving data processing pipelines - You will ensure the efficiency and scalability of analytical solutions by independently designing, managing, and optimizing processes for handling large volumes of data. Close collaboration with Data Analysts (Mid/Senior/Expert) and Data Engineers - You will work daily with experienced professionals, co-developing advanced analytical products for brands advertising on Allegro as well as supporting internal needs of the Advertising Analytics Team. Developing data architecture - You will design, expand, and refine the data architecture that powers analytical products. Monitoring data quality and consistency - You will be responsible for maintaining data integrity, reliability, and effectiveness by continuously monitoring and improving data quality and consistency. Optimizing data processing costs in Google Cloud Platform - You will help manage cloud resources efficiently and optimize costs associated with data processing workflows. Automating and scaling analytical processes - You will enhance operational efficiency by developing solutions that optimize workflows for both our team and key stakeholders. Supporting the growth of Advertising BU initiatives - You will work closely with business teams including Sales, Strategy, Special Projects, and Brand & Display Business Project Management. Collaborating with cross-functional teams - Your analytical solutions will play a key role in shaping advertising strategies for brands and improving data analysis workflows. Senior-level mentoring and support for Data Engineers in Advertising - You will drive best practices, ensure strategic alignment of solutions, and facilitate knowledge sharing within the team, supporting competency development and enhancing your leadership skills. Engaging in company-wide analytics scaling initiatives - You will have the opportunity to make an impact and contribute to the growth and optimization of analytics processes across the organization. This is the right job for you if: Have a minimum of 5 years of experience in the role of Data Engineer, processing and analyzing large datasets (SQL, Python, PySpark, Airflow), with a strong focus on designing and optimizing ETL processes. Have strong SQL skills and can optimize queries for both performance and cost efficiency. Are proficient in Python, effectively process data using PySpark and Airflow, and follow best practices in data engineering. Have experience working in cloud environments (GCP, AWS, or Azure) and understand database algorithms and structures. Can independently manage their scope of work, propose improvements, and successfully plan and execute projects in line with the roadmap. Take full ownership of meeting deadlines and achieving business objectives, while effectively planning and estimating the time required to complete tasks and managing priorities. Collaborate effectively with various stakeholders and analysts, ensuring impactful solutions while fostering team growth. Seek opportunities to optimize processes and propose innovative solutions that increase operational efficiency. What we offer: A hybrid work model. Well-located offices (with fully equipped kitchens and bicycle parking facilities) and excellent working tools (height-adjustable desks, interactive conference rooms) A wide selection of fringe benefits in a cafeteria plan ‚Äì you choose what you like (e.g. medical, sports or lunch packages, insurance, purchase vouchers) English classes that we pay for related to the specific nature of your job Work in a team you can always count on ‚Äì we have on board top-class specialists and experts in their areas Training budget and an internal educational platform, MindUp (with training courses on work organization, means of communication, motivation to work and various technologies and substantive issues) If you want to learn more, check it out for yourself Apply to Allegro and see why it is #dobrzetubyƒá (#goodtobehere)",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,173,Senior Data Engineer,HSBC Service Delivery,"Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity If you‚Äôre looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. HSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions. We are currently seeking an experienced professional to join our team in the role of Senior Data Engineer. What you‚Äôll do Establish best practices for data engineering and ensure team adherence. Perform code testing and conduct reviews with team members to ensure high-quality, efficient, and well-tuned code. Design and implement data-driven solutions in collaboration with stakeholders. Develop and maintain data pipelines and CI/CD pipelines, optimizing data integration processes for accuracy and efficiency. Own & review release process. What you need to have to succeed in this role Fluency in English, both verbal and written. Previous experience of an international and multi-cultural context is a plus. 5 years + experience as PL/SQL-Unix Developer. Time management and prioritization skills as well as the ability to work autonomously. Knowledge and previous experience in Business Finance and/or Data Engineering contexts will be appreciated. Technical skills (must have): Oracle PL/SQL, Procedures and packages development, SQL query optimization, Advanced error handling, Unix shell. Other technical skills: Perl, Jenkins / Ansible, Agile methodologies, Github, JIRA, Confluence, Vue.js, Spring Boot, ControlM. What we offer Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery discounts Financial support with trainings and education Social fund Flexible working hours Free parking Intellectual property benefit If your CV meets our criteria, you should expect the following steps in the recruitment process: Online assessment. Telephone screen Job interview with the hiring manager. We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,174,Senior Data Engineer (Databricks),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Senior Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Data Platform Transformation for energy management association body. This project addressed critical data management challenges, boosting user adoption, performance, and data integrity. The team is implementing a comprehensive data catalog, leveraging Databricks and Apache Spark/PySpark, for simplified data access and governance. Secure integration solutions and enhanced data quality monitoring, utilizing Delta Live Table tests, established trust in the platform. The intermediate result is a user-friendly, secure, and data-driven platform, serving as a basis for further development of ML components. Design of the data transformation and following data ops pipelines for global car manufacturer. This project aims to build a data processing system for both real-time streaming and batch data. We‚Äôll handle data for business uses like process monitoring, analysis, and reporting, while also exploring LLMs for chatbots and data analysis. Key tasks include data cleaning, normalization, and optimizing the data model for performance and accuracy. üöÄ Your main responsibilities: Design and optimize scalable data processing pipelines for both streaming and batch workloads using Big Data technologies such as Databricks, Apache Airflow, and Dagster. Architect and implement end-to-end data platforms, ensuring high availability, performance, and reliability. Lead the development of CI/CD and MLOps processes to automate deployments, monitoring, and model lifecycle management. Develop and maintain applications for aggregating, processing, and analyzing data from diverse sources, ensuring efficiency and scalability. Collaborate with Data Science teams on Machine Learning projects, including text/image analysis, feature engineering, and predictive model deployment. Design and manage complex data transformations using Databricks, DBT, and Apache Airflow, ensuring data integrity and consistency. Translate business requirements into scalable and efficient technical solutions while ensuring optimal performance and data quality. Ensure data security, compliance, and governance best practices are followed across all data pipelines. üéØ What you‚Äôll need to succeed in this role: At least 5 years of commercial experience implementing, developing, or maintaining Big Data systems. Strong programming skills in Python : writing a clean code, OOP design. Strong SQL skills, including performance tuning, query optimization, and experience with data warehousing solutions. Experience in designing and implementing data governance and data management processes. Deep expertise in Big Data technologies, including Apache Airflow, Dagster, Databricks , and other modern data orchestration and transformation tools. Experience implementing and deploying solutions in cloud environments (with a preference for Azure ). Knowledge of how to build and deploy Power BI reports and dashboards for data visualization. Excellent understanding of dimensional data and data modeling techniques. Consulting experience and the ability to guide clients through architectural decisions, technology selection, and best practices. Ability to work independently and take ownership of project deliverables. Familiarity with Spark, Azure Event Hub or Kafka. Master‚Äôs or Ph.D. in Computer Science, Big Data, Mathematics, Physics, or a related field. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn , Instagram ).","[{""min"": 21000, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,175,Data Center Engineer ‚Äì Database,Link Group,"Data Center Engineer ‚Äì Database We are seeking an experienced Data Center Engineer with a strong background in database administration to support the performance, availability, and security of enterprise database systems within a high-availability data center environment. This role involves hands-on management of relational and/or NoSQL databases, ensuring data integrity, optimizing performance, and supporting business continuity processes. Key Responsibilities: Administer, monitor, and maintain database systems (e.g. Oracle, MS SQL Server, PostgreSQL, MySQL, MongoDB) Ensure database performance, availability, and scalability in mission-critical environments Perform database backups, restorations, and support disaster recovery testing Implement security policies, user access controls, and data encryption where applicable Optimize queries and database structures to improve performance and resource usage Collaborate with infrastructure, application, and security teams to support integrated operations Create and maintain documentation for configurations, procedures, and data policies Participate in incident response and root cause analysis related to database issues Requirements: Proven experience in database administration within enterprise or data center environments Strong knowledge of SQL, database performance tuning, indexing, and replication Experience with backup and recovery strategies, including tools like RMAN, Veeam, or native DB tools Understanding of high availability and clustering technologies (e.g. Always On, Oracle RAC) Familiarity with Linux/Windows environments and scripting (e.g. PowerShell, Bash) Strong analytical and troubleshooting skills Good communication and teamwork abilities Willingness to work in a 24/7 shift rotation Certifications such as Oracle DBA, Microsoft Certified: Azure Database Administrator, or MongoDB Certified DBA are a plus",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,176,Data Science Analyst,Trans.eu Group SA,"Who are we looking for: We are looking for a Data Science Analyst with strong analytical thinking skills and a proactive approach to solving complex problems. You should be adept at interpreting data to provide key insights that support business decisions. The role & the team: In this role, you will translate business requirements into analytical solutions and process large datasets in Snowflake and AWS . You will utilize machine learning methods and create interactive Tableau dashboards to monitor products and drive their continuous improvement. Your work will be crucial in identifying trends and areas for enhancement within our product functionalities. What will you do: Conduct exploratory analyses to identify trends, anomalies, and key areas for improving product functionalities. Translate business requirements into analytical solutions and create analytical documentation. Process large datasets in Snowflake and AWS, utilizing machine learning methods for analytics. Design, build, and optimize interactive dashboards in Tableau for ongoing product monitoring. Monitor the quality of delivered solutions to ensure their continuous development and improvement. Ensure the quality of the analytical solutions you provide. Required skills: At least 3 years of experience in advanced data analysis. Advanced knowledge of SQL (e.g., Snowflake). Knowledge of Python . Good knowledge of Tableau for designing, building, and optimizing interactive dashboards. Knowledge of Data Science (model evaluation, types of machine learning models, clustering methods). Understanding of basic machine learning algorithms. Ability to work with Git for code versioning and team collaboration. Analytical thinking and problem-solving skills for complex business and technical issues. Ability to interpret data, draw conclusions, and identify key insights to support decision-making. Fluent Polish and advanced English. Bonus points for: Experience with cloud platforms ( AWS , Snowflake ). Knowledge of Airflow . Interest in new technologies in the Data Science field. What your future teammates love the most: Private LuxMed healthcare, free preventive check-ups Comfort Work Setup: funding for upgrading your home workstation International workation trips & team-building events Family trips to our private lakeside resort Sport & Fun : Multisport Card & company sport groups ‚Äì yoga, basketball, crossfit and more Subsidized English lessons with a native speaker Life insurance with UNIQA Office perks: awesome breakfasts, flexible working hours, spine-relieving massages About us: We are at the heart of the digital revolution in transportation. Since 2004, we‚Äôve been building one of the largest logistics platforms in Europe , bringing together carriers, forwarders, and shippers in one place. We automate processes, enhance transaction security, and make transport more efficient. We‚Äôre an international tech company operating in 13 countries , and we‚Äôre just getting started. We develop cutting-edge digital solutions for transport, freight forwarding, and logistics, powered by the expertise of top developers, customer experience specialists, machine learning engineers, and AI experts. If you‚Äôre looking to shape the future of transport technology ‚Äì this is the place for you! At Trans.eu , we believe in Smart & Fair Logistics ‚Äì a more efficient and transparent industry. That‚Äôs why we‚Äôve expanded our capital group, building a network of specialized companies that help us drive this mission forward.",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,177,Cuda engineer,Accelerated.Finance,"We are seeking a highly skilled and motivated CUDA/C/LLM Engineer to join our innovative team. This role focuses on designing, developing, and optimizing high-performance software solutions with a strong emphasis on GPU programming, low-level system development, and large language model (LLM) integration. The ideal candidate brings expertise in CUDA and C programming, along with a solid understanding of cryptocurrency mining and proof-of-work (PoW) mechanisms, to drive cutting-edge projects in AI and distributed systems. Key Responsibilities: Develop and optimize GPU-accelerated applications using CUDA for tasks such as data processing, model inference, training, and cryptocurrency mining workloads. Write efficient, maintainable, and scalable code in C for performance-critical systems, including mining algorithms and PoW implementations. Collaborate with machine learning teams to integrate and fine-tune large language models (LLMs) into production environments. Design and implement proof-of-work systems, optimizing hash functions and mining efficiency on GPU architectures. Profile and debug complex systems to identify bottlenecks and implement solutions for improved performance in both AI and mining applications. Stay up-to-date with advancements in GPU technology, LLM architectures, parallel computing, and blockchain-related developments. Work cross-functionally with software engineers, data scientists, and blockchain researchers to deliver innovative solutions. Qualifications: Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Electrical Engineering, or a related field (or equivalent experience). Strong proficiency in CUDA and C programming, with hands-on experience in GPU optimization. In-depth knowledge of cryptocurrency mining, proof-of-work algorithms (e.g., SHA-256, Ethash), and their implementation on GPUs. Familiarity with large language models and deep learning frameworks (e.g., PyTorch, TensorFlow) and their deployment. Experience with parallel computing, multi-threading, and memory management in high-performance applications. Understanding of computer architecture, particularly GPU and CPU interactions, and their application in mining and AI workloads. Ability to work independently and in a team, with excellent problem-solving and communication skills. Bonus: Experience with blockchain technologies, low-level kernel development, or contributions to open-source mining or AI projects. Why Join Us? You will get Base Salary + Equity + significant performance based bonus. Join a forward-thinking team at the intersection of AI, high-performance computing, and blockchain technology. Work on groundbreaking projects that harness GPUs and LLMs to solve real-world challenges, from advanced AI systems to optimized mining solutions. Competitive salary, benefits, and opportunities for growth included.",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,178,Data Warehouse Development and Maintenance Expert,Nationale-Nederlanden,"Chcesz mieƒá realny wp≈Çyw na rozw√≥j nowoczesnych rozwiƒÖza≈Ñ danych w du≈ºej organizacji? Szukamy do≈õwiadczonego specjalisty, kt√≥ry pomo≈ºe nam projektowaƒá i rozwijaƒá zaawansowanƒÖ architekturƒô danych oraz wspieraƒá zespo≈Çy biznesowe w podejmowaniu trafnych decyzji. Do Twoich zada≈Ñ nale≈ºeƒá bƒôdzie: Tworzenie i rozw√≥j architektury Hurtowni Danych oraz system√≥w Business Intelligence wspierajƒÖcych kluczowe procesy biznesowe Refaktoryzacja i optymalizacja istniejƒÖcych proces√≥w przetwarzania danych Projektowanie i implementacja proces√≥w ETL integrujƒÖcych dane z wielu ≈∫r√≥de≈Ç Modelowanie struktur danych zgodnie z najlepszymi praktykami (Kimball, Inmon) Wsp√≥≈Çpraca z zespo≈Çami developerskimi, analitycznymi i biznesowymi w celu dostarczania warto≈õciowych rozwiƒÖza≈Ñ Tworzenie i aktualizacja dokumentacji technicznej zgodnie z obowiƒÖzujƒÖcymi standardami Nasza oferta jest dla Ciebie, je≈õli: Masz minimum 5 lat do≈õwiadczenia w projektach zwiƒÖzanych z hurtowniami danych lub du≈ºymi bazami relacyjnymi Znasz bardzo dobrze MS SQL Server oraz narzƒôdzia SSIS, SSRS i Power BI Posiadasz praktyczne do≈õwiadczenie w projektowaniu proces√≥w ETL i modelowaniu danych Wyr√≥≈ºniasz siƒô analitycznym my≈õleniem, samodzielno≈õciƒÖ i inicjatywƒÖ Jeste≈õ otwarty/a-, potrafisz efektywnie wsp√≥≈Çpracowaƒá w zespole i dzieliƒá siƒô wiedzƒÖ Aktywnie poszukujesz nowych rozwiƒÖza≈Ñ, technologii i trend√≥w w obszarze hurtowni danych i BI, stale rozwijajƒÖc swoje kompetencje, Komunikujesz siƒô w jƒôzyku angielskim na poziomie umo≈ºliwiajƒÖcym pracƒô z dokumentacjƒÖ i zespo≈Çami miƒôdzynarodowymi. Dodatkowym atutem bƒôdzie: Znajomo≈õƒá innych narzƒôdzi ETL (Informatica / Talend) Do≈õwiadczenie w pracy z technologiami chmurowymi (Azure Data Factory, Databricks, Lakehouse, Azure DevOps) Znajomo≈õƒá GitLab CI/CD i do≈õwiadczenie w automatyzacji proces√≥w developerskich Znajomo≈õƒá narzƒôdzi do monitorowania wydajno≈õci ZABBIX, FOGLIGHT, GRAFANA Widzisz te benefity i szeroko siƒô u≈õmiechasz: Pracujemy w trybie hybrydowym, a nasze nowoczesne biuro znajduje siƒô na zielonym Powi≈õlu, Zapewnimy profilaktykƒô zdrowotnƒÖ i zabezpieczymy przysz≈Ço≈õƒá! Mo≈ºesz skorzystaƒá z Medicover, Multisport, ubezpieczenia, Dostaniesz dostƒôp do platformy benefitowej, w kt√≥rej bƒôdziesz m√≥g≈Ç/-a wybraƒá najlepsze dla siebie benefity oraz rabaty, Stawiamy na rozw√≥j, dlatego bƒôdziesz mia≈Ç/a dostƒôp do szkole≈Ñ, webinar√≥w, webcast√≥w, Dla ≈õwie≈ºo upieczonych tatusi√≥w mamy dodatkowe dwa tygodnie p≈Çatnego urlopu ojcowskiego, Zale≈ºy Ci na kompleksowym wdro≈ºeniu do pracy? Nasi in≈ºynierowie otrzymujƒÖ dedykowany onboarding jak poruszaƒá siƒô w obszarze Technologii Biznesowych w NN, Dla Ciebie obni≈ºymy ceny produkt√≥w firmy Samsung, Trenujesz? Dbasz o siebie? To dobrze siƒô sk≈Çada mo≈ºesz do≈ÇƒÖczyƒá do sekcji sportowej, Chcesz przeje≈ºd≈ºaƒá do pracy na rowerze? Zapraszamy, czeka na Ciebie ≈õwietnie przygotowana infrastruktura dla rowerzyst√≥w, Stawiamy na rozw√≥j, dlatego bƒôdziesz mia≈Ç/-a dostƒôp do szkole≈Ñ, webinar√≥w, webcast√≥w. Chcesz rozwijaƒá siƒô w ≈õrodowisku, kt√≥re stawia na jako≈õƒá, nowoczesne technologie i partnerskƒÖ wsp√≥≈Çpracƒô? Aplikuj i tw√≥rz z nami przysz≈Ço≈õƒá danych!",[],Unclassified,Unclassified
Full-time,Mid,Permanent,Remote,179,Data Engineer,INFOPLUS TECHNOLOGIES,"Job Description Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources data sources using SQL and AWS ‚Äòbig data‚Äô technologies. Build analytics tools (Tableu) that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Work with system integration and middlewares (MuleSoft, Talend, Solace and etc) Perform proof of concept with customer in data integration and data load Managing structure and unstructured data set Managing and design data privacy, integrity and security solution Managing data with high confidentiality, integrity and privacy Skills Experience with working in agile methodologies (Scrum or SAFe) Good teamwork and communication skill (Higher Performance Team) Able to be self organized and focus in delivering values to the business Always work with integrity, passion and courage Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) and Nosql unstructured database Familiarity with a variety of databases (Microsoft SQL is mandatory). Familiarity reporting tools and dashboards like Tableu. Experience building and optimizing ‚Äòbig data‚Äô data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‚Äòbig data‚Äô data stores. Preferable middleware knowledge, example: MuleSoft, Solace. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.","[{""min"": 220000, ""max"": 250000, ""type"": ""Gross per year - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,180,Backend Engineer (Data & AI),Appliscale,"About the role Our client is an early-stage, venture-backed startup transforming the $800B franchising industry. Their innovative AI platform helps leading franchise brands automate operations, leverage data insights, and scale faster and smarter. We're hiring a product-focused Backend Software Engineer eager to build impactful solutions, work closely with AI technology, and thrive in a fast-paced startup environment. This is an opportunity to contribute directly to product development, seeing your code and ideas quickly in users' hands. Responsibilities Please note, availability to attend daily afternoon/evening meetings is a specific requirement for this role as most of the team is located in the US Develop scalable backend systems, data pipelines, and APIs using Python, TypeScript, and AWS infrastructure Collaborate cross-functionally with product, ML, and infrastructure teams to integrate and deploy AI features in a multi-tenant SaaS environment Required qualifications Minimum of 2 years full-time commercial backend software development experience, ideally with Python and TypeScript Bachelor's or higher degree in Computer Science, Software Engineering, or a related field Comfortable building and managing services in AWS environments (EC2, Lambda, ECS, Airflow) Experienced using AutoML frameworks, time-series DBs Product-minded engineer who enjoys collaborating closely with product and business teams Startup-oriented: thrives in ambiguity, eager to learn quickly, iterate fast, and build impactful solutions Excellent communication skills and high fluency in English, it‚Äôs our daily business language Nice to have AI-curious: experience deploying ML models into production is a strong plus but not required","[{""min"": 14000, ""max"": 20000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 16000, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Mid,B2B,Hybrid,181,Data Engineer,Jit Team,"Salary: 100 - 150 PLN net/h + VAT (B2B contract) Remote: elastic hybrid work model (2 times a month for people located outside Wroc≈Çaw, 2 times a week for people located in Wroc≈Çaw and up to 40 km from the city) Why choose this offer? Stable Project in an International Company : Be part of a globally recognized organization that values stability and growth The international work environment will give you the opportunity to interact with the English language on a daily basis The Jit community will bring you a nice time during regular integration meetings Project We are looking for Data Engineer who will join a well-organized team that is part of a rapidly growing risk and analytics practice, providing high-end research and analytical services to leading financial institutions across the globe. You'll be working in an international work environment that encourages innovation and collaboration. The focus is on delivering cutting-edge solutions in areas such as equity and fixed income research, risk management, and data-driven insights using AI/ML technologies. Responsibilities: Help the business to manage regulatory risk in a more effective, efficient, and commercial way through the adoption of data science ( AI/ML and advanced analytics ) Design and deliver fully documented analytical solutions that meet business objectives and requirements. Identify, monitor, and mitigate key and emerging risks while ensuring senior management is well-informed Maintain and enhance existing models through regular performance monitoring and validation Facilitate communication and engagement with stakeholders to promote understanding, adoption, and exploration of data science products and services Requirements: Minimum 3 years of professional experience as a Data Engineer University degree in technology, data analytics, or a related discipline, or equivalent relevant work experience in computer science or data science Strong understanding of applied mathematics, statistics, data science principles, and advanced computing techniques including machine learning, modeling, NLP, and Generative AI Proven experience working within the Hadoop ecosystem and strong proficiency in analytical programming languages such as Python and SQL Knowledge of cloud platforms (GCP, AWS, Azure), Spark, and/or graph theory is a strong advantage Experience with data visualization tools such as Qlik or Tableau Solid grasp of data architecture concepts including cloud environments, ETL processes , ontology, and data modeling Understanding of regulatory compliance and risk management, with direct experience deploying controls and analytics to mitigate those risks Experience in financial services (preferably within a tier one bank) or a related industry Good command of English , both spoken and written (minimum B2 level) Technologies you'll work with Python SQL Hadoop Cloud Qlik/Tableau ETL AI/ML Client ‚Äì why choose this particular client from the Jit portfolio? Our Client is the largest and top-ranked provider of high-end research and analytics services to the world's leading commercial and investment banks, insurance companies, corporations and asset management firms. It has deep expertise in the areas of equity research, fixed income research, valuations, pricing complex derivatives, structured finance, risk management, actuarial analysis, and business intelligence. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 17000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,182,Senior Data Engineer (Oracle),7N,"You will be part of our Data Team where you, together with the team, will be responsible for technical management & support of a database with an application on top, migration and sunsetting the same database. Furthermore, you will be performing application management work including managing contracts, licenses, and life cycle activities. Work Mode : Hybrid ‚Äì 2 days in the Warsaw office","[{""min"": 30240, ""max"": 36960, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,183,Technical Lead Erlang Engineer,Spyrosoft,"Join our team in Warsaw , where we‚Äôre collaborating on a cutting-edge fintech venture with a global industry leader. Together with our Partner ‚Äì Klarna , we‚Äôre building an IT hub designed to drive innovation in digital payment solutions . We‚Äôre on the lookout for top-tier engineers who thrive in dynamic, forward-thinking environments. Spyrosoft is leading the recruitment process, facilitating a seamless experience for candidates who are ready to shape the future of online shopping and payments. This opportunity is ideal for engineers who value independence, proactiveness, and flexibility. Our engagement begins with a B2B contract through Spyrosoft , transitioning to a direct contract with our Partner . We offer a hybrid work model in Warsaw‚Äôs vibrant Wola district. English fluency and eligibility to work in Poland are essential, as is the successful completion of a background check to meet the rigorous standards of the financial domain. Our process: CV selection Initial recruitment screening Technical interview Online logic test Cultural fit interview Project Description: You will be part of a project focused on building a robust, scalable system with high availability, designed to support complex data flows and transaction processing. The system leverages Erlang/OTP for fault-tolerant, concurrent applications and operates within a modern AWS-based cloud environment. Key components include PostgreSQL for data storage and Kafka for message streaming. The system also integrates payment functionalities and aims for high performance and reliability in financial transaction processing. Tech Stack: Erlang + OTP AWS PostgreSQL Apache Kafka Elixir (nice to have) Python (nice to have) Requirements: Minimum 1 year of relevant experience. Hands-on experience with Erlang/OTP for system development. Strong understanding of concurrent programming and building fault-tolerant systems. Proven track record in working with AWS services, architecture, deployment, management, and scaling of cloud applications. Solid experience in managing Postgres databases and leveraging Kafka for real-time data processing and integration. Familiarity with Elixir is a plus, especially with a willingness to deepen Erlang expertise. Experience with the design and development of scalable, reliable payment and collection systems is a significant advantage. Deep understanding of cloud infrastructure, especially AWS tooling. Basic knowledge of Python is beneficial to broaden the tech stack and support versatile development tasks. Strong analytical and problem-solving skills. Ability to challenge the status quo and drive continuous improvement. Excellent verbal and written communication skills, enabling effective collaboration across teams and stakeholders. Ability to mentor and support peers while contributing to team success. A degree in Computer Science, Information Technology, or a related field. Fluency in English, both written and spoken. Main Responsibilities: Design, develop, and maintain backend systems using Erlang/OTP. Implement and optimize cloud infrastructure using AWS services. Integrate and manage PostgreSQL and Kafka in a production environment. Collaborate with cross-functional teams to understand requirements and deliver scalable solutions. Participate in architectural decisions and propose improvements to enhance system performance and reliability. Support the development of payment and collection modules with a focus on reliability and scalability. Mentor junior developers and support knowledge sharing within the team. Ensure best practices in code quality, testing, and documentation. Communicate effectively with internal teams and external stakeholders to align technical goals.","[{""min"": 220, ""max"": 255, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,184,Junior/ Mid Data Engineer,Altimetrik Poland,"Altimetrik Poland is a digital enablement company. In an agile way, we deliver bite-size outcomes to enterprises and startups from all industries, to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a big focus on core development, attacking challenging and complex problems of the biggest companies in the world. For our client, founded in 1884 as a Swiss luxury watchmaker, known for precision-made chronometers designed for aviators, we seek a Junior/ Mid Data Engineer to join our team and drive the development and success of the high-end timepieces. Responsibilities: Collaboration: collect and understand requirements and work closely with the business analysts, data architects, BI engineers and other data engineers. Developing data pipelines: develop, optimize, and maintain reliable and scalable ETL/ELT pipeline on a cloud platform through the collection, storage, processing, and transformation of large datasets Support production issues related to application functionality and integrations And if you possess.. Related Experience: 2+ years of experience in the role of data engineer Hands-on experience in gathering, cleaning, processing and visualizing data in Databricks with Python/Pyspark Proficient in SQL development skills with the ability to write complex, efficient queries for data integration Experience in Azure cloud ecosystem (Azure Data Factory, Azure Storage Account, Azure SQL database, . Experience in data modelling in a Data lake environment Must have strong experience in data warehouse concepts Analytical skills to support Business Analysts and the ability to translate user stories Good diagnosis skills for identifying job issues Ability to manage and complete multiple tasks within tight deadlines Proficient in spoken and written communication skills (verbal and non-verbal) ‚Ä¶ then we are looking for you! We work 100% remotely or from our hub in Krak√≥w We grow fast. We learn a lot. We prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 15000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,185,Senior BI Developer with Snowflake,Holisticon Connect,"Holisticon Connect is a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä Design, build, and develop data warehouses based on Snowflake; Create and optimize ETL/ELT processes using Matillion; Integrate data from various sources (databases, APIs, files); Develop reports and dashboards in Power BI for internal clients; Maintain, monitor, and further develop existing BI solutions; Collaborate with project teams and business stakeholders to gather and analyze requirements; Participate in data migration from legacy systems (e.g., Oracle, SSIS) to Snowflake; Ensure high-quality technical and business documentation; Implement best practices for data management, security, and version control; Actively participate in Agile team meetings (e.g., daily stand-ups, sprint planning, retrospectives); At least 4 years' experience in a BI Developer role; Advanced and proven experience as a Snowflake developer, responsible for building ETL processes and creating tables and views within the Snowflake environment; Solid skills in Power BI ; Eager to work as a full-stack BI Developer (both backend and frontend); Strong English skills (min. C1 level , daily communication with international clients); Proactive and creative - skills to drive improvements and engage with both technical and non-technical stakeholders; Strong documentation skills and a knack for business analysis. Experience with Matillion ; Experience with Oracle (we‚Äôre migrating to Snowflake, but legacy knowledge is a plus); Experience with SSAS/SSIS/SSRS ; Familiarity with Visual Studio; Understanding of version control concepts (branching, merging, pushing; Git integrated with Matillion). Background in procurement, supply chain, or business data analysis related to orders and internal corporate stakeholders; Background in Managed Services delivery models - you know how to take end-to-end ownership of BI solutions, ensuring their reliability, scalability, and alignment with client needs throughout the entire lifecycle; Experience working in Agile teams. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private life so you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Fully remote work or in our office in Wroc≈Çaw; B2B Contract: 135 ‚Äì 150 PLN net/hour + VAT Free benefits such as Luxmed , Multisport , and life insurance in Nationale Nederlanden ; Attractive referral system (9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budget with additional paid hours; Passion Day - an extra day off for your hobby to spend as you please; Flexible working hours with no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment + 2 additional monitors and accessories.","[{""min"": 135, ""max"": 155, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,186,SAP Master Data Subject Matter Expert,emagine Polska,"Location: Poland, 100% Remote Expected Start Date: ASAP Contract Period: 12 months (+possible extension) Language Requirement: English B2/C1 Salary: 190-205 z≈Ç/h B2B contract Introduction & Summary: We are seeking a highly skilled Master Data Subject Matter Expert with over 5 years of relevant experience in Master Data Management, particularly with SAP ECC and/or S/4HANA. The ideal candidate will possess a deep understanding of Master Data processes, cross-functional knowledge across various business processes, including O2C, P2P, and R2R, and exceptional soft skills for effective communication and stakeholder engagement. Evaluate and improve current Master Data processes as part of SAP S/4HANA implementation. Participate in data clean-up and harmonization efforts. Design and document SAP Master Data configuration activities. Support implementation of new SAP features for Master Data enhancements. Assist in writing test plans and support end-user training. Create and maintain comprehensive Master Data documentation. Analyze large data sets for migration and validation purposes. Provide end-user support and troubleshooting for Master Data-related issues. Work closely with project teams to prepare for ERP rollout. Minimum of 5 years of experience in Master Data management. Operational experience with SAP ECC and/or S/4HANA. Deep knowledge of Business Partner setup. Cross-functional understanding of Master Data objects. Experience in the retail sector is a plus. Experience in SAP implementation or transformation programs is advantageous. Expertise in Microsoft Office tools, especially Excel and PowerPoint. Working knowledge of Jira is preferred. Nice to Have: Experience in retail industry operations. Participation in SAP implementation or rollout projects. Experience in data mapping and migration into SAP. This position offers an opportunity to contribute to significant ERP initiatives in a dynamic environment. The role is remote with flexible working hours, depending on project needs.","[{""min"": 190, ""max"": 205, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,187,Data Scientist,in4ge,"Poszukujemy do≈õwiadczonej osoby na stanowisko Data Scientist, kt√≥ra do≈ÇƒÖczy do naszego zespo≈Çu w Warszawie. Osoba na tym stanowisku bƒôdzie odpowiedzialna za analizƒô zbior√≥w danych, projektowanie i wdra≈ºanie modeli predykcyjnych oraz wsparcie proces√≥w decyzyjnych przy u≈ºyciu zaawansowanych narzƒôdzi analitycznych. Do codziennych obowiƒÖzk√≥w bƒôdzie nale≈ºeƒá przetwarzanie i analiza du≈ºych wolumen√≥w danych, przygotowywanie raport√≥w oraz prezentacja wynik√≥w analiz interesariuszom biznesowym. Praca obejmuje r√≥wnie≈º wsp√≥≈Çpracƒô z zespo≈Çem programist√≥w i analityk√≥w oraz udzia≈Ç w projektowaniu rozwiƒÖza≈Ñ usprawniajƒÖcych procesy biznesowe. Zakres roli: Prowadzenie projekt√≥w data science i wsp√≥≈Çpraca z klientami przy rozwiƒÖzywaniu ich problem√≥w biznesowych i identyfikowaniu najlepszych technik statystycznych. Prowadzenie projekt√≥w data science i wsp√≥≈Çpraca z klientami przy rozwiƒÖzywaniu ich problem√≥w biznesowych i identyfikowaniu najlepszych technik statystycznych. Tworzenie i rozwijanie framework√≥w modelowania oraz skalowanie proof-of-concept do pe≈Çnych rozwiƒÖza≈Ñ. Tworzenie i rozwijanie framework√≥w modelowania oraz skalowanie proof-of-concept do pe≈Çnych rozwiƒÖza≈Ñ. Przek≈Çadanie wynik√≥w danych i modeli na strategiczne i taktyczne insights dla podejmowania decyzji biznesowych. Przek≈Çadanie wynik√≥w danych i modeli na strategiczne i taktyczne insights dla podejmowania decyzji biznesowych. Wsp√≥≈Çpraca z zespo≈Çami Product/Engineering. Wsp√≥≈Çpraca z zespo≈Çami Product/Engineering. Oczekujemy: Wykszta≈Çcenia wy≈ºszego typu Master w dziedzinie ilo≈õciowej (statystyka, in≈ºynieria, nauki ≈õcis≈Çe) lub r√≥wnowa≈ºnego do≈õwiadczenia. Wykszta≈Çcenia wy≈ºszego typu Master w dziedzinie ilo≈õciowej (statystyka, in≈ºynieria, nauki ≈õcis≈Çe) lub r√≥wnowa≈ºnego do≈õwiadczenia. Min. 3 lat do≈õwiadczenia w wykorzystywaniu analityki do rozwiƒÖzywania problem√≥w produktowych lub biznesowych. Min. 3 lat do≈õwiadczenia w wykorzystywaniu analityki do rozwiƒÖzywania problem√≥w produktowych lub biznesowych. Umiejƒôtno≈õci programowania (Python, R, SQL), pracy z bazami danych i analizy statystycznej (alternatywnie stopie≈Ñ PhD). Umiejƒôtno≈õci programowania (Python, R, SQL), pracy z bazami danych i analizy statystycznej (alternatywnie stopie≈Ñ PhD). Proponujemy: Do≈õwiadczenie w dostarczaniu insights z ML klientom (definiowanie problem√≥w, modelowanie, interpretacja). Do≈õwiadczenie w dostarczaniu insights z ML klientom (definiowanie problem√≥w, modelowanie, interpretacja). Do≈õwiadczenie w u≈ºywaniu lub wdra≈ºaniu rozwiƒÖza≈Ñ digital analytics i measurement. Do≈õwiadczenie w u≈ºywaniu lub wdra≈ºaniu rozwiƒÖza≈Ñ digital analytics i measurement. Do≈õwiadczenie w Computer Vision i NLP w kontek≈õcie marketing analytics z umiejƒôtno≈õciƒÖ wykorzystania generative AI. Do≈õwiadczenie w Computer Vision i NLP w kontek≈õcie marketing analytics z umiejƒôtno≈õciƒÖ wykorzystania generative AI. Znajomo≈õƒá algorytm√≥w statystycznych u≈ºywanych w Marketing Analytics. Znajomo≈õƒá algorytm√≥w statystycznych u≈ºywanych w Marketing Analytics. üí° Nie przegap dopasowanych ofert! Mamy wiele rekrutacji, a nowe projekty pojawiajƒÖ siƒô na bie≈ºƒÖco. Pamiƒôtaj, ≈ºe zaznaczajƒÖc zgodƒô na przetwarzanie danych w celu przysz≈Çych proces√≥w , bƒôdziemy mogli zaprosiƒá Ciƒô do udzia≈Çu w kolejnych procesach, dopasowanych do Twojego do≈õwiadczenia i oczekiwa≈Ñ! PS Zamierzamy kontaktowaƒá siƒô z TobƒÖ wy≈ÇƒÖcznie wtedy, kiedy bƒôdziemy dla Ciebie ciekawe projekty, bez tej zgody nie bƒôdzie to mo≈ºliwe. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people",[],Data Science,Data Science
Full-time,Senior,Permanent or B2B,Remote,188,Principal of Data Analytics Consulting,EPAM Systems,"We are looking for Principal of Data Analytics Consulting to join our team. You will work alongside a dynamic team to identify complex business problems and create solution-oriented strategies for some of the most recognized brands in the Financial Services industry. Responsibilities Build trusted partner relationships with senior client stakeholders and internal account managers Manage and drive collaborative, value-centered approaches to defining and solving data problems Coordinate with Senior Leadership to co-develop account plans and strategies for data-driven opportunities & business development Generate revenue within the client account portfolio by identifying data opportunities and enabling a wide range of data & analytics engagements in the financial sector Perform as principal consultant leading complex data transformational initiatives Establish & Contribute to the strategic deliverables such as mission statements, assessment results & roadmaps Participate in data-related pre-sales activities by assessing opportunities, responding to RFPs, creating proposals, and helping to close new deals Lead team development while supporting strategic financial accounts Impact client value and business growth by achieving target revenue goals Improve the business operations by achieving the target billable utilization Create & contribute to EPAM Data Practice people initiatives Requirements Minimum of 12 years of IT experience At least 8 years in Manager/Owner/Coordinator roles Hands-on experience influencing data solution design and technology selections for enterprise data platforms, warehouses, data lakes, and data science platforms Strong understanding of IT management consulting Expert in driving large data-driven programs and the revenue growth within a financial account portfolio Highly skilled in building data & analytics business development organizations Proven history of managing complex technology initiatives while communicating with senior business & IT stakeholders Familiarity with banking, trading, and data aggregation Knowledge of pre-sales activities Ability to develop and grow client relationships as well as facilitate and drive strategy definition Experience working with data analytics trends Exposure to interacting with customers, managing expectations, and explaining solutions & project deliverables to senior stakeholders Comfortable operating at the strategic level while being close enough to the details to add value to clients and the team C1 English level proficiency We offer International projects with top brands Work with global teams of highly skilled, diverse peers Healthcare benefits Employee financial programs Paid time off and sick leave Upskilling, reskilling and certification courses Unlimited access to the LinkedIn Learning library and 22,000+ courses Global career opportunities Volunteer and community involvement opportunities EPAM Employee Groups Award-winning culture recognized by Glassdoor, Newsweek and LinkedIn EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,189,Senior Database Developer,1dea,"Do warszawskiego oddzia≈Çu firmy z UK - FinTech, praca nad wewnƒôtrznym produktem - szukamy Senior Database Developer'a. Zakres obowiƒÖzk√≥w: Tworzenie i rozwijanie nowych funkcjonalno≈õci oraz modyfikacja istniejƒÖcych system√≥w bazodanowych. Optymalizacja i utrzymanie aplikacji wykorzystywanych przez klient√≥w firmy. Wprowadzanie ulepsze≈Ñ technicznych oraz administracja bazami danych (instalacja poprawek, aktualizacje). Wsparcie zespo≈Ç√≥w biznesowych i wsp√≥≈Çpraca z Project Managerami. RozwiƒÖzywanie incydent√≥w oraz problem√≥w zwiƒÖzanych z bazami danych. Wymagania: Minimum 5 lat do≈õwiadczenia w pracy jako Database Developer, w tym min. 2 lata z Oracle PL/SQL. Wymagane do≈õwiadczeniem w migracji z Oracle do PostgreSQL Bardzo dobra znajomo≈õƒá SQL i PL/SQL oraz architektury baz danych Oracle. Do≈õwiadczenie w pracy z systemami kontroli wersji (np. SVN, Git). Znajomo≈õƒá metod debugowania i testowania jednostkowego kodu PL/SQL. Umiejƒôtno≈õƒá pracy w zespole i dobra organizacja pracy. Dobra znajomo≈õƒá jƒôzyka angielskiego B2/C1 (praca w miƒôdzynarodowym ≈õrodowisku). Lokalizacja biura: Warszawa ≈ör√≥dmie≈õcie, 1 dzie≈Ñ/ tydzie≈Ñ w biurze Umowa: B2B, d≈Çugoterminowe, bezpo≈õrednie zatrudnienie Rekrutacja: 2 etapy online","[{""min"": 24000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,190,BI Developer with Snowflake,Holisticon Connect,"Holisticon Connect is a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä Design, build, and develop data warehouses based on Snowflake; Create and optimize ETL/ELT processes using Matillion; Integrate data from various sources (databases, APIs, files); Develop reports and dashboards in Power BI for internal clients; Maintain, monitor, and further develop existing BI solutions; Collaborate with project teams and business stakeholders to gather and analyze requirements; Participate in data migration from legacy systems (e.g., Oracle, SSIS) to Snowflake; Ensure high-quality technical and business documentation; Implement best practices for data management, security, and version control; Actively participate in Agile team meetings (e.g., daily stand-ups, sprint planning, retrospectives); 2 years' experience in a BI Developer role; At least 1 year of proven experience as a Snowflake developer, responsible for building ETL processes and creating tables and views within the Snowflake environment; Solid skills in Power BI ; Eager to work as a full-stack BI Developer (both backend and frontend); Strong English skills (min. C1 level, daily communication with international clients); Proactive and creative - skills to drive improvements and engage with both technical and non-technical stakeholders; Strong documentation skills and a knack for business analysis. Experience with Matillion ; Experience with Oracle (we‚Äôre migrating to Snowflake, but legacy knowledge is a plus); Experience with SSAS/SSIS/SSRS ; Familiarity with Visual Studio; Understanding of version control concepts (branching, merging, pushing; Git integrated with Matillion). Background in procurement, supply chain, or business data analysis related to orders and internal corporate stakeholders; Background in Managed Services delivery models - you know how to take end-to-end ownership of BI solutions, ensuring their reliability, scalability, and alignment with client needs throughout the entire lifecycle; Experience working in Agile teams. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private life so you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Fully remote work or in our office in Wroc≈Çaw; B2B Contract: 120 ‚Äì 140 PLN net/hour + VAT Free benefits such as Luxmed , Multisport , and life insurance in Nationale Nederlanden ; Attractive referral system (9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budget with additional paid hours; Passion Day - an extra day off for your hobby to spend as you please; Flexible working hours with no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment + 2 additional monitors and accessories.","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Mandate,Hybrid,191,Celonis Process Mining Specialist (Maternity Cover),Vaillant Group Business Services,"What we achieve together In this role you lead and drive the Digital Twin implementation process while delivering exceptionally high levels of service to ensure the optimal solution for internal customers with the Celonis technology. You design, optimize and monitor data models and data connections (ETL) to build the best possible and real-time capable data architecture. In addition, you support the implementation of process mining solutions across enterprise end-to-end processes (Design-to-Operate, Procure-to-Pay, Order-to-Cash). You translate business requirements into technical requirements, assess the feasibility, plan the technical implementation and ensure the overall quality of implementation. You also design and implement innovative analyses and execution apps and enrich them with Machine Learning algorithms (Python) or Task Mining to make the customer's processes transparent and automated. Within your responsibilities is also the analysis of the data to identify process inefficiencies, while monitoring compliance and data security measures. What makes us successful together Experience : You have at least 3 years of commercial experience in IT-Consulting, Management Consulting, Process Improvement/Excellence or a similar area. You also bring a solid track record in successfully developing and shipping data driven solutions in Celonis. Know-how and skills: You have proficiency in SQL, other programming languages (Python, R, Matlab) as a plus and a strong interest in Big Data, Data Analytics, Data Mining, Process Mining and Digital Twin. Personality : With your positive attitude and trustworthy personality, you can build strong stakeholder relationships. You understand and interpret business processes and communicate proactively and clearly. In addition, you have Excellent analytical skills, well organized and known for being a quick learner. Language skills : You speak English fluently; German language skills would be a plus. What makes us special Environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee. Package of additional benefits: private medical care, multi-sport card. A fast growing, agile and very dynamic team that challenges established routines and helps transforming the Vaillant Group to a data informed business. Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings.","[{""min"": 15000, ""max"": 20000, ""type"": ""Gross per month - Mandate""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,192,Senior Oracle Database Administrator,emagine Polska,"PROJECT DESCRIPTION: Work model: Hybrid (2 days/week form office Warsaw) Assignment type: B2B Start: ASAP Project length: long term Rate: 175 pln/h net + vat Project language: English Industry: Banking The Senior DBA will be responsible for managing Oracle-based applications, ensuring continuous service and integrity through implementation, testing, troubleshooting, and support for critical databases. RESPONSIBILITIES: Prepare the prePROD environment for enabling FSFO. Sync the DataGuard in case of issues or errors. Rebuild the Standby as needed. Create DB service and validate PDB-level services. Create a new database and migrate old PDB to a new container. Activate FSFO and troubleshoot related issues. Perform tests on prePROD, including switchover and failover. Apply DB-level patching. Execute changes related to OS (Unix processes, load management, daemon) as needed. Handle RAC issues with disaster recovery. Set up OGG Microservices and OEM. Implement changes in production. MUST HAVE: 10+ years of managing Oracle Database versions 12c to 19c (RAC, Data Guard). Operating system experience in Linux. Proficiency with Oracle management tools (Data Guard, RMAN, Data Pump). Understanding of architecture design principles. Strong problem-solving skills and ability to work independently and in a team. Experience with creating PDB services and handling Data Guard issues. Strong practical experience in a production RAC (ASM) environment (19c). Self-starter with attention to detail. Knowledge of Oracle GoldenGate. Understanding of storage systems. Knowledge on FSFO problem-solving, observers, and change/incident management processes. Experience with Oracle SR handling and OEM architecture.","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,193,Senior Business and Systems Analyst,DataRiseLab Sp. z o.o.,"ABOUT US We are an IT Company offering our Clients with a comprehensive support in development and maintenance of data management solutions (data warehouses, data integration, BI reporting, Big Data), defining data and information management strategies in the organization, creating advanced data analytics solutions as well as cloud solutions and business applications. We specialize in delivery of solutions based on Microsoft and IBM technologies for Clients from Poland and other European countries. JOB OFFERING We are looking for a Senior Business and System Analyst who has experience in conducting analysis of business requirements for BI/reporting solutions and who will interact with business users to collect necessary information / specifications and translate them to development team. RESPONSIBILITIES Understanding the Client‚Äôs business objectives and translating them to the development team Collecting functional and non-functional requirements from the Client Requirements management and prioritization throughout the duration of the project Creating mockups of proposed solution for the Client Coordinating meetings with Client Performing data analysis in SQL-type databases. Close cooperation with development team Supporting the solution testing process Preparation and maintenance of analytical documentation REQUIREMENTS Experience: Minimum 3 years of experience in business/system analyst role, with the main focus on gathering and documenting functional and non-functional requirements business requirements, etc. Previous experience in Data / Business Intelligence projects Previous experience working with Agile methodology Nice to have experience in Financial Sector Non-technical skills: Strong problem solving and analytical abilities Excellent communication skills in Polish and English Good organization and time management skills Proactive approach and willingness to learn and further develop Technical skills: Advanced excel skills Good SQL skills Nice to have: experience working with Power BI and Azure DevOps BENEFITS B2B contract Remote work Flexible working hours Healthcare, Multisport benefit The administrator of personal data processed as part of the recruitment process is DataRiseLab Sp. z o.o. with headquarters in Wroc≈Çaw (53-004) at ul. Kawalerzyst√≥w 28/2 (hereinafter referred to as: ‚ÄûDRL‚Äù or ‚ÄûAdministrator‚Äù). The Administrator will process personal data for the purpose of recruitment for the position offered. After expressing a separate consent, personal data will also be processed in order to participate in future recruitments conducted by DRL, notifications send about job offers in DRL and about work-related events that are organized by DRL or with the participation of DRL (e.g. job fairs).","[{""min"": 15000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,194,Big Data Engineer,Link Group,"üöÄ Big Data Engineer üìç Remote or Hybrid (EMEA preferred) | Full-time | AdTech Platform | Python/Java/Scala We‚Äôre looking for an experienced Big Data Engineer to join our high-impact team building the backbone of a global advertising platform delivering personalized content to millions of media-enabled devices. Your work will directly influence data-driven decision making, real-time targeting, and analytics pipelines for one of the most advanced AdTech ecosystems in the industry. Build and maintain robust, scalable data pipelines to support attribution, targeting, and analytics Collaborate closely with Data Scientists, Engineers, and Product Managers Design and implement efficient storage and retrieval layers for massive datasets Optimize data infrastructure and streaming processing systems (e.g. Flink, Apache Ignite) Drive quality through unit tests, integration tests, and code reviews Develop and maintain Airflow DAGs and other orchestration pipelines Translate business needs into robust, technical data solutions Lead or support A/B testing and data-driven model validation Contribute to R&D initiatives around cloud services and architecture 5+ years of experience in backend/data engineering using Python , Java , or Scala Strong experience with Big Data frameworks (Hadoop, Spark, MapReduce) Solid knowledge of SQL/NoSQL technologies (e.g. Snowflake, PostgreSQL, DynamoDB) Hands-on with Kubernetes , Airflow , and AWS (or similar cloud platforms) Stream processing experience: Flink, Ignite Experience with large-scale dataset processing and performance optimization Familiarity with modern software practices: Git, CI/CD, clean code, Design Patterns Fluent in English (B2+) Degree in Computer Science, Telecommunications, or related technical field Experience with GoLang or GraphQL Hands-on with microservices or serverless solutions Experience in container technologies (Docker, Kubernetes) Previous work in AdTech , streaming media, or real-time data systems","[{""min"": 100, ""max"": 130, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Practice / Internship,Junior,Internship,Remote,195,Data Analytics Engineering Trainee,EPAM Systems,"Striving to gain market-oriented knowledge and skills to jumpstart your career in IT? Apply for this program and shape your professional path with EPAM experts. Details If you are interested in creating data products and exploring the power of data to turn raw information into valuable insights for business growth, then this training program is what you need. By participating, you will have the opportunity to acquire new skills or expand existing knowledge through hands-on experience in three major areas: Data Integration ‚Äì development and support of a wide range of data transformations and migrations Data Visualization ‚Äì creation of interactive and complex data visualizations and analysis tools Data Quality ‚Äì data validation and transformation at every stage of the project After successfully completing all program stages, you will gain market-oriented soft and hard skills, which you may further apply at EPAM or elsewhere in the IT industry. What do we offer? Industry-based education. As a leading software engineering company, we will help you explore emerging technologies and best practices that the market demands. Top-notch learning materials. Our Data & Analytics specialists with extensive project experience have designed and tested the educational content in numerous training runs. Practice-oriented approach. This comprehensive program focuses on providing you with hands-on experience and practical application of the concepts learned. Deep dive into the specialization. Our graduates become highly skilled specialists ready to face complex technical challenges and work with the world's leading customers. Support from experienced mentors. We will guide you at all training stages, covering your open questions and sharing feedback on assigned tasks. Career advancement. Upon successful completion of both Fundamentals and Specialization stages, we will consider you for open positions based on your demonstrated skills and available opportunities at EPAM. Training process The program consists of two stages: Fundamentals stage It will last 3 months and require ~15 hours of weekly engagement. You will explore self-study materials, complete one assigned task each week, ensure its approval by mentors and discuss your questions during weekly group sessions led by experts. Additionally, you will participate in 2-3 individual assessments with mentors, which will involve a theoretical review and live coding exercises. If you show good results and successfully pass a technical interview, we will invite you to the next stage. Specialization stage It will last 4 months and require ~20 hours of weekly engagement. At this stage, learning will become more intensive, involving daily assignments, daily group Q&A sessions and exploration of self-study materials. What is required for training: English level from B2 (Upper-Intermediate) and higher Basic knowledge of Relational Database Management System (DBMS) theory Understanding of Structured Query Language (SQL) Familiarity with Python basics Nice to have: Degree from a technical university or other educational institution with a technical specialization Experience in banking and technical spheres Please read this info before registration This program is for citizens of Poland and people who have relocated to this country for a permanent stay. The program start date may change, so the selection period may be adjusted accordingly. Please regularly check for updates on this page and via email. Considering the limited number of places in this program's group, the selection results will be decisive factors in enrollment, with the applicants with the highest scores being processed first. We strive to keep the registration and testing process fair for everyone. If we notice any cheating, we will have to reject your application. If you are interested in applying while enrolled in another EPAM Campus program or employed at EPAM, please discuss it with your Training Coordinator or Resource Manager first.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,196,Data Engineer (Databricks),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI companies. As a Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Data Platform Transformation for energy management association body. This project addressed critical data management challenges, boosting user adoption, performance, and data integrity. The team is implementing a comprehensive data catalog, leveraging Databricks and Apache Spark/PySpark, for simplified data access and governance. Secure integration solutions and enhanced data quality monitoring, utilizing Delta Live Table tests, established trust in the platform. The intermediate result is a user-friendly, secure, and data-driven platform, serving as a basis for further development of ML components. Design of the data transformation and following data ops pipelines for global car manufacturer. This project aims to build a data processing system for both real-time streaming and batch data. We‚Äôll handle data for business uses like process monitoring, analysis, and reporting, while also exploring LLMs for chatbots and data analysis. Key tasks include data cleaning, normalization, and optimizing the data model for performance and accuracy. üöÄ Your main responsibilities: Design scalable data processing pipelines for streaming and batch processing using Big Data technologies like Databricks, Airflow and/or Dagster. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using Databricks/DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. üéØ What you‚Äôll need to succeed in this role: At least 3 years of commercial experience implementing, developing, or maintaining Big Data systems. Strong programming skills in Python : writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity with Big Data technologies like Airflow or Dagster , Databricks, Spark and DBT. Experience implementing and deploying solutions in cloud environments (with a preference for Azure ). Knowledge of how to build and deploy Power BI reports and dashboards for data visualization. Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master‚Äôs or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn , Instagram ).","[{""min"": 15120, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,197,Administratorka/Administrator hurtowni danych i BI,PKO Bank Polski,"Do≈ÇƒÖcz do nas jako: Administratorka/Administrator hurtowni danych i BI Departament Rozwoju Platform In≈ºynierii i Analityki Danych - Warszawa Chmielna 89 administrujemy systemami z obszaru Business Intelligence oraz platform raportowych, optymalizujemy systemy Business Intelligence oraz procesy ETL\ELT, tworzymy procesy CI/CD, automatyzujmy, monitorujemy i obs≈Çugujemy procesy ETL\ELT zasilajƒÖce Hurtowniƒô Danych, automatyzujemy procesy przygotowania ≈õrodowisk testowych i wdro≈ºe≈Ñ, automatyzujemy powo≈Çywanie ≈õrodowisk, przeprowadzamy testy tworzymy dokumentacjƒô technicznƒÖ. masz do≈õwiadczenie w pracy ze ≈õrodowiskami/narzƒôdziami: Reporting Services, Power Bi i modelami wielowymiarowymi MS Analysis Services, masz do≈õwiadczenie w administrowaniu bazami danych ORACLE, MS SQL, potrafisz pos≈Çugiwaƒá siƒô jƒôzykami SQL, PLSQL (Oracle), Transact-SQL (Microsoft SQL Server), masz do≈õwiadczenie developerskie, w szczeg√≥lno≈õci zwiƒÖzane z automatyzacjƒÖ i optymalizacjƒÖ proces√≥w ETL\ELT, przygotowywaniem ≈õrodowisk oraz wdro≈ºe≈Ñ, masz wykszta≈Çcenie wy≈ºsze (techniczne IT, matematyka, fizyka lub pokrewne), znasz jƒôzyk angielski na poziomie komunikatywnym, jeste≈õ osobƒÖ samodzielnƒÖ, komunikatywnƒÖ i otwartƒÖ. aplikacjƒô raportowƒÖ EPM Infor, narzƒôdzia DevOps, w szczeg√≥lno≈õci platformƒô GitLab, jƒôzyki skryptowe, w szczeg√≥lno≈õci python, bash, narzƒôdzia do konteneryzacji (podman, docker). Docenienie dzia≈Çania i zaanga≈ºowania Opr√≥cz wynagrodzenia, ka≈ºdy z nas ma okre≈õlone cele ‚Äì kt√≥re doceniamy w ramach system√≥w premiowych. Oferta specjalna Wspieramy Twoje finanse i oferujemy produkty naszej Grupy Kapita≈Çowej na preferencyjnych warunkach ( m.in . kredyt hipoteczny, kartƒô kredytowƒÖ czy ubezpieczenie) oraz pomagamy odk≈Çadaƒá na emeryturƒô w PPE. Elastyczno≈õƒá benefit√≥w R√≥≈ºnimy siƒô, dlatego to Ty wybierasz z jakich benefit√≥w skorzystasz. Mamy dla Ciebie system kafeteryjny lub dzia≈Çania w ramach ZF≈öS m.in . dofinansowanie opieki nad dzieƒámi, wakacji czy po≈ºyczkƒô na remont. Mo≈ºliwo≈õci rozwoju U nas masz szerokie perspektywy rozwoju i mo≈ºesz uczyƒá siƒô, jak lubisz. Oferujemy dostƒôp do r√≥≈ºnych form nauki ‚Äì stacjonarnie i cyfrowo. Wsparcie w zdrowiu i odpoczynku Dbamy o naszƒÖ formƒô mentalnƒÖ i fizycznƒÖ. Mo≈ºesz skorzystaƒá z kart sportowych, opieki medycznej LuxMed, stomatologa, programu wellbingowego #FokusNaCiebie czy dodatkowych dni wolnych. Pracuj w miejscu, kt√≥re zna ka≈ºdy.#TyWybiera",[],Database Administration,Database Administration
Full-time,Senior,B2B,Office,198,Data Engineer,1dea,"Dla jednego z du≈ºych klient√≥w poszukujemy osoby do roli: Data Engineer Warunki zaanga≈ºowania: Obszar: Finansowy Lokalizacja: Irlandia, 5 dni w tygodniu Start: ASAP (akceptujemy kandydatury z max 1msc okresem wypowiedzenia) Stawka (ustalana indywidualnie): 150 - 170 PLN net / h Zaanga≈ºowanie: B2B (outsourcing z 1dea), full-time, d≈Çugofalowo Zakres obowiƒÖzk√≥w Projektowanie, tworzenie i utrzymywanie wysokiej jako≈õci potok√≥w danych (data pipelines). Analiza danych oraz modelowanie danych zgodnie z wymaganiami biznesowymi. Tworzenie i utrzymywanie proces√≥w ETL do efektywnego pobierania i transformacji danych (znajomo≈õƒá SnapLogic bƒôdzie dodatkowym atutem). Zapewnienie wydajno≈õci, bezpiecze≈Ñstwa i skalowalno≈õci rozwiƒÖza≈Ñ danych. Wsp√≥≈Çpraca z analitykami, naukowcami danych i innymi interesariuszami w zakresie projektowania i wdra≈ºania rozwiƒÖza≈Ñ. Udzia≈Ç w przeglƒÖdach kodu i projekt√≥w w celu utrzymania wysokiego poziomu standard√≥w in≈ºynierii danych. Tworzenie i utrzymywanie proces√≥w CI/CD dla p≈Çynnej integracji i dostarczania danych. Wsp√≥≈Çpraca z liderami technicznymi i architektami nad ulepszaniem rozwiƒÖza≈Ñ. Wymagania Minimum 5 lat do≈õwiadczenia na stanowisku Data Engineera w ≈õrodowisku korporacyjnym. Zaawansowana znajomo≈õƒá architektury danych i technologii bazodanowych. Bieg≈Ço≈õƒá w pracy z MS SQL, w tym ze ≈õrodowiskiem SQL MI w chmurze Azure . Do≈õwiadczenie w analizie danych, modelowaniu danych oraz procesach ETL. Znajomo≈õƒá koncepcji hurtowni danych. Do≈õwiadczenie w pracy z narzƒôdziami CI/CD (Git, Jenkins, Docker). Zrozumienie i do≈õwiadczenie w metodykach Agile. Oferujemy Zatrudnienie na podstawie umowy B2B na czas nieokre≈õlony Do≈ÇƒÖczysz do firmy z solidnƒÖ pozycjƒÖ na rynku Firma zapewnia nowoczesny sprzƒôt, oprogramowanie i konfiguracjƒô Profesjonalne doradztwo i wsparcie w rozwoju kariery od do≈õwiadczonego zespo≈Çu specjalist√≥w 1dea Cenimy sobie kole≈ºe≈Ñsko≈õƒá, otwarto≈õƒá, szacunek, wzajemnƒÖ pomoc i wsparcie w rozwijaniu kompetencji zar√≥wno w≈Çasnych, jak i koleg√≥w i kole≈ºanek z zespo≈Çu Wspieramy kulturƒô kreatywno≈õci. Ka≈ºdy cz≈Çonek zespo≈Çu ma mo≈ºliwo≈õƒá proponowania w≈Çasnych pomys≈Ç√≥w i rozwiƒÖza≈Ñ, a jego g≈Ços jest zawsze brany pod uwagƒô","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Office,199,Head of Database & Lake Platform Security,HSBC Service Delivery,"Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity Global Defense Engineering is responsible for fielding solutions that help defend HSBC against a wide range of threats to the business, customers, clients, partners, and staff. The team works in concert with partner teams across HSBC to implement novel defensive capabilities that are effective and adaptable against a constantly evolving threat landscape. The function operates under the vision: ‚ÄúEnabling HSBC to be safely successful everywhere the firm chooses to do business.‚Äù What you‚Äôll do Define secure configuration baselines for database management system software, including but not limited to Oracle, Db2, SAP ASE, SQL Server, Db2 z/OS, MongoDB, and PostgreSQL, Teradata, HADOOP. Work with database technical subject matter experts to agree secure configuration baselines. Work with database technical subject matter experts to define/develop/implement checks for compliance scans. Work with database technical subject matter experts to provide remediation guidance for IT Service Owners. Work with the Configuration Baseline Management team to ensure they receive configuration compliance data. Interact with stakeholders across the organisation to understand their security needs and expectations. Define and maintain capability strategy, supported by Enterprise Architecture, Security Architecture and, Control Owners, in response to business strategies, regulator expectations, technology and practice advancement, best practice, and threat actor evolution [will overlap with Architecture]. Ensure success with delivery partners (in alignment with support functions). Runs / drives respective Delivery forum, QBRs, SteerCos and Capability PODs. Maintain and prioritise a capability backlog based on objectives and value released to identify what teams work on next. Supports the prioritisation of backlogs from supporting technology and operations/service teams. Close working with Control Owners: Oversees Control Owner activity from a technical point-of-view, e.g. accurate assessment of control defect severities. Close working with Service Owners: understands general performance of associated services, exceptions, customer feedback and service uplift roadmaps. Close working with Technology/Platform Owners: understands general performance of associated IT services, significant bugs, technology health, customer feedback and technology uplift roadmaps (including technical debt resolution). Run a Pod per L2 capability with Architecture, Engineering, Service Delivery, Control Owner, Programme Manager, and Product Management Own all medium-rated and below risk Control Issues, Audit points and Regulatory findings. What you need to have to succeed in this role Minimum 5 years‚Äô in-depth experience with multiple database technologies from the list of Oracle, Db2, SAP ASE, SQL Server, Db2 z/OS, MongoDB, and PostgreSQL, Teradata, HADOOP. Demonstrated experience with database platform security. Minimum 2 years‚Äô experience leading a technical team. Demonstrated understanding of and experience with Center for Internet Security (CIS) benchmarks. Strong stakeholder management skills, with demonstrated experience of understanding and meeting the needs of multiple stakeholders. Excellent communication skills, including the ability to translate complex technical concepts into business-friendly language. Customer-centric consultancy approach. Strong analytical and problem-solving skills. Ability to manage budgets and allocate resources effectively. Reliant and adaptive to changing situations, with strong desire to delegate and empower the team. What we offer Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery discounts Financial support with trainings and education Social fund Flexible working hours Free parking If your CV meets our criteria, you should expect the following steps in the recruitment process: Online behavioural test Telephone screen Interview with the hiring manager. We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,200,Engineering Manager (AI & Big Data),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As an Engineering Manager, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: AI optimization engine for airport operation management. This project involves creating an AI-driven optimization engine to streamline airport operations, using real-time data from sources like radar, air traffic, and passenger information. GenAI knowledge retrieval platform for the leading automotive company. This project involves the creation of an Azure-backed platform that leverages cutting-edge LLMs to deliver powerful insights from an enterprise-scale knowledge base. AI recommendation engine for the leading innovation company. This project involves the design of AWS-powered solutions to provide domain-specific recommendations by employing sophisticated LLMs. This role is ideal for a leader who combines technical expertise with strong leadership skills , ready to drive innovative projects in data science and big data. Discover our perks and benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. In this position, you will: Be responsible for project management and technical leadership for data science and big data projects. Maintaining top-quality and client-focused deliverables to meet and exceed their expectations towards delivered solutions. Manage and mentor a team of data scientists, ensuring they are engaged, motivated, productive, and are provided with regular feedback to support growth. Act as a trusted advisor, proposing effective approaches and data science architectures tailored to client needs. Building technical solutions that deliver measurable value. Identify opportunities to expand service offerings and project potential with clients. Take part in pre-sales activities, connecting technical solutions to client needs. Participate in the recruitment process of new talents. What you‚Äôll need to succeed in this role: Bachelor‚Äôs or higher in Computer Science, Mathematics, Physics, or related field. Proven track record in managing technical teams and leading end-to-end projects. Experience working with corporate clients. Hands-on experience with Data Science applications (NLP, Computer Vision, Generative AI, Machine Learning, Predictive Modeling). Hands-on experience in ETL, data preparation, and data wrangling techniques. Strong critical thinking and problem-solving abilities. Excellent communication and presentation skills. Advanced English Skills ‚Äì C1 proficiency level or higher. Proficiency in Python and cloud platforms (preferably Azure, AWS).","[{""min"": 24000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,Permanent,Hybrid,201,Data Engineer,Reply Polska Sp. z o. o.,"Responsibilities Data System Design : Design and implement robust, scalable data processing systems: this involves selecting appropriate storage technologies, designing schemas, and planning integration strategies. Data Integration and ETL Development : Develop and maintain pipelines for data transformation, integration, and ETL processes. Ensure data quality and accessibility Performance Optimization : Monitor, tune, and optimize data applications and database performance. Address any issues that may affect data processing speeds or analytics capabilities Consulting and Strategy : Provide expert advice and consultancy services to clients on data strategies, architecture choices, and technological advancements Analytics and Business Intelligence Support : Assist in developing analytics platforms and business intelligence solutions, ensuring that data can be effectively transformed into actionable insights Client Interaction : Work closely with clients to understand their business needs and technical requirements. Translate these requirements into effective data engineering solutions. Benefits Motivizer Benefits Platform to choose and manage all your benefits in one place. You receive a budget (550 PLN monthly). You can choose medical care package, meal tickets, sports cards (we have Multisport and on preferential terms, we have membership cards to one of the most popular Gyms), cinema tickets, shop vouchers, discounts and many more. Language Courses ‚Äì you'll have access to a multi-language learning platform enabling you to practice you language skills and learn new ones! Regular and systematic further training opportunities - both internally and from external providers. We support your ongoing learning and development. Cooperation within an internal community is our everyday reality. We have networking events, coding challenges, and company parties for different occasions. Qualifications Educational Background: Bachelor‚Äôs or master‚Äôs degree in CS, Engineering, IT, or a related field. Very good knowledge of English and Polish. Programming Language: Strong programming skills in languages such as Java, Scala or Kotlin. Cloud: Experience with public cloud providers AWS/Azure Database Technologies: Knowledge of SQL and NoSQL databases. Expertise in Big Data Technologies: Familiarity with big data frameworks and tools like Apache Hadoop, Spark, Kafka, Flink or others, formats such as Apache Iceberg. Data Modeling and Warehousing: Familiarity with data modeling and warehousing techniques. Strong Analytical Skills: Ability to analyze complex data structures and derive insights to provide strategic guidance. Excellent Communication: Strong interpersonal and communication skills to effectively collaborate with team members and clients. Problem Solving: Strong problem-solving skills and the ability to propose creative, efficient solutions to complex problems. Plus: Any additional experience with Docker, Kubernetes, DevOps practices, CI/CD, DBT. Availability to work in a hybrid mode with at least 2 visits in the office per month. About Data Reply Data Reply, as part of the Reply Group, offers a wide range of services to help customers to become data driven. The team is active in various industries and business areas and works closely with clients to enable them to achieve meaningful results through the effective use of data. Data Reply offers many years of experience in transformation projects on the topic of ""Data Driven Enterprise"". Our experts focus on the development of Streaming and Event-Driven applications, Data Platforms and Machine Learning solutions - automated, efficient and scalable - without compromising IT security.","[{""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,202,Remote Data Engineer/ Analytics Engineer,Ework Group,"Ework Group - founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking for Senior Analytics Engineer üîπ ‚úîÔ∏èThe Position Join our Global Commercial IT team as a Senior Analytics Engineer and take part in transforming our Common Data Platform (CDP), which integrates and delivers business-critical data across 43 European and Canadian affiliates. You‚Äôll be joining a product team responsible for building and maintaining our AWS-based Data Lake, DBT Cloud transformation pipelines, and Snowflake data warehouse, enabling high-quality commercial and engagement insights across channels. This role goes beyond coding ‚Äì we‚Äôre looking for someone with strong technical expertise, a proactive mindset, and the ability to build foundations where there are none, improve data quality and reliability, and collaborate closely with business-facing product owner. ‚úîÔ∏è Key Responsibilities: Design, build, and maintain robust ETL/ELT pipelines in DBT Cloud, ingesting data from sources like OCE IQVIA, Adobe Analytics, Qualtrics, Salesforce CIAM, and others Contribute to the migration of legacy Snowflake and Qlik environments into the unified EUCAN CDP instance Implement data quality validation frameworks (e.g., DBT tests, freshness checks, anomaly detection) Support and improve our RUN Management process: monitoring, reruns, root-cause analysis and preventive actions Maintain and document model ownership, schema logic and transformations within our CDW_CORE > CDW_STAGE > CDW_PROD architecture Collaborate closely with Product Owner, Data Architect, and affiliate stakeholders to ensure delivery of trusted, scalable and governed data products Mentor junior engineers and act as a point of escalation for complex data issues ‚úîÔ∏è Tech Stack & Environment: DBT Cloud (core ELT engine) Snowflake (Data Warehouse) AWS S3 (Data Lake, ingestion zone) GitHub, Azure DevOps Data sources: IQVIA OCE, Salesforce Marketing Cloud, Adobe AEM/Analytics, Shopify, Qualtrics, Accutics, Facebook/LinkedIn/Instagram ‚úîÔ∏è Must-Have Qualifications: 5+ years of experience as a Data Engineer working with cloud-based ELT pipelines Expert-level SQL and DBT modeling skills Deep understanding of data warehousing concepts (dimensional modeling, incremental loading, performance optimization) Experience with Snowflake and cloud data architecture Strong communication and documentation skills ‚Äì must work well in distributed teams Proactive attitude: ownership over data quality, incident handling and delivery discipline ‚úîÔ∏è Nice-to-Haves: Experience in the pharmaceutical or commercial and Sales data domains (CRM, HCP, field force) Exposure to GDPR and data privacy requirements in commercial data ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 169, ""max"": 195, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Remote,203,Senior Data Scientist,Revolut,"About Revolut People deserve more from their money. More visibility, more control, and more freedom. Since 2015, Revolut has been on a mission to deliver just that. Our powerhouse of products ‚Äî including spending, saving, investing, exchanging, travelling, and more ‚Äî help our 55+ million customers get more from their money every day. As we continue our lightning-fast growth,‚Äå 2 things are essential to our success: our people and our culture. In recognition of our outstanding employee experience, we've been certified as a Great Place to Work‚Ñ¢. So far, we have 10,000+ people working around the world, from our offices and remotely, to help us achieve our mission. And we're looking for more brilliant people. People who love building great products, redefining success, and turning the complexity of a chaotic world into the simplicity of a beautiful solution. About the role We approach Data Science at Revolut the same way that we approach everything else ‚Äì with class, logical thinking and lots of style. Let‚Äôs break it down: we take the most complex problems and create tailor-made solutions for our customers. If you‚Äôre thinking the Data team is kept in some sort of a secret den, doomed to never see the impact of their work, don‚Äôt worry - that‚Äôs not how we do things. They‚Äôre some of our best and brightest problem-solvers, deployed to the front lines to work in product teams and deliver rockstar solutions. We start with deep data analysis to understand our customers, their objectives and any issues they might have. We then use various data points and advanced machine learning algorithms, to come up with the best possible option for each client. We experiment, iterate and build fully automated solutions, based on algorithms which self-improve with time. We‚Äôre looking for next-level Data Scientists to board our FinTech rocket ship and shape the future of financial services apps. Phew! It‚Äôs a big task, but you won‚Äôt do it alone. You‚Äôll be working with the toughest and most gifted professionals in Product, Design, Data Science and Engineering, on impactful projects that‚Äôll make our company move forward. What you‚Äôll be doing Improving existing algorithms and building new Proofs of Concept Delivering real impact to the product through rigorous data-driven solutions Researching and then delivering PoCs into data products Collaborating with product owners, engineers and data scientists to continually solve complex data problems What you'll need Experience building and leading a significant data science team within a scaling organisation Bachelor's/Master's/PhD in STEM (Mathematics, Computer Science, Engineering) Excellent knowledge of data science tools, including, python coding, SQL and production tools. Deep understanding of fundamentals of probability and statistics. Big picture thinking - correctly diagnosing problems and productionising research. Excellent communication and collaboration skills to partner with Product Owners and business heads Nice to have Advanced degree in a quantitative discipline (e.g., Masters or PhD) Strong experience with additional programming languages (such as: Java, Scala, C++‚Ä¶) Previous experience in anti fraud departments Experience working at a large tech company worth >$15B School/University Olympic medal competitions in: Physics, Maths, Economics or Programming Building a global financial super app isn‚Äôt enough. Our Revoluters are a priority, and that‚Äôs why in 2021 we launched our inaugural D&I Framework, designed to help us thrive and grow everyday. We're not just doing this because it's the right thing to do. We‚Äôre doing it because we know that seeking out diverse talent and creating an inclusive workplace is the way to create exceptional, innovative products and services for our customers. That‚Äôs why we encourage applications from people with diverse backgrounds and experiences to join this multicultural, hard-working team.",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,204,Data Architect,Spyrosoft,"Proven experience designing enterprise-level data architectures in Azure and/or AWS environments. Expertise in Databricks, Lakehouse architecture, and data processing frameworks (e.g., Spark). Hands-on leadership in metadata, data lineage, and governance tools (Collibra, Alation, Informatica EDC). Hands-on experience implementing Master Data Management strategies and tools. Strong proficiency in SQL/python, data modeling, and data integration patterns. Excellent English language skills, both written and spoken, especially for technical communication. Exceptional executive communication skills with the ability to brief the board, not just engineers, collaborate, share ideas and build consensus across teams. Experience working in multicultural, international environments. Our Partner is a prestigious Saudi Arabian conglomerate that stands today as one of the Middle East‚Äôs most influential family businesses, blending commercial expansion, global partnerships, and impactful philanthropy. Over eight decades, it has diversified into seven major sectors, including automotive, energy and financial services, operating in over 30 countries. Together, we are looking for a skilled Data Architect who has led AI and digital transformations at scale. In this senior-level role, you‚Äôll design and build robust data architectures with a strong focus on Master Data Management, Databricks, and Lakehouse solutions on Azure or AWS. You‚Äôll collaborate with international, cross-functional teams, share your vision, and help drive alignment on data strategy. Strong communication and interpersonal skills will be key as you work with both technical and business stakeholders in a multicultural environment. If you‚Äôre passionate about shaping modern data ecosystems and enabling organizations to turn raw data into trusted, high-impact insights‚Äîwe‚Äôd love to hear from you. Architect and implement scalable data solutions using Lakehouse, Databricks and cloud platforms (Azure or AWS). Design and oversee Master Data Management (MDM) frameworks, ensuring consistency, accuracy, and governance of core business data. Design or optimize ETL/ELT pipelines, integrating data from diverse sources and ensuring high availability. Collaborate with DevOps and infrastructure teams to ensure resilient cloud-native architecture and incident-ready solutions. Develop and enforce data governance principles and manage tools like Collibra, Alation, Informatica EDC. Create data models and semantic layers that serve as the foundation for BI and analytics solutions. Support teams by transforming business requirements into architecture blueprints and data-driven designs. Engage with diverse teams to communicate ideas and co-create architectural visions, promoting shared ownership and alignment. Mentor team members, driving data architecture best practices across the organization.","[{""min"": 170, ""max"": 230, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,205,ETRM Data Scientist,INFOPLUS TECHNOLOGIES,"We are looking for a highly skilled ETRM Data Scientist with 7+ years of experience to join our advanced analytics team. The ideal candidate will have deep expertise in time-series forecasting, scalable ML systems, and MLOps, with hands-on experience in energy trading (ETRM) environments. This is a high-impact role for someone passionate about building intelligent solutions in a cloud-native setting. Mandatory Skills Data Science & Machine Learning Expertise in time-series forecasting , predictive modelling , and deep learning . Experience with ML algorithms such as ARIMA , LSTM , Prophet , Linear Regression , Random Forest . Strong proficiency in scikit-learn , XGBoost , Darts , TensorFlow , PyTorch , Pandas , and NumPy . Proven knowledge of ensemble techniques ( stacking , boosting , bagging ) for robust model design. Ability to optimize and retrain production ML models based on evolving data and business needs. MLOps Implementation Proficiency in Python-based MLOps frameworks for automated pipelines, monitoring, and retraining. Hands-on experience with Azure Machine Learning Python SDK : Designing parallel model training workflows. Utilizing distributed computing for large-scale data processing. Big Data Analytics Strong experience with PySpark for distributed data processing and large-scale analytics. Azure Cloud Expertise Azure Machine Learning: Model deployment, training orchestration, lifecycle management. Azure Databricks: Data engineering and collaborative development using PySpark/Python. Azure Data Lake: Managing scalable storage and analytics pipelines for large datasets. Preferred Skills K-Means Clustering for segmentation and pattern analysis. Bottom-Up Forecasting for hierarchical business insight generation. Azure Data Factory for pipeline orchestration and ETL integration. Basic understanding of power/energy trading and ETRM systems. Exposure to Generative AI (GenAI) such as GPT or similar technologies.","[{""min"": 32000, ""max"": 36800, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,B2B,Hybrid,206,Data Engineer,Credit Agricole Bank Polska S.A.,"Jeste≈õmy Tribem CLV (Customer Lifetime Value) w potocznej nomenklaturze szeroko rozumianym jako CRM. BƒôdƒÖc cz≈Çonkiem naszego zespo≈Çu uczestniczy≈Ç/a by≈õ w pracach sk≈Çadu MLOPS odpowiedzianego za budowanie w oparciu o rozwiƒÖzania chmurowe platformy analitycznej dostarczajƒÖcej informacji o preferencjach produktowych klienta . Praca w naszym sk≈Çadzie opera siƒô na zaufaniu i przejrzysto≈õci. W celu budowania wiedzy i zrozumienia naszych produkt√≥w uczestniczymy we wszystkich etapach budowania naszych rozwiƒÖza≈Ñ a na ka≈ºdym etapie prac mo≈ºna liczyƒá na wsp√≥≈Çpracƒô i pomoc jak r√≥wnie≈º mi≈ÇƒÖ i ≈ºyczliwƒÖ atmosferƒô. Do≈õwiadczenie w pracy na stanowisku Data Engineera (minimum 5 lat). Bardzo dobra znajomo≈õƒá ≈õrodowiska Microsoft Azure, w tym: Azure Databricks (Spark) Azure Data Factory (orkiestracja przep≈Çyw√≥w danych) Azure Data Storage / Delta Lake Azure DevOps (CI/CD, zarzƒÖdzanie uprawnieniami, automatyzacja). Umiejƒôtno≈õƒá projektowania i wdra≈ºania rozwiƒÖza≈Ñ chmurowych dla zaawansowanych analiz i oblicze≈Ñ. Gotowo≈õƒá do pe≈Çnienia tak≈ºe roli wspierajƒÖcej testowanie rozwiƒÖza≈Ñ. Umiejƒôtno≈õƒá pracy zespo≈Çowej i komunikatywno≈õƒá. Do≈õwiadczenie z obszaru Machine Learning, MLOps i MLflow. Praktyczne do≈õwiadczenie z narzƒôdziami i technologiami takimi jak: Python, Terraform, YAML, Parquet, Git, PowerShell, Azure CLI. Znajomo≈õƒá dobrych praktyk zwiƒÖzanych z przetwarzaniem danych i optymalizacjƒÖ pipeline""√≥w. umowƒô o wsp√≥≈Çpracy (B2B), szkolenia i programy skupione wok√≥≈Ç rozwoju mocnych stron skierowane do wszystkich. R√≥≈ºnorodno≈õƒá oferty szkoleniowej: szkolenia techniczne i kompetencyjne, warsztaty, konferencje, mentoring. mo≈ºliwo≈õƒá rozwoju kariery poprzez wydarzenia, programy rozwojowe, akademie i rekrutacje wewnƒôtrzne w r√≥≈ºnych obszarach banku i Grupy Credit Agricole w Polsce, programy wellbeingowe, platformƒô wsparcia psychologicznego, dzia≈Çania CSRowe, akcje #mniejplastiku, miejsce, w kt√≥rym mo≈ºesz byƒá sobƒÖ bez wzglƒôdu na wiek, p≈Çeƒá, stopie≈Ñ sprawno≈õci czy jakƒÖkolwiek innƒÖ cechƒô, mo≈ºliwo≈õƒá wymiany do≈õwiadcze≈Ñ w ramach miƒôdzynarodowej Grupy Credit Agricole, mo≈ºliwo≈õƒá wykonywania zada≈Ñ zar√≥wno zdalnie jak i w nowoczesnym biurze, wsp√≥≈Çpracƒô w organizacji, kt√≥ra stawia cz≈Çowieka w centrum dzia≈Ça≈Ñ. Projektowanie, budowa i utrzymanie rozwiƒÖza≈Ñ do przetwarzania danych w chmurze Microsoft Azure. Przetwarzanie danych w Azure Databricks z u≈ºyciem Apache Spark. Orkiestracja proces√≥w danych w Azure Data Factory. Automatyzacja wdro≈ºe≈Ñ oraz konfiguracja ≈õrodowisk z wykorzystaniem Azure DevOps (CI/CD pipelines), Terraform, YAML, Azure CLI. Wsparcie zespo≈Ç√≥w biznesowych poprzez testowanie rozwiƒÖza≈Ñ i walidacjƒô danych. Praca z danymi w czasie rzeczywistym (real-time streaming).",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,207,Software Engineer,Pretius,"W Pretius poszukujemy Software Engineera do projektu dotyczƒÖcego rozwoju popularnej platformy eCommerce. Lokalizacja: zdalnie Wynagrodzenie: 140-170 pln netto/h O projekcie: Generowanie nazw produkt√≥w (r√≥wnie≈º z u≈ºyciem AI), automatyczna ocena ich jako≈õci i wyboru najlepszej nazwy dla produktu Weryfikacja poprawno≈õci numer√≥w GTIN (EAN) oraz ich precyzyjne przyporzƒÖdkowywanie do konkretnych produkt√≥w Automatyczne t≈Çumaczenie nazw i parametr√≥w produkt√≥w na wszystkie jƒôzyki obs≈Çugiwane na platformie Allegro Oczekiwania: 5+ lat do≈õwiadczenia na podobnym stanowisku Do≈õwiadczenie w pracy w Kotlinie, wykorzystywaniu AI do projektowania, budowania i rozwijania rozwiƒÖza≈Ñ Umiejƒôtno≈õƒá efektywnej wsp√≥≈Çpracy w zespole, proaktywno≈õƒá i samodzielno≈õƒá w dzia≈Çaniu Dba≈Ço≈õƒá o wysokƒÖ jako≈õƒá dostarczanych rozwiƒÖza≈Ñ, z uwzglƒôdnieniem testowania i optymalizacji Stack: Kotlin/Spring, MongoDB, Apache Spark/Apache Beam, BigQuery Mile widziane: Prompt Engineering AI Co oferujemy w Pretius? Stawiamy na d≈Çugofalowe relacje oparte na uczciwych zasadach i rzetelno≈õci Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Mo≈ºliwo≈õƒá pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnƒôtrzne, konferencje, certyfikacje","[{""min"": 140, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,208,Platform Integration Engineer (Salesforce & Qualtrics),ITDS,"As a Platform Developer , you will be working for our client, a global leader in healthcare and agriculture. The client is dedicated to revolutionizing the digital experience across global markets by integrating state-of-the-art SaaS platforms into their customer and internal operations. Your contributions will directly support strategic initiatives, streamline business processes, and enable data-driven decision-making at a global scale. Working closely with IT, business stakeholders, and external vendors, you will architect scalable solutions that elevate both user engagement and data management capabilities. This is an opportunity to make a tangible impact in a fast-paced, mission-driven environment that‚Äôs transforming the future of crop science through digital platforms. Develop and distribute surveys using multiple channels including WhatsApp, SMS, and email Design and implement automated workflows for survey logic and data processes Support integration between Salesforce and Qualtrics based on business events Manage Salesforce workflows, rules, and data objects related to survey responses Collaborate with Salesforce development teams to address evolving integration needs Integrate survey platforms with Google Cloud to support data lakes and warehouse strategies Work with data teams to design ingestion strategies and perform gap analysis Configure AI-powered survey analytics and insights for reporting Partner with cross-functional teams to gather requirements and deliver technical solutions 5‚Äì7 years of experience in SaaS development and platform integration Proven expertise in Qualtrics platform administration and workflow configuration Strong knowledge of Salesforce configuration, workflows, and data architecture Basic knowledge of Salesforce Service Cloud Experience with integration tools, APIs, and cloud platforms like Google Cloud Solid understanding of data modeling, ingestion strategies, and architecture Proficiency in Apex, Lightning Components, SOQL, and Salesforce permissions management Knowledge of XML, HTML, CSS, SOAP/REST Ability to translate business requirements into scalable technical solutions Familiarity with AI analysis tools within survey platforms Strong collaboration and communication skills in cross-functional environments Experience working in Agile development frameworks We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #7440 üìå You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 25200, ""max"": 29400, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,209,Senior Data Engineer (GCP + Python),emagine Polska,"Industry: Energy System, Startup Rate: 42-50 Euro/h on B2B Location: Poland (Remote) Contract length: 6-9 months with possible extension Introduction & Summary The primary objective of this role is to spearhead a data migration project focused on transferring data pipelines from a third-party orchestration tool to native Google Cloud Platform (GCP) services. The ideal candidate should possess 5+ years of experience as a Data Engineer with strong competencies in Google Cloud Platform, Python, and Terraform. Main Responsibilities Design and implement the data migration architecture. Establish infrastructure representation using Terraform. Select, configure, and manage an orchestrator. Implement reliable CI/CD practices for testing and deployments. Migrate data pipelines to the GCP environment and validate performance. Onboarding a new full-time employee. Key Requirements At least 5 years of experience as Data Engineer. Proven experience with Google Cloud Platform (GCP). Proficiency in Python and SQL for pipeline development. Hands-on expertise with Terraform as an infrastructure-as-code tool. Nice to Have Experience with DBT (Data Build Tool). Previous migration experience of data pipelines from tools like Twirl to Composer. Understanding of CI/CD best practices in data projects.","[{""min"": 28500, ""max"": 35280, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,210,Senior Data Integration Engineer,EPAM Systems,"We are looking for a Senior Data Integration Engineer to join the Client‚Äôs security team, help develop and support data products (data pipelines, data integration, data analytics dashboard). Our team is distributed between the US and the EU (Poland, Portugal). This role offers a hybrid model, with 3 days per week working from the client's office in Wroclaw, Gdansk or Krakow. RESPONSIBILITIES Assess Client‚Äôs internal infrastructure, get used to Client‚Äôs services and tools Work closely with the Client to understand their data needs, consult them and develop reporting and analytics solutions Use internal Client‚Äôs tools, build new data pipelines and ETL workflows for various use cases Implement PoCs, perform data integration and migration activities Implement data quality checks and monitor data pipelines to ensure they perform as expected Investigate and support released data products Own the code, plan and do sanity check before initiating code review Develop and maintain technical documentation Participate closely in the refinement and scope definition Present the results and outcomes REQUIREMENTS 2+ years of hands-on experience in SQL Proven expertise in dimensional data modeling, data warehousing Good communication, problem solving, and collaboration skills English: B2 Availability from 8 AM to 10 AM Pacific Time (17: 00 - 18: 00 Poland Time) is required. About once a week, the candidate should be available until 19: 00 Poland Time NICE TO HAVE Experience in programming and scripting languages such as Python, Bash, or Java Data-driven CI/CD development WE OFFER We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,211,Senior Database Engineer,Navblue,"Job Summary: Aviation. It connects our world, brings people together, provides opportunities, accelerates economic growth, and is just so very cool! Come work for NAVBLUE, a leading services company owned by Airbus, dedicated to Flight Operations, Air Traffic Management solutions and services for airlines, airports, and Air Navigation Service Providers (ANSPs). We combine aircraft manufacturer expertise, flight operations know-how, and agile development to enhance operational efficiency, optimize resources, and increase productivity for a safe and sustainable aviation future. Our global teams deliver a reliable, optimum, and customized user experience to more than 500 customers worldwide. We are looking for a seasoned Software Engineer with extensive experience in designing, implementing, and optimizing database solutions in a microservices-based environment. As a member of our team, you will contribute to the full lifecycle of our data persistence layer, from schema design and performance tuning to ensuring robust replication, disaster recovery, and seamless integration within our cloud-native microservice ecosystem. Responsibilities: Design & Develop Database Solutions: Architect, design, and implement highly optimized relational (e.g., MySQL, PostgreSQL, AWS Aurora, SQL Server) and NoSQL (e.g., MongoDB, DynamoDB, Redis) database schemas, ensuring data integrity, performance, and scalability for microservices. Performance Optimization & Tuning: Proactively analyze and optimize complex queries, implement efficient indexing strategies, and manage partitioning/sharding to ensure peak database performance and handle high throughput. Reliability & Disaster Recovery: Design, implement, and maintain robust backup, disaster recovery, and high-availability solutions, including replication (master-slave/multi-master) and failover configurations, to ensure data durability and system uptime. Cloud Database Management: Deploy, configure, and manage database instances within cloud infrastructure (e.g., AWS RDS, Aurora), leveraging cloud-native features for scalability and operational efficiency. Focus on quality by promoting coding best practices, a test-first mindset and highest security standards. Contribute to building new and improving existing development processes. Work within a small agile teams delivering new features and fixing defects. Lead technical designs, taking a holistic view of the product, and collaborate with multiple stakeholders to define the best approach to address upcoming challenges and deliverables. Define and drive the team's technical direction, mentor junior engineers, and proactively identify, propose, and implement new processes or architectural improvements to enhance team efficiency, code quality, and timely delivery. Contribute to software architecture discussions, translate system-level designs and architectural blueprints into robust, maintainable, and high-quality code, applying the latest best practices in software engineering. Required Skills/Experience: 6+ years in roles directly responsible for the availability, performance, and security of critical databases. Expertise in Database Technologies: Strong command of MySQL, PostgreSQL, AWS Aurora, SQL Server and experience with MongoDB, DynamoDB, or Redis, including schema design and complex query writing. Database Performance & Scalability: Proven ability in query optimization (e.g., EXPLAIN plans), operating system optimization, advanced indexing strategies, and implementing partitioning/sharding and caching (Redis). Cloud Database Operations: Hands-on experience deploying, managing, and optimizing databases within AWS infrastructure (RDS, Aurora) and virtual machine infrastructure (SQL Server). Database Reliability & Security: Experience with high availability (replication, failover), backup automation, PITR, and data security (encryption, SQL injection prevention). Automation & DevOps: Proficiency in integrating database changes into CI/CD pipelines using schema migration tools (DbUp, EF migrations, Flyway, Liquibase) and Git for version control. Scripting & Troubleshooting: Strong scripting skills (Python/Bash) for automation and ability to analyze logs and monitor performance using tools like AWS Cloudwatch, Datadog, Prometheus, Grafana, or pgBadger. Solid understanding of DevOps practices, including CI/CD pipelines (e.g., GitLab CI, Cloudbees, Jenkins, GitHub Actions), containerization with Docker, and monitoring/logging tools. Demonstrated experience in leading software development teams, fostering a collaborative and high-performance culture, and effectively representing the team's technical vision and needs to stakeholders, including architects and senior management Strong capability in identifying technical challenges and bottlenecks, constructively proposing and implementing effective solutions (either individually or by guiding the team), while actively building team engagement, fostering a positive atmosphere, and championing team spirit Master of Science Degree in software engineering or a related field Proficiency in English spoken and written Nice-to-Haves: Experience with ETL/ELT pipeline design and tools (e.g., Apache Airflow). Familiarity with Change Data Capture (CDC) solutions. Knowledge of database services on other cloud platforms (e.g., Azure SQL Database, Google Cloud Spanner). Understanding of ORM frameworks (Entity Framework, Dapper, SQLAlchemy, Hibernate) from a database performance perspective. Experience with data governance or data lineage concepts and tools. Active contributions to open-source database projects or the broader database community. Proficiency in designing and implementing GraphQL backends or similar API patterns that interact heavily with databases. Understanding of airline operations, flight planning, or air navigation principles. Passion for the aviation industry. We offer: Stable employment based on a full-time job contract International working environment in a dynamic company Access to the latest knowledge and technologies enabling professional development Training and development possibilities Participating in international projects and international trips Competitive salary dependent on experience and qualifications Flexible working hours and work-from-home opportunities Private medical coverage for you and your family Sport card Life insurance for you and your family Co-funding for meals Employee stock ownership plan How to Apply: Candidates who are interested in joining the NAVBLUE team are invited to submit their resume and cover letter, highlighting their work experiences and skills via email to talent@navblue.aero . We thank all applicants for applying. Only selected applicants will be contacted. Navblue is committed to creating an environment and a culture where everyone feels like they belong no matter who they are or where they are from. We are committed to providing equal employment opportunities to all individuals based on job-related qualifications and ability to perform a job. We do not discriminate against any employee or applicant for employment because of race, colour, sex, age, national or ethnic origin, religion, sexual orientation, gender identity or expression, marital status, family status, genetic characteristics, record of offences, and basis of disability or any protected class. Accommodations will be available on request for candidates throughout the entire recruitment and selection process. NAVBLUE is operating within the Airbus Helicopters Polska Structure . About Us: NAVBLUE, an Airbus Company, is a leading global provider of flight operations solutions, including aeronautical charts, navigation data solutions, flight planning, aircraft performance software (take-off/landing, weight and balance), and crew planning solutions. You‚Äôll be able to shape the future of the digital aviation industry by working on several of the best in the industry flagship products enabling pilots, dispatchers, flight engineers and other aviation personnel on a daily basis to deliver safe, efficient, and reliable flight operations all over the world. You‚Äôll have the opportunity to support millions of flights each year and help NAVBLUE customers maximize efficiency, reduce costs, ensure compliance with complex national and international safety regulations, and effectively deliver their services. You‚Äôll join a team with a focus on digital and collaborative innovation that is passionate and customer-focused. Over the last few years, Airbus has been supportive of various initiatives such as Going Digital, Performance Based Navigation Services, Air Traffic Management Modernization Programs, FlySmart on iOS and other digital projects related to new aircraft technologies; the launch of NAVBLUE was therefore a natural step to further develop its Flight Operations and Air Traffic Management Portfolio. NAVBLUE is a fully owned subsidiary of Services by Airbus, fueled by the agility of Airbus ProSky and Navtech (acquired in 2016), and the pioneering spirit of Airbus, NAVBLUE was created in July 2016 with one mission: lead aviation into the digital age. Airbus and all subsidiaries, including NAVBLUE are proud to have been recognized as a Global Top Employer for 2025 . Based on eight criteria: physical workplace, work atmosphere and social, health financial and family benefits, vacation and time off, employee communication, performance manager, training and skills development and community involvement. It was determined that we offer some of the most progressive and forward-thinking programs within the area. This achievement reflects our commitment to nurturing and empowering our people to reach their full potential. We‚Äôre grateful to our people, whose dedication and collaboration make this possible. NAVBLUE is based in Hersham (UK), Cardiff (UK), Toulouse (France), Waterloo, ON (Canada), Bangkok (Thailand) and Gda≈Ñsk (Poland) with other offices all around the world. The Future is Yours for the Taking: https: //youtu.be/vdY6gYuceYY","[{""min"": 15800, ""max"": 24000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,212,Data Vault Architect with Snowflake,Upvanta sp. z o.o.,"Assess the client's existing data platform and define a clear and actionable transformation roadmap Define the platform strategy and long-term vision aligned with business needs Lead stakeholder engagement and manage change processes (change management) Coordinate and plan delivery efforts across multiple teams Potential to lead a technical delivery team Proven experience with Data Vault 2.0 modeling and architecture Strong hands-on experience with Snowflake Practical knowledge of DBT (Data Build Tool) for data transformation workflows Familiarity with AWS cloud services (e.g., S3, Lambda, Glue) Solid background in stakeholder management and delivering data transformation projects Team leadership experience is a plus, or a willingness to take on a managerial role",[],Data Architecture,Data Architecture
Full-time,Mid,B2B,Hybrid,213,Big Data / Hadoop Developer,makeitright,"Specjalista Big Data / Hadoop üìç Tryb pracy: hybrydowy (2 dni w tygodniu w biurze) üí∞ Stawka maksymalna: 13 PLN/h B2B üìÖ Start: najp√≥≈∫niej listopad 2025 üåê Bran≈ºa ko≈Ñcowego klienta: bankowo≈õƒá üó£ Wymagane jƒôzyki: polski i angielski üìå Metodyka pracy: Agile Do≈õwiadczenie w pracy z technologiami Big Data oraz Hadoop Projektowanie i wdra≈ºanie wydajnego, elastycznego, skalowalnego i ≈Çatwego w utrzymaniu globalnego rozwiƒÖzania raportowego dla sektora bankowego ≈öcis≈Ça wsp√≥≈Çpraca z Product Ownerem, Architektami RozwiƒÖza≈Ñ, Analitykami, Testerami i Programistami Projekt realizowany jest dla klienta ko≈Ñcowego (firma z sektora bankowego) Proces rekrutacyjny obejmuje rozmowƒô kwalifikacyjnƒÖ oraz zadanie praktyczne","[{""min"": 120, ""max"": 130, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,214,Reporting Developer (Power BI/Tableau),Britenet,"About the role Project carried out for the lottery industry. The work is conducted remotely, with occasional visits to the office in Warsaw. Our expectations Minimum 3 years of experience in a similar position Proficient in Power BI and familiar with Tableau Basic knowledge of project management principles, especially using Tableau/Power BI Understanding of best practices in data visualization Experience working with SQL for data extraction and manipulation Experience with Microsoft SQL Server and ETL processes Knowledge of data preparation, modeling, and visualization techniques Strong communication and teamwork skills Ability to translate business requirements into effective solutions Willingness to learn and adapt to new technologies and tools Very good command of English (minimum B2 level) Openness to occasional on-site work in the Warsaw office Main responsibilities Managing projects related to PowerBI/Tableau implementation, data visualization, and analytics Collaborating with other teams to gather requirements, design solutions, and deliver Tableau dashboards and reports Analyzing data trends and providing strategic insights to support business decision-making Working with team members on project tasks and deliverables Participating in project meetings and contributing to project status updates","[{""min"": 90, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Hybrid,215,Staff Architect (Data),The Stepstone Group Polska,"Who we are At The Stepstone Group, we have a simple yet very important mission: The right job for everyone. Using our data, platform, and technology, we create opportunities for job seekers and companies around the world to find a perfect match, in a fair and equitable way. With over 20 brands across 30+ countries, we strive for inclusive, unbiased hiring. At our Tech Hub in Warsaw, located near Wilanowska Metro, we are a team of over 300 ambitious specialists dedicated to building and scaling world-class IT products. As part of The Stepstone Group, a global leader in job-tech and digital recruiting solutions, we‚Äôre shaping the future of work and technology. Join our team of 4,000+ employees and be part of reshaping the labour market and becoming the world‚Äôs leading job-tech platform. Our way of working: being together At The Stepstone Group, we believe that meaningful work happens best together. We‚Äôre building a culture where people connect, learn from each other, and share ideas openly. That‚Äôs why working 3 days per week from our Warsaw office is a core part of how we collaborate. We know that face-to-face time makes a difference ‚Äî for our people, our culture, and our impact. üìå The job at glance The Staff Architect ‚Äì Data Architecture is an experienced technologist within the Enterprise Architecture team, dedicated to shaping, implementing, and governing the enterprise data architecture. This role is pivotal in ensuring consistency, interoperability, and strategic alignment of data across the organization. You will lead and partner in the development and governance of the enterprise data model, data policies, integration patterns, and data contracts between our core domains and capabilities. You will work closely and collaboratively with engineering, product, and architecture stakeholders to ensure data is treated as a strategic asset‚Äîenabling scalable, secure, and intelligent solutions that benefit everyone. This role welcomes candidates from all backgrounds and genders and values diverse perspectives. It calls for a blend of deep technical expertise in data architecture and engineering, strong communication and synthesis skills, and a strategic mindset to align data initiatives with domain delivery. Working closely with other senior individual contributors and technical leaders, the Staff Architect ensures ongoing alignment of data architecture practices across portfolios. They provide mentorship, oversight, and guidance on the discipline of data architecture, influencing its evolution across Chapters and engineering teams. The Staff Architect also plays a key role in supporting and growing an inclusive IC community by sharing knowledge, promoting best practices, and fostering a culture of belonging and excellence in data architecture. This position is ideal for experienced professionals who can navigate both functional and technical data landscapes and drive impactful outcomes with autonomy and expertise. üìå Responsibilities You will contribute to the implementation and governance of the enterprise data architecture, ensuring alignment with business and technical strategies. You will support the definition and maintenance of the enterprise data model, promoting consistency and interoperability across domains. You will help establish and enforce data policies, integration patterns, and data contracts between domains and capabilities. You will collaborate with domain architects, engineering leaders, and product teams to simplify and standardize data flows and interfaces. You will support the full ML/AI lifecycle, from data acquisition and preparation to model deployment and monitoring. You will contribute to architecture roadmaps, principles, and standards related to data. You will participate in resolving complex data-related architectural decisions and challenges. You will help improve the maturity of data architecture assets and practices across the organization. You will help create an inclusive culture where all team members feel heard, valued, and empowered to contribute. üìå Key stakeholders VP of Architecture Engineering Directors & Managers Product Directors & Managers Domain Architects Principal & Staff Engineers Data Scientists & ML Engineers Business stakeholders and colleagues across diverse teams and backgrounds üìå Key performance indicators Delivery of data architecture milestones Health and adoption of the enterprise data model Quality and consistency of data contracts and integration patterns Support for ML/AI lifecycle and data platform scalability Engineering and data governance health Delivery of OKRs Contribution to an inclusive, supportive, and collaborative work environment Skills, Knowledge, and Experience Strong technical background in data architecture, data engineering, and data management Proven experience in enterprise data modeling, data governance, and data integration Familiarity with the ML/AI lifecycle and enabling data platforms for intelligent applications Effective communication and synthesis skills to engage confidently with both technical and non‚Äëtechnical stakeholders Experience with solution architecture and cross‚Äëdomain data initiatives Ability to influence and align technical solutions with business goals, while collaborating respectfully with diverse teams Strong understanding of modern data platforms, cloud‚Äënative data services, and data security practices Commitment to building an inclusive, supportive environment where all voices and perspectives are welcomed üñ•Ô∏è Software & Hardware Mac or Dell laptop ‚Äî your choice, so you can work the way you feel most productive Windows or macOS system ‚Äî whichever fits your style 2 monitors (24"", 27"", or 32"") for an ergonomic, flexible setup Adjustable desks for your comfort, health, and well‚Äëbeing Slack & Teams to keep communication open, smooth, and inclusive Atlassian tools (JIRA, Confluence) to help everyone stay connected and organized üéÅ What we offer Your life outside work matters too ‚Äî and we believe your job should enrich it, not take it over. Here‚Äôs what you‚Äôll find with us: Premium medical and dental care for you and your loved ones Life insurance for peace of mind Flex Benefits ‚Äì Worksmile Cafeteria System (Multisport, vouchers, tickets, etc.) Employee Referral Program Hackathons, Knowledge Sharing Hours, and in‚Äëhouse projects where all voices are welcomed Tech and sport communities open to everyone Team get‚Äëtogethers and integration parties to build genuine connections Charity initiatives & 2 extra volunteer days to support causes you care about English/German classes to boost your confidence and skills Game room and chill‚Äëout zone to relax, recharge, and share time with colleagues ü§ù Our promise We‚Äôre committed to creating a workplace where everyone feels valued, respected, and supported.Equal opportunities are at the heart of what we do ‚Äî and diversity, equity, and inclusion are essential to our success as a global company. We actively welcome applications from people of all gender identities, backgrounds, family situations, sexual orientations, disability statuses, ethnicities, beliefs, and ages. We know diverse teams make better decisions, build better products, and create a place where everyone can thrive. If you want to help shape the future of work ‚Äî and believe in doing it together ‚Äî we‚Äôd love to hear from you.","[{""min"": 20000, ""max"": 35000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,216,Data Scientist,Kevin Edward,"We are seeking an experienced Data Scientist with a strong background in predictive analytics and machine learning, specifically within the Energy Sector . The ideal candidate will have 7+ years of experience applying advanced modeling techniques like ARIMA , LSTM , Prophet , Linear Regression , and XGBoost to solve complex problems in energy trading and risk management (ETRM) environments. The role requires expertise in Python , PySpark , and modern machine learning frameworks like TensorFlow , PyTorch , and Darts . You will work on building and deploying machine learning models for energy forecasting, risk management, and trading optimization. Key Responsibilities Develop and implement machine learning models to forecast energy prices, demand, and other key market indicators, leveraging tools such as ARIMA , LSTM , Prophet , XGBoost , and Linear Regression . Use frameworks like TensorFlow , PyTorch , and Darts to design, train, and deploy predictive models. Integrate machine learning models into production environments using MLOps practices, ensuring seamless deployment and monitoring. Build and maintain ETRM (Energy Trading and Risk Management) solutions, specifically focusing on advanced data analysis and predictive modeling for risk and portfolio management. Work closely with cross-functional teams, including traders, data engineers, and IT, to understand data requirements and deliver actionable insights. Implement Python SDK for model development, training, and testing, while utilizing PySpark for distributed computing and large-scale data processing. Develop and optimize algorithms for energy forecasting, including short-term and long-term price predictions, demand forecasting, and energy production forecasts. Continuously improve model performance and adapt to changing market conditions, using data-driven insights to inform strategic decisions. Required Qualifications 7+ years of experience as a Data Scientist, preferably within the Energy Sector or a related field such as commodities or trading. Expertise in ETRM systems and processes, with a strong understanding of energy markets and trading dynamics. Strong proficiency with machine learning techniques, including: ARIMA , LSTM , Prophet , Linear Regression , XGBoost . Deep learning frameworks like TensorFlow and PyTorch . Forecasting models using Darts for time series analysis. Experience in MLOps for deploying and maintaining machine learning models in production environments. Advanced knowledge of programming in Python , including Python SDK for model deployment and automation. Strong experience with PySpark for distributed data processing and large-scale analytics. Familiarity with cloud computing platforms, particularly Azure for data storage, model training, and deployment. Experience with data wrangling, feature engineering, and data preprocessing for large datasets. Excellent problem-solving skills with a passion for innovation and driving business value through data science. Preferred Qualifications Experience working with ETRM platforms like Allegro , Exxeta , or Trayport . Background in energy trading, utilities, or commodity sectors. Familiarity with regulatory compliance within the energy sector. Strong understanding of data visualization tools like Power BI , Tableau , or Matplotlib for presenting insights. Experience working in an Agile development environment.",[],Data Science,Data Science
Full-time,Mid,B2B,Hybrid,217,SAS Developer (SAS Viya),Connectis,"Wsp√≥lnie z naszym klientem z sektora bankowego poszukujemy do≈õwiadczonej osoby na stanowisko SAS Developera , kt√≥ra do≈ÇƒÖczy do zespo≈Çu rozwijajƒÖcego systemy CRM i procesy danych w ≈õrodowisku SAS. Projekt obejmuje m.in . rozw√≥j i migracjƒô proces√≥w ETL, integracjƒô system√≥w oraz modernizacjƒô ≈õrodowiska SAS z wykorzystaniem SAS Viya i chmury Azure . üí° TWOJA ROLA: Konfiguracja log√≥w, archiwizacji i polityk backupowych zgodnie z procedurami bankowymi. Tworzenie, rozw√≥j i optymalizacja proces√≥w ETL (SAS 4GL, Oracle, MSSQL). Harmonogramowanie i monitorowanie proces√≥w SAS oraz Oracle. Administracja i utrzymanie ≈õrodowiska serwerowego SAS (Unix). Instalacja i aktualizacja komponent√≥w SAS, w tym hot-fix√≥w. Migracja proces√≥w do SAS Viya. üîç CZEGO OCZEKUJEMY OD CIEBIE? Min. 4 lata do≈õwiadczenia w pracy z SAS w obszarze tworzenia i utrzymania proces√≥w ETL. Znajomo≈õƒá narzƒôdzi: SAS DI Studio, SAS Management Console, SAS Enterprise Guide. Samodzielno≈õƒá w analizie i rozwiƒÖzywaniu problem√≥w technicznych. Praktyczne do≈õwiadczenie w pracy z chmurƒÖ ‚Äì preferowana Azure. Do≈õwiadczenie z SAS Viya i architekturƒÖ serwerowƒÖ SAS. Znajomo≈õƒá baz danych: Oracle, MSSQL. ‚ú® OFERUJEMY: Uczestnictwo w spotkaniach integracyjnych oraz meetupach technologicznych, umo≈ºliwiajƒÖcych dzielenie siƒô wiedzƒÖ i do≈õwiadczeniem. Wsparcie dedykowanego opiekuna Connectis, kt√≥ry zawsze jest dostƒôpny, by pom√≥c Ci w sprawach zwiƒÖzanych z projektem. Rozw√≥j kariery i d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô w firmie o ugruntowanej pozycji na rynku. Hybrydowy model pracy ‚Äì 1 dzie≈Ñ w tygodniu w biurze klienta (Warszawa). Wsp√≥≈Çpracƒô przy du≈ºym i stabilnym projekcie z sektora finansowego. Mo≈ºliwo≈õƒá rozwoju w kierunku nowoczesnych technologii SAS i Azure. 5000 PLN za polecenie znajomego do naszych projekt√≥w. Atrakcyjne wynagrodzenie oraz forma wsp√≥≈Çpracy B2B. Dostƒôp do najnowszych technologii i narzƒôdzi pracy. Zaanga≈ºowany zesp√≥≈Ç i wsparcie merytoryczne. Szybki i zdalny proces. Dziƒôkujemy za wszystkie zg≈Çoszenia. Pragniemy poinformowaƒá, ≈ºe skontaktujemy siƒô z wybranymi osobami. 12201/AP","[{""min"": 135, ""max"": 195, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Hybrid,218,Data Engineer (Cybersecurity),Antal Sp. z o.o.,"Data Engineer ‚Äì B2B Contract (Hybrid ‚Äì Krak√≥w or Warsaw) Location: Hybrid ‚Äì 6 days per month on-site (preferably Krak√≥w or Warsaw) Contract type: B2B Workload: Full-time We are looking for an experienced Data Engineer to join a team focused on data processing and analytics in the cybersecurity domain. This role involves designing and building robust data architecture and scalable data pipelines that power reporting, data analytics, and future machine learning models. Design, build, and test multi-layered data architecture and efficient, scalable data pipelines. Design, build, and test multi-layered data architecture and efficient, scalable data pipelines. Perform deep analysis of large, complex datasets to extract insights and generate statistical metrics for business reporting and data estate assessment. Perform deep analysis of large, complex datasets to extract insights and generate statistical metrics for business reporting and data estate assessment. Develop systems and software for data acquisition, aggregation, and refinement. Develop systems and software for data acquisition, aggregation, and refinement. Integrate data across various sources, systems, and platforms. Integrate data across various sources, systems, and platforms. Optimize query performance and overall data processing efficiency. Optimize query performance and overall data processing efficiency. Work on data ingestion, sourcing, aggregation, API integration, and feature engineering. Work on data ingestion, sourcing, aggregation, API integration, and feature engineering. Collaborate with stakeholders to align deliverables with business needs and better understand the data context. Collaborate with stakeholders to align deliverables with business needs and better understand the data context. Write clean, reusable, and maintainable code using the team's DevOps practices. Write clean, reusable, and maintainable code using the team's DevOps practices. Follow Agile methodologies, including test-driven development (TDD). Follow Agile methodologies, including test-driven development (TDD). Continuously grow technical and domain-specific skills in collaboration with the team. Continuously grow technical and domain-specific skills in collaboration with the team. At least 12 months of hands-on experience with Spark, PySpark, and/or Databricks (or a comparable modern data platform). At least 12 months of hands-on experience with Spark, PySpark, and/or Databricks (or a comparable modern data platform). Strong hands-on experience with Python and SQL in end-to-end data engineering workflows. Strong hands-on experience with Python and SQL in end-to-end data engineering workflows. Experience building and maintaining data pipelines and ETL workflows across disparate datasets. Experience building and maintaining data pipelines and ETL workflows across disparate datasets. Working knowledge of Azure DevOps, scripting (Azure CLI), Git/version control, and CI/CD processes. Working knowledge of Azure DevOps, scripting (Azure CLI), Git/version control, and CI/CD processes. Practical experience with Databricks. Practical experience with Databricks. Ability to communicate complex technical topics clearly and effectively to diverse audiences. Ability to communicate complex technical topics clearly and effectively to diverse audiences. A proactive, eager-to-learn attitude, especially regarding cybersecurity. A proactive, eager-to-learn attitude, especially regarding cybersecurity. Hands-on experience working within Agile teams. Hands-on experience working within Agile teams. B2B contract and support of the Contractor Care Team Private Medical Care Cafeteria system Life insurance Zapraszamy do odwiedzenia naszej strony www.antal.pl","[{""min"": 30000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,219,Database Architect,StoneX Poland,"Connecting clients to markets ‚Äì and talent to opportunity. With 4,500+ employees and over 300,000 commercial, institutional, payments, and retail clients, we operate from more than 70 offices spread across six continents. As a Fortune 100, Nasdaq-listed provider, we connect clients to the global markets ‚Äì focusing on innovation, human connection, and providing world-class products and services to all types of investors.Whether you want to forge a career connecting our retail clients to potential trading opportunities, or ingrain yourself in the world of institutional investing, StoneX Group is made up of four business segments that offer endless potential for progression and growth. Position Purpose: We are seeking an experienced Database Architect to design, govern, and evolve our enterprise data architecture. Our current environment is primarily based on on-premises Microsoft SQL Server & Oracle, but we are actively using and exploring a hybrid future with cloud-based technologies including MongoDB, Azure Cosmos DB, Azure SQL and PostgreSQL Flexible Server. You will be instrumental in guiding this transition while ensuring alignment with business goals, compliance, and scalability. You‚Äôll help shape the future of our data architecture, building secure, scalable, and forward-looking solutions. This is a unique opportunity to operate at the intersection of business and technology while influencing the data strategy at an enterprise level. Primary duties will include: Architecture & Design Submit reports to management that outline the changing data needs of the company and propose corresponding architectural strategies. Define the vision and roadmap for the evolution of the enterprise data platform, ensuring it supports future business needs, innovation, and scalability. Define and maintain enterprise-wide data architecture strategy across relational, NoSQL, and distributed platforms. Create and manage conceptual, logical, and physical data models. Select and recommend appropriate database technologies for varying business use cases. Cloud & Modernization Lead the modernization of legacy on-premises SQL Server systems to cloud-native platforms with a focus on MongoDB, Cosmos DB, and PostgreSQL Flexible Server in Azure. Governance & Compliance Establish and enforce data governance policies. Ensure compliance with GDPR, HIPAA, SOX, and other regulatory frameworks. Performance & Availability Working in tandem with the Database Operations team to guide performance engineering and capacity planning practices and to set standards for monitoring, disaster recovery, and high-availability architecture. Collaboration & Leadership Work closely with the Database Operations Team, to align architectural direction with operational practices and project execution. Partner with the Networks and Systems teams to architect infrastructure solutions that are scalable, resilient, and aligned with the evolving demands of the data platform. Serve as the bridge between IT and business. Provide architectural oversight and mentorship. Participate in or lead enterprise architectural review boards and steering committees. Tooling & Automation Provide architectural support and database expertise to Platform Engineering-led initiatives. Adoption of DevOps for databases. Help define and standardize tooling across the database lifecycle. To land this role you will need: Define and maintain enterprise-wide data architecture strategy across on-premises SQL Server/Oracle and emerging cloud database platforms such as MongoDB, Cosmos DB, and PostgreSQL Flexible Server. 10+ years of experience in database development and administration, with at least 2 years focused on cloud-based PostgreSQL solutions. Certifications: TOGAF, Azure Solutions Architect, PostgreSQL Certified Engineer. Experience with data lakes, lakehouses, or real-time streaming platforms. Familiarity with Agile and Scrum methodologies. Education / Certification Requirements: Bachelor‚Äôs degree. Master‚Äôs degree a plus. Working environment: Hybrid; our Cracow office is located at Mogilska 35 street. Parking space for employees. #LI-Hybrid #LI-MA1",[],Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,220,Senior Data Engineer - Integrations,Asquare GmbH,"As a Senior Data Engineer - Integrations at Adsquare, you will be responsible for building our massively scalable data processing pipelines and integrations - transferring terabytes of data to external partners. You will join a team of 5 Backend Engineers and a Technical Product Owner, making you the 7th team member, enhancing the team with the Data Engineering expertise You will work under the guidance of the Technical Team Lead A lot of sparring opportunities with data professionals from other teams at Adsquare are available In the future we are planning to add further data (analytics) engineering resources to the team Work in a self-organised agile team with high level of autonomy and you will actively shape your teams culture Design, build, and standardise privacy-first big data architectures, large scale data pipelines, and advanced analytics solutions in AWS Develop complex integrations with third-party partners transferring terabytes of data Align with other Data experts at Adsqaure on data (analytics) engineering best practices and standards and introduce those standards and data engineering expertise to the team in order to enhance existing data pipelines and build new ones Successfully partner up with our Product team to constantly develop further and improve our platform features You have 4+ years of experience on similar position You have significant experience with Python. Familiarity with Java or Scala is a plus Hands on experience building scalable solutions in AWS Proficiency in NoSQL and SQL databases and in high-throughput data-related architecture and technologies (e.g. Kafka, Spark, Hadoop, MongoDB, AWS Batch, AWS Glue, Athena, Airflow, dbt) Excellent SQL and data transformation skills Excellent written and verbal communication skills with an ability to simplify complex technical information Experience guiding and mentoring junior team members in a collaborative environment Fully remote work, as well as an option to work from our Berlin office To encourage education and professional growth, we offer an individual yearly budget of 1.200‚Ç¨ Regular team events and company events We equip you with the latest hardware and provide you with all the tools you need to thrive Adsquare is the global audience & location intelligence company making marketing a whole lot smarter. With eight offices around the world we are a truly international company but one united team working towards our vision: empowering companies to accelerate business growth by staying at the forefront of data-driven marketing. We are pioneers in advertising and data. We were born mobile, before it was a necessity, and have pioneered data-driven advertising in display and now DOOH. We have the smartest people, working with the best tech, so we can drive the pace of change. We are visionaries. We see a better way for marketers and are doing everything we can to make that a reality. We believe in innovation and the power of technology. We believe in constantly improving what we‚Äôre doing because the work is never done. We believe in what we do and rise to meet our own high standards. If there‚Äôs a problem, we fix it, humbly and without fuss or delay. Although our technology is best-of-breed, we are unassuming and modest in our approach. We are passionately candid, both with each other and with our clients. We take the time to explain our approach to their problem, confident there is nothing to hide.We are consistent and true to our word. We don‚Äôt take shortcuts but focus on being our clients‚Äô most reliable partner. We make promises - and we keep them. As a result, clients enjoy spending time with us, because we are good people.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,221,ETL Developer,EndySoft,"Position Overview: We are seeking an experienced ETL Developer to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust ETL processes to ensure the efficient flow of data from various sources to data warehouses or data lakes. This role involves collaborating with business and technical teams to support data-driven decision-making and analytics initiatives. MD rate: 16600 - 20000PLN Roles and Responsibilities: Design, develop, and optimize ETL pipelines to extract, transform, and load data from multiple data sources. Collaborate with data architects and business analysts to gather and understand data requirements. Implement and maintain data integration workflows using ETL tools such as Informatica , Talend , SSIS , or Apache NiFi . Perform data validation and quality checks to ensure data integrity and accuracy. Troubleshoot and resolve issues related to ETL processes and data flows. Monitor and enhance the performance of ETL jobs to meet business SLA requirements. Maintain and document technical solutions, including data mappings, workflows, and procedures. Work closely with other data team members to support data warehouse and data lake initiatives. Required Skills and Experience: Proficiency in SQL for querying and transforming data. Hands-on experience with ETL tools such as Informatica , Talend , SSIS , or similar. Strong knowledge of data modeling techniques, including star schema and snowflake schema . Experience in data integration and data warehousing concepts. Familiarity with cloud platforms (e.g., AWS Glue , Azure Data Factory , Google Dataflow ) for ETL processes. Strong problem-solving skills and the ability to troubleshoot complex data issues. Experience with scripting languages such as Python , Shell , or Bash for automation. Excellent communication and collaboration skills to work effectively with cross-functional teams. Nice to Have: Experience with big data tools such as Spark , Kafka , or Hadoop . Knowledge of NoSQL databases like MongoDB or Cassandra . Familiarity with DataOps practices and CI/CD pipelines for ETL workflows. Exposure to data governance and metadata management tools. Understanding of data security and compliance requirements. Experience with version control systems like Git . Exposure to Agile/Scrum methodologies. Additional Information: This role provides an opportunity to work on complex data integration projects and contribute to the development of scalable data solutions. If you are passionate about transforming raw data into actionable insights and thrive in a fast-paced environment, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,222,DevOps Engineer (AI Team),Jit Team,"Salary: 1000 - 1200 PLN/day on B2B Work model: elastic hybrid from Gdynia / Gda≈Ñsk / Warszawa (at least 2 days per week from the office) Why choose this offer? You can expect a flexible work organization The international work environment will give you the opportunity to interact with the English language on a daily basis Scandinavian organizational culture will provide you with work-life balance, you will gain time for additional training (financed by Jit) The Jit community will bring you a nice time during regular integration meetings Project You will be involved in a financial project focused on building and deploying generative and predictive AI models to enhance operations in Financial Crime Prevention. We are currently seeking experienced DevOps Engineer to support the infrastructure, orchestration, and deployment of our AI-powered applications. You‚Äôll be part of a fast-moving team, experimenting with new methods and tools to deliver scalable, production-ready AI solutions. Responsibilities you'll have Design, implement, and manage cloud-based infrastructure for generative AI solutions Develop and maintain orchestration workflows using tools like AWS Step Functions, EventBridge, and Lambda Support data processing pipelines on Glue, EMR, and EKS Collaborate with Data Scientists and Engineers on data transformation using PySpark, Python, and Hadoop ecosystem tools Build and optimize CI/CD pipelines using Jenkins and Terraform Expected competences and knowledge Solid experience with orchestration services such as AWS Step Functions, EventBridge, Managed Workflows for Apache Airflow (MWAA), and AWS Lambda Good knowledge of data processing frameworks and platforms like AWS Glue, EMR, and EKS Hands-on experience working with AWS S3 for data storage and Athena for querying and analysis Proven experience developing Big Data ETL pipelines using PySpark with Python; experience with Spark and Scala is a plus Strong skills in data manipulation and transformation using Python and Pandas Experience working with the Hadoop ecosystem , including tools like Hive, Impala, Sqoop, HDFS, and Oozie Familiarity with CI/CD practices and tools, particularly Jenkins English min. B2 Nice to have: Experience with MLflow and AWS SageMaker for machine learning lifecycle management Familiarity with AWS Bedrock and other generative AI services AWS Certified Cloud Practitioner or higher certification Experience with MLflow and AWS SageMaker for machine learning lifecycle management Familiarity with AWS Bedrock and other generative AI services AWS Certified Cloud Practitioner or higher certification Technologies you'll work with AWS Python Hadoop Jenkins, Terraform Apache Airflow AWS Lambda AWS Glue, EMR, EKS S3, Athena PySpark, Pandas Client ‚Äì why choose this particular client from the Jit portfolio? Jit Team has had an over-decade-long relationship with the leading financial group in the Nordic countries, and we are privileged to be our client's premier partner in Poland. At present, over 200 Jit personnel are engaged in the completion of more than 60 projects for this Norwegian major provider of financial services with a global presence and a strong focus on modern technology. Our customer's work atmosphere is epitomized by the Scandinavian culture , which is conducive to people who place emphasis on work-life balance and feedback culture . Furthermore, all projects are executed in international teams, giving constant exposure to the English language. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 21000, ""max"": 25200, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,Permanent,Hybrid,223,Specjalista / Specjalistka ds. Administracji Bazami Danych,PLAY (P4 Sp. z o.o.),"Administracja bazami danych (instalacja, konfiguracja, optymalizacja, backup, aktualizacje) Reagowanie na awarie i wsp√≥≈Çpraca z serwisami zewnƒôtrznymi Utrzymanie bezpiecze≈Ñstwa i dokumentacji systemowej Koordynacja zmian w ≈õrodowiskach produkcyjnych i testowych Udzia≈Ç w projektach IT ‚Äì analiza wymaga≈Ñ, wdra≈ºanie rozwiƒÖza≈Ñ, wsp√≥≈Çpraca z zespo≈Çami Dzielenie siƒô wiedzƒÖ i wsparcie merytoryczne dla innych dzia≈Ç√≥w Umowa o pracƒô Podstawa premii rocznej w wysoko≈õci 20% wynagrodzenia Praca hybrydowa (office + home office) R√≥≈ºnorodny rozw√≥j: szkolenia, projekty, rekrutacje wewnƒôtrzne Telefon i internet Play wraz z dodatkowymi us≈Çugami ( m.in . telewizja, nawigacja) oraz zni≈ºki pracownicze Opiekƒô medycznƒÖ wsp√≥≈ÇfinansowanƒÖ przez pracodawcƒô Ubezpieczenie na ≈ºycie w pe≈Çni finansowane przez pracodawcƒô Szeroki wyb√≥r benefit√≥w na platformie kafeteryjnej Dodatkowy benefit ≈õwiƒÖteczny Dodatkowy dzie≈Ñ urlopu na profilaktykƒô zdrowotnƒÖ",[],Unclassified,Unclassified
Full-time,Senior,B2B,Remote,224,Python Developer with AI,Pretius,"At Pretius we are looking for Python Developer with AI to a project in the financial industry, international team. Location: remote Salary: 150-210 pln netto/h Project / Role Build and validate intelligent assistants with cutting-edge LLMs (e.g., GPT-4, Falcon 2, LLAMA 3, Mixtral) using techniques like RAG and agent frameworks (e.g., Langraph, CrewAI) Recommend technical approaches and architectures for business challenges Implement efficient Python-based data pipelines for AI model training and deployment Communicate insights to both technical and non-technical audiences Contribute to project documentation Requirements 4+ years of relevant experience Proficiency in Python and LLM frameworks (e.g., Langchain) Experience with transformer-based models and large datasets (e.g. Pandas, NumPy, SQL) Strong knowledge of ML/AI concepts: types of algorithms, machine learning frameworks,model efficiency metrics, model life-cycle, AI architectures Experience with data engineering, including preprocessing, transformation, and pipeline automation Fluent in English (C1) Nice to have: Familiarity with cloud-based ML services (AWS, Azure, GCP) What we offer? We focus on long-term relationships based on fair principles and reliability Co-financing of the Multisport card and Medicover private healthcare Modern office available Team bonding activities, internal courses, conferences, certifications","[{""min"": 25200, ""max"": 35280, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,B2B,Remote,225,Medium Data Engineer,co.brick sp. z o.o.,"Nasz klient to wiodƒÖca spo≈Çeczno≈õƒá i marketplace dla bran≈ºy Human Resources, skupiajƒÖcy siƒô na technologiach przysz≈Ço≈õci pracy (Future of Work). Dzia≈Ça na rynkach europejskich i ameryka≈Ñskich, oferujƒÖc aktualno≈õci, analizy i trendy rynkowe dla lider√≥w organizacji na ca≈Çym ≈õwiecie. Siedziba firmy znajduje siƒô w Londynie. Oferujemy üìç Praca 100% zdalna | üåç Klient z siedzibƒÖ w Londynieüí∞ Kontrakt B2BüìÖ Start: ASAP lub od po≈Çowy lipca‚è≥ Wsp√≥≈Çpraca d≈Çugoterminowa ObowiƒÖzki ≈öcis≈Ça wsp√≥≈Çpraca z dzia≈Çami firmy w celu zrozumienia potrzeb analitycznych i raportowych ≈öcis≈Ça wsp√≥≈Çpraca z dzia≈Çami firmy w celu zrozumienia potrzeb analitycznych i raportowych Budowa Single User View oraz raportowania opartego na zdarzeniach w czasie rzeczywistym Budowa Single User View oraz raportowania opartego na zdarzeniach w czasie rzeczywistym Tworzenie analiz i raport√≥w wspierajƒÖcych r√≥≈ºne dzia≈Çy organizacji Tworzenie analiz i raport√≥w wspierajƒÖcych r√≥≈ºne dzia≈Çy organizacji Automatyzacja raportowania dla CEO w oparciu o dane z wielu platform Automatyzacja raportowania dla CEO w oparciu o dane z wielu platform Generowanie w≈Çasnych pomys≈Ç√≥w i koncepcji analitycznych Generowanie w≈Çasnych pomys≈Ç√≥w i koncepcji analitycznych Zapewnienie jak najbli≈ºszego real-time raportowania Zapewnienie jak najbli≈ºszego real-time raportowania Raportowanie postƒôp√≥w prac w narzƒôdziu Monday.com Raportowanie postƒôp√≥w prac w narzƒôdziu Monday.com Wymagania ‚úÖ Minimum 2 lata do≈õwiadczenia w: integracji wielu ≈∫r√≥de≈Ç danych w centralnym data lake integracji wielu ≈∫r√≥de≈Ç danych w centralnym data lake pracy z r√≥≈ºnorodnymi API i konektorami do platform pracy z r√≥≈ºnorodnymi API i konektorami do platform tworzeniu dynamicznych dashboard√≥w raportowych tworzeniu dynamicznych dashboard√≥w raportowych ‚úÖ Znajomo≈õƒá narzƒôdzi: Python Python BigQuery BigQuery Looker (lub inne narzƒôdzia BI/dashboards) Looker (lub inne narzƒôdzia BI/dashboards) GCP (lub inne rozwiƒÖzania chmurowe) GCP (lub inne rozwiƒÖzania chmurowe) Fivetran, Zapier/Make Fivetran, Zapier/Make ‚úÖ Umiejƒôtno≈õƒá wsp√≥≈Çpracy w zespo≈Çach miƒôdzydzia≈Çowych‚úÖ Samodzielno≈õƒá i inicjatywa w tworzeniu rozwiƒÖza≈Ñ",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,226,Analityk SQL/ Analityczka SQL,Santander Consumer Bank,"SporzƒÖdzanie, projektowanie i tworzenie raport√≥w korporacyjnych oraz analiz na potrzeby controllingowe przy u≈ºyciu narzƒôdzi informatycznych (m. in. EXCEL, VBA, PL/SQL / SQL, PBI), Opracowywanie algorytm√≥w, modeli i narzƒôdzi niezbƒôdnych do prawid≈Çowego prognozowania i wyliczenia warto≈õci finansowych Przeprowadzanie i weryfikacja analiz rentowo≈õci oraz wyniku na oferowanych produktach Udzia≈Ç w procesie bud≈ºetowania i prognozowania wynik√≥w finansowych Sp√≥≈Çki w okresach kr√≥tko i d≈Çugoterminowych, Udzia≈Ç w projektach( w tym: przygotowywanie wymaga≈Ñ, przypadk√≥w testowych oraz udzia≈Ç we wdro≈ºeniach nowych rozwiƒÖza≈Ñ) Bardzo dobra znajomo≈õƒá jƒôzyka PL/SQL; SQL Bardzo dobra znajomo≈õƒá pakietu MS Office (ze szczeg√≥lnym uwzglƒôdnieniem MS Excel) Minimum 3 letnie do≈õwiadczenie w obszarze controllingu, prognozowania, sprawozdawczo≈õci zarzƒÖdczej, analizie danych Wykszta≈Çcenie wy≈ºsze (preferowane kierunki o profilu ekonomiczno-finansowym) Umiejƒôtno≈õƒá pracy pod presjƒÖ czasu i zarzƒÖdzania swoimi zadaniami Sumienno≈õƒá i dok≈Çadno≈õƒá w realizacji zada≈Ñ Wysokie zdolno≈õci interpersonalne i komunikacyjne oraz umiejƒôtno≈õƒá pracy zespo≈Çowej Znajomo≈õƒá jƒôzyka angielskiego na poziomie B1 Znajomo≈õƒá jƒôzyka polskiego na poziomie native Znajomo≈õƒá Power BI/Power Query Znajomo≈õƒá VBA Do≈õwiadczenie w bankowo≈õci lub sp√≥≈Çce leasingowej Podstawowa znajomo≈õƒá zasad rachunkowo≈õci",[],Database Administration,Database Administration
Full-time,Mid,Permanent,Hybrid,227,"Analityk Hurtowni Danych - SQL, Azure",Upvanta sp. z o.o.,"Poszukujemy Analityka Hurtowni Danych (DWH), kt√≥ry bƒôdzie wspiera≈Ç rozw√≥j, utrzymanie i optymalizacjƒô hurtowni danych wykorzystywanej w analizach biznesowych na du≈ºƒÖ skalƒô. Je≈õli masz do≈õwiadczenie z SQL, Azure i Power BI, swobodnie poruszasz siƒô w ≈õwiecie modelowania danych, a jednocze≈õnie potrafisz ≈ÇƒÖczyƒá potrzeby techniczne z biznesowymi ‚Äì ta rola mo≈ºe byƒá w≈Ça≈õnie dla Ciebie. Miejsce pracy: Wroc≈Çaw (praca hybrydowa ‚Äì wymagane sta≈Çe miejsce zamieszkania we Wroc≈Çawiu lub w bliskiej okolicy) Zakres obowiƒÖzk√≥w: Analiza biznesowo-systemowa w kontek≈õcie hurtowni danych oraz projektowanie i implementacja struktur hurtowni. Analiza biznesowo-systemowa w kontek≈õcie hurtowni danych oraz projektowanie i implementacja struktur hurtowni. Modelowanie danych i tworzenie oraz interpretacja modeli ERD. Modelowanie danych i tworzenie oraz interpretacja modeli ERD. Optymalizacja zapyta≈Ñ SQL oraz analiza du≈ºych wolumen√≥w danych (setki milion√≥w transakcji). Optymalizacja zapyta≈Ñ SQL oraz analiza du≈ºych wolumen√≥w danych (setki milion√≥w transakcji). Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi i biznesowymi w celu dostosowania rozwiƒÖza≈Ñ do wymaga≈Ñ klienta. Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi i biznesowymi w celu dostosowania rozwiƒÖza≈Ñ do wymaga≈Ñ klienta. Tworzenie i wdra≈ºanie po≈ÇƒÖcze≈Ñ miƒôdzy hurtowniƒÖ danych a ≈∫r√≥d≈Çami danych, takimi jak sFTP, Azure DB, onPremDB, QuickBase oraz Power BI. Tworzenie i wdra≈ºanie po≈ÇƒÖcze≈Ñ miƒôdzy hurtowniƒÖ danych a ≈∫r√≥d≈Çami danych, takimi jak sFTP, Azure DB, onPremDB, QuickBase oraz Power BI. Utrzymanie hurtowni danych i procedur automatyzacji, diagnozowanie problem√≥w wydajno≈õciowych oraz naprawa b≈Çƒôd√≥w w ≈õrodowisku (np. Azure Data Factory, Azure SQL Server, maszyny wirtualne). Utrzymanie hurtowni danych i procedur automatyzacji, diagnozowanie problem√≥w wydajno≈õciowych oraz naprawa b≈Çƒôd√≥w w ≈õrodowisku (np. Azure Data Factory, Azure SQL Server, maszyny wirtualne). Wykorzystanie system√≥w kontroli wersji (Git) do zarzƒÖdzania kodem oraz wsp√≥≈Çpracy z zespo≈Çami. Wykorzystanie system√≥w kontroli wersji (Git) do zarzƒÖdzania kodem oraz wsp√≥≈Çpracy z zespo≈Çami. Tworzenie dokumentacji technicznej i analitycznej zwiƒÖzanej z implementacjƒÖ i utrzymaniem hurtowni danych. Tworzenie dokumentacji technicznej i analitycznej zwiƒÖzanej z implementacjƒÖ i utrzymaniem hurtowni danych. Dodawanie procedur sk≈Çadowanych, tabel, trigger√≥w (T-SQL, Git, Visual Studio) oraz wdra≈ºanie zmian w ≈õrodowiskach produkcyjnych. Dodawanie procedur sk≈Çadowanych, tabel, trigger√≥w (T-SQL, Git, Visual Studio) oraz wdra≈ºanie zmian w ≈õrodowiskach produkcyjnych. Wymagania: Wykszta≈Çcenie wy≈ºsze z zakresu informatyki lub pokrewne. Wykszta≈Çcenie wy≈ºsze z zakresu informatyki lub pokrewne. Minimum 2 lata do≈õwiadczenia w implementacji i zarzƒÖdzaniu hurtowniami danych. Minimum 2 lata do≈õwiadczenia w implementacji i zarzƒÖdzaniu hurtowniami danych. Bardzo dobra znajomo≈õƒá jƒôzyka SQL (T-SQL, PostgreSQL) na poziomie ≈õredniozaawansowanym. Bardzo dobra znajomo≈õƒá jƒôzyka SQL (T-SQL, PostgreSQL) na poziomie ≈õredniozaawansowanym. Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych (setki milion√≥w transakcji). Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych (setki milion√≥w transakcji). Umiejƒôtno≈õƒá modelowania danych oraz tworzenia modeli ERD. Umiejƒôtno≈õƒá modelowania danych oraz tworzenia modeli ERD. Zdolno≈õci analityczne oraz umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w w kontek≈õcie hurtowni danych. Zdolno≈õci analityczne oraz umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w w kontek≈õcie hurtowni danych. Znajomo≈õƒá systemu kontroli wersji (Git) i narzƒôdzi do zarzƒÖdzania kodem. Znajomo≈õƒá systemu kontroli wersji (Git) i narzƒôdzi do zarzƒÖdzania kodem. Praktyczna znajomo≈õƒá Microsoft Azure oraz Power BI. Praktyczna znajomo≈õƒá Microsoft Azure oraz Power BI. Mile widziane: Znajomo≈õƒá Oracle. Znajomo≈õƒá Oracle. Do≈õwiadczenie w modelowaniu i implementacji hurtowni danych w metodologii Data Vault. Do≈õwiadczenie w modelowaniu i implementacji hurtowni danych w metodologii Data Vault. Znajomo≈õƒá narzƒôdzi do automatyzacji implementacji hurtowni danych (np. Wherescape). Znajomo≈õƒá narzƒôdzi do automatyzacji implementacji hurtowni danych (np. Wherescape). Znajomo≈õƒá technologii SSAS Tabular. Znajomo≈õƒá technologii SSAS Tabular. Znajomo≈õƒá Python. Znajomo≈õƒá Python. Brzmi interesujƒÖco?Je≈õli chcesz pracowaƒá z du≈ºymi zbiorami danych, mieƒá realny wp≈Çyw na architekturƒô hurtowni i rozwijaƒá siƒô w ≈õrodowisku opartym na nowoczesnych technologiach, czekamy na TwojƒÖ aplikacjƒô.",[],Database Administration,Database Administration
Full-time,Mid,Permanent,Hybrid,228,Data Controls Design Manager,HSBC Service Delivery,"If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity The Data Controls Design Manager jobholder will be responsible for designing and developing innovative, user-centric solutions, processes and frameworks for Data Storage and Records Management (DSTO) controls for Retention Schedules, Retention and Disposal and Legal Holds Management. ‚¶Å Design and develop overall processes related to Data Storage and Records Management (DSTO) controls for Retention Schedules, Retention and Disposal and Legal Holds Management. ‚¶Å Design and develop the frameworks for structured and unstructured records, retention schedules and legal holds, including Mandatory Procedures, Operating Instructions, Technical Implementation Instructions, process flows and any other required documentation. ‚¶Å Work with various stakeholders, including Regional, Country, Business and Infrastructure teams representatives, Product Owner, Change the Bank teams and vendors to enhance and/or create tooling required to effectively manage records and data within the bank, provide tracking and introduce automated processes. ‚¶Å Develop instructions and guidance and provide support and training to stakeholders as required. ‚¶Å Maintain the design of Retention Schedule KCIs. Introduce amendments to the KCIs as required. ‚¶Å Drive all activities related to MSIIs, as required, including drafting new issues and completing closure packs. ‚¶Å Knowledge of and working experience with Information Lifecycle Management, Records Management and Data Privacy, including tooling solutions. Ability to understand complex and abstract ideas and technical solutions. ‚¶Å Experience with Risks and Controls, including Helios. ‚¶Å Experience with developing Control documentation, such as policies, procedures, instructions, guidance, documents etc. ‚¶Å Previous project and change management experience, including project planning and execution. ‚¶Å Problem-solving skills and design thinking attitude. ‚¶Å Excellent collaboration and communication skills; ability to articulate design work to all levels of stakeholders. ‚¶Å Self-directed person with a strong sense of ownership and an ability to work independently. ‚¶Å Competitive salary ‚¶Å Annual performance-based bonus ‚¶Å Additional bonuses for recognition awards ‚¶Å Multisport card ‚¶Å Private medical care ‚¶Å Life insurance ‚¶Å One-time reimbursement of home office set-up (up to 800 PLN). ‚¶Å Corporate parties & events ‚¶Å CSR initiatives ‚¶Å Nursery discounts ‚¶Å Financial support with trainings and education ‚¶Å Social fund ‚¶Å Flexible working hours ‚¶Å Free parking ‚¶Å Online assessment ‚¶Å Telephone screen ‚¶Å Interview with the hiring manager We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Unclassified,Unclassified
Full-time,Mid,B2B,Hybrid,229,Data Engineer,emagine Polska,"Project information: Industry : Insurance and IT services Rate: up to 160 z≈Ç/h netto + VAT Location : Warsaw (first 2-3 months of office visits once a week, then occasionally) Project language : Polish, English Summary The Data Engineer will be responsible for designing, building, and maintaining Data Hubs that integrate multiple data sources for efficient analytics and operational purposes, with a focus on real-time data processing. Main Responsibilities Data Hub Development ‚Äì Design and implement scalable Data Hubs to support enterprise-wide data needs. Data Pipeline Engineering ‚Äì Build and optimize ETL/ELT pipelines for efficient data ingestion, transformation, and storage. Logical Data Modeling ‚Äì Structure Data Hubs to ensure efficient access patterns and support diverse use cases. Real-Time Analytics ‚Äì Enable real-time data ingestion and updating models. Data Quality & Monitoring ‚Äì Develop monitoring features to ensure high data reliability. Performance Optimization ‚Äì Optimize data processing for large-scale datasets. Automation & CI/CD ‚Äì Implement CI/CD pipelines for automating data workflows. Collaboration ‚Äì Align data solutions with enterprise needs through teamwork. Monitoring & Maintenance ‚Äì Continuously improve data infrastructure reliability. Agile Practices ‚Äì Participate in Scrum/Agile methodologies. Documentation ‚Äì Create and maintain clear documentation for data models and pipelines. Key Requirements Strong Python skills (or other relevant language) Experience with Azure Data Factory, ADLS, and Azure SQL Hands-on experience in building ETL/ELT pipelines Experience with real-time data processing Understanding of data preparation for AI/ML applications Experience in building data validation and monitoring features Proficiency in SQL for data transformation Familiarity with CI/CD and infrastructure-as-code principles Understanding data security and compliance best practices Proficient in English (B2 level minimum) Nice to Have Data Governance knowledge Experience with containerization technologies (Docker, Kubernetes/AKS) Agile collaboration experience Ability to produce high-quality technical documentation","[{""min"": 140, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,230,Lead Data Scientist,Bayer Sp. z o.o.,"Lead Data Scientist For Digital Hub Warsaw, we are looking for: Lead Data Scientist Are you ready to make a significant impact in the world of data science and AI agents? We are seeking a talented Lead Data Scientist to become a vital part of our dynamic Data Assets, Analytics and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in implementing global, cutting-edge AI solutions across commercial, product supply, marketing, and R&D domains. Our diverse, international team, spanning Poland and Germany, is dedicated to managing the entire product life cycle ‚Äì from proof of concept to the seamless operation of fully industrialized products. We pride ourselves on delivering innovative solutions that leverage traditional Machine Learning, Generative AI, and Mathematical Optimization, all powered by a modern tech stack featuring Python, Azure and Databricks. If you‚Äôre passionate about driving data-driven decision-making and AI driven process automation and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Creates, employs, evaluates, optimizes, and maintains generative AI, machine learning, time series forecasting, mathematical optimization, simulation, and NLP models and workflows that deliver actionable insights and meet business objectives. Participates in the development, delivery, and maintenance of Agentic AI Platform by implementing and proposing new features, designing evaluation experiments, and researching the latest advancements in the GenAI field. Extracts valuable information from large structured and unstructured datasets, identifying key variables and metrics that influence consumer behavior, commercial activities, product supply, R&D and RMSQC. Builds prototypes to prove modeling concepts. Industrializes and scales up successful advanced analytics prototypes into IT products. Creates intuitive and interactive data visualizations, reporting tools and dashboards to communicate complex analytical findings to technical and non-technical stakeholders as well as enable scenario creation and management. Develops generalized frameworks and libraries for repetitive data science activities. Leads exploratory projects that investigate new data sources, tools, and analytical techniques, keeping Bayer at the forefront of data science in the consumer health sector. Builds data processing pipelines, analyzes data used for modelling and performs other data related activities if required. Manages code in Github repositories and performs peer code reviews. Fosters a culture of continuous learning within the data science team, taking active part in workshops and training sessions to share knowledge and skills. Presents compelling data driven stories to all levels of the organization, including peers, senior management, and internal customers to drive both strategic and operational changes in the business. Acts as a subject matter expert in data science, advising on best practices and emerging technologies that can enhance Bayer Consumer Health‚Äôs data science capabilities. Develops and train other members of the Data Science team. Qualifications & Competencies (education, skills, experience): Master‚Äôs or PhD degree with 8+ years of experience in Data Science or related fields. Proven educational background or applied experience in at least one of the following: Machine Learning, Statistics, Mathematics, Computer Science, Quantitative Finance/Economics/Marketing, Biostatistics, Bioinformatics, or other related quantitative disciplines. Expert proficiency and practical experience in machine learning, generative AI, forecasting, and mathematical optimization. Expert knowledge of data science tools, libraries, frameworks, and platforms: Python, SQL, Vector Stores, Spark, Azure, Databricks, Github, LangChain, RAG, prompt and context engineering. Ability to write production grade code using object-oriented programming paradigm. Proven track record of developing advanced analytics products within a cloud environment and delivering valuable analysis through the application of domain and business knowledge. Ability to create architecture for advanced analytics products. Problem solving and analytical skills. Interpersonal and communication skills, active listening, consulting, challenges, presentation skills. Leadership, strategic and design thinking. Fluent in English, both written & spoken. What do We offer: A flexible, hybrid work model. Great workplace in a new modern office in Warsaw. Career development, 360¬∞ Feedback & Mentoring programm. Wide access to professional development tools, trainings, & conferences. Company Bonus & Reward Structure. VIP Medical Care Package (including Dental & Mental health). Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù). Life & Travel Insurance. Pension plan. Co-financed sport card - FitProfit. Meals Subsidy in Office. Additional days off. Budget for Home Office Setup & Maintenance. Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs. Tailored-made support in relocation to Warsaw when needed.","[{""min"": 26000, ""max"": 34000, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Senior,Permanent or B2B,Hybrid,231,CRM Business Analyst,Multi Parts Poland Sp. z o.o.,"Multi Parts ‚Äì since 1988, one of the Aftermarket‚Äôs leading manufacturers of replacement parts for passenger and commercial vehicles. Combining a deep understanding of the common failures impacting modern vehicles with expert engineering and manufacturing capabilities, Multi Parts offers customers a unique turn-key product solutions. Multi Parts has locations in the USA (HQ), China, Vietnam and Europe (Poland). This position will report to the Vice President of Business System and participate in the responsibilities to support, maintain, and improve the Business Systems and Processes for MULTI PARTS and related companies. Primary Duties Respond to internal information requests with thoughtful, concise, and accurate reports, data, and analysis using corporate provided tools such as Dynamics CRM, SSMS, SSRS, and MS Power BI. Perform internal data analysis tasks to understand trends, patterns, and insights as directed by management and timely collect and record KPI data. Assist with the implementation, documentation, and training of processes changes within the organization. Provide second-line support to internal users for the primary business ERP Systems (Exact Globe and MS Dynamics), EDI, Shipping, Business Intelligence, and other critical business software systems. Use the company ticketing and work tracking system to record and track work activities. Actively protect the data integrity and confidentiality of all MPS data and adhere to corporate information protection practices including access controls, data security, confidentiality, and anonymity. Minimum Requirements 3-5 years experience working as a Business Analyst or related position Associates degree in a technical field of study or a high school diploma (or equivalent diploma) plus related work experience Certification is at least one technical area such as CompTIA, Microsoft, Amazon, or other qualified certification agency in a related area High level proficiency in using Excel to transform and model data. Experience using SQL or R to create reports in SQL Server Reporting Services, Power Bi, or equivalent General experience in ERP systems, business operations, and business processes. Strong written communication skills for creating clear process documents, reports, analysis, and presentations. We offer: Competitive salary Professional Training at employer‚Äôs cost Medical Care MultiSport card Lunch card Teambuilding events It is the expectation that all employees of MULTI PARTS to act in good faith and in the Company‚Äôs best interest while performing their duties. This job description in no way states or implies that these are the only duties to be performed by this employee. This position is required to follow all other instructions and other related duties as assigned by their manager and Executive Management. MULTI PARTS reserves the right to update, revise or change this job description and related duties at any time.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Hybrid,232,Data Engineer,Alior Bank SA,"Data Engineer Mamy dla Ciebie pracƒô, kt√≥ra polega na: administracji biznesowej platformy przetwarzania danych , budowie i rozwoju modelu danych na platformie danych, zarzƒÖdzaniu procesami przetwarzania danych na platformie danych, zarzƒÖdzaniu u≈ºytkownikami i ich zasobami na platformie danych, optymalizacji zasob√≥w platformy danych, w tym zarzƒÖdzanie regu≈Çami ograniczajƒÖcymi koszty przetwarza≈Ñ, projektowaniu i utrzymaniu Data Mart√≥w na platformie danych, obs≈Çudze incydent√≥w zwiƒÖzanych z zasilaniem platformy danych , wsparciu u≈ºytkownik√≥w platformy danych. Je≈õli jeste≈õ osobƒÖ, kt√≥ra: posiada znajomo≈õƒá szeroko pojƒôtej tematyki Hurtowni Danych, Big Data i BI, bardzo dobrze zna SQL, zna i stosuje pryncypia architektoniczne w zarzƒÖdzaniu ≈õrodowiskiem przetwarzania danych, umie definiowaƒá wymagania do system√≥w w oparciu o potrzeby u≈ºytkownik√≥w, potrafi zarzƒÖdzaƒá komunikacjƒÖ z u≈ºytkownikami, jest samodzielna i otwarta na rozw√≥j. Dodatkowe atuty: ma do≈õwiadczenie w pracy z Teradata, ma do≈õwiadczenie w pracy z przynajmniej jednym z wiodƒÖcych system√≥w chmurowego przetwarzania danych (np. Snowflake, DataBricks) , rozumie wyzwania proces√≥w ML / AI, pos≈Çuguje siƒô Python, pracowa≈Ça w instytucji finansowej Oferujemy: umowƒô o pracƒô pracƒô hybrydowƒÖ pakiet benefit√≥w m.in. kafeteria, prywatna opieka medyczna z pakietem stomatologicznym, karta Multisport, ubezpieczenie, konkursy wewnƒôtrzne, programy zni≈ºkowe i lojalno≈õciowe 16 godzin na wolontariat ‚ÄûDzie≈Ñ na U‚Äù pracƒô w oparciu o warto≈õci wsp√≥lnie wypracowane przez Aliorowc√≥w ‚Äì odpowiedzialno≈õƒá, otwarto≈õƒá, innowacyjno≈õƒá, zorientowanie na klienta, Alior Uniwersytet ‚Äì przestrze≈Ñ do rozwoju kompetencji zawodowych oraz personalnych nagrodƒô pieniƒô≈ºnƒÖ za skuteczne polecenie w ramach Programu Polece≈Ñ.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,233,Specjalista ds. MS SQL,Bank Pocztowy,"O nas Bank Pocztowy ≈õwiadczy bezpieczne i proste us≈Çugi finansowe w plac√≥wkach w≈Çasnej sieci, a tak≈ºe w Urzƒôdach Pocztowych na terenie ca≈Çego kraju. Specjalizujemy siƒô w najprostszych produktach bankowych oferowanych w polskich z≈Çotych dla Klient√≥w detalicznych, a tak≈ºe Klient√≥w instytucjonalnych. Prowadzimy r√≥wnie≈º dzia≈Çalno≈õƒá w segmencie rozliczeniowym i skarbowym. Kluczowa jest dla nas atmosfera pracy, sprzyjajƒÖca rozwijaniu nowych inicjatyw i poszerzaniu wiedzy. Jeste≈õmy nastawieni na osiƒÖgniƒôcie cel√≥w biznesowych, kt√≥re sƒÖ dla nas najwa≈ºniejsze. Wierzymy, ≈ºe dobry zesp√≥≈Ç zrealizuje nawet najbardziej wymagajƒÖce projekty. Mo≈ºesz siƒô o tym przekonaƒá, gdy do≈ÇƒÖczysz do naszej firmy. Specjalista ds. MS SQL Tw√≥j zakres obowiƒÖzk√≥w Administracja bazami danych MS SQL oraz serwerami bazodanowymi Windows Monitoring, analiza wydajno≈õci, aktualizacja statystyk, reorganizacja indeks√≥w ZarzƒÖdzanie uprawnieniami, konfiguracja zewnƒôtrznych ≈∫r√≥de≈Ç danych (linked server) Prace rozwojowe i utrzymanie dokumentacji w ramach zarzƒÖdzanego ≈õrodowiska Nasze wymagania Dobra znajomo≈õƒá rozwiƒÖza≈Ñ Microsoft oraz rozwiƒÖza≈Ñ bazodanowych MS SQL Zdolno≈õci organizacyjne i analityczne Komunikatywno≈õƒá i umiejƒôtno≈õƒá wsp√≥≈Çpracy w zespole Odporno≈õƒá na stres oraz umiejƒôtno≈õƒá pracy pod presjƒÖ czasu Umiejƒôtno≈õƒá szybkiego uczenia siƒô i poznawania nowych technologii teleinformatycznych Znajomo≈õƒá jƒôzyka angielskiego na poziomie umo≈ºliwiajƒÖcym pos≈Çugiwanie siƒô dokumentacjƒÖ Mile widziane Znajomo≈õƒá system√≥w monitorowania: Redgate, DBplus itp. Znajomo≈õƒá podstawowych us≈Çug Windows AD Znajomo≈õƒá technologii klastrowych Microsoft Znajomo≈õƒá technologii chmurowych (AWS, Azure) Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z bezpiecze≈Ñstwem IT To oferujemy Realizacjƒô ciekawych projekt√≥w i dostƒôp do nowoczesnych technologii Pracƒô w instytucji finansowej z przyjaznym ≈õrodowiskiem pracy Pracƒô hybrydowƒÖ w naszym biurze w Bydgoszczy Stabilne warunki zatrudnienia w ramach umowy o pracƒô R√≥wnowagƒô pomiƒôdzy pracƒÖ a ≈ºyciem prywatnym Pakiet benefit√≥w",[],Database Administration,Database Administration
Full-time,Mid,B2B,Remote,234,Data Engineer / Scientist,Kyotu Technology,"üîç Data Engineer / Scientist (Mid) Location: Wroc≈Çaw / Warszawa / remote from anywhere in Poland Contract: B2B / Employment contract Capacity: Full-time You know that before a fancy dashboard or predictive model happens, someone needs to build a solid pipeline, clean up messy datasets from three continents, and deal with a file called final_final_REALLY_final.csv ? That‚Äôs exactly the kind of challenge we‚Äôre talking about. Kyotu Technology is a boutique software house based in Wroc≈Çaw and Warsaw , working fully remotely or in hybrid mode from anywhere in Poland. We partner with companies from Germany, Switzerland, Western Europe, the United States, and the Middle East , focusing on complex, production-grade systems ‚Äî the kind where code moves real money, not just pixels. Our teams are highly experienced and independent. We build things from scratch, with strong ownership and zero hand-holding. We‚Äôre looking for someone who‚Äôs comfortable in the world of data: capable of building and maintaining data pipelines, integrating different sources, preparing clean and structured datasets ‚Äî and occasionally diving into analysis or experimentation. You don‚Äôt need to know every buzzword. But you should be confident with: Building and maintaining ETL/ELT pipelines Using SQL and Python to automate and process data Working with tools like dbt, Airflow, Pandas, Jupyter Understanding data warehouse/lake architectures (Snowflake, BigQuery, Redshift, etc.) Making sense of data and describing it clearly for others Experience with machine learning (e.g., scikit-learn, feature engineering, prompt tuning) is a plus, but not required. Curious minds welcome. Build and optimize pipelines that pull data from APIs, SQL databases, files ‚Äî and, yes, occasionally weird Excel sheets Validate and clean incoming data (because no one wants to base decisions on fairytales) Collaborate with AI/ML teams to provide high-quality, structured inputs Occasionally explore, visualize, or summarize data to support product or business goals You won‚Äôt be working in a vacuum. You‚Äôll have a team, proper code reviews, QA, DevOps support, and PMs who understand tech. We work in agile-ish cycles. If something works ‚Äî we keep it. If it doesn‚Äôt ‚Äî we fix it. English matters ‚Äî most projects are international and require written and spoken communication with clients. Hourly rate: 120‚Äì170 PLN net/hour (B2B) ‚Äî negotiable depending on experience Flexible working hours and full remote freedom ‚Äî or drop by our Wroc≈Çaw or Warsaw offices if you like Engaging, end-to-end projects ‚Äî not just ticket-pushing Autonomy and influence ‚Äî if something doesn‚Äôt make sense, you‚Äôll help change it","[{""min"": 20160, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,Permanent,Hybrid,235,Business Intelligence Analyst,Tylko,"We are looking for a Business Intelligence Analyst that will support our company in making better data-driven decisions. In this role, you will partner up with our Operations and Product departments, analysing a broad scope of data from our Product Portfolio sales, through production times, delivery KPI‚Äôs, as well as quality and customer complaints. About the Team Analysts are key members of the Finance Department, contributing significantly to decision-making and strategic planning. We are the proud owners of our data warehouse and all dashboards used in our company. Once you join our Team you will be responsible for: Being a business partner, supporting a data driven decision-making process Preparing reports, analyses, and business recommendations based on data from analytical tools (BigQuery) Data visualization and creating dashboards in Looker Studio Recommending optimization actions related to product portfolio, logistics, quality, and customer service Debugging tracking errors to ensure the proper functioning of analytical tools Why Tylko? Being a part of the Tylko team goes beyond clocking in every day and keeping your head down. We celebrate inclusivity and work together to build a culture of happy, healthy employees who are truly heard, and provide the following great benefits for every single member: One bonus day off every month Comprehensive healthcare Multisport A discount on all Tylko products And of course, a dog-friendly office! Job requirements +5 years of experience in a similar position in a physical-product business , i.e. retail e-commerce, or FMCG Experience in designing reports and dashboards (preferably in Looker Studio or PowerBI, Qlik, Tableau, etc.) Good knowledge of SQL and experience working with structured and unstructured databases Good communication skills, responsibility, and a proactive approach to task execution Fluent Polish and good command of English Working from our Warsaw office 3 days a week Ôªø It would be an additional plus if you know: Google Stack Tools such as: Google Tag Manager, Google Analytics/GA4, Google Firebase, Google Sheets, Google Apps Script, Google Cloud Platform (BigQuery, Cloud Functions, Pub/Sub) how to apply econometric principles in practice Python or R how to coordinate a project",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,236,Senior Backend Engineer (with Data Science Skills) at PeakData,PeakData,"PeakData provides AI-powered market intelligence to optimize drug launch execution and resource allocation for pharmaceutical companies. Our platform delivers actionable insights on healthcare professionals (HCPs) and healthcare organizations (HCOs), empowering commercial leaders with real-time, data-driven decision-making. We're looking for a Senior Engineer with strong data science capabilities to join our Data Platform team. In this role, you‚Äôll design and build cloud-native data solutions that support large-scale processing, analytics, and AI-powered automation across our platform. This is a hands-on, senior-level role. You will be expected to work independently, own end-to-end pipelines and infrastructure, and drive initiatives forward both individually and within the team. You should have a strong foundation in Python, SQL, AWS , and/or GCP , with experience using or integrating LLMs into data workflows. You‚Äôll work with and expand upon: Python for data pipelines and automation SQL (PostgreSQL) for transformation and analytics AWS (S3, Glue, Lambda, ECS, Bedrock) as primary cloud environment GCP (Vertex AI) for select workloads and integrations Medallion architecture with RAW/CLEANED/CURATED layers LLM integrations for automation, enrichment, and insight generation Data quality frameworks and orchestration tools (e.g., Argo) Design, implement, and maintain scalable and efficient data pipelines across AWS and GCP Build data products and services supporting both internal analytics and client-facing insights Own ETL/ELT workflows from ingestion to curation Implement observability and alerting for pipeline health and data integrity Integrate LLMs into workflows to support enrichment, automation, or intelligent data handling Act as a technical lead for data engineering projects, driving execution independently Collaborate cross-functionally with Data Science, Product, and Engineering teams Contribute to architectural decisions and long-term data platform evolution Champion best practices for performance, security, and scalability Apply data science techniques where appropriate (e.g., clustering, statistical inference) Prototype and validate LLM-powered solutions using tools like AWS Bedrock or Vertex AI Use prompt engineering and evaluation frameworks to fine-tune LLM interactions Help bridge engineering and AI innovation across the platform 6+ years of experience in data engineering or back-end systems with data-heavy workloads Strong hands-on skills with Python and SQL Deep understanding of AWS cloud data tooling (S3, Lambda, Glue, Step Functions, etc.) Working experience with GCP services, especially BigQuery and Vertex AI Exposure to LLMs and how they integrate into data workflows Experience building data pipelines at scale with monitoring and alerting Ability to work independently and take ownership of technical topics Experience with Argo, Airflow or similar orchestration frameworks Familiarity with IaC tools (Terraform) for deploying infrastructure Experience with data quality monitoring , validation frameworks, or anomaly detection Previous work in healthcare, life sciences, or regulated data environments Proactive : You take initiative and don‚Äôt wait for tasks to be assigned Autonomous : You can own projects from design to production with minimal oversight Curious : You explore new approaches (especially LLMs/AI) and bring them to the table Collaborative : You work well with cross-functional teams Customer-aware : You understand the real-world impact of your pipelines and models Purpose-driven work: support pharmaceutical innovation and better patient outcomes Ownership: real autonomy in shaping our data systems and how they scale Innovation: work on LLM integration and next-gen data workflows A collaborative, fast-moving environment Competitive compensation Access to both AWS and GCP ecosystems in production If you're a hands-on data engineer who enjoys owning end-to-end systems, loves solving real business problems, and thrives in a hybrid cloud + AI environment ‚Äî we want to talk to you.","[{""min"": 28000, ""max"": 36000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,237,Cloud Data Architect (AWS/Azure),Future Processing,"posiadasz min. 5 lat do≈õwiadczenia w IT, w tym min. 3,5 roku w pracy z danymi w chmurze AWS lub Azure (potwierdzone projektami komercyjnymi wdro≈ºonymi na produkcjƒô) oraz min. rok pracy w roli Lead Developera/Architekta lub podobnej , masz wiedzƒô w zakresie us≈Çug Data w AWS lub Azure bardzo dobrze znasz Databricks , ze szczeg√≥lnym naciskiem na najnowsze funkcjonalno≈õci tej platformy (np. Delta Live Tables i Unity Catalog), znasz narzƒôdzia, mechanizmy i procesy, tj. CI/CD, IaaC/Terraform, Git, umiesz tworzyƒá i modyfikowaƒá modele danych oraz dobieraƒá odpowiedni model do rozwiƒÖzywanego problemu ( DWH, Data Marts, Data Lake, Delta Lake, Data Lakehouse ), architektury Lambda i Kappa nie kryjƒÖ przed TobƒÖ tajemnic, wiesz, jak budowaƒá rozwiƒÖzania do analizy danych w trybie ‚Äûnear to real time‚Äù, monitoring, diagnostyka oraz rozwiƒÖzywanie problem√≥w w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanowaƒá infrastrukturƒô oraz obliczyƒá jej koszt, posiadasz wiedzƒô na temat migracji rozwiƒÖza≈Ñ on-premise do chmury oraz znasz podstawowe typy migracji, wiesz, jak stosowaƒá mechanizmy zwiƒÖzane z bezpiecznym przechowywaniem i przetwarzaniem danych w chmurze, bra≈Çe≈õ/a≈õ udzia≈Ç w spotkaniach przedsprzeda≈ºowych, cechuje Ciƒô my≈õlenie strategiczne i analityczne oraz dobra samoorganizacja pracy, samodzielno≈õƒá, adaptacyjno≈õƒá, komunikatywno≈õƒá oraz proaktywno≈õƒá, posiadasz umiejƒôtno≈õƒá mentorowania oraz szkolenia os√≥b o mniejszym do≈õwiadczeniu oraz prowadzenia dzia≈Ça≈Ñ majƒÖcych na celu ich wsparcie rozwojowe, prowadzenie prezentacji i prelekcji podczas wydarze≈Ñ bran≈ºowych nie spƒôdza Ci snu z powiek, jeste≈õ otwarty na wyjazdy s≈Çu≈ºbowe, pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie zaawansowanym, min. C1 . konsultowanie rozwiƒÖza≈Ñ z klientami - zar√≥wno obecnymi, jak i nowymi. Doradzanie w wyborze rozwiƒÖzania technicznego adekwatnego do problemu biznesowego klienta. DƒÖ≈ºenie do wyboru rozwiƒÖzania, kt√≥re jest optymalne kosztowo i odpowiada na potrzebƒô rozwiƒÖzujƒÖcƒÖ problem klienta. udzia≈Ç w spotkaniach z klientem na wczesnym etapie - pitch naszego do≈õwiadczenia, procesu, podej≈õcia do technologii, dopytywanie, zbieranie/doszczeg√≥≈Çawianie wymaga≈Ñ; tworzenie wk≈Çadu merytorycznego do ofert - schemat proponowanej architektury, wyliczenia Total Cost of Ownership/Return of Investment/koszt√≥w chmury za pomocƒÖ kalkulator√≥w dostawcy chmury; estymowanie projektu/wyceny zaanga≈ºowania przedstawicieli DS w projekcie; budowanie argument√≥w przekonujƒÖcych klienta do naszego rozwiƒÖzania, pokazywanie przewag w stosunku do innego podej≈õcia, udzia≈Ç w spotkaniach prezentujƒÖcych ofertƒô, odpowiadanie na pytania klienta, prezentowanie oferty, za kt√≥rƒÖ stoi nasz fragment rozwiƒÖzania, prace badawczo-rozwojowe w zakresie analizy funkcjonalno≈õci i przydatno≈õci nowych technologii i narzƒôdzi w rozwiƒÖzaniach biznesowych klienta, regularny kontakt z osobami decyzyjnymi w obszarze Data po stronie klienta (VP, IT Director), tworzenie PoC w obszarze Data w celu zaprezentowania wynik√≥w prowadzonych prac R&D, projektowanie i tworzenie ca≈Ço≈õci platformy przetwarzania danych uwzglƒôdniajƒÖc wszystkie jej czƒô≈õci oraz powiƒÖzania z pozosta≈Çymi rozwiƒÖzaniami (przyk≈Çadowo BI i ML) z uwzglƒôdnieniem ekosystemu chmurowego, optymalizowanie ca≈Ço≈õciowych rozwiƒÖza≈Ñ/system√≥w przechowywania i analizy danych, utrzymywanie relacji z zespo≈Çem technicznym po stronie klienta, koordynowanie pracy in≈ºynier√≥w zaanga≈ºowanych w tworzenie rozwiƒÖzania, nadzorowanie przebiegu ca≈Çego projektu, od poczƒÖtku do ko≈Ñca, zaanga≈ºowanie w rozw√≥j linii biznesowej Data Solutions (pozyskiwanie pracownik√≥w, klient√≥w, szkolenia, udzia≈Ç w konferencjach, mentoring itp.).","[{""min"": 165, ""max"": 245, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,Permanent,Hybrid,238,Data Analyst,HSBC Service Delivery,"Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity CTO Data GDT Wholesale, MSS & ESG is creating a world class ‚Äúdata-driven‚Äù organization that leads our competitors and inspires our employees. We are building a revolutionary data analytics ecosystem to generate business insights and provide great customer experience from well-managed and trusted data assets. Our global team is partnering with IT to deliver an ecosystem of curated, enriched and protected sets of data ‚Äì created from global, raw, structured and unstructured sources. Our Wholesale Big Data Lake is the largest aggregation of data ever within HSBC. We have over 1000 sources and a rapidly growing book of work. We are utilising the latest technologies to solve business problems and deliver value and truly unique insights. Developing the most appropriate and secure Technology solutions using iterative software development/programming in line with the solution design, to meet customer needs, ensuring continuous improvement. What you‚Äôll do Define and obtain source data required to successfully deliver insights and use cases. Determine the data mapping required to join multiple data sets together across multiple sources. Analyse structured and unstructured data to identify key business trends. Design and develop data models to support analytics initiatives. Create interactive dashboards and reports for stakeholders. Collaborate with data engineers and business teams to define requirements. Ensure data quality, integrity, and governance standards are met. Identify opportunities for process optimisation through data analysis. Support machine learning and predictive analytics projects. Document the data design and solutions to the high end for future reference. Support data-related tasks such as data mapping, data lineage, data governance, data quality, and metadata management What you need to have to succeed in this role Minimum 5 years of experience in data analytics, business intelligence, or related areas. Strong grasp and understanding of fundamental data modelling concepts such as entity-relationship modelling, data flow diagrams, and the ability to create conceptual data models from business requirements. Strong SQL skills for data querying and transformation. Ability to interpret complex datasets and generate business insights. Familiarity with Python or R for data analysis. Strong problem-solving skills and attention to detail. Excellent communication skills to present findings effectively. Knowledge of financial services or risk analytics (preferred). What we offer Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery discounts Financial support with trainings and education Social fund Flexible working hours Free parking If your CV meets our criteria, you should expect the following steps in the recruitment process: Online behavioural test Telephone screen Zoom interview with the hiring manager. We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,239,Data Engineer (Snowflake),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: udzia≈Ç w miƒôdzynarodowym projekcie realizowanym dla bran≈ºy logistycznej. Wykorzystywany stos technologiczny w projekcie: Snowflake (Cortex AI), AWS, DataLake, Data Warehouse, ETL, Tableau, PowerBI, Python, Docker, Kubernetes, projektowanie i rozwijanie skalowalnych pipeline'√≥w danych, hurtowni danych oraz data lake'√≥w w oparciu o Snowflake, integracja i automatyzacja proces√≥w przetwarzania danych (ETL/ELT) z r√≥≈ºnych ≈∫r√≥de≈Ç, wykorzystywanie us≈Çug AWS oraz Cortex AI do budowy inteligentnych rozwiƒÖza≈Ñ opartych na chmurze, rozwijanie aplikacji analitycznych i prototyp√≥w AI z u≈ºyciem Pythona i frameworka Streamlit, konteneryzacja aplikacji i ich uruchamianie przy pomocy Docker oraz Kubernetes, wsp√≥≈Çpraca z zespo≈Çami analitycznymi i interesariuszami biznesowymi w celu dostarczania odpowiednich rozwiƒÖza≈Ñ danych, monitorowanie oraz optymalizacja pipeline'√≥w danych i proces√≥w przetwarzania, udzia≈Ç w projektach z obszaru AI i uczenia maszynowego, w tym tworzenie rozwiƒÖza≈Ñ konwersacyjnych (np. chatboty), praca 100% zdalna, stawka do 150 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz min. 5-letnie do≈õwiadczenie na stanowisku Data Engineer, posiadasz do≈õwiadczenie w projektowaniu i utrzymywaniu hurtowni danych oraz pipeline‚Äô√≥w w Snowflake, biegle pos≈Çugujesz siƒô Cortex AI i narzƒôdziami AWS, programujesz swobodnie w Pythonie, znasz framework Streamlit, pracowa≈Çe≈õ/a≈õ z technologiami konteneryzacji, takimi jak Docker i Kubernetes, masz do≈õwiadczenie w tworzeniu i optymalizacji workflow√≥w ETL/ELT, dobrze rozumiesz procesy analizy danych i potrafisz rozwiƒÖzywaƒá problemy w z≈Ço≈ºonym ≈õrodowisku, potrafisz efektywnie wsp√≥≈Çpracowaƒá w miƒôdzynarodowym zespole, pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie p≈Çynnym - min. B2+, mile widziane do≈õwiadczenie w projektach opartych na conversAI oraz machine learning. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 19200, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,240,Insurence SQL/ETL Engineer,Link Group,"Twoje zadania: Budowa i rozw√≥j proces√≥w ETL przy u≈ºyciu Azure Data Factory i SQL Server Procedures Mapowanie i ekstrakcja danych z r√≥≈ºnych ≈∫r√≥de≈Ç do hurtowni danych Tworzenie data mart√≥w pod konkretne potrzeby analityczne i raportowe Projektowanie i wdra≈ºanie rozwiƒÖza≈Ñ raportowych Udzia≈Ç w analizie danych oraz przygotowywaniu ich do u≈ºytku biznesowego Wsp√≥≈Çpraca z zespo≈Çem analityk√≥w oraz specjalist√≥w bran≈ºowych Wymagania: Bardzo dobra znajomo≈õƒá SQL (T-SQL, procedury sk≈Çadowane) Do≈õwiadczenie w pracy z Microsoft SQL Server Praktyczna znajomo≈õƒá narzƒôdzi ETL , zw≈Çaszcza Azure Data Factory Do≈õwiadczenie w budowie rozwiƒÖza≈Ñ hurtowni danych (Azure SQL Data Warehouse) Umiejƒôtno≈õci analizy danych i pracy z du≈ºymi zbiorami Samodzielno≈õƒá i proaktywno≈õƒá w dzia≈Çaniu Mile widziane: znajomo≈õƒá Pythona (np. do automatyzacji proces√≥w)","[{""min"": 70, ""max"": 90, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,Any,Remote,241,Power Apps Developer (Lead),Lingaro,"Tasks: Acting as a lead developer or technical lead on Power Platform projects Designing and implementing complex solutions leveraging Power Platform ‚Äì Power Apps, Power Automate, Power Pages and Power BI Educating junior resources Remaining update-to-date on product improvements/releases and work closely with internal teams to stay current on product developments Participating in the entire solution implementation process ‚Äì from taking pre-sales activities, analysis, collecting customer requirements to design and development of target solution Skills: Must have: Minimum 4-6 years in IT industry Minimum 3 years of experience in Power Platforms Very good knowledge of Power Apps, Power Pages, Power Automate Very good technical knowledge, problem-solving and critical thinking Ability to translate technical solution advantages into business value for the customers English language proficiency is a must, both verbal and written Experiences in delivering e2e application and automation starting from requirements gathering, understanding business needs, design, develop, testing and maintenance. Aware about newest features in Power Platforms Nice to have: Knowledge of Power BI, DAX, Power Query M, SQL languages Knowledge of Dynamics 365 and Dataverse Experiences with Copilot Studio Knowledge about data modelling, performance tuning, business requirements gathering and translation them to a solution Experience with CI/CD and version controlling. Knowledge of Python Good knowledge of Microsoft Azure platform (Data Factory, Data Lake, Synapse, Logic Apps, Azure Functions ) or other cloud solution (Snowflake, GCP, AWS) Knowledge of Microsoft Fabric Knowledge of Boomi, BluePrism Experience: Working for international customers and teams Working in agile or agile/waterfall projects Working with stakeholders on different seniority and maturity level Participating in creating E2E BI solutions Acting as Power Platform Leader in projects We offer: Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Unclassified,Unclassified
Full-time,Senior,B2B,Remote,242,Lead Data Engineer,Link Group,"About the Role We‚Äôre looking for a Lead Data Engineer to take ownership of the architecture and delivery of data solutions in a modern, cloud-based environment. You will lead technical direction, mentor team members, and collaborate closely with cross-functional teams to ensure scalable, reliable, and high-performance data platforms. Design and build robust data pipelines , ETL/ELT workflows , and data architecture Lead the engineering team in best practices, code quality, and solution design Collaborate with stakeholders, product owners, and architects to define data strategies Set and enforce standards around data governance, quality, and performance Review code, provide technical mentorship, and support engineering growth Take ownership of end-to-end solution delivery ‚Äî from data ingestion to consumption Support DevOps, CI/CD, and cloud infrastructure related to data workloads 6+ years of experience in Data Engineering Proven experience in a technical leadership or lead engineer role Expert in Python , SQL , and cloud-native data processing tools Strong knowledge of AWS (preferred), GCP or Azure also welcome Experience with orchestration tools ( Airflow , Dagster , etc.) Proficiency with modern data platforms like Databricks , Snowflake , Redshift Familiar with data modeling , data lakes , lakehouse architectures , and streaming (Kafka, Kinesis, etc.) Knowledge of CI/CD , Terraform , and GitOps workflows Experience with team management or mentoring junior engineers Knowledge of dbt , Spark , Delta Lake , Glue , or similar Exposure to MLOps , analytics engineering , or BI/data visualization tools Understanding of data security , compliance , and privacy regulations","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,243,Specjalista ds. Bada≈Ñ i Rozwoju,PSE Innowacje sp. z o.o.,"PSE Innowacje jest sp√≥≈ÇkƒÖ powsta≈ÇƒÖ w 2012 na zlecenie operatora systemu przesy≈Çowego - PSE S.A. Od 2012 roku realizujemy takie zadania jak: prowadzenie analiz i bada≈Ñ, w tym analiz techniczno-ekonomicznych, prowadzenie prac badawczo-rozwojowych, budowa nowych oraz rozw√≥j i modernizacja istniejƒÖcych system√≥w informatycznych wspierajƒÖcych prowadzenie ruchu sieciowego. NaszƒÖ misjƒÖ jest dba≈Ço≈õƒá o niezawodnƒÖ i efektywnƒÖ pracƒô systemu elektroenergetycznego w Polsce oraz jego sta≈Çy rozw√≥j. Jeste≈õmy jednostkƒÖ do innowacyjnych zada≈Ñ specjalnych w bran≈ºy elektroenergetycznej. Poszukujemy do naszego Centrum Kompetencji Badania i Rozw√≥j w Warszawie lub Katowicach: Specjalist(k)ƒô ds. Bada≈Ñ i Rozwoju - Obszar kompetencji: Big Data, Prognozowanie i Optymalizacja üìç Miejsce pracy: - Warszawa lub Katowice; - Praca hybrydowa. üéØ Opis stanowiska: Osoba na tym stanowisku do≈ÇƒÖczy do obszaru merytorycznego Big Data, Prognozowanie i Optymalizacja w charakterze Specjalisty ds. Analiz i Wdro≈ºe≈Ñ IT i bƒôdzie odpowiedzialna za prowadzenie prac analitycznych z ukierunkowaniem na zagadnienia dot. OZE, wdra≈ºanie produkcyjne wypracowanych rozwiƒÖza≈Ñ BiR oraz wsparcie zespo≈Çu ds. modelowania matematycznego poprzez dostarczenie danych, narzƒôdzi oraz udzia≈Ç w pracach projektowych. üõ† Podstawowy zakres obowiƒÖzk√≥w na stanowisku: Analizowanie du≈ºych, r√≥≈ºnorodnych zbior√≥w danych na potrzeby prowadzonych prac analitycznych i badawczych; Wytwarzanie kodu ≈∫r√≥d≈Çowego na potrzeby prac badawczo-rozwojowych w jƒôzyku R/Python; Budowanie framework√≥w obliczeniowych w jƒôzyku R/Python, zgodnie z przyjƒôtym w Firmie standardem kodowania; Realizacja projekt√≥w na zr√≥≈ºnicowanym poziomie zaawansowania, poczƒÖwszy od budowy prototypu ko≈ÑczƒÖc na wdro≈ºeniu na ≈õrodowisko produkcyjne; Udzia≈Ç w budowaniu modeli prognostycznych z obszaru elektroenergetyki o r√≥≈ºnych horyzontach czasowych, od modeli ultra-kr√≥tkoterminowych po modele d≈Çugoterminowe; Testowanie modeli prognostycznych; Przygotowywanie raport√≥w, prezentacji i dokumentacji projektowej z uzyskanych wynik√≥w prac wraz z wnioskami i rekomendacjami; Praca w zespo≈Çach zarzƒÖdzanych w metodyce AgilePM; ≈öwiadczenie us≈Çug serwisowych dla wytwarzanych produkt√≥w; üìù Od kandydat√≥w oczekujemy: Wykszta≈Çcenia wy≈ºszego kierunkowego (mile widziana: matematyka, statystyka, energetyka, informatyka, ekonometria); Min. 3-letniego do≈õwiadczenia w pracy o charakterze Data Science (preferowane zagadnienia: rynek energii elektrycznej, gazu ziemnego, itp.); Znajomo≈õci zagadnie≈Ñ zwiƒÖzanych z rynkiem elektroenergetycznym, w tym dot. OZE; Praktycznej i bieg≈Çej znajomo≈õƒá jƒôzyk√≥w R oraz Python; Umiejƒôtno≈õƒá pracy z istniejƒÖcym kodem ≈∫r√≥d≈Çowym (tzw. legacy code), w tym jego analizy, modyfikacji i utrzymania; Do≈õwiadczenia w budowaniu aplikacji zgodnie z dobrymi praktykami in≈ºynierii oprogramowania; Udzia≈Ç w projektach zako≈Ñczonych wdro≈ºeniem produkcyjnym; Umiejƒôtno≈õci pisania test√≥w jednostkowych; Znajomo≈õci systemu kontroli wersji Git/GitLab do zarzƒÖdzania kodem i wsp√≥≈Çpracy zespo≈Çowej; Umiejƒôtno≈õci analitycznego my≈õlenia oraz formu≈Çowania ocen i wniosk√≥w; Komunikatywno≈õci i umiejƒôtno≈õci pracy w zespole; Dobrej organizacji pracy; Znajomo≈õci jƒôzyka angielskiego na poziomie min. B2; Chƒôci do podnoszenia kwalifikacji zawodowych; üíé Mile widziane: Do≈õwiadczenie w budowaniu aplikacji w R z wykorzystaniem R/Shiny; Znajomo≈õƒá jƒôzyka SQL; Kreatywno≈õƒá i aktywno≈õƒá w podejmowaniu inicjatywy; Znajomo≈õƒá praktyk CI/CD; üöÄ Oferujemy: Udzia≈Ç w ciekawych projektach majƒÖcych strategiczny wp≈Çyw na sektor energetyczny w Polsce i na ≈õwiecie; Pracƒô w przyjaznej atmosferze i wsparcie zespo≈Çu nastawionego na dzielenie siƒô wiedzƒÖ oraz do≈õwiadczeniami; KlarownƒÖ ≈õcie≈ºkƒô rozwoju zawodowego oraz szkolenia; Wewnƒôtrzny program mentoringowy wspierajƒÖcy zar√≥wno nowych jak i obecnych pracownik√≥w; Bogaty pakiet benefit√≥w: opieka medyczna, karta sportowa, ubezpieczenie na ≈ºycie; Program onboardingowy pozwalajƒÖcy na szybkƒÖ i przyjaznƒÖ adaptacje do pracy; Chcesz do≈ÇƒÖczyƒá do naszego zespo≈Çu? Aplikuj ju≈º teraz üíª",[],Unclassified,Unclassified
Full-time,Mid,Permanent,Remote,244,Bot & Fraud Detection Specialist,Akamai Technologies,"Does developing creative machine learning models addressing application security threats get your head churning? Join us and use data science to make a positive impact! Join our Global Web Security group! Our team is part of the Cloud Security organization. We are responsible for the technologies powering Akamai's security products and protecting some of the world's major internet brands. Working in partnership with other development teams, we design and create highly scalable solutions. Partner with the best As a Bot & Fraud Detection Specialist, you will help customers maximize value from our security products. You'll tailor detections using data science, cybersecurity, and consulting. You will optimize configurations and identify improvement opportunities. While not customer-facing, you'll support clients through the Professional Services team. You will ensure reliable, effective technical solutions aligned with customer needs. As a Bot & Fraud Detection Specialist, you will be responsible for: Analysing customer traffic, tailor detection algorithms for maximum accuracy, and optimize configurations. Collaborating with internal teams to implement detection enhancements and advocate for customer priorities. Debugging production issues, conducting root cause analysis, and proposing enhancements. Improving our tooling to identify proactively accuracy issues and facilitate traffic analysis. Do what you love To be successful in this role you will: Have relevant work experience. Be comfortable with data analysis techniques (Pandas, Polars, NumPy, SQL) and foundational knowledge of statistics. Love digging into large datasets to identify patterns and showcasing innovative problem-solving skills. Have good communication skills to collaborate effectively across teams and interest in cybersecurity. Work in a way that works for you FlexBase, Akamai's Global Flexible Working Program, is based on the principles that are helping us create the best workplace in the world. When our colleagues said that flexible working was important to them, we listened. We also know flexible working is important to many of the incredible people considering joining Akamai. FlexBase, gives 95% of employees the choice to work from their home, their office, or both (in the country advertised). This permanent workplace flexibility program is consistent and fair globally, to help us find incredible talent, virtually anywhere. We are happy to discuss working options for this role and encourage you to speak with your recruiter in more detail when you apply. Learn what makes Akamai a great place to work Connect with us on social and see what life at Akamai is like! We power and protect life online, by solving the toughest challenges, together. At Akamai, we're curious, innovative, collaborative and tenacious. We celebrate diversity of thought and we hold an unwavering belief that we can make a meaningful difference. Our teams use their global perspectives to put customers at the forefront of everything they do, so if you are people-centric, you'll thrive here. Working for you At Akamai, we will provide you with opportunities to grow, flourish, and achieve great things. Our benefit options are designed to meet your individual needs for today and in the future. We provide benefits surrounding all aspects of your life: Your health Your finances Your family Your time at work Your time pursuing other endeavors Our benefit plan options are designed to meet your individual needs and budget, both today and in the future. About us Akamai powers and protects life online. Leading companies worldwide choose Akamai to build, deliver, and secure their digital experiences helping billions of people live, work, and play every day. With the world's most distributed compute platform from cloud to edge we make it easy for customers to develop and run applications, while we keep experiences closer to users and threats farther away. Join us Are you seeking an opportunity to make a real difference in a company with a global reach and exciting services and clients? Come join us and grow with a team of people who will energize and inspire you!",[],Data Science,Data Science
Full-time,Senior,Permanent,Remote,245,"Senior Data Engineer (AWS, Databricks)",Upvanta sp. z o.o.,"Poszukujemy do≈õwiadczonego Senior Data Engineera, kt√≥ry do≈ÇƒÖczy do naszego zespo≈Çu danych i bƒôdzie wspiera≈Ç rozw√≥j nowoczesnej platformy danych opartej na chmurze AWS. Je≈õli masz do≈õwiadczenie w pracy z du≈ºymi zbiorami danych, narzƒôdziami takimi jak Databricks, DBT i tworzysz solidny, skalowalny kod w Pythonie, to mo≈ºe byƒá oferta dla Ciebie. ‚úÖ Twoje zadania: Projektowanie i budowa skalowalnych pipeline‚Äô√≥w danych w oparciu o Databricks i DBT Projektowanie i budowa skalowalnych pipeline‚Äô√≥w danych w oparciu o Databricks i DBT Tworzenie i optymalizacja przetwarzania danych w ≈õrodowisku AWS (S3, Glue, Lambda, Athena, EMR itp.) Tworzenie i optymalizacja przetwarzania danych w ≈õrodowisku AWS (S3, Glue, Lambda, Athena, EMR itp.) Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç i zapewnienie ich jako≈õci Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç i zapewnienie ich jako≈õci Utrzymywanie i rozwijanie hurtowni danych Utrzymywanie i rozwijanie hurtowni danych Praca w zespole z analitykami, data scientistami i product ownerami Praca w zespole z analitykami, data scientistami i product ownerami Udzia≈Ç w decyzjach architektonicznych i dobrych praktykach in≈ºynierii danych Udzia≈Ç w decyzjach architektonicznych i dobrych praktykach in≈ºynierii danych üß† Wymagania: Minimum 6 lat do≈õwiadczenia jako Data Engineer Minimum 6 lat do≈õwiadczenia jako Data Engineer Bieg≈Ço≈õƒá w Pythonie i pisaniu czystego, testowalnego kodu Bieg≈Ço≈õƒá w Pythonie i pisaniu czystego, testowalnego kodu Do≈õwiadczenie z DBT oraz Databricks (lub Spark) Do≈õwiadczenie z DBT oraz Databricks (lub Spark) Znajomo≈õƒá chmury AWS ‚Äì mile widziana certyfikacja Znajomo≈õƒá chmury AWS ‚Äì mile widziana certyfikacja Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych i rozproszonymi systemami Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych i rozproszonymi systemami Znajomo≈õƒá SQL na poziomie zaawansowanym Znajomo≈õƒá SQL na poziomie zaawansowanym Umiejƒôtno≈õƒá pracy w metodyce Agile Umiejƒôtno≈õƒá pracy w metodyce Agile Dobra znajomo≈õƒá jƒôzyka angielskiego (min. B2) Dobra znajomo≈õƒá jƒôzyka angielskiego (min. B2) üéÅ Co oferujemy: Atrakcyjne wynagrodzenie (B2B) Atrakcyjne wynagrodzenie (B2B) Elastyczne godziny pracy i pe≈Çna praca zdalna lub hybrydowa Elastyczne godziny pracy i pe≈Çna praca zdalna lub hybrydowa Praca w do≈õwiadczonym zespole, w kt√≥rym dzielimy siƒô wiedzƒÖ i wsp√≥lnie decydujemy o architekturze Praca w do≈õwiadczonym zespole, w kt√≥rym dzielimy siƒô wiedzƒÖ i wsp√≥lnie decydujemy o architekturze Udzia≈Ç w budowie nowoczesnej platformy danych od podstaw Udzia≈Ç w budowie nowoczesnej platformy danych od podstaw Karta Multisport, prywatna opieka medyczna, pakiet ubezpiecze≈Ñ Karta Multisport, prywatna opieka medyczna, pakiet ubezpiecze≈Ñ Sprzƒôt do pracy Sprzƒôt do pracy",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,246,Analityk Danych ‚Äì bran≈ºa bankowa,Sii,"Dla naszego klienta z bran≈ºy bankowej poszukujemy do≈õwiadczonego Analityka Danych. Osoba na tym stanowisku bƒôdzie pe≈Çniƒá kluczowƒÖ rolƒô w tworzeniu rozwiƒÖza≈Ñ wspierajƒÖcych rozw√≥j produkt√≥w inwestycyjnych zar√≥wno w aplikacji mobilnej, jak i w innych kana≈Çach (desktop, narzƒôdzia doradcy). Koordynacja i wsparcie projektu w aplikacji mobilnej ‚Äì w ≈õcis≈Çej wsp√≥≈Çpracy z zespo≈Çami IT, biznesowymi oraz dostawcami Kreowanie i rozw√≥j funkcjonalno≈õci wspierajƒÖcych obs≈Çugƒô w aplikacji mobilnej Udzia≈Ç w projektowaniu narzƒôdzi oraz funkcji wspierajƒÖcych ofertƒô produkt√≥w inwestycyjnych r√≥wnie≈º w innych kana≈Çach (np. desktop, systemy doradc√≥w) Przygotowywanie analiz danych, kt√≥re wspierajƒÖ decyzje biznesowe i rozw√≥j oferty produktowej Identyfikacja zale≈ºno≈õci i zalece≈Ñ na podstawie danych ‚Äì z my≈õlƒÖ o poprawie do≈õwiadczenia u≈ºytkownika i efektywno≈õci biznesowej Co najmniej 5 lat do≈õwiadczenia w obszarze IT Wykszta≈Çcenie wy≈ºsze (preferowane kierunki ≈õcis≈Çe, np. matematyka, informatyka, ekonometria) Znajomo≈õƒá jƒôzyk√≥w zapyta≈Ñ do baz danych: SQL i Python ‚Äì z umiejƒôtno≈õciƒÖ optymalizacji zapyta≈Ñ pod kƒÖtem wydajno≈õci, tak≈ºe przy du≈ºych wolumenach danych Bardzo dobra znajomo≈õƒá Power BI, Microsoft Excel i PowerPoint Wysoko rozwiniƒôte zdolno≈õci analityczne oraz umiejƒôtno≈õƒá szybkiego, trafnego wnioskowania i identyfikowania zale≈ºno≈õci Do≈õwiadczenie w analizie danych klienta (customer data analysis) Umiejƒôtno≈õƒá swobodnej komunikacji w jƒôzyku angielskim (w mowie i pi≈õmie) Wymagana p≈Çynna znajomo≈õƒá jƒôzyka polskiego Wymagane przebywanie na terenie Polski Dyspozycyjno≈õƒá do pracy 1-2 dni w tygodniu w biurze (Warszawa, Pozna≈Ñ lub Wroc≈Çaw) Tytu≈Ç Great Place to Work od 2015 roku - to dziƒôki opiniom pracownik√≥w otrzymujemy tytu≈Ç i wdra≈ºamy nowe pomys≈Çy Stabilno≈õƒá zatrudnienia ‚Äì 2,1 MLD PLN przychodu, brak d≈Çug√≥w, od 2006 roku na rynku Dzielimy siƒô zyskiem z pracownikami - od 2022 roku przeznaczyli≈õmy na ten cel ju≈º ponad 60 milion√≥w PLN Bogaty pakiet benefit√≥w - prywatna opieka zdrowotna, platforma kafeteryjna, zni≈ºki na samochody i wiƒôcej Komfortowe miejsce pracy - pracuj w naszych biurach klasy A lub zdalnie DziesiƒÖtki fascynujƒÖcych projekt√≥w dla presti≈ºowych marek z ca≈Çego ≈õwiata ‚Äì mo≈ºesz je zmieniaƒá dziƒôki aplikacji Job Changer 1 000 000 PLN rocznie na Twoje pomys≈Çy - takƒÖ kwotƒÖ wspieramy pasje i akcje wolontariackie naszych pracownik√≥w Stawiamy na Tw√≥j rozw√≥j - meetupy, webinary, platforma szkoleniowa i blog technologiczny ‚Äì Ty wybierasz Fantastyczna atmosfera stworzona przez wszystkich Sii Power People 1 Wy≈õlij do nas swoje CV 2 We≈∫ udzia≈Ç w rozmowie rekrutacyjnej 3 Poznaj projekty dopasowane do Twoich potrzeb 4 Podejmij decyzjƒô i zacznij swojƒÖ przygodƒô z Sii! Sii to czo≈Çowy dostawca doradztwa technologicznego, transformacji cyfrowej oraz us≈Çug in≈ºynieryjnych i biznesowych w Polsce.Zatrudniamy ju≈º ponad 7 500 specjalist√≥w i realizujemy projekty z r√≥≈ºnorodnych bran≈º dla klient√≥w z wielu kraj√≥w na ca≈Çym ≈õwiecie. Tytu≈Ç Great Place to Work zdobyty 10 razy z rzƒôdu to dow√≥d na to, ≈ºe w Sii tworzymy przyjazne ≈õrodowisko pracy. W badaniu a≈º 90% naszych pracownik√≥w odpowiedzia≈Ço, ≈ºe Sii jest ≈õwietnym miejscem pracy, a 95% z nich sƒÖdzi, ≈ºe panuje tu ≈õwietna atmosfera.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,247,SQL Developer,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 3-letnie do≈õwiadczenie zawodowe na stanowisku Programisty SQL Do≈õwiadczenie projektowe zwiƒÖzane z tworzeniem z≈Ço≈ºonych zapyta≈Ñ SQL, procedur oraz funkcji bazy danych Do≈õwiadczenie projektowe zwiƒÖzane ze strojeniem i optymalizacjƒÖ zapyta≈Ñ SQL Do≈õwiadczenie projektowe zwiƒÖzane z hurtowniƒÖ danych, procesami ETL Znajomo≈õƒá praktyk CI/CD Umiejƒôtno≈õƒá pracy w systemie kontroli wersji GIT Do≈õwiadczenie w programowaniu w PL/pgSQL, PL/SQL Znajomo≈õƒá bazy danych PostgreSQL/EDB Znajomo≈õƒá JSON Znajomo≈õƒá zagadnie≈Ñ FDW/po≈ÇƒÖczenia miƒôdzy r√≥≈ºnymi bazami Do≈õwiadczenie w programowaniu w jƒôzykach skryptowych w ≈õrodowisku LINUX Do≈õwiadczenie w programowaniu w jƒôzyku Python Dobra organizacja pracy i odpowiedzialno≈õƒá za powierzone zadania Komunikatywno≈õƒá, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista Dociekliwo≈õƒá w diagnozowaniu i rozwiƒÖzywaniu bie≈ºƒÖcych problem√≥w Mile widziane Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Do≈õwiadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá programowania w jƒôzyku PL/SQL (np. PostgreSQL Certification lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Kluczowe zadania Tworzenie i rozwijanie z≈Ço≈ºonych zapyta≈Ñ SQL, procedur sk≈Çadowanych oraz funkcji bazy danych Optymalizacja i strojenie zapyta≈Ñ SQL w celu poprawy wydajno≈õci system√≥w Projektowanie i wdra≈ºanie rozwiƒÖza≈Ñ zwiƒÖzanych z hurtowniƒÖ danych oraz procesami ETL Praca z bazƒÖ danych PostgreSQL/EDB oraz implementacja rozwiƒÖza≈Ñ z wykorzystaniem PL/pgSQL i PL/SQL Wykorzystywanie i zarzƒÖdzanie danymi w formacie JSON oraz obs≈Çuga zagadnie≈Ñ zwiƒÖzanych z FDW i po≈ÇƒÖczeniami miƒôdzy r√≥≈ºnymi bazami danych Programowanie w jƒôzykach skryptowych w ≈õrodowisku Linux oraz rozwijanie aplikacji i narzƒôdzi w Pythonie Praca w systemie kontroli wersji GIT oraz stosowanie praktyk Continuous Integration/Continuous Deployment (CI/CD)","[{""min"": 80, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Practice / Internship,Junior,Internship,Remote,248,Data Science Trainee,EPAM Systems,"Striving to gain market-oriented knowledge and skills to jumpstart your career in IT? Apply for this program and shape your professional path with EPAM experts. Details If you are eager to thrive in the data-driven world and master one of the most trending professions, then this expert-led program is what you need. By participating, you will have the opportunity to: Gain a strong foundation in software engineering Dive deep into statistics and data analysis Learn regression, classification, and clustering techniques Join one of our mentoring programs in specialized Data Science domains After completing this program, high-performing candidates may be offered opportunities to further their learning based on their level, skills, and available positions at EPAM. The program is designed to guide you through two engaging stages: In this stage, you'll build a strong foundation in Data Science. Here's what to expect: Weekly learning. Explore video lessons and self-study materials, then practice through tasks and tests ‚Äì all within set deadlines to ensure steady progress. Mentor guidance. Submit your weekly practical assignments for feedback and approval from expert mentors. Interactive support. Join weekly group Q&A sessions with professionals to discuss questions and deepen your understanding. Perform well to pass the technical interview and advance to the next level! This stage is all about taking your skills to the advanced level with a more intensive approach. You will join one of the proposed mentoring programs: Computer Vision Natural Language Processing Machine Learning Engineering Time Series Recommendation Systems By the end, you'll acquire job-ready skills to tackle real-world data challenges confidently. What is required for training: Second-to-last or final year university students and recent graduates Individuals aged 18 years and older Skills requirements: English level B2 (Upper-Intermediate) or higher Proficiency in mathematical analysis (main topics of derivatives, integrals, extrema of functions in multidimensional real space) Skills in linear algebra (vectors, matrices, tensors, linear equations, eigenvalues and eigenvectors, quadratic forms) Knowledge of probability theory (definition of probability, conditional probability, Bayes theorem, expectation) Command of statistics (basic concepts, hypothesis testing, concept of likelihood, estimation of distribution parameter) Understanding of basic optimization concepts and methods (stationary points, Lagrange multipliers, gradient descent) Knowledge of data structures and algorithms (sorting algorithms, algorithm complexity) Familiarity with Python fundamentals (NumPy and Pandas in particular) What do we offer? Industry-based education. As a leading software engineering company, we will help you explore emerging technologies and best practices that the market demands. Top-notch learning materials. Our Data Science specialists with extensive project experience have designed and tested the educational content in numerous training runs. Practice-oriented approach. This comprehensive program focuses on providing you with hands-on experience and practical application of the concepts learned. Deep dive into the specialization. Our graduates become highly skilled specialists ready to face complex technical challenges and work with the world's leading clients. Support from experienced mentors. We will guide you at all training stages, covering your open questions and sharing feedback on assigned tasks. Career advancement. Upon successful completion of both fundamentals and specialization stages, we will consider you for open positions based on your demonstrated skills and available opportunities at EPAM. Please read this info before registration This training is for citizens of Poland and specialists relocated to this country for permanent stay. The program start date may change, so the selection period may be adjusted accordingly. Please regularly check for updates on this page and via email. Active participants of trainings available on the campus.epam.com platform and EPAM Systems Company Employees are not allowed to register for the training. Please, contact your manager regarding the positions available. Considering the limited number of program slots, the application order and assessment results will be decisive factors for being enrolled.",[],Data Science,Data Science
Full-time,Mid,B2B,Remote,249,Data Engineer with AWS,emagine Polska,"üí∞ Rate: B2B ‚Äì around 38 EUR/h. üåç Work model: fully remote. ‚è≥ Project length: 12 months + extensions.‚è∞ Start date for assignment: ASAP/ 1 month.üìï Project language: English, Polish.üíº Industry: pharmaceutical. ‚öôÔ∏è Recruitment process : 2 interviews with the client.üíª Workload: Full time. Summary: The Data Engineer role focuses on developing and integrating data solutions to enhance production processes and support business needs within the Manufacturing Intelligence (MI) team. Main Responsibilities: Software development activities, including writing, reviewing, refactoring, testing, and documenting code. Build stable and scalable data pipelines for production environments. Provide technical expertise and act as a trusted advisor on projects. Collaborate with Agile teams to understand and manage technical work. Participate in Agile events such as Program Increment planning and system demos. Key Requirements: A master‚Äôs degree in computer science, engineering, chemistry, biology, or relevant fields. Experience with data engineering principles (data warehousing, batch processing, data streaming, etc.). Experience building CI/CD pipelines . Coding/scripting skills in Bash, Python , or similar languages. Hands-on experience with AWS services (Lambda, Glue, IAM, etc.). Experience with Infrastructure as Code (IaC) for managing cloud resources. Strong mathematical, statistical, and problem-solving skills. Nice to Have: Strategic and innovative mindset. Strong communication skills. Systematic approach to problem-solving.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,250,Data Engineer with Databricks,Margo,"We are looking for data engineers perfectly familiar with data literacy and proficient with data platform architecture patterns. The data platform is leveraging MS Azure services, including ingestion, staging, processing, and serving. The client now entering a phase of scaling Databricks solution, then we are looking for resources with relevant skills on this technology. The position is ideal for you if you have: 3+ years of experience as a Data Engineer Experience with Azure Databricks and Power BI Fluent in English (written and spoken) Joining Margo you can expect: Ability to work in an international consulting company on ambitious projects, Permanent contract or B2B cooperation, Benefits such as medical care and sports card, Co-finantrainings, certification exams and post-graduate studies, Internal training and the possibility of using our know-how, Possibility to use our library free of charge, Individual approach and development opportunities (career path planning, ability to change the project and position, possibility to get involved in outside-project activities with additional remuneration), Possibility to influence the shape of the company, openness to your ideas and willingness to implement them, Excellent working atmosphere, integration events. Data engineer, with proven skills with Databricks. Missions : support our dev team to develop our data pipelines & data transformation & data exposure services Actively Contribute to the continuous improvement of our dev patterns Document developments & unit testing Improve platform observability capabilities Key skills : Proficient in SQL, Python and PySpark At least 2 years of experience in Databricks, with proven experience deploying Unity Catalog solutions Proven experience working in Azure, and more specifically with the following services within Azure: Data Factory, SQL Database, Data Lake, Logic Apps Skilled in Data modelization, whatever the supporting tool. Comfortable with business-specific discussions.","[{""min"": 170, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,251,Tech Lead (Azure),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. projekt budowy nowoczesnej Data Platform w chmurze Azure, obejmujƒÖcej warstwƒô Lakehouse (Bronze / Silver / Gold) oraz Analytical Platform (MLOps), modelowanie struktur bazodanowych w podej≈õciu DDD (Data Domain Driven Design), opracowywanie modeli danych na podstawie dokumentacji z obszaru Data Governance (glosariusz danych, modele konceptualne/logiczne), projektowanie przep≈Çywu danych ze ≈∫r√≥de≈Ç do warstw platformy danych (Ingest: direct query, API, event streaming), tworzenie i dokumentowanie Data Contracts, wsp√≥≈Çpraca z w≈Ça≈õcicielami system√≥w ≈∫r√≥d≈Çowych i docelowych w zakresie integracji danych i SLA, implementacja struktur danych w architekturze medallion (Bronze / Silver / Gold) przy u≈ºyciu ETL na Azure Data Platform, doradztwo w zakresie doboru narzƒôdzi, architektury integracyjnej i praktyk CI/CD, mo≈ºliwo≈õƒá realnego wp≈Çywu na standardy technologiczne i projektowe, praca 100% zdalna, a dla chƒôtnych mo≈ºliwo≈õƒá pracy z biura we Wroc≈Çawiu, stawka do 200 z≈Ç/h przy B2B, w zale≈ºno≈õci od do≈õwiadczenia. masz do≈õwiadczenie w modelowaniu danych (ERD), przygotowywaniu Data Contracts i implementacji struktur domenowych w ≈õrodowisku Data Warehouse znasz procesy Data Ingestion i architekturƒô nowoczesnych DWH w Azure (Azure Synapse, Data Lake, Databricks), masz do≈õwiadczenie z platformami MLOps / Analytical Platform, mile widziane do≈õwiadczenie w: tworzeniu dokumentacji mapowania danych ≈∫r√≥d≈Çowych, zarzƒÖdzaniu metadanymi i jako≈õciƒÖ danych (np. Azure Purview), komunikujesz siƒô p≈Çynnie w jƒôzyku angielskim (B2/C1). d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed.","[{""min"": 26880, ""max"": 33600, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,252,Data Engineer,Datumo,"We‚Äôre looking for a Data Engineer ready to push boundaries and grow with us. Datumo specializes in providing Data Engineering and Cloud Computing consulting services to clients from all over the world, primarily in Western Europe, Poland and the USA. Core industries we support include e-commerce üõí, telecommunications üì° and life sciences üß¨. Our team consists of exceptional people whose commitment allows us to conduct highly demanding projects. Our team members tend to stick around for more than 3 years, and when a project wraps up, we don't let them go - we embark on a journey to discover exciting new challenges for them. It's not just a workplace; it's a community that grows together! Must-have: ‚úÖ at least 3 years of commercial experience in programming ‚úÖ proven record with a selected cloud provider GCP (preferred), Azure or AWS ‚úÖ good knowledge of JVM languages (Scala or Java or Kotlin), Python, SQL ‚úÖ experience in one of data warehousing solutions: BigQuery/Snowflake/Databricks or similar ‚úÖ in-depth understanding of big data aspects like data storage, modeling , processing , scheduling etc. ‚úÖ data modeling and data storage experience ‚úÖ ensuring solution quality through automatic tests, CI/CD and code review ‚úÖ proven collaboration with businesses ‚úÖ English proficiency at B2 level, communicative in Polish Nice to have: üåü knowledge of dbt, Docker and Kubernetes, Apache Kafka üåü familiarity with Apache Airflow or similar pipeline orchestrator üåü another JVM (Java/Scala/Kotlin) programming language üåü experience in Machine Learning projects üåü understanding of Apache Spark or similar distributed data processing framework üåü familiarity with one of BI tools: Power BI/Looker/Tableau üåü willingness to share knowledge (conferences, articles, open-source projects) What‚Äôs on offer: üî• 100% remote work, with workation opportunity üî• 20 free days üî• onboarding with a dedicated mentor üî• project switching possible after a certain period üî• individual budget for training and conferences üî• benefits: Medicover Private Medical Care , co-financing of the Medicover Sport card üî• opportunity to learn English with a native speaker üî• regular company trips and informal get-togethers Development opportunities in Datumo: üöÄ participation in industry conferences üöÄ establishing Datumo's online brand presence üöÄ support in obtaining certifications (e.g. GCP, Azure, Snowflake) üöÄ involvement in internal initiatives, like building technological roadmaps üöÄ training budget üöÄ access to internal technological training repositories Discover our exemplary project: üîå IoT data ingestion to cloud The project integrates data from edge devices into the cloud using Azure services. The platform supports data streaming via either the IoT Edge environment with Java or Python modules, or direct connection using Kafka protocol to Event Hubs. It also facilitates batch data transmission to ADLS. Data transformation from raw telemetry to structured tables is done through Spark jobs in Databricks or data connections and update policies in Azure Data Explorer. ‚òÅÔ∏è Petabyte-scale data platform migration to Google Cloud The goal of the project is to improve scalability and performance of the data platform by transitioning over a thousand active pipelines to GCP. The main focus is on rearchitecting existing Spark applications to either Cloud Dataproc or Cloud BigQuery SQL, depending on the Client‚Äôs requirements and automate it using Cloud Composer. üìà Data analytics platform for investing company The project centers on developing and overseeing a data platform for an asset management company focused on ESG investing. Databricks is the central component. The platform, built on Azure cloud, integrates various Azure services for diverse functionalities. The primary task involves implementing and extending complex ETL processes that enrich investment data, using Spark jobs in Scala. Integrations with external data providers, as well as solutions for improving data quality and optimizing cloud resources, have been implemented. üõí Realtime Consumer Data Platform The initiative involves constructing a consumer data platform (CDP) for a major Polish retail company. Datumo actively participates from the project‚Äôs start, contributing to planning the platform‚Äôs architecture. The CDP is built on Google Cloud Platform (GCP), utilizing services like Pub/Sub, Dataflow and BigQuery. Open-source tools, including a Kubernetes cluster with Apache Kafka, Apache Airflow and Apache Flink, are used to meet specific requirements. This combination offers significant possibilities for the platform. Recruitment process: 1Ô∏è‚É£Quiz - 15 minutes 2Ô∏è‚É£ Soft skills interview - 30 minutes 3Ô∏è‚É£ Technical interview - 60 minutes Find out more by visiting our website - https: //www.datumo.io If you like what we do and you dream about creating this world with us - don‚Äôt wait, apply now!","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,253,Data Engineer (with Microsoft Fabric),Sigli,"As a Data Engineer, you will be instrumental in developing and scaling our data integration capabilities. You will work with a modern, cloud-native stack and be a key player in building the future of our platform. What about the project: Our client is a forward-thinking technology company on a bold mission: to build the AI foundation for SaaS growth. Their platform transforms data chaos into clarity ‚Äì empowering businesses with an integrated, intelligent core for better decision-making, deeper insights, and scalable automation. Founded and backed by seasoned entrepreneurs from successful Scandinavian ventures, the company is purpose-built for long-term impact. Their vision combines technical excellence with a strong product mindset ‚Äì creating tools that are not only powerful, but built to last. They foster a culture where engineers solve meaningful problems with autonomy, transparency, and mutual trust. It‚Äôs a place where curiosity is welcomed, and every team member plays a key role in shaping the future of AI-driven SaaS infrastructure. What you will do: Design, build, and manage scalable data solutions within Microsoft Fabric, including Data Factory, lakehouse architecture, and Fabric workspaces. Develop and maintain data pipelines with a focus on real-time streaming, ETL/ELT processing, and data transformation best practices. Ensure data quality and governance by implementing robust frameworks for validation, cleansing, and lineage tracking. Work with REST APIs, webhooks, and custom integrations to connect diverse data sources and enable automated workflows. Apply knowledge of tenant-level data partitioning, security boundaries, and resource isolation to build secure, compliant data environments. Collaborate on CI/CD practices in Microsoft Fabric, including Git integration, deployment pipelines, and workspace lifecycle management. Use Python for data processing, automation scripts, and the development of custom integration components. Leverage Azure data services for storage, compute, and analytics, ensuring cost-effective and performant solutions. Apply Infrastructure as Code techniques (e.g., ARM templates, Bicep, Terraform) to provision and manage cloud resources reproducibly. Support integration with SaaS platforms, ensuring alignment with common middleware patterns and SaaS-specific operational models. What you need: 3+ years of experience as a Data Engineer. Proven experience with Microsoft Fabric architecture, including Data Factory. Hands-on experience with Fabric workspaces, data pipelines, and lakehouse architecture. Knowledge of tenant-specific data partitioning, security boundaries, and resource allocation. Proficiency with REST APIs, webhooks, and real-time data streaming. Experience with data transformation, ETL/ELT processes, and data quality frameworks. Knowledge of common SaaS integration patterns and middleware solutions. Strong Python programming skills for data processing, automation, and custom integrations. A strong background in Azure data services. Experience with Infrastructure as Code (ARM templates, Bicep, or Terraform). Expertise in Microsoft Fabric CI/CD using deployment pipelines, Git integration, and workspace lifecycle management. An understanding of SaaS business models, scaling challenges, and operational requirements. Upper-intermediate English level. What's in it for you: Ownership ‚Äì we trust that you will do the right things to deliver maximum impact. Transparency ‚Äì we say what we think and every voice is heard and respected, even when our opinions differ. Service ‚Äì whether it‚Äôs for our customers or teammates, we always support each other. No bureaucracy, no micromanagement. Flexible working schedule ‚Äì you plan your working day based on your tasks and meetings.",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,254,Senior Data Engineer,SD Worx,"SD Worx is a leading European provider of Payroll & HR services with global reach. We have offices in Europe and an office in Mauritius. Our goal? We bring people solutions to life, so companies of any size can turn Human Resources into a source of value for the business and the people in it. Our people solutions span the entire employee journey, from getting people paid to attracting, rewarding, and developing talent. Are you ready to join us? What do we have to offer? Stable employment conditions: permanent employment contract (after a 3-month probation period) Flexible working hours and remote work opportunities Private medical care (LuxMed) MyBenefit Cafeteria system / Multisport card Lunch card (Edenred) Financial allowance for remote work Loans for employees Workation: possibility to work from any SD Worx location (4 weeks per year, EU only) Life insurance Holiday allowance Free language courses (during your working hours) Annual bonus Integration events Free parking spaces for employees Referral program PPK (Employee Capital Plans): 3.5% employer contribution Learning opportunities: through an individual development plan and professional training Career growth: whether you want to become more of an expert in your field or expand your knowledge horizontally, there‚Äôs always room to grow within SD Worx! About the role: Drive the development of robust data pipelines and storage solutions using cutting-edge technologies, ensuring optimal performance, scalability, and security Mentor and guide a team of Data Engineers, ensuring project success and technical excellence Partner with stakeholders to define and implement strategic, data-driven solutions Stay updated on industry trends and emerging technologies, and obtain relevant certifications to ensure the application of best practices in data architecture Using an analytical, data-driven approach to support business cases Working with stakeholders to design data processing systems, data pipelines, and deliver insightful analytics while ensuring data integrity Building and automating key components of our infrastructure As an ideal candidate, you have: A Master‚Äôs or bachelor‚Äôs degree in IT, a related technical field, or equivalent practical experience A Strong interest in data manipulation (storage, crunching, pipeline implementation) At least 4 years of relevant experience as a Data/DevOps/Software Engineer. A Strong SQL knowledge Understanding of data structures and algorithms With a data-oriented mindset, you possess a good knowledge of data architecture and data infrastructure Experience with Azure Experience with DevOps and CI/CD concepts (Azure DevOps) Experience with a cloud data warehouse - preferably Snowflake Proficient with Python scripting & app development. Fluency in English Conceptual and creative thinking Experience in B2B and customer-driven environments Great communication skills It is not a requirement, but other preferred qualifications would be: Experience working with deployment and orchestration technologies (i.e., Docker, Kubernetes, Terraform, Airflow, etc.) Experience with DBT Experience with Power BI (and embedded) Experience with PowerShell scripting From many places, we work as one, moving from better to best together. SD Worx lives diversity in the workplace. Diversity provides inspiration and innovation in our company. We particularly welcome applications from qualified talent, regardless of origin, nationality, gender, skin colour, ethnic and social background, religion, age, disability, sexual orientation, and stage of life.","[{""min"": 19000, ""max"": 23000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,255,Data Engineer,Mirumee Software,"Hi! üëã We are Mirumee , a Python and TypeScript software house specializing in developing unique products. Since 2009 , we‚Äôve been helping businesses transform by delivering sustainable, API-first, efficient, and modern solutions. Our expertise empowers brands across Healthcare, Commerce, and Self-Publishing , from high-volume online retailers to disruptive innovators featured on the Forbes Next Billion Dollar Startups list. At Mirumee , culture matters. We thrive in open-source communities and believe in technical excellence, open communication, and mutual respect . We actively contribute to open-source projects - check out our work here: GitHub.com/mirumee . But first of all, we care about technical excellence, open communication, and treating each other with respect as equal human beings . We are seeking an experienced Senior Data Engineer to join an e-commerce project in the cannabis branch in the US, who combines strong technical execution with curiosity, ownership, and a collaborative mindset. You‚Äôll help shape and scale our data platform while proactively engaging with cross-functional teams to drive clarity, resilience, and value in our data pipelines and tooling. The ideal candidate brings experience with modern data stacks (e.g., Airflow, DBT, cloud-native tooling), strong fundamentals in SQL and Python, and a willingness to engage beyond narrowly defined tasks. If you enjoy working through ambiguity, taking initiative, and helping set new standards of quality, we‚Äôd love to meet you! Your profile Strong SQL skills with experience writing performant, maintainable queries on large datasets Fluency in Python , especially with libraries like Pandas, and a strong testing discipline Experience with Airflow , DBT , or similar orchestration and transformation tools Familiarity with BI tools such as Sigma or Tableau Experience with AWS and IaaC best practices (especially serverless data services) Ability to break down complex projects into iterative steps and communicate progress effectively A growth-oriented mindset ‚Äîyou‚Äôre resourceful, self-directed, and open to feedback Someone curious and engaged , regularly asking thoughtful questions and offering insights beyond your immediate responsibilities A positive, collaborative teammate who brings energy and focus to the work and team Good command of English (both written and spoken) Responsibilities Design, build, and maintain scalable data pipelines using Airflow, DBT, and Python. Partner with stakeholders to clarify requirements, uncover edge cases, and ensure quality delivery - even when the problem space isn‚Äôt fully defined. Own and improve our data models and warehouse practices, helping ensure reliability, performance, and clarity Proactively monitor and respond to data issues (e.g., Sentry alerts, data quality checks), driving resolution and follow-through. Collaborate with others through well-scoped pull requests, clear documentation, and knowledge sharing Champion testing and observability, ensuring our pipelines Benefits 26 days of paid service disruption per is a standard for us üå¥ Personal development in the fast-growing industry based on your and our needs: boredom is something foreign to us Work wherever the way you want: we don't care if you start at 6 or 10 am and give you the choice to choose between a beautiful office, home office spot or combine both! We give you 5 extra days for health recovery (b2b) and the basics to take care of prophylactically - private healthcare that works (Signal Iduna) & sports card (Multisport Plus) for free Work-life balance (and we mean it!) High-end tools, physical (MacBook), and software ones And much more that we want to share during interviews! We can't wait to meet YOU . üôåüèº","[{""min"": 17000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,256,Senior Data Engineer,Link Group,"Maintain and optimize Azure SQL Database and Azure SQL Managed Instances Design, implement, and manage data pipelines to support real-time and near real-time data exchange Analyze and improve complex SQL queries and performance bottlenecks Work closely with solution architects and product teams to propose architectural changes and data model improvements Monitor and troubleshoot data issues, proposing sustainable and scalable fixes Contribute to the overall data architecture strategy and ensure alignment with business needs Proven experience as a Senior Data Engineer or Data Architect Strong expertise in Azure SQL Database and Azure SQL Managed Instance Deep knowledge of SQL performance tuning and query optimization Experience with data modeling , ETL/ELT pipelines , and data integration High-level understanding of data architecture principles and patterns Hands-on experience with Azure Data Factory , Azure Data Lake , or other Azure data services Familiarity with CI/CD , Git , and agile workflows Experience with Power BI , Databricks , or Synapse Analytics Familiarity with security and compliance standards in the data domain Knowledge of AI/ML integration in data pipelines Previous work on maturity or assessment platforms is a plus 100% remote work with flexible hours Opportunity to shape a high-impact platform used by global organizations Long-term project with potential for growth and ownership Competitive compensation based on experience","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,257,Data & Visualisation Specialist (Tableau),Antal Sp. z o.o.,"Data & Visualisation Specialist (Tableau) Location: hybrid, Cracow Contract Type: B2B Workload: Full-time Are you looking to make a real impact in a global financial project? We are working Data & Visualisation Specialist (Tableau) to join our Clients team. Company Description Our Client's team is dedicated to fostering a data-driven culture across the organization. Their strategy focuses on protecting the data through robust management policies, engaging colleagues with enhanced training opportunities, and building sustainable capabilities to unlock value for customers. Principal Accountabilities Developing data visualizations in the form of advanced dashboards and reports Understanding infrastructure requirements and best practices to support a Tableau deployment Data Source Management ‚Äì ensuring data integrity, updating data as needed Managing Project site on Tableau Server Documenting processes, data sources and dashboards Analyze & drive data sharing best practices around user access Strong stakeholder management and influencing skills Essential Criteria Proficiency in Tableau 3+ years‚Äô work experience in data analysis Ability to query and display large data sets while maximizing the performance of Tableau workbooks Working knowledge of Tableau administrator/architecture Ability to work collaboratively with cross-functional teams and stakeholders at all levels A high degree of mathematical competence Analytically minded Developing dashboards in QlikSense experience (nice to have) Major Challenges Manage self in a positive, engaging, thoughtful and constructive manner, from a role which often has a material influence on outcomes. Operate in a fast changing operating environment requiring quality decision making with often incomplete information. Maintaining well-regarded, thoughtful communications with principal stakeholders. Ensure data sharing compliance in a complex regulatory landscape What We Offer b2b contract and support of the Contractor Care Team Private Medical Care Cafeteria system Life insurance To learn more about Antal, please visit www.antal.pl","[{""min"": 30000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,Permanent or B2B,Remote,258,Data Engineer,dotLinkers,"Type of contract: contract of employment (UoP) / B2B Salary ranges: up to 24 000 PLN a month Working model: 100% remote Join our client, one of the leading logistics and transport solutions providers. About the role: Looking for a skilled Lead Data Engineer to drive the design and implementation of robust data systems, while effectively connecting technical teams with business goals. This role involves hands-on development, strategic planning, and cross-team collaboration. Responsibilities: Designing scalable data systems and tools to support analytics, modeling, and decision-making Building advanced data pipelines and platforms on Azure Collaborating closely with technical and business teams Establishing best practices and reusable standards in data engineering Engaging with international projects and external partners Requirements: 5+ years in IT, data engineering, or information systems Expertise in Azure, Spark (Scala/PySpark), Databricks, Kafka, Event Hubs, Python, Java, SQL/NoSQL Experience with DevOps tools and practices (CI/CD, Terraform, Kubernetes) Skilled in streaming data and big data environments Strong leadership and communication skills (English and Polish) Experience working with distributed teams The offer: Flexible remote work with occasional office visits Benefits include private healthcare, insurance, and a sports card High-end equipment Development budget for learning and growth","[{""min"": 22000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 22000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,259,Fullstack Engineer/Technical Owner,PAYBACK,"You want to take responsibility and collaborate your ideas to our unique product? You are ambitious and eager to and move things further? Then you are exactly right with PAYBACK. We are looking forward to you getting to know you. You write well-designed, testable and efficient code and perform pair programming and code reviews You create a technical vision for the components owned by your cross-functional team and on their integration in our overall landscape You develop and implement innovative data and AI products using classical machine learning methods and generative AI (GenAI) You will be involved in all phases of the development lifecycle, which includes deployment, testing and monitoring - ‚ÄòYou Build It, You Run It‚Äô You will drive planning, decisioning and implementation processes on design, principles, guidelines, NFR fulfillment and documentation quality You will lead testing and QA efforts as well as drive the quality of automated testing and continous deployment workflows You will collaborate with product and business stakeholders to approach new feature requirements, actively manage technical debts and improve software performance You support continuous improvement by investigating alternatives such as technologies & architectures and trigger adoption discussions with our internal guilds You ensure the smooth operation of our AI systems, optimize existing applications and always stay up to date with the latest technology You provide guidance to fellow developers, foster a culture of learning by sharing your expertise and work with managers to ensure the right talents are acquired, developed and retained You have a solid, proven track record in design and development of complex applications in object-oriented Python You bring hands-on knowledge of relational databases, SQL & ORM technologies and PostgreSQL You are open to the approach of a full-stack developer (incl. testing, web frontend, backend, monitoring, DevOps thinking, data) and have experience with test frameworks such as pytest You have experience with Cloud technologies, preferably Google Cloud and the related architecture concepts, such as Terrafom, PubSub, BigQuery. Ideally, you also have understanding of Cloud Storage, Cloud Run and/or VertexAI You can demonstrate a deep understanding of software architecture principles, design patterns as well best practices in security and operations You are willing to constantly expand your knowledge in new areas and technologies Ideally, you have several years of professional experience in the development and operationalization of data and AI products, as well as a deep understanding and practical experience in managing ML models using modern MLOps practices Ideally, you also have experience developing and maintaining Next.JS-based frontends You speak English fluently and have excellent communication skills to efficiently align with other technical leads, architects, product owners and further stakeholders (*please send your cv in English) Employment contract? Of course. With us you do not have to worry about stable employment. Benefits?We have them! Among other: corporate incentive program, sport card, private medical care. Lunch card?With the cooperation extended and permanent contract, you will receive additional funds to use for meal purchases. Working in a hybrid model? Of course! ! You work with us 2 days a week from home. Work wherever you want? In PAYBACK you have the opportunity. Working 100% remotely, also from European countries for 15 days a year. ‚ÄéFlexible working hours? Sounds great! We start working between 7 to 10. Trainings?Of course. We provide training to develop hard and soft skills. Convenient location? Sure! We invite you to our new office at Rondo Daszy≈Ñskiego, but we are currently also working remotely. Dress code?We definitely say no. There are no rigid dress code rules in our company, sneakers are more than welcome. Friendly atmosphere at work? Yes! In PAYBACK, people are the most important asset‚Äé. ‚ÄéSomething is missing? Open communication is our priority, so dare to ask!‚Äé",[],Unclassified,Unclassified
Full-time,Mid,B2B,Remote,260,Data Scientist with German,in4ge sp. z o.o.,"We are looking for a Data Scientist to join a project focused on migrating large volumes of data from the Microsoft Dynamics system. This role requires both strong technical skills in data analysis and transformation, as well as excellent communication abilities to collaborate closely with a German-speaking client throughout the project. Analyze, clean, and prepare data for migration. Perform data migration from Microsoft Dynamics to a new environment. Ensure data consistency, quality, and integrity throughout the migration process. Act as a point of contact for the German-speaking client on all data-related topics. Collaborate with the project team on data structure and mapping. Experience in a Data Scientist, Data Analyst, or Data Engineer role. Fluent German ‚Äì required for daily communication with the client. Proven experience working with large datasets, including data cleaning, structuring, and migration. Familiarity with Microsoft Dynamics or similar ERP/CRM systems is a plus. Hands-on knowledge of tools such as SQL, Python, Power BI, or similar. Strong analytical mindset and ability to work independently. Excellent communication skills and client-facing experience. Spanish language skills. Previous experience in data migration projects. Knowledge of data cleaning, data mapping, and ETL processes. Fully remote work with flexible working hours - EMEA Timezone. Long-term collaboration on B2B contract. Opportunity to work on complex cloud projects for international clients. Professional growth in a highly skilled and supportive team. Collaborative and open working culture.","[{""min"": 15000, ""max"": 27000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,261,Data Engineer ETRM domain,INFOPLUS TECHNOLOGIES,"Job Description We are seeking a skilled ETRM Data Engineer to join our data engineering team and support critical initiatives in the Energy Trading and Risk Management (ETRM) domain. This role involves building robust data pipelines, integrating trading systems, and ensuring data quality across platforms such as Azure Data Factory , Databricks , and Snowflake . You will collaborate closely with traders , analysts , and IT teams to design and implement scalable, high-performance data solutions that power decision-making in fast-paced trading environments. Key Responsibilities Design, develop, and maintain scalable data pipelines and ETRM systems Lead data integration projects within the energy trading ecosystem Integrate data from ETRM platforms such as Allegro , RightAngle , and Endur Build and optimize data storage solutions using Data Lake and Snowflake Develop and orchestrate ETL/ELT workflows using Azure Data Factory and Databricks Write efficient, production-grade Python / PySpark code for data processing and analytics Build and expose APIs using FastAPI for data services Ensure data quality, consistency, and reliability across complex systems Work closely with stakeholders to translate business requirements into technical data solutions Optimize and enhance data architecture for scalability and performance Mandatory Skills Strong experience with Azure Data Factory (ADF) Proficient in Data Lake architecture and best practices Hands-on expertise with Snowflake and SQL Solid experience in Python and PySpark Knowledge of FastAPI for building scalable APIs Proven work with Databricks in production environments Nice to Have Domain experience in ETRM / energy trading systems Familiarity with Streamlit for internal dashboards Experience integrating with Allegro , RightAngle , or Endur trading platforms","[{""min"": 32000, ""max"": 36800, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,262,Data Engineer,emagine Polska,"PROJECT INFORMATION: Industry: Construction Assignment type: B2B Start: September Work model: Hybrid model (2 days/week in office - Warsaw) Project length: 12 months + extensions Project language: English We are looking for a dedicated Data Engineer/ BI Developer to join our Danish client's BI scrum team of six members, who develop and maintain our BI back-end solutions. The team you will join works in close collaboration with our highly innovative departments and companies, where you identify and build the foundation for their new BI reports. Reports covering everything from working environment, diversity, IOT machine data to the more financial. Our team is at the forefront of digitization and automation, focused on streamlining the way we work by processing and presenting data through Business Intelligence tools. The role involves collaborating with various departments to develop insightful BI reports that enhance decision-making and operational efficiency. Main Responsibilities Develop and maintain BI back-end solutions. Collaborate with various teams to identify and implement new BI reporting frameworks. Analyze data trends and provide actionable insights to colleagues. Facilitate the transition to cloud environments and integrate new data sources. Contribute to the development and execution of our digital strategies. Identify and implement new data sources and improve existing data frameworks. Work closely with cross-functional teams to define requirements for BI initiatives. Analyze complex data sets and translate findings into actionable insights. Contribute to cloud integration and Data Lakehouse projects. Key Requirements +3 years of experience in BI area Experience in Business Intelligence and data analysis. Strong understanding of Python and SQL at an advanced level. Strong Experience with Databricks Basic experience with DevOps practices Knowledge of cloud-based solutions, particularly Azure. Capacity to perform dimensional data modeling. Experience with structured and unstructured data sources (API, SQL). Adept at communicating complex ideas effectively. Good problem-solving skills and a proactive attitude toward technological advancements. Nice to Have Familiarity Data Factory. Knowledge of MS Dynamics and process automation. Familiarity with Power BI and data transformation tools.","[{""min"": 170, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,263,"Data Engineer - Spark, Scala/Java",Link Group,"Developing, managing, and optimizing data pipelines. Engaging with team developers and architects to shape the process implementation framework. Estimating workload and regularly updating progress on solution development. Cooperating closely with stakeholders to refine solution details and validate requirements. Supporting business teams in understanding technical possibilities, offering optimal solutions, and clarifying constraints. Establishing efficient workflows, proposing, and executing process enhancements. Proficiency in Scala or Java with substantial experience. Minimum 4‚Äì5 years of relevant experience Hands-on expertise in working with Spark. Familiarity with CI/CD methodologies and tools such as GitHub Actions. Knowledge of cloud platforms (GCP) and infrastructure as code (Terraform) is a plus. Experience with Airflow is an added advantage. Background in data pipeline testing is beneficial. A business- and product-centric approach with direct experience in stakeholder collaboration. Banking sector experience is a plus. Prior work experience in Scrum and agile methodologies. Strong analytical skills with the ability to address intricate technical challenges. Passion for tackling challenges that foster both personal and professional development. A collaborative mindset with a willingness to learn and grow. Proficiency in English is essential.","[{""min"": 22000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,264,üëâ Senior GenAI Engineer (Future Opening),Xebia sp. z o.o.,"üü£ About project: This role focuses on developing and deploying AI applications using tools such as Azure AI Studio, vector databases, and Retrieval-Augmented Generation (RAG) frameworks. You will collaborate closely with senior team members to deliver robust, high-quality solutions that drive innovation. üü£ You will be: designing, developing, and implementing AI solutions using Python, with a focus on LLMs, vision models, and generative AI technologies, building, testing, and optimizing RAG applications, leveraging effective prompt engineering techniques to improve model performance, integrating AI models with Azure AI Studio, SharePoint, and Azure Blob Storage for efficient deployment and data handling, utilizing vector databases and Agentic frameworks (e.g., LlamaIndex) to enhance the functionality and intelligence of AI systems, implementing event-driven architectures using tools like Event Hub and Kafka for real-time data processing and scalability, collaborating with the AI Lead and team members to troubleshoot issues, test models, and ensure successful deployment, writing clean, efficient, and well-documented code adhering to best practices and version control standards, staying current with emerging AI tools, frameworks, and technologies to continuously improve development processes and outcomes. üü£ Your profile: Bachelor‚Äôs degree in computer science, Data Science, Engineering, or a related field, 4+ years of hands-on experience in AI/ML development, with an emphasis on generative AI and related technologies, strong experience with Python, REST APIs, Git proven expertise in developing and deploying LLMs, vision models, vector databases, and RAG applications, strong proficiency in Azure AI Studio, Azure Blob Storage, Event Hub, Kafka, familiarity with Agentic frameworks and tools like LlamaIndex for advanced AI development, ability to thrive in a collaborative team environment while managing multiple tasks effectively, good verbal and written communication skills in English (min. B2). üü£ Nice to have: exposure to cloud fundamentals (Azure preferred) and containerization tools like Docker, experience with CI/CD pipelines for AI model deployment, understanding RESTful services and API integration. üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 23500, ""max"": 33500, ""type"": ""Net per month - B2B""}, {""min"": 18000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,265,Specjalista ds. Analiz w Departamencie Sprzeda≈ºy,Polkomtel Sp. z o. o.,"Opis stanowiska: Przygotowywanie analiz dla managementu w oparciu o dane rynkowe Analiza danych historycznych ‚Äì identyfikacja sezonowo≈õci, trend√≥w oraz anomalii Wspieranie projekt√≥w sprzeda≈ºowych od strony analitycznej ‚Äì rekomendacja struktury raportowania, przygotowanie podsumowa≈Ñ oraz prezentacja wynik√≥w Przek≈Çadanie danych liczbowych na konkretne rekomendacje biznesowe oraz praktyczne wnioski wspierajƒÖce realizacjƒô cel√≥w Udzia≈Ç w spotkaniach z klientami wewnƒôtrznymi (dzia≈Çy Sprzeda≈ºy i Trade Marketingu), przygotowywanie analiz i raport√≥w zgodnie z ich potrzebami Przygotowywanie analiz ad hoc oraz raport√≥w cyklicznych z mo≈ºliwo≈õciƒÖ ich dalszej automatyzacji Wsparcie w monitorowaniu i optymalizacji istniejƒÖcych proces√≥w raportowania Automatyzacja proces√≥w raportowych oraz analitycznych z wykorzystaniem narzƒôdzi takich jak Power Query, SQL Projektowanie oraz optymalizacja zapyta≈Ñ SQL w celu poprawy efektywno≈õci i wydajno≈õci Wymagania: Minimum 3 lata do≈õwiadczenia na podobnym stanowisku Bardzo dobra znajomo≈õƒá pakietu MS Office, w szczeg√≥lno≈õci: MS Excel (formu≈Çy, tabele przestawne, Power Query, VBA na poziomie bardzo dobrym) MS Access (dobra znajomo≈õƒá) Bardzo dobra znajomo≈õƒá jƒôzyka SQL ‚Äì warunek konieczny Mile widziana znajomo≈õƒá ≈õrodowisk takich jak Microsoft SQL Server (MS SQL), Teradata Umiejƒôtno≈õƒá pracy na du≈ºych zbiorach danych Umiejƒôtno≈õƒá organizacji i planowania pracy w≈Çasnej oraz pracy zespo≈Çowej Otwarto≈õƒá, inicjatywa oraz nastawienie na realizacjƒô cel√≥w Chƒôƒá rozwoju i samodzielnego wdra≈ºania nowych pomys≈Ç√≥w i rozwiƒÖza≈Ñ Mile widziane: Do≈õwiadczenie z narzƒôdziami do wizualizacji danych (np. Power BI, Tableau) Zrozumienie zagadnie≈Ñ zwiƒÖzanych z przetwarzaniem du≈ºych zbior√≥w danych (Big Data) Oferujemy: Zatrudnienie w oparciu na umowƒô o pracƒô Praca w zgranym zespole Praca w biurze z mo≈ºliwo≈õciƒÖ pracy zdalnej raz w tygodniu DobrƒÖ atmosferƒô i przyjazne ≈õrodowisko pracy Pakiet benefit√≥w (MultiSport, opieka medyczna, oferty pracownicze)",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,266,Senior Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Senior Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Development and maintenance of a large platform for processing automotive data. A significant amount of data is processed in both streaming and batch modes. The technology stack includes Spark, Cloudera, Airflow, Iceberg, Python, and AWS. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Centralized reporting platform for a growing US telecommunications company. This project involves implementing BigQuery and Looker as the central platform for data reporting. It focuses on centralizing data, integrating various CRMs, and building executive reporting solutions to support decision-making and business growth. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. üöÄ Your main responsibilities: Develop and maintain a high-performance data processing platform for automotive data, ensuring scalability and reliability. Design and implement data pipelines that process large volumes of data in both streaming and batch modes. Optimize data workflows to ensure efficient data ingestion, processing, and storage using technologies such as Spark, Cloudera, and Airflow. Work with data lake technologies (e.g., Iceberg) to manage structured and unstructured data efficiently. Collaborate with cross-functional teams to understand data requirements and ensure seamless integration of data sources. Monitor and troubleshoot the platform, ensuring high availability, performance, and accuracy of data processing. Leverage cloud services (AWS) for infrastructure management and scaling of processing workloads. Write and maintain high-quality Python (or Java/Scala) code for data processing tasks and automation. üéØ What you'll need to succeed in this role: At least 5 years of commercial experience implementing, developing, or maintaining Big Data systems, data governance and data management processes. Strong programming skills in Python (or Java/Scala): writing a clean code, OOP design. Hands-on with Big Data technologies like Spark , Cloudera, Data Platform, Airflow, NiFi, Docker, Kubernetes, Iceberg, Hive, Trino or Hudi. Excellent understanding of dimensional data and data modeling techniques. Experience implementing and deploying solutions in cloud environments. Consulting experience with excellent communication and client management skills, including prior experience directly interacting with clients as a consultant. Ability to work independently and take ownership of project deliverables. Fluent in English (at least C1 level). Bachelor‚Äôs degree in technical or mathematical studies. ‚ûï Nice to have: Experience with an MLOps framework such as Kubeflow or MLFlow. Familiarity with Databricks, dbt or Kafka. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn ).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,267,Senior Data Engineer,Sparta Global,"Senior Data Analytics Specialist ‚Äì Financial Services (Krak√≥w) Join one of our leading financial clients in Krak√≥w and play a key role in delivering accurate, timely, and insightful data and management information. You‚Äôll be supporting Risk, Compliance, and Finance functions within a major global financial institution. Role Overview As a Data Analytics Specialist, you will be responsible for ingesting data from internal and external sources, performing quality assessments, and creating visualisations. The role also includes peer benchmarking, platform security, capacity management, and active contribution to global risk data projects. Key Responsibilities Perform data analysis, ensuring data integrity and suitability for intended use. Collaborate with model development and monitoring teams to understand data requirements and governance standards. Contribute to planning the data and business process architecture roadmap. Prepare and present insights to senior stakeholders, fostering strong cross-functional relationships. Identify opportunities for process improvement and drive efficiency initiatives. About You ‚Äì Qualifications & Experience A bachelor‚Äôs degree in IT, Computer Science, or a numerate discipline. At least 5 years‚Äô experience in data analytics or a related role. Proficiency in programming and data tools such as SAS, Python, PySpark, and SQL. Solid understanding of business analysis, data architecture, and project management principles. Strong organisational and analytical skills with the ability to manage multiple tasks effectively. Familiarity with JIRA, Confluence, and an aptitude for working under pressure. Fluency in English (spoken and written).","[{""min"": 20000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,268,Lead Azure Data Engineer - Cybersecurity ‚òÅÔ∏èüí•,ITDS,"Join us, and enhance data analytics capabilities for a secure digital future! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office) As a Lead Data Engineer , you will be working for our client, a global leader in cybersecurity and data analytics. The Client‚Äôs team focuses on advancing cybersecurity capabilities through innovative data engineering and analytics solutions. You will be contributing to the development and enhancement of their data lake and analytics platform, supporting cutting-edge technologies and cloud infrastructures. Your role will involve collaborating with data engineers and cybersecurity professionals to build scalable data pipelines and ensure the continuous growth of security analytics systems. Your main responsibilities: Designing and implementing data pipelines for cybersecurity use cases Ingesting and provisioning raw datasets and curated data assets Driving improvements in the reliability and frequency of data ingestion Supporting and enhancing data ingestion infrastructure and pipelines Automating and optimizing data engineering workflows Identifying and onboarding new data sources for cybersecurity use Conducting exploratory data analysis for new schemas Collaborating with platform engineers for cloud infrastructure and platform engineering Ensuring the operational efficiency of production data pipelines Monitoring and enhancing cloud-based data transport and data cleaning processes You're ideal for this role if you have: 5+ years of experience in data engineering or cloud infrastructure engineering Proficiency in programming languages like Python, Java, or C# Experience with cloud technologies, especially Azure (Azure Data Factory, Azure Databricks, etc.) Strong understanding of data engineering principles and data pipeline design Expertise in building and maintaining ETL workflows across disparate datasets Knowledge of data acquisition, cloud-based data pipelines, and data transport Experience with SQL, Kusto query language, or similar query languages Familiarity with cloud cost optimization and data asset curation Ability to work in a fast-paced, collaborative environment Knowledge of cybersecurity principles (preferred but not required) It is a strong plus if you have: Experience with Infrastructure-as-Code tools such as Terraform or Ansible Familiarity with big data technologies like Kafka, Spark, and streaming services Previous exposure to Security Information & Event Management (SIEM) systems Experience in real-time analytics deployment for large-scale datasets Understanding of cybersecurity frameworks such as NIST, ISO27001, or OWASP Knowledge of cloud security practices and network protocols Experience with cloud-based security orchestration, automation, and response (SOAR) technologies We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #6917 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 1040, ""max"": 1450, ""type"": ""Net per day - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,269,SQL Server Production DB Administrator,Aristocrat Interactive,"Join Aristocrat in Bringing Joy to Life Through the Power of Play. Be part of our growing global team where people come first because they fuel our success. Here, it‚Äôs All About the Player and we create a world of its own for everyone, everywhere with premium casino and world-class digital and mobile products. Our value of Good Business, Good Citizen ensures that corporate growth and responsible gameplay go hand in hand to help our industry remain sustainable. Aristocrat offers a highly diverse, inclusive, and equitable culture as well as the professional tools and resources to ensure your Talent is Unleashed. We achieve success through Collective Brilliance. Individually, we are great, but together, we are unstoppable. Aristocrat enhances the player experience‚Äîand careers‚Äîwith opportunities featuring meaningful challenges, strong advancement potential, and global exposure. Explore a Career with Our Team: Aristocrat Interactive We are seeking a quality and experienced DBA Production Expert ‚Äî infrastructure and applicative. The DBA will be responsible for all DB aspects in the production environment. We have a hybrid model of work: 2-3 times in a week working from the office What You will Do Performs performance tuning, Query optimization, database installations, configurations, updates, backups and monitoring Provide support for multiple environments ‚Äî development, test and production Anticipates, mitigates, identifies, responds and resolves problems affecting database performance, efficiency, and availability SQL Server Installation, Upgrades, Migrations and Configuration Troubleshoot and resolve database performance/connectivity issues Follow Problem Management processes to troubleshoot and resolve recurring issues, proactively monitor and respond to MSSQL database alerts. Improve and support processes to ensure zero application downtime on critical issues or during DB deployment of new application releases Work according to documentation, guidelines and policies of working procedures Provide support for the operation teams 24/7 ‚Äî on call in off work hours only for critical issues (one week a month) What We're Looking For At least 3 years of experience as a DBA/MS SQL Server Developer Deep and wide understanding of databases‚Äô structure and architecture Writing complicated T-SQL, stored procedures and functions Motivated, team player with ability to learn fast and great analysis skills What We offer High-level compensation on an employment contract and regular performance based salary and career development reviews; Medical insurance (health), employee assistance program; Paid vacation, holidays and sick leaves; Multisport Card; English classes with native speakers, trainings, conferences participation; Referral program; Team buildings, corporate events. Why Aristocrat? Aristocrat is a world leader in gaming content and technology, and a top-tier publisher of free-to-play mobile games. Aristocrat has three operating business units, spanning regulated land-based gaming (Aristocrat Gaming), social casino (Product Madness), and regulated online real-money gaming (Aristocrat Interactive). Our team of over 7,300 employees worldwide is united by our company‚Äôs mission to bring joy to life through the power of play. We deliver great performance for our B2B customers and bring joy to the lives of the millions of people who love to play our casino and mobile games. And while we focus on fun, we never forget our responsibilities. We strive to lead the way in responsible gameplay, and to lift the bar in company governance, employee wellbeing and sustainability. We‚Äôre a diverse business united by shared values and an inspiring mission to bring joy to life through the power of play. Aristocrat is proud to be an equal opportunity employer. We celebrate diversity and do not discriminate based on gender, race, religion, color, national origin, sex, sexual orientation, age, veteran status, disability status, or any other applicable characteristics protected by law. Diversity and Inclusion are integral to our values of Talent Unleashed, Collective Brilliance, Good Business, Good Citizen, and It‚Äôs All About the Player.",[],Database Administration,Database Administration
Full-time,Mid,B2B,Remote,270,Data Analyst (she/he/they),≈ªabka Future,"W ≈ªabce Future napƒôdzamy cyfrowƒÖ transformacjƒô Grupy ≈ªabka. Tworzymy nowoczesny, skalowalny ekosystem technologiczny, kt√≥ry ≈ÇƒÖczy dane, narzƒôdzia i procesy w sp√≥jnƒÖ ca≈Ço≈õƒá. Dzia≈Çamy szybko, z pomys≈Çem i w zgranych zespo≈Çach. Stawiamy na skalowalne technologie, realny wp≈Çyw i ciƒÖg≈Çe doskonalenie üíô",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Hybrid,271,Data Visualization Specialist (Tableau),ITDS,"Data Visualization Specialist (Tableau) Join us, and shape how data tells its story! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As a Data Visualization Specialist, you will be working for our client, a leading global financial institution focused on optimizing financial resource management through innovative IT solutions. You will be joining a high-performing team to design and deliver advanced data visualizations that turn complex datasets into meaningful insights. Your work will directly support strategic decision-making by developing and maintaining dashboards, ensuring data integrity, and promoting best practices in data sharing. This is an exciting opportunity to shape visualization standards and influence how data is consumed across the organization. Your main responsibilities: Developing advanced dashboards and reports using Tableau Managing data sources to ensure accuracy and consistency Collaborating with stakeholders to gather analytical requirements Maintaining and overseeing project sites on Tableau Server Documenting dashboards, data sources, and development processes Promoting best practices for data sharing and user access management Ensuring optimal performance of Tableau workbooks for large datasets Supporting infrastructure and architectural requirements for Tableau deployment Following internal control standards and compliance procedures Working with cross-functional teams to deliver effective visual solutions You're ideal for this role if you have: 3+ years of professional experience in data visualization or analysis Proven proficiency in Tableau development and dashboard optimization Ability to handle large data sets while ensuring performance efficiency Experience managing Tableau Server project sites Strong analytical mindset with a high level of mathematical competence Familiarity with data governance and compliance standards Ability to collaborate effectively with cross-functional stakeholders Excellent communication and documentation skills Strong attention to detail and problem-solving abilities Demonstrated ability to work in fast-paced, high-stakes environments It is a strong plus if you have: Working knowledge of Tableau administration and architecture Experience developing dashboards in QlikSense Understanding of infrastructure best practices for enterprise data tools Familiarity with regulatory environments in financial institutions Knowledge of offshoring practices and documentation workflows We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7098 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 20800, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,272,Senior Data Engineer,Xelerit,"‚úÖBardzo dobra znajomo≈õƒá Pythona i SQL . ‚úÖPraktyczne do≈õwiadczenie w pracy z Databricks . ‚úÖDo≈õwiadczenie w pracy z web API , projektowaniu i tworzeniu pipeline'√≥w ETL oraz budowaniu wydajnych modeli danych . ‚úÖUmiejƒôtno≈õƒá efektywnej pracy w cyklach sprintowych w dynamicznym ≈õrodowisku startupowym. ‚úÖZnajomo≈õƒá jƒôzyka angielskiego na poziomie min. B2 ‚úÖDo≈õwiadczenie z Weaviate LLM lub ElasticSearch ‚úÖZnajomo≈õƒá technik web scrapingu ‚úÖZnajomo≈õƒá Terraform ‚úÖDo≈õwiadczenie w tworzeniu pipeline'√≥w CI w GitHub ‚úÖProjektowanie, rozwijanie i optymalizacja proces√≥w ETL , integracjƒô z web API oraz przetwarzanie i modelowanie danych. ‚úÖTworzenie i utrzymywanie pipeline'√≥w danych w ≈õrodowisku Databricks . ‚úÖBudowa i optymalizacja modeli danych , zapewniajƒÖcych wydajno≈õƒá i skalowalno≈õƒá przetwarzanych danych. ‚úÖTworzenie i rozwijanie mechanizm√≥w wyszukiwania danych przy wykorzystaniu Weaviate LLM lub ElasticSearch . ‚úÖRealizacja zada≈Ñ zwiƒÖzanych z web scrapingiem w przypadku konieczno≈õci pozyskania danych ze ≈∫r√≥de≈Ç zewnƒôtrznych. ‚úÖWsp√≥≈Çpraca z zespo≈Çem programist√≥w oraz udzia≈Ç w planowaniu i realizacji sprint√≥w w metodyce Agile . ‚úÖTworzenie i rozwijanie pipeline'√≥w CI/CD w ≈õrodowisku GitHub Actions . ‚úÖWdra≈ºanie i utrzymywanie infrastruktury w modelu Infrastructure as Code przy u≈ºyciu Terraform. ‚úÖPraca w dynamicznym ≈õrodowisku startupowym üí≤ D≈Çugoterminowa wsp√≥≈Çpraca B2B, nasze wide≈Çki: 24 000-30 000 pln net +VAT miesiƒôcznie üåû 26 dni p≈Çatnych dni wolnych rocznie üìç Praca zdalna + od czasu do czasu spotykamy siƒô w przestrzeni coworkingowej oraz integracjach w Gda≈Ñsku (nic na si≈Çƒô, tylko dla chƒôtnych) ü•á Bud≈ºet na rozw√≥j osobisty + Opieka medyczna: Medicover +Karta sportowa : Medicover Sport ü§∏‚Äç‚ôÇÔ∏è Lu≈∫na atmosfera , brak biurokracji i formalno≈õci","[{""min"": 24000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,273,Regulatory Reporting IT Analyst / DevOps,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: üåçhybrid work model from Warsaw - 3 days HO, 2 days from the office üìù ASAP project start Responsibilities: Design and develop solutions for regulatory reporting on the Snowflake platform Collaborate with developers and SMEs to gather requirements and translate them into technical designs Build and maintain ETL pipelines and orchestration workflows Apply DevOps practices to ensure high code quality, automation, and efficient deployment Support testing, troubleshooting, and documentation of the implemented solutions Requirements: Minimum 3 years of a professional experience Solid experience with DevOps practices and tools Strong hands-on expertise in Snowflake Proficiency in Python development Experience designing and building ETL pipelines Familiarity with orchestration tools (e.g., Airflow, Prefect, etc.) Very good command of English (oral and written) Nice to have: Knowledge of DORA (DevOps Research and Assessment) principles Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 140, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,274,Data Engineer MS Fabric,1dea,"Dla jednego z du≈ºych klient√≥w poszukujemy osoby do roli: Data Engineer MS Fabric! Warunki zaanga≈ºowania: Lokalizacja: 100% zdalnie Start: ASAP (akceptujemy kandydatury z max 1msc okresem wypowiedzenia) Stawka (ustalana indywidualnie): 130 - 160 PLN net / h Zaanga≈ºowanie: B2B (outsourcing z 1dea), full-time, d≈Çugofalowo Minimum 3 lata do≈õwiadczenia jako Data Engineer, Znajomo≈õƒá technologii: Azure Data Factory, Azure, PySpark, Microsoft Fabric, Solidne podstawy w zakresie modelowania danych oraz architektury hurtowni danych. Jƒôzyk angielski na poziomie min. B2 Przejrzysty model wsp√≥≈Çpracy: Zatrudnienie przez 1dea na podstawie umowy B2B Stabilne i bezpieczne ≈õrodowisko pracy: Do≈ÇƒÖczysz do firmy z solidnƒÖ pozycjƒÖ na rynku Nowoczesne wyposa≈ºenie: Firma zapewnia nowoczesny sprzƒôt, oprogramowanie i konfiguracjƒô Elastyczny czas pracy: Mo≈ºliwo≈õƒá pracy w elastycznych godzinach Praca zdalna: mo≈ºliwa w 100% Profesjonalne doradztwo i wsparcie: Profesjonalne doradztwo i wsparcie w rozwoju kariery od do≈õwiadczonego zespo≈Çu specjalist√≥w 1dea Przyjemna atmosfera w zespole: Cenimy sobie kole≈ºe≈Ñsko≈õƒá, otwarto≈õƒá, szacunek, wzajemnƒÖ pomoc i wsparcie w rozwijaniu kompetencji zar√≥wno w≈Çasnych, jak i koleg√≥w i kole≈ºanek z zespo≈Çu Kultura kreatywno≈õci: Wspieramy kulturƒô kreatywno≈õci. Ka≈ºdy cz≈Çonek zespo≈Çu ma mo≈ºliwo≈õƒá proponowania w≈Çasnych pomys≈Ç√≥w i rozwiƒÖza≈Ñ, a jego g≈Ços jest zawsze brany pod uwagƒô","[{""min"": 130, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,275,Programista PL/SQL ze znajomo≈õciƒÖ integracji,4IT Solutions,"Poszukujemy Programisty PL/SQL ze znajomo≈õciƒÖ integracji dla naszego klienta ‚Äì globalnego lidera w dziedzinie baz danych, rozwiƒÖza≈Ñ chmurowych i oprogramowania biznesowego. To wyjƒÖtkowa okazja do pracy dla jednej z wiodƒÖcych firm tworzƒÖcych kompleksowe oprogramowanie bazodanowe oraz nowoczesne rozwiƒÖzania chmurowe oparte na sztucznej inteligencji. Dziƒôki ciƒÖg≈Çemu rozwojowi technologii oraz inwestycjom w nowoczesne rozwiƒÖzania, firma pozostaje kluczowym graczem w sektorze IT, dostarczajƒÖc innowacyjne produkty dla firm na ca≈Çym ≈õwiecie. Dynamiczny rozw√≥j oraz globalna spo≈Çeczno≈õƒá ekspert√≥w IT sprawiajƒÖ, ≈ºe jest to idealne miejsce dla ambitnych specjalist√≥w. Na poczƒÖtek konkrety: Mo≈ºliwa forma wsp√≥≈Çpracy: B2B Stawka: 130-150 PLN/h Tryb pracy: hybryda Warszawa (1-2x w tygodniu w biurze) Wymiar pracy: 1FTE O projekcie: Projekt dotyczy rozwoju kodu w PL/SQL oraz integracji system√≥w w architekturze SOA dla klienta z sektora publicznego. Zakres obowiƒÖzk√≥w: Modelowanie danych oraz tworzenie struktur danych na potrzeby integracji system√≥w w architekturze SOA Rozw√≥j i optymalizacja kodu PL/SQL, w tym implementacja zapyta≈Ñ, procedur sk≈Çadowych i interfejs√≥w us≈Çug z naciskiem na wydajno≈õƒá i skalowalno≈õƒá Analiza i optymalizacja zapyta≈Ñ SQL na du≈ºych zbiorach danych Praca zgodnie z wymaganiami SLA (Service Level Agreement) Wymagania: Minimum 5-letnie do≈õwiadczenie na podobnym stanowisku Bardzo dobra znajomo≈õƒá SQL i PL/SQL Do≈õwiadczenie w pracy z du≈ºymi zbiorami danych Umiejƒôtno≈õƒá optymalizacji zapyta≈Ñ w bazie danych Oracle Kompetencje w projektowaniu oraz implementacji interfejs√≥w us≈Çug w architekturze SOA Praktyczne do≈õwiadczenie w wykorzystywaniu SOAP, REST, XQuery, XSLT w projektach integracyjnych Mile widziane: Certyfikaty Oracle PL/SQL Znajomo≈õƒá Oracle Service Bus, Oracle SOA Suite, Oracle Weblogic ServerZa Oferujemy: Atrakcyjna lokalizacja biura Brak dress code‚Äôu Mo≈ºliwo≈õƒá pracy zdalnej Przyjazna niekorporacyjna atmosfera Spotkania integracyjne Opis procesu rekrutacji: Wstƒôpna rozmowa telefoniczna z naszym przedstawicielem - 4IT Solutions (ok 20min) Zdalna rozmowa techniczna z osobami z zespo≈Çu naszego Klienta Spotkanie zapoznawcze z Managerem w biurze ... i finalizujemy rozmowy","[{""min"": 130, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,Permanent,Hybrid,276,Senior Data Analyst - Product Data,Bayer Sp. z o.o.,"Senior Data Analyst ‚Äì Product Data Are you ready to make a significant impact in the world of data analytics and data management? We are seeking a talented Data Analyst to become a vital part of our dynamic Data Assets, Analytics, and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in building and maintaining our core data assets across various domains, ensuring their completeness, semantics and quality. You will work closely with data owners, product managers, data engineers, data architects, data stewards, data governors and data scientist to enable our data analytics solutions, enhance the strategic value of our data assets and enable cutting-edge AI solutions and support data-driven decision-making. If you‚Äôre passionate about transforming data into actionable insights and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Collaborate with data owners, architects, engineers, stewards, governors, scientists and product managers to understand objectives and requirements for data assets. Serve as a liaison between technical teams and business stakeholders to ensure data assets meet both business requirements and technical standards. Take ownership of a data asset roadmap. Co-develop data strategy and governance frameworks. Proactively identify new datasets from across the organization and collaborate with data architects and data engineers to integrate them into the core data assets Define transformation logic and enrich data to create valuable KPIs and features in the consumption layer. Develop and maintain clear semantics and metadata for data assets, ensuring that all data is well-documented and easily understandable. Ensure data quality, availability, and completeness through implementation of quality checks, validation processes, and continuous monitoring in collaboration with data architects and data engineers. Serve as the primary point of contact for users of data assets, providing guidance and support to help them understand and utilize the data effectively. Analyze complex datasets to extract actionable insights that streamline the development of analytics and AI solutions. Become a go-to expert for product data. Qualifications & Competencies: Master's degree in Statistics, Computer Science, Data Management, Data Science or a related field. 5+ years of experience as a Data Analyst or Data Steward, preferably within the consumer-packaged goods, FMCG, pharmaceutical or healthcare industry. Strong knowledge of data management principles, data quality frameworks, and metadata management practices and tools. Understanding of data lineage and data cataloging concepts. Business acumen in the area of product supply analysis. Familiarity with SAP product-related modules (e.g. Product Lifecycle Management, Materials Management, Quality Management, Advanced Planner and Optimizer, S/4HANA Supply Chain) and Supply Chain Planning Solutions (e.g. OMP). Experience with data manipulation and analysis using Azure Databricks, SQL and Python. Familiarity with relational databases (PostgreSQL, MSSQL) and data modelling. Excellent analytical and problem-solving skills with a keen attention to detail. Strong understanding about data compliance & security standards such as data privacy regulations (GPDR, HIPAA), EU AI Act, management of confidential data, and experience with measures to mitigate data risks. Strong communication skills, with the ability to present complex data in a clear and understandable manner. Interest and experience with AI tools supporting data analysis and stewardship is a plus. Experience with preparing data for AI solutions (e.g. traditional machine learning models, AI Agents) is a plus. Experience in IT product management is a plus. Ability to work collaboratively in a team-oriented environment. Fluent in English, both written and spoken. What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,277,Database Engineer,VIER,"VIER is a fast-growing company in the IT sector. We are always looking for motivated and creative minds who like to make a difference. At VIER, ‚Äòteamwork‚Äô and ‚Äòco-design‚Äô are not just empty words. For us, people take centre stage. If you are prepared to work proactively and are not afraid of change, we look forward to taking you on a journey with us. Our actions are guided by our Guiding Principles, which emphasise openness and transparency. We also have a unique participative application process in which your future team decides whether you come on board. VIER rethinks customer dialogue and communication. With our solutions, we make contact-based business processes more efficient. We improve the customer experience and the user experience. We combine artificial intelligence with human intelligence, expertise with intuition, years of experience with innovation and research. ‚ú® This is what you can expect as a Database Engineer with us: New and further development and operation of our databases in our private cloud/development platform. You are responsible for the technical development and implementation of cloud-native specific concepts and database solutions. Consistent automation will help you to administer and provision our complex database landscape. You support our internal customers (development teams) by providing technical advice on database technology. You keep your finger on the pulse and constantly deepen your knowledge in the database environment and implement it. You actively shape the DevOps culture in the team and in the organization. ‚≠ê This is what you bring to the table: For this position we are looking for someone with several years of relevant professional experience (at least 7 years) as a Database Engineer/DBA. You have expertise with relational databases, especially MySQL, CockroachDB, Galera Cluster. Time series databases and distributed databases are no foreign words for you. Automation is part of your DNA. You should have extensive experience in Ansible. You have experience with modern monitoring (e.g. Grafana, Prometheus, PMM). You are curious and enjoy acquiring knowledge and are interested in current database technologies in the cloud-native environment. You are an absolute team player. You are fluent in English. Willingness to work at night if necessary. You have sound knowledge in the administration of Linux server systems.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,278,BI Systems Analyst,Britenet,"Do≈ÇƒÖcz do innowacyjnego ≈õrodowiska i zmieniaj ≈õwiat razem z Britenet w projekcie dla jednego z naszych klient√≥w. Jeste≈õmy europejskƒÖ firmƒÖ IT z silnymi polskimi korzeniami. Kreuj z nami ≈õmia≈Çe rozwiƒÖzania i inspiruj do tego innych. Nasze oczekiwania: Do≈õwiadczenie w pracy z co najmniej jednym z narzƒôdzi: Tagetik (mile widziane do≈õwiadczenie we wsparciu u≈ºytkownik√≥w) IBM Cognos Controller / TM1 (Planning Analytics) CDM / Disclosure Management Znajomo≈õƒá proces√≥w finansowo-kontrolingowych (mile widziane do≈õwiadczenie z obszaru raportowania sprawozda≈Ñ finansowych) Umiejƒôtno≈õƒá pracy z u≈ºytkownikami biznesowymi (komunikatywno≈õƒá, proaktywno≈õƒá) Gotowo≈õƒá do pracy hybrydowej z biura w Warszawie Mile widziane: Znajomo≈õƒá SQL, ETL, integracji system√≥w finansowych Do≈õwiadczenie z systemami ERP lub BI (np. SAP, Oracle, Power BI) Kluczowe zadania: Wsparcie u≈ºytkownik√≥w ko≈Ñcowych i konsultacje w zakresie korzystania z aplikacji Udzia≈Ç w utrzymaniu i rozwoju rozwiƒÖza≈Ñ opartych na: Tagetik (planowanie, konsolidacja finansowa) IBM Cognos Controller / TM1 CDM ‚Äì obs≈Çuga procesu publikacji raport√≥w gie≈Çdowych Wsp√≥≈Çpraca z zespo≈Çami finansowymi, controllingu oraz IT Wsparcie projekt√≥w wdro≈ºeniowych i zmian systemowych Monitorowanie poprawno≈õci dzia≈Çania system√≥w i rozwiƒÖzywanie incydent√≥w",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,279,Data Scientist,Experis Manpower Group,"Tasks: Analyze Standard Operating Procedures (SOPs), system logs, and structured datasets to support AI agent training and optimization. Validate AI-generated outputs and provide actionable feedback for continuous model improvement. Develop dashboards and define metrics to monitor and report on AI impact and performance. Collaborate closely with software developers and business analysts to align AI solutions with business needs. Requirements: Proven experience (4+ years) in data science, with a strong foundation in data analysis and machine learning. Proficiency in Python and SQL, with hands-on experience in data visualization tools (e.g., Power BI, Tableau, or similar). Familiarity with enterprise data systems such as SAP, OTM, or Denali is highly desirable. Strong analytical thinking, attention to detail, and ability to communicate technical insights to non-technical stakeholders. Ability to work independently in a remote or hybrid environment and manage multiple priorities effectively. Offer: 100% remote work MultiSport Plus Group insurance Medicover Premium e-learning platform","[{""min"": 190, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Senior,Permanent or B2B,Remote,280,Senior Consultant - Data Management & Governance,EPAM Systems,"We are seeking an accomplished, forward-thinking leader in Data Management & Governance to work collaboratively with our client in delivering exceptional business outcomes. In this role, you‚Äôll engage closely with senior executives, shape impactful strategies, and leverage modern data technologies to drive organizational transformation. If you‚Äôre passionate about solving complex challenges and redefining data management excellence, this is the perfect opportunity. Our client is investing heavily in their Data Management & Governance consulting practice, combining efficient data governance with cutting-edge technology to build a dynamic, high-growth environment centered on innovation. With an agile culture, they empower you to tackle complex, high-visibility challenges for leading global organizations. This isn‚Äôt just a job ‚Äì it‚Äôs your chance to be at the forefront of data-driven transformation and make a lasting impact. If you‚Äôre eager to join a high-performing team where your expertise shapes strategic initiatives, we‚Äôd love to hear from you. Responsibilities Engage with senior executives to define and execute enterprise-wide Data Management & Governance strategies, often becoming a ‚Äútrusted advisor‚Äù Lead assessments to help clients advance from current to optimal data governance maturity by developing strategies, roadmaps, and implementation plans Leverage deep business and IT expertise to build Data Management & Governance solutions that drive innovation and create value across client organizations Deliver end-to-end services, from strategy ideation and use case elaboration to MVP implementation and scaling for enterprise adoption Guide architectural teams and lead discussions on modern Data Management architecture, ensuring best practices for data lifecycle management Support business development efforts, including crafting proposals, Statements of Work, and delivering client presentations Develop consulting offerings and reusable templates to enhance delivery efficiency Requirements Bachelor's or Master's degree in information technology or a related field At least 3 years of experience in a consulting role, leading Data Management & Governance initiatives In-depth knowledge in 2-3 of the following areas: Data Governance Operating Models & Policies, Data Quality, Master Data Management, Metadata Management & Data Cataloguing, Data Lineage, or Data Compliance Familiarity with data governance frameworks like DAMA or DCAM, and an understanding of their practical implementation Proven ability to build relationships with directors and C-level stakeholders, understand business needs, and deliver data-driven insights Experience conducting maturity assessments, gap analyses, and developing strategic roadmaps with implementation guidelines Skilled in running client workshops to identify priorities, assess current states, and evaluate readiness for enterprise Data Management & Governance initiatives Knowledge of market trends, technology selection models, and best practices tailored for large-scale application Hands-on experience with the modern data stack, working in agile environments with advanced technologies Ability to work independently, manage small projects with multiple workstreams, or oversee parts of larger engagements Strong verbal and written English skills for leading discussions and producing clear, concise documentation Nice to have Experience in leading data transformation projects Technology expertise with one or several commercial or open-source tools, e.g. Collibra, Alation, Informatica, Ataccama, Azure Purview, Atlas, Talend, Soda Understanding of modern concepts as Data Observability, DataOps and Data Mesh Technology expertise in modern Big Data & Cloud stack: Spark, Snowflake, Databricks, etc Practical expertise in implementation of Data Governance for modern cloud data lakes We offer We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Unclassified,Unclassified
Full-time,Mid,B2B,Remote,281,Data Engineer,in4ge sp. z o.o.,"We are looking for an experienced Data Engineer to join the Data & Analytics team. The ideal candidate will have deep expertise in data engineering with a focus on building Graph Databases using Neo4j, as well as hands-on experience in data ingestion, optimization techniques and Google Cloud Platform (GCP) services. Design, develop, and optimize Graph Database solutions using Neo4j. Build and maintain robust data pipelines for large-scale data ingestion and processing on GCP. Design and optimize data models, schemas, and queries to ensure high performance and scalability. Develop, maintain, and optimize data workflows using GCP data services such as BigQuery, Dataflow (Apache Beam), Dataproc (Apache Spark/Hadoop), or Composer (Apache Airflow). Implement best practices for data warehousing and data lake architectures. Collaborate with cross-functional teams to translate business requirements into technical solutions. Ensure data integrity, quality, and security in all data engineering processes. Contribute to the continuous improvement of engineering standards, tools, and processes. 4+ years of hands-on experience as a Data Engineer, with a minimum of 2 years working specifically with GCP data services. Proven experience building and optimizing Graph Databases with Neo4j. Strong proficiency in SQL with expertise in schema design, query optimization, and handling large datasets. Advanced skills in BigQuery, including complex SQL, partitioning, clustering, and performance tuning. Hands-on experience with at least one GCP data processing service: Dataflow (Apache Beam), Dataproc (Apache Spark/Hadoop), or Composer (Apache Airflow). Proficiency in at least one programming or scripting language such as Python, Java, or Scala. Solid understanding of data warehousing and data lake concepts and best practices. Experience with version control systems such as Git. English proficiency at C1 level (both written and spoken). Professional Data Engineer (PDE) certification or a similar relevant certification. Nice to Have Familiarity with data governance and data security best practices. Experience working in Agile teams and methodologies. Knowledge of machine learning pipelines and integration with data platforms. Fully remote work with flexible working hours. Long-term collaboration on B2B contract. Opportunity to work on complex cloud projects for international clients. Professional growth in a highly skilled and supportive team. Collaborative and open working culture. üí° Don‚Äôt miss out on tailored opportunities! We have many ongoing recruitments, and new projects are constantly coming in. By giving your consent to process your data for future recruitment processes , we‚Äôll be able to invite you to roles that match your experience and expectations! PS: We‚Äôll only reach out to you when we have projects that might genuinely interest you ‚Äî without your consent, we won‚Äôt be able to do that. Our recruitment process is transparent and focused on finding the right candidate for our clients. When you apply, you can count on our objectivity, respect, and full professionalism. We look forward to receiving your CV. We connect you with the right people.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,282,Data Engineer,ITDS,"Data Engineer Join us, and create cutting-edge pipelines for seamless data transformation! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As a Data Engineer, you will be working for our client, a global financial institution that is driving DevOps transformation through data analytics and engineering. You will be part of a team that provides key metrics and analytical products to enhance software engineering practices across the organization. Your role will focus on developing data transformation pipelines, ensuring data quality, and supporting a cloud data platform to improve the overall DevOps experience. You will collaborate with diverse global teams to deliver enriched datasets, dashboards, and insights that enable strategic decision-making. Your main responsibilities: Designing, developing, testing, and deploying data ingest, quality, refinement, and presentation pipelines Operating and iterating on a cloud data platform to support internal goals Building and maintaining ETL processes and data transformation pipelines Ensuring data quality and implementing automated data validation solutions Developing data marts and optimizing schema designs for performance and usability Collaborating with business stakeholders to understand data needs and deliver actionable insights Working with cloud-based big data technologies, particularly Google Cloud Platform (GCP) and BigQuery Utilizing orchestration and scheduling tools such as Airflow and Cloud Composer Supporting continuous integration and continuous delivery (CI/CD) processes Following Agile methodologies and working within a product-oriented culture You're ideal for this role if you have: At least 7 years of professional experience in SQL development Strong experience in data engineering and ETL processes Expertise in GCP, BigQuery, and data build tools (DBT) Hands-on experience with Apache Airflow and Cloud Composer Proficiency in data modeling and designing optimized data schemas Experience with data streaming technologies such as Kafka Familiarity with BI tools, especially Looker Studio Understanding of DevOps principles and working in a DevOps environment Experience with Continuous Integration and Continuous Delivery (CI/CD) practices Strong communication skills and ability to work with global teams It is a strong plus if you have: Experience in building and operating a cloud data platform Knowledge of data architecture and data marts Proficiency in Git, Shell scripting, and Python Ability to quickly learn and adapt to new technologies Experience collaborating with technical staff and project managers for efficient delivery Proactive approach to identifying improvement opportunities and solving issues Comfort in working in fast-paced, changing, and ambiguous environments","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,283,Data Architect,Connectis,"Wraz z naszym partnerem z bran≈ºy ubezpieczeniowej, poszukujemy eksperta/ekspertki na stanowisko Data Architect , kt√≥ry do≈ÇƒÖczy do strategicznego projektu budowy DataHubu ‚Äì rozwiƒÖzania klasy mini hurtowni danych, wspierajƒÖcego dzia≈Çalno≈õƒá operacyjnƒÖ, raportowƒÖ, analitycznƒÖ oraz rozwiƒÖzania z zakresu generatywnej sztucznej inteligencji. Projektowanie i nadz√≥r nad wdra≈ºaniem skalowalnych, wydajnych modeli danych integrujƒÖcych wiele ≈∫r√≥de≈Ç. Tworzenie dokumentacji architektonicznej i mentoring mniej do≈õwiadczonych in≈ºynier√≥w. Zapewnienie jako≈õci danych: automatyczna walidacja, wykrywanie anomalii, monitoring. Definiowanie standard√≥w, wzorc√≥w i najlepszych praktyk w zakresie in≈ºynierii danych. Projektowanie rozwiƒÖza≈Ñ do przetwarzania danych strumieniowego i wsadowego. Ustalanie polityk dostƒôpu, zasad zarzƒÖdzania danymi i ram bezpiecze≈Ñstwa. Bliska wsp√≥≈Çpraca z zespo≈Çami DevOps, Data Engineering oraz AI/ML. üîç CZEGO OCZEKUJEMY OD CIEBIE? Do≈õwiadczenie z Azure Databricks oraz Azure DevOps. Bardzo dobra znajomo≈õƒá Microsoft Azure. Znajomo≈õƒá Apache Spark / PySpark. Bieg≈Ço≈õƒá w Python i SQL. Mile widziane: Do≈õwiadczenie z DBT (Data Build Tool). Praktyczna znajomo≈õƒá narzƒôdzi CI/CD Znajomo≈õƒá modelu Data Vault 2.0. ‚ú® OFERUJEMY: Tryb pracy zdalnej z okazjonalnymi spotkaniami zespo≈Çu w biurze w centrum Warszawy (raz w miesiƒÖcu). Udzia≈Ç w strategicznym projekcie realizowanym dla miƒôdzynarodowego Partnera Biznesowego. Realny wp≈Çyw na architekturƒô i rozw√≥j rozwiƒÖzania wdra≈ºanego w wielu krajach. Dedykowane wsparcie opiekuna Connectis przez ca≈Çy czas trwania wsp√≥≈Çpracy. Wsp√≥≈Çpracƒô z do≈õwiadczonym zespo≈Çem in≈ºynier√≥w danych i DevOps√≥w. Mo≈ºliwo≈õƒá przed≈Çu≈ºenia wsp√≥≈Çpracy w kolejnych fazach programu. 5000 PLN za polecenie znajomego do naszych projekt√≥w. Szybki, zdalny proces rekrutacyjny. Dziƒôkujemy za wszystkie zg≈Çoszenia. Pragniemy poinformowaƒá, ≈ºe skontaktujemy siƒô z wybranymi osobami. 12170/DK","[{""min"": 150, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,Permanent,Hybrid,284,Senior Data Scientist - Cyber Analytics & AI Innovation,VISA,"Company Description Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose ‚Äì to uplift everyone, everywhere by being the best way to pay and be paid. Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa. Visa‚Äôs Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world‚Äôs most sophisticated processing networks capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. While working with us you‚Äôll get to work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cyber security, and B2C platforms. The Opportunity: We are looking to hire a Data Scientist to lead the AI/ML initiatives in the Cybersecurity Products team. This position will be based out of Warsaw, Poland. In this role, you will: Be responsible for driving, designing, and building cutting edge innovation in the space of cybersecurity through Artificial Intelligence and Machine Learning - current scope of problems includes behavior biometrics, risk-based authentication, Account Takeover protection, advanced threat detection, smart incident response, AI model threat analysis, and many more. Drive the continued innovation and engineering of our existing behavior-based adaptive authentication product and bot/fraud protection product with a high-performance team of data scientists and engineers. Build innovative solutions and collaborate with engineering and product partners across the global Visa organization that help secure Visa against a variety of threats and attacks. Provide consultation to more experienced leaders in order to recommend solutions which solve security & other business challenges. You must have strong technical depth and experience in application of Machine Learning, Deep Learning, and Data Science techniques. On top of that, you should also have a genuine interest in cybersecurity and a desire to build solutions that deliver real impacts to the world. We will rely on your leadership to establish a roadmap and vision as this team engages in existing and new emerging areas. Support transfer technical knowledge to facilitate implementation of the business solution provided. Document all projects developed, including clear and efficient coding, and write other documentation as needed. Identify relevant market trends by country, based on a deep analysis of payment industry information. Interacting with several internal and external stakeholders for the strategic definition of analysis and initiatives. Continuously develop and present innovative ideas to improve current business practices within Visa. Essential Functions: Cyber Analytics Product: Research innovation in digital authentication using behavior biometrics capabilities, build applied AI based models and engineer them into the product called Visa Behavior Analytics. You will engage in data science and applied AI related activities for a Visa engineered product to protect against account takeover related threats, continuously enhancing it to combat threats in the secure authentication and perimeter defense space. Cyber/AI R&D: Research innovation in applying AI to the more general field of cybersecurity, including the protection with and against AI driven technologies, as well as the AI models themselves. You will be using your core competencies around AI and data science and help drive the teams to build models and solutions that work at scale, harnessing Petabytes of data while applying it to products that need to respond with cyber analytics in milliseconds. Influence & Collaborate: Be able to present results to a cross section of employees, including C-Level and other senior leadership at Visa. You will engage with internal technology, and cyber teams along with global product orgs. In addition, you will collaborate with colleagues in technology and product offices to establish effective, productive business relationships. This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs. Basic Qualifications: 2+ years of relevant work experience and a Bachelor‚Äôs degree, OR 5+ years of relevant work experience. Preferred Qualifications: 3 or more years of work experience with a Bachelor‚Äôs Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD,MD). 3+ years of experience in modern data mining and data science techniques(e.g., regressions, decision trees, ensemble algorithms, neural networks, time series analytics, clustering, anomaly detection, text analytics, etc.). Candidates with a PhD in a quantitative field, such as Statistics, Mathematics, Operational Research, Computer Science, Economics, or engineering preferred. Experience in developing and deploying products using Docker, Kubernetes, and the containerization technology stack. Experience in development of advanced machine learning and deep learning models such as RNN, LSTM, Graph Neural Networks. Experience and proficiency in working with large language models (LLMs), and familiarity with the associated solution architecture and infrastructure, including Nvidia GPUs. Experience in leading, building, and supporting scalable and reliable AI/ML-powered systems, enabling rapid prototyping and advanced analytics using modern big data and AI/ML technologies (e.g., Spark, Kafka, TensorFlow,PyTorch) in an agile environment. Software Engineering background: The candidate must be proficient in Python and at least one object-oriented programming language (Java, Golang, C++,etc.) Golang experience is desired. Candidate must possess software engineering skills and be able to take end-to-end ownerships of analytical models. Data Wrangling: The candidate must have proficient data wrangling skills with Python, SQL, and other data processing tools/scripts. Experience with the end-to-end machine learning lifecycle and MLOps, including data preprocessing and feature extraction, model training and evaluation, deployment, and monitoring of AI models in production environments. Experience working with Docker in both development and deployment workflows, ensuring smooth transitions from development to production environments. Distributed Systems: practical experience with NoSQL data platforms (e.g., Cassandra, Lakehouse, DynamoDB) and caching technologies like Redis is a plus. A solid understanding of the Linux networking subsystem, contributing to the stability and performance of deployed AI/ML systems. A solid understanding of the web applications and APIs, contributing to the front-end accessibility and integration of AI-driven solutions. Cloud domain: Familiarity with infrastructure and analytics services on cloud(e.g., AWS, Azure) is a plus. Domain Knowledge - Candidate with background in one or multiple of the following domains is a plus: Cybersecurity, AI security/privacy research and Biometrics This role qualifies for Autorskie Koszty Uzyskania Przychodu (KUP), in accordance with applicable Polish tax regulations. Eligible employees may benefit from preferential tax treatment on income derived from the creation of intellectual property, subject to meeting statutory criteria. Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,285,Senior Data Engineer,co.brick,"We're Hiring: Data Engineers & Data Architects üìç Location: Remote üí∞ Rate: Up to 170 PLN/h üïí Engagement: Full-time üìÖ Start: July üìÜ Duration: Minimum 12 months (with possible extension) We're currently looking for experienced Data Engineers and Data Architects to join a long-term project for established UK-based client. About the role: You‚Äôll be working on a project focused on decommissioning legacy software and migrating data to modern systems . Required experience: ‚úîÔ∏è 7+ years in data-related roles ‚úîÔ∏è Strong skills in: ‚Ä¢ Snowflake and DBT ‚Ä¢ Python ‚Ä¢ AWS (preferred) or Azure ‚úîÔ∏è Excellent communication skills (C1+ English level) ‚úîÔ∏è Open and collaborative mindset Nice to have: ‚ûï Experience with Fivetran We‚Äôre looking for communicative, proactive individuals who thrive in remote environments and want to work with a solid, international team. Looking forward to hearing from you!","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,286,Starszy Programista Baz Danych,P&P Solutions,"üìç Lokalizacja : Hybrydowo / Zdalnie üïí Wymiar pracy : Pe≈Çny etat, B2Büìë Wide≈Çki: 125-150 z≈Ç/h netto na b2b Poszukujemy Starszego Programisty Baz Danych do realizacji strategicznego projektu dla instytucji publicznej w obszarze ochrony zdrowia . Projekt obejmuje dzia≈Çania zwiƒÖzane z utrzymaniem, rozwojem i migracjƒÖ danych w ≈õrodowisku relacyjnych baz danych ‚Äì ze szczeg√≥lnym uwzglƒôdnieniem PostgreSQL. Do≈ÇƒÖczysz do zespo≈Çu ekspert√≥w odpowiedzialnych za projektowanie, optymalizacjƒô i transformacjƒô danych w ramach kluczowych system√≥w informatycznych. To doskona≈Ça okazja do zaanga≈ºowania siƒô w projekt o wysokim znaczeniu spo≈Çecznym i du≈ºej skali technologicznej. Minimum 5 lat do≈õwiadczenia zawodowego w pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL, MSSQL, Oracle). Minimum 4 lata do≈õwiadczenia w programowaniu w proceduralnym jƒôzyku bazodanowym (np. PL/SQL, PL/PgSQL). Bardzo dobra znajomo≈õƒá jƒôzyka SQL . Do≈õwiadczenie z technologiƒÖ PostgreSQL ‚Äì mile widziane pe≈Çne zrozumienie architektury i konfiguracji tej bazy. Praktyka w strojenie zapyta≈Ñ SQL oraz procedur PL/PgSQL . Wiedza z zakresu zarzƒÖdzania, konfiguracji i optymalizacji PostgreSQL . Do≈õwiadczenie w migracji danych z system√≥w klasy enterprise . Znajomo≈õƒá proces√≥w analizy i transformacji danych na potrzeby migracji. Udzia≈Ç w projektach realizowanych w obszarze ochrony zdrowia ‚Äì zw≈Çaszcza rejestry i czƒô≈õƒá ‚Äûbia≈Ça‚Äù Projektowanie, implementacja i optymalizacja procedur oraz zapyta≈Ñ w relacyjnych bazach danych (PostgreSQL, PL/SQL, itp.); Udzia≈Ç w projektach migracji danych z system√≥w klasy enterprise ‚Äì analiza, transformacja i integracja danych; Strojenie zapyta≈Ñ SQL oraz procedur w celu poprawy wydajno≈õci system√≥w bazodanowych; Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi oraz analitykami w zakresie integracji danych i wymaga≈Ñ systemowych; Utrzymanie, rozw√≥j i konfiguracja istniejƒÖcych struktur baz danych; Projektowanie i implementacja mechanizm√≥w przetwarzania danych zgodnie z wymaganiami klienta; Tworzenie i utrzymywanie dokumentacji technicznej; Zapewnienie zgodno≈õci z wewnƒôtrznymi standardami jako≈õci i bezpiecze≈Ñstwa danych (np. WCAG, RODO ‚Äì je≈õli dotyczy); Wsp√≥≈Çpraca z zespo≈Çami odpowiedzialnymi za architekturƒô danych i DevOps w zakresie ciƒÖg≈Ço≈õci dzia≈Çania oraz wdro≈ºe≈Ñ. Wide≈Çki do 150 z≈Ç/h netto na b2b. Mo≈ºliwo≈õƒá pracy w 100% zdalnie. Mo≈ºliwo≈õƒá udzia≈Çu w projekcie o znaczeniu publicznym i realnym wp≈Çywie na funkcjonowanie sektora ochrony zdrowia. Wsp√≥≈Çpracƒô z do≈õwiadczonym zespo≈Çem ekspert√≥w IT. D≈ÇugoterminowƒÖ wsp√≥≈Çpracƒô i stabilno≈õƒá projektu (projekt publiczny).","[{""min"": 125, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Mid,B2B,Hybrid,287,Customer Data Business Analyst,emagine Polska,"PROJECT INFORMATION: Industry: Banking Location: 100% remote Project language: English Start: ASAP Summary: The role involves supporting the Reg X programme, an initiative dedicated to data transformation within the bank, ensuring data quality and standardization across the organization. Main Responsibilities: Correcting industry codes related to customer and client data. Supporting the development of a new database/platform focused on customer data. Performing data reconciliation using large datasets in MS Excel. Creating process documentation and standard operating procedures (SOPs). Engaging with stakeholders for effective reference data analysis. Key Requirements: Experience in managing Entity Data/Customer Data. Basic SQL knowledge for executing SQL scripts. Experience with Atlassian Confluence. Ability to conduct external research for entity relationships using reports. Proficiency in handling large datasets in MS Excel. Expertise in Excel functions, including pivots and VLOOKUPs. Experience in creating process documentation. Strong stakeholder management and reference data analysis skills. Understanding of corporate entity relationships. Nice to Have: Knowledge of ETL processes. Background in mathematics.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Hybrid,288,Programista/Programista Hurtowni Danych,Provident Polska,"Tw√≥j zakres obowiƒÖzk√≥w: Rozwijanie Hurtowni Danych w zakresie: przek≈Çadania wymaga≈Ñ biznesowych na projekt techniczny projektowania i tworzenia proces√≥w integracji danych (pakiety ETL) tworzenia kod√≥w ≈∫r√≥d≈Çowych dla proces√≥w zasilajƒÖcych hurtownie danych przeprowadzania test√≥w wdra≈ºanych rozwiƒÖza≈Ñ tworzenia dokumentacji technicznej wdra≈ºanych rozwiƒÖza≈Ñ Monitoring i utrzymanie Hurtowni Danych Tworzenie zoptymalizowanych kod√≥w procedur T-SQL, budowanie pakiet√≥w SSIS (BIML) Analizowanie b≈Çƒôd√≥w oraz wdra≈ºanie i rekomendowanie zmian Nasze wymagania: Wykszta≈Çcenie wy≈ºsze (preferowane: informatyka, matematyka, fizyka lub pokrewne) Min. rok do≈õwiadczenia komercyjnego na podobnym stanowisku Znajomo≈õƒá tematyki hurtowni danych i technologii w niej wykorzystywanych (Dimentional modeling R. Kimball-a) Bardzo dobra znajomo≈õƒá jƒôzyka T-SQL i co najmniej dobra znajomo≈õƒá SQL Server Integration Services (SSIS) Umiejƒôtno≈õƒá samodzielnego rozwiƒÖzywania z≈Ço≈ºonych problem√≥w technicznych Umiejƒôtno≈õƒá przek≈Çadania wymaga≈Ñ biznesowych na projekt techniczny Zdolno≈õci komunikacyjne Inicjatywa, kreatywno≈õƒá, dok≈Çadno≈õƒá i efektywno≈õƒá Jƒôzyk angielski w mowie i pi≈õmie na poziomie min. dobrym Dostƒôpno≈õƒá w wymiarze pe≈Çnego etatu Na plus postrzegana bƒôdzie znajomo≈õƒá: SQL Server Analysis Services (SSAS) Snowflake Cloud Data Platform Amazon Web Services (AWS), np. S3, Athena Azure DevOps/Git Co mo≈ºesz zyskaƒá, pracujƒÖc z nami: Stabilne zatrudnienie ‚Äì 93% os√≥b jest zatrudnionych na umowƒô o pracƒô, na czas nieokre≈õlony. Bezpiecze≈Ñstwo ‚Äì jeste≈õmy na polskim rynku ju≈º 28 lat. Przyjazne ≈õrodowisko pracy ‚Äì 12 razy z rzƒôdu otrzymali≈õmy nagrodƒô Top Employer. Pracƒô hybrydowƒÖ ‚Äì zazwyczaj 2 razy w tygodniu widzimy siƒô w biurze (metro Dworzec Gda≈Ñski). Ekstra 3 dni p≈Çatnego urlopu ‚Äì je≈õli wykorzystasz ca≈Çy urlop w danym roku kalendarzowym. PrywatnƒÖ opiekƒô medycznƒÖ z us≈ÇugƒÖ gwarancji terminu (Medicover). Pe≈Çne wdro≈ºenie pod okiem mentora, w tym pakiet profesjonalnych szkole≈Ñ wdro≈ºeniowych. Dostƒôp do platformy rozwojowej, obejmujƒÖcej szkolenia e-learningi, podcasty i webinary. Aktywno≈õci wspierajƒÖce rozw√≥j w organizacji np. cykl szkole≈Ñ ‚ÄûSkuteczny Manager‚Äù dla os√≥b obejmujƒÖcych stanowiska kierownicze. Telefon s≈Çu≈ºbowy (r√≥wnie≈º do u≈ºytku prywatnego). Dostƒôp do platformy kafeteryjnej ProviBenefity, zasilanej co miesiƒÖc kwotƒÖ do wykorzystania lub dofinansowanie do Twojej karty Multisport ‚Äì wybierasz spo≈õr√≥d 5 rodzaj√≥w kart. Ubezpieczenie na ≈ºycie (UNUM ≈ªycie TUiR S.A.) na preferencyjnych warunkach. ≈öwiadczenia ≈õwiƒÖteczne oraz dofinansowanie do wypoczynku ‚ÄûWczasy pod gruszƒÖ‚Äù dla Ciebie oraz dla Twoich dzieci. Wsparcie psychologiczne pracownik√≥w, obejmujƒÖce m.in. opiekƒô psychologa (r√≥wnie≈º dzieciƒôcego), psychoterapeuty, dietetyka, coaching.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,289,Senior Data Scientist (pharma),7N,"Senior Data Scientist (pharma) About the project For our client operating in the pharmaceutical industry, we are looking for a Senior Data Scientist to work on advanced data science projects related to medical image analysis. Work Mode: 100% remote Research, develop and implement medical image processing pipelines utilizing state of the art computer-vision-based approaches in the medical image analysis space. Implement and maintain local and cloud-based data and computational environments and platforms to enable the work. Build data pipelines (data curation, preparation, cleaning, and consolidation) as an integral part of data science activity. Utilize AI/ML (Artificial Intelligence/Machine Learning) to develop complex methodologies and analyses, all around various medical imaging modalities. Perform informed semi-automated quality assessments of the acquired imaging data and of the derived measures. Align with clinical experts on the requirements, constraints and deliverables of a project. PhD degree with 3+ years‚Äô relevant direct non-academic professional experience or PhD degree with strong post-doc experience in medical image analysis Hands-on experience with deep learning for computer vision using frameworks like PyTorch and Sklearn Strong practical knowledge in medical image analysis in multiple modalities - X-ray , US , MRI , CT , Digital Pathology Solid understanding of deep learning theory: CNNs , Transformers , RNN , GenAI Ability to develop and validate algorithms against clinical data Ability to do quick prototyping/proofs of concept up to production ready models Ability to perform in-depth data analysis and present results to engineering and leadership teams Ongoing support from a dedicated agent , taking care of your project continuity, client contact, necessary formalities, work comfort and development, Consultant Development Program ‚Äì advice on growth planning based on the latest trends and market needs in IT, including consultations with agents and growth mentors, Access to 7N Learning & Development ‚Äì a development and educational platform with webinars, a library of articles and industry reports, and regular invitations to one-time and recurring development events ‚Äì technical, business, and lifestyle, Spectacular integration events , both for you (e.g., annual Kick-Off trip , Christmas parties, or Summer Olympics sports events) and for your loved ones (e.g., family picnics, movie premieres), Professional development not only during the project ‚Äì you can get involved in knowledge transfer to others within the 7N Services offering directed at 7N clients, Relationships and access to the knowledge of the most experienced IT experts in the market ‚Äì the average professional tenure of our consultants in Poland is over 10 years, A complete benefits package , including funding for medical care, life insurance, sports cards for you and your loved ones, as well as discounts in stores in Poland and abroad. Constantly searching for projects, difficult rate negotiations, lack of development support ‚Äì sounds familiar? At 7N, you gain not only stability of contracts but also the personal involvement of a dedicated agent who ensures your professional comfort and continuous access to development initiatives. Our mission is to provide stable and rewarding collaborations that drive your success as an IT expert and the success of our clients. We build long-lasting relationships based on Scandinavian values and 30 years of experience creating IT solutions for over 200 organizations.","[{""min"": 25200, ""max"": 31080, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,Permanent,Hybrid,290,Data Engineer Market Data Asset,Bayer Sp. z o.o.,"Data Engineer Market Data Asset The Data Assets, Analytics & AI organization at Bayer Consumer Health focuses on driving digital transformation and innovation by creating best-in-class analytical solutions that enable data-driven decision making and performance optimization for Bayer Consumer Health. You will be part of the Data Assets, Analytics & AI organization and will be responsible for building data assets. You partner with business stakeholders, data architects, data scientists, analytics leads as well as other engineers. You will build data pipelines, data models and provision data for Analytics products and data scientists. You will follow proper development processes and contribute to the enhancement of implementation frameworks. If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Key Tasks & Responsibilities: Implement efficient data pipelines that integrate data from different sources and business domains to develop globally harmonized data models and KPI calculations by following data management and data quality standards. Make sure you follow data management and data quality standards. Ensure that data is well-managed to build stable, reusable and quality assured data assets. Ensure that data products adhere to the data protection & compliance standards. Implement effective data access management policies, data privacy policies and secure data provisioning based on corporate guidelines. Contribute to the enhancement of implementation frameworks based on the needs of the analytics products consuming your data asset and based on your own needs to process the data. Collaborate with other IT functions (enabling functions data asset teams, analytics teams, platform product managers & integration architects) to ensure the aforementioned activities are executed effectively. Collaborate with other data engineers in your team (internal or external engineers) on solving tasks and implement work packages. Estimate owned product backlog items, ensure that own deliverables are properly tested, documented and handed over to the operations team. Qualifications & Competencies (education, skills, experience): Bachelor‚Äôs/Masters degree in Computer Science, Engineering, or a related field. 3+ years of working experience in the field of Data & Analytics, preferably in the CPG industry 3+ years of proficient coding experience with Python for data engineering, including SQL and PySpark (DataFrame API, Spark SQL, MLlib), with hands-on experience in various databases (SQL/NoSQL), key libraries (e.g., pandas, SQLAlchemy), parallel processing, and advanced data transformation and performance optimization techniques. Ability to develop, and maintain data engineering applications using Python classes and PySpark, ensuring code modularity, reusability, and maintainability. Advanced data engineering & technology knowledge (Azure Data Lake Gen2, Azure Data Factory and Databricks as well as data management knowhow (data cataloguing, data quality management). Solid understanding of data modeling, ETL processing and lakehouse concepts. Solid knowledge of CI/CD processes and tools (GitHub VCS, GitHub Actions, Azure DevOps Pipelines) Solid data content knowledge of market data (Sell-out, Sell-through) for the CPG or health care industry which is licensable from data providers like IQVIA, Nielsen and IRI. Good understanding of data governance processes to support mapping of data from multiple sources. Experience working within international/intercultural teams and using Agile methodologies (Scrum, Kanban) to organize the work. Strong problem solving and analytical skills. Excellent interpersonal and communication skills, active listening, consulting, challenging in a constructive way. Fluent in English, both written & spoken, intercultural awareness and willingness to travel. What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL. JEROZOLIMSKIE 158","[{""min"": 14000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,291,Senior Ontologist,Upvanta sp. z o.o.,"Min. 5 lat do≈õwiadczenia w tworzeniu i zarzƒÖdzaniu ontologiami (preferowane ≈õrodowisko miƒôdzynarodowe) Bieg≈Ço≈õƒá w pracy ze standardami W3C (RDF, OWL) oraz narzƒôdziami do modelowania ontologii Znajomo≈õƒá technologii Semantic Web , mile widziana wiedza z zakresu AI i ML Umiejƒôtno≈õƒá t≈Çumaczenia z≈Ço≈ºonych wymaga≈Ñ biznesowych na skuteczne rozwiƒÖzania semantyczne Wysokie umiejƒôtno≈õci analityczne i komunikacyjne Do≈õwiadczenie w pracy w ≈õrodowisku macierzowym oraz we wsp√≥≈Çpracy z interesariuszami na r√≥≈ºnych poziomach organizacji Mile widziane do≈õwiadczenie w sektorze: farmaceutycznym, ochrony zdrowia lub life sciences Dodatkowym atutem bƒôdzie znajomo≈õƒá obszar√≥w finans√≥w, produkcji lub ≈Ça≈Ñcucha dostaw Wsp√≥≈Çpracƒô zdalnƒÖ z miƒôdzynarodowym zespo≈Çem ekspert√≥w Kontrakt B2B z mo≈ºliwo≈õciƒÖ przed≈Çu≈ºenia (12+ miesiƒôcy) Realny wp≈Çyw na rozw√≥j jƒôzyka danych i innowacyjne podej≈õcie do zarzƒÖdzania wiedzƒÖ Stabilno≈õƒá i transparentno≈õƒá wsp√≥≈Çpracy",[],Unclassified,Unclassified
Full-time,Senior,B2B,Remote,292,Data Engineer with Data Vault,emagine Polska,"PROJECT INFORMATION: Industry: FinTech Client: company from Sweden Remote work: 100% remote Consultant‚Äôs location: Poland Project language: English Business trips: None Project length: 6 months contracts + prolongations (we‚Äôre looking for long-term) Start: ASAP / Flexible Assignment type: B2B Stack in the order of importance on the project: Data Vault, SQL, Python, dbt. Nice to have : AWS, Snowflake Summary: The Data Engineer role is central to enhancing data infrastructure as part of a team committed to innovative financial solutions. The position aims to drive data innovation and improve data management practices to support various business initiatives. Responsibilities: Develop and maintain scalable Data pipelines using tools like Matillion and SQL. Collaborate with data infrastructure and Data teams to design optimal solutions for data ingestion and reporting. Seamlessly ingest data into Snowflake to enable advanced analytics and reporting. Serve as a technical expert, supporting business users in leveraging data effectively. Ensure data integrity, security, and governance across systems and workflows. Document and communicate technical solutions to ensure clarity across the organization. Must Haves: Strong SQL skills and a solid understanding of cloud-based data platforms. Experience with dbt (ideally with Data Vault and Snowfake). Hands-on experience with Data Vault modeling. Knowledge of best practices in data warehousing and data modeling. Excellent communication and collaboration skills to work cross-functionally. Ability to work independently or as part of a team to deliver high-quality results. Experience with Python. Previous work experience in finance or fintech industries - payment cards, loans. Nice to have: Experience with Matillion. Experience with Snowflake (or with Databricks). Experience with AWS. We offer: Long-term cooperation. Transparently built relations based on trust and fair play. Co-financed benefits: Medicover card, Multisport card.","[{""min"": 212, ""max"": 254, ""type"": ""Net per day - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,293,Mid/Senior Data Engineer,CodiLime,"Get to know us better CodiLime is a software and network engineering industry expert and the first-choice service partner for top global networking hardware providers, software providers and telecoms. We create proofs-of-concept, help our clients build new products, nurture existing ones and provide services in production environments. Our clients include both tech startups and big players in various industries and geographic locations (US, Japan, Israel, Europe). While no longer a startup - we have 250+ people on board and have been operating since 2011 we‚Äôve kept our people-oriented culture. Our values are simple: Act to deliver. Disrupt to grow. Team up to win. The project and the team The goal of this project is to build a centralized, large-scale business data platform for one of the biggest global consulting firms. The final dataset must be enterprise-grade, providing consultants with reliable, easily accessible information to help them quickly and effectively analyze company profiles during Mergers & Acquisitions (M&A) projects. You will contribute to building data pipelines that ingest, clean, transform, and integrate large datasets from over 10 different data sources, resulting in a unified database with more than 300 million company records. The data must be accurate, well-structured, and optimized for low-latency querying. The platform will power several internal applications, enabling a robust search experience across massive datasets and making your work directly impactful across the organization. The data will provide firm-level and site-level information, including firmographics, technographics, and hierarchical relationships (e.g., GU, DU, subsidiary, site). This platform will serve as a key data backbone for consultants, delivering critical metrics such as revenue, CAGR, EBITDA, number of employees, acquisitions, divestitures, competitors, industry classification, web traffic, related brands, and more. Technology stack: Languages: Python, SQL Data Stack: Snowflake + DBT, PostgreSQL, Elasticsearch Processing: Apache Spark on Azure Databricks Workflow Orchestration: Apache Airflow Cloud Platform: Microsoft Azure- Compute / Orchestration: Azure Databricks (Spark clusters), Azure Kubernetes Service (AKS), Azure Functions, Azure API Management.- Database & Storage: Azure Database for PostgreSQL, Azure Cosmos DB, Azure Blob Storage- Security & Configuration: Azure Key Vault, Azure App Configuration, Azure Container Registry (ACR)- Search & Indexing: Azure AI Search CI/CD: GitHub Actions Static Code Analysis: SonarQube AI Integration (Future Phase): Azure OpenAI What else you should know: Team Structure: Data Architecture Lead Data Engineers Backend Engineers DataOps Engineers Product Owner Work culture: Agile, collaborative, and experienced work environment. As this project will significantly impact the organization, we expect a mature, proactive, and results-driven approach. You will work with a distributed team across Europe and India. We work on multiple interesting projects at the time, so it may happen that we‚Äôll invite you to the interview for another project if we see that your competencies and profile are well suited for it. As a part of the project team, you will be responsible for: Data Pipeline Development: Designing, building, and maintaining scalable, end-to-end data pipelines for ingesting, cleaning, transforming, and integrating large structured and semi-structured datasets Optimizing data collection, processing, and storage workflows Conducting periodic data refresh processes (through data pipelines) Building a robust ETL infrastructure using SQL technologies. Assisting with data migration to the new platform Automating manual workflows and optimizing data delivery Data Transformation & Modeling: Developing data transformation logic using SQL and DBT for Snowflake. Designing and implementing scalable and high-performance data models. Creating matching logic to deduplicate and connect entities across multiple sources. Ensuring data quality, consistency, and performance to support downstream applications. Workflow Orchestration: Orchestrating data workflows using Apache Airflow, running on Kubernetes. Monitoring and troubleshooting data pipeline performance and operations. Data Platform & Integration: Enabling integration of 3rd-party and pre-cleaned data into a unified schema with rich metadata and hierarchical relationships. Working with relational (Snowflake, PostgreSQL) and non-relational (Elasticsearch) databases Software Engineering & DevOps: Writing data processing logic in Python. Applying software engineering best practices: version control (Git), CI/CD pipelines (GitHub Actions), DevOps workflows. Ensuring code quality using tools like SonarQube. Documenting data processes and workflows. Participating in code reviews Future-Readiness & Integration: Preparing the platform for future integrations (e.g., REST APIs, LLM/agentic AI). Leveraging Azure-native tools for secure and scalable data operations Being proactive and motivated to deliver high-quality work, Communicating and collaborating effectively with other developers, Maintaining project documentation in Confluence. As a Data Engineer, you must meet the following criteria: Strong experience with Snowflake and DBT (must-have) Experience with data processing frameworks, such as Apache Spark (ideally on Azure Databricks) Experience with orchestration tools like Apache Airflow, Azure Data Factory (ADF), or similar Experience with Docker, Kubernetes, and CI/CD practices for data workflows Strong SQL skills, including experience with query optimization Experience in working with large-scale datasets Very good understanding of data pipeline design concepts and approaches Experience with data lake architectures for large-scale data processing and analytics Very good coding skills in Python- Writing clean, scalable, and testable code (unit tests)- Understanding and applying object-oriented programming (OOP) Experience with version control systems: Git Good knowledge of English (minimum C1 level) Beyond the criteria above, we would appreciate the nice-to-haves: Experience with PostgreSQL (ideally Azure Database for PostgreSQL) Experience with GitHub Actions for CI/CD workflows Experience with API Gateway, FastAPI (REST, async) Experience with Azure AI Search or AWS OpenSearch Familiarity with developing ETL/ELT processes (a plus) Optional but valuable: familiarity with LLMs, Azure OpenAI, or Agentic AI system Flexible working hours and approach to work: fully remotely, in the office or hybrid Professional growth supported by internal training sessions and a training budget Solid onboarding with a hands-on approach to give you an easy start A great atmosphere among professionals who are passionate about their work The ability to change the project you work on","[{""min"": 16500, ""max"": 27500, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,294,Data Engineering Tech Lead,Lingaro,"Tasks: Provide leadership and guidance to the data engineering team, including mentoring, coaching, and fostering a collaborative work environment. Set clear goals, assign tasks, and manage resources to ensure successful project delivery. Work closely with developers to support them and improve data engineering processes. Support team members with troubleshooting and resolving complex technical issues and challenges. Provide technical expertise and direction in data engineering, guiding the team in selecting appropriate tools, technologies, and methodologies. Stay updated with the latest advancements in data engineering and ensure the team follows best practices and industry standards. Collaborate with stakeholders to understand project requirements, define scope, and create project plans. Support project managers to ensure that projects are executed effectively, meeting timelines, budgets, and quality standards. Monitor progress, identify risks, and implement mitigation strategies. Act as a trusted advisor for the customer. Oversee the design and architecture of data solutions, collaborating with data architects and other stakeholders. Ensure data solutions are scalable, efficient, and aligned with business requirements. Provide guidance in areas such as data modeling, database design, and data integration. Align coding standards, conduct code reviews to ensure proper code quality level. Identify and introduce quality assurance processes for data pipelines and workflows. Optimize data processing and storage for performance, efficiency and cost savings. Evaluate and implement new technologies to improve data engineering processes on various aspects (CICD, Quality Assurance, Coding standards). Act as main point of contact to other teams/contributors engaged in the project. Maintain technical documentation of the project, control validity and perform regular reviews of it. Ensure compliance with security standards and regulations. What We're Looking For: Minimum of 5 years of experience in data engineering or a related field. Strong technical skills in data engineering, including proficiency in programming languages such as Python, SQL and Scala. Familiarity with Azure cloud platform and services, and experience in implementing data solutions in a cloud environment. Expertise in working with various data tools and technologies, such as ETL frameworks, data pipelines, and data warehousing solutions. Proven experience in leading and managing a team of data engineers, providing guidance, mentorship, and technical support. In-depth knowledge of data management principles and best practices, including data governance, data quality, and data integration. Strong project management skills, with the ability to prioritize tasks, manage timelines, and deliver high-quality results within designated deadlines. Excellent problem-solving and analytical skills, with the ability to identify and resolve complex data engineering issues. Knowledge of data security and privacy regulations, and the ability to ensure compliance within data engineering projects. Excellent communication and interpersonal skills, with the ability to effectively collaborate with cross-functional teams, stakeholders, and senior management. Continuous learning mindset, staying updated with the latest advancements and trends in data engineering and related technologies. A bachelor's or master's degree in Computer Science, Information Systems, or a related field is typically required. Additional certifications in data integration tools or platforms are advantageous. Missing one or two of these qualifications? We still want to hear from you! If you bring a positive mindset, we'll provide an environment where you feel valued and empowered to learn and grow. Offer: Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,295,Senior Data Engineer,CLOUDFIDE,You are,[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,296,Data Solution Architect (Azure),Scalo,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in .: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! projekt budowy nowoczesnej Data Platform w chmurze Azure, obejmujƒÖcej warstwƒô Lakehouse (Bronze / Silver / Gold) oraz Analytical Platform (MLOps), modelowanie struktur bazodanowych w podej≈õciu DDD (Data Domain Driven Design), opracowywanie modeli danych na podstawie dokumentacji z obszaru Data Governance (glosariusz danych, modele konceptualne/logiczne), projektowanie przep≈Çywu danych ze ≈∫r√≥de≈Ç do warstw platformy danych (Ingest: direct query, API, event streaming), tworzenie i dokumentowanie Data Contracts, wsp√≥≈Çpraca z w≈Ça≈õcicielami system√≥w ≈∫r√≥d≈Çowych i docelowych w zakresie integracji danych i SLA, implementacja struktur danych w architekturze medallion (Bronze / Silver / Gold) przy u≈ºyciu ETL na Azure Data Platform, doradztwo w zakresie doboru narzƒôdzi, architektury integracyjnej i praktyk CI/CD, mo≈ºliwo≈õƒá realnego wp≈Çywu na standardy technologiczne i projektowe, praca 100% zdalna, a dla chƒôtnych mo≈ºliwo≈õƒá pracy z biura we Wroc≈Çawiu, stawka do 240 z≈Ç/h przy B2B, w zale≈ºno≈õci od do≈õwiadczenia. masz do≈õwiadczenie w modelowaniu danych (ERD), przygotowywaniu Data Contracts i implementacji struktur domenowych w ≈õrodowisku Data Warehouse znasz procesy Data Ingestion i architekturƒô nowoczesnych DWH w Azure (Azure Synapse, Data Lake, Databricks), masz do≈õwiadczenie z platformami MLOps / Analytical Platform, mile widziane do≈õwiadczenie w: tworzeniu dokumentacji mapowania danych ≈∫r√≥d≈Çowych, zarzƒÖdzaniu metadanymi i jako≈õciƒÖ danych (np. Azure Purview), komunikujesz siƒô p≈Çynnie w jƒôzyku angielskim (B2/C1). d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 33600, ""max"": 40320, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Mid,B2B,Remote,297,Hadoop Admin / Data Engineer,ALTEN Polska,"Why us? We are part of the ALTEN Group ‚Äì a leading European provider of engineering and technology consulting services. Join ALTEN Polska and let‚Äôs grow together! üåé Location : Krak√≥w/ Warsaw/ Remote Hadoop Admin role is expected to contribute to the design and build system capabilities on the key on-premise Hadoop big data platform. The person would be entrusted to develop solutions/design ideas focusing on product requirements, resiliency and automation of the underlying data processing clusters. Implementing new capabilities and identify design ideas to enable the software to meet the acceptance and success criteria, these requirements may be backend (containerized services, APIs) and front end (UI), as well as various integration and automation tasks. An ideal candidate should be comfortable in multiple areas of the software stack and corporate IT deliveries. They will work with solution architects to build platform components and overall capabilities within the ecosystem of IT systems within Data Technologies As a key member of the technical team alongside Engineers, Data Scientists and Data Users, you will be expected to define and contribute at a high-level to many aspects of our collaborative Agile development process: Big Data development, automated testing of new and existing components in an Agile, DevOps and dynamic environment Working with data delivery teams to setup new Hadoop users. This job includes setting up Linux users, setting up Kerberos principals and testing HDFS, Spark, and MapReduce access for the new user Executes the review / acceptance / tuning and ensures environment availability 24x7 General operational excellence. This includes good troubleshooting skills, understanding of system‚Äôs capacity and bottlenecks. Must Haves: Strong problem-solving skills and adaptability to a complex environment Providing technical support and design Hadoop Big Data platforms (preferably Cloudera distributions like Hive, Beeline, Spark, HDFS, Kafka, Yarn, Zookeeper etc.), handle and identify possible failure scenario (Incident Management), respond to end users of Hadoop platform on data or application issues, report and monitor daily SLA that identifies vulnerabilities and opportunities for improvement. Hands-on experience with large scale Big Data environment builds, capacity planning, performance tuning and monitoring, including end-to-end Cloudera cluster installation. Handling Hadoop security activities using Apache Ranger, Knox, TLs, Kerberos and Encryption zone management. Expertise in software installation and configuration, orchestration, and automation with tools such as Jenkins/Ansible. Improve the current estate by incorporating the use of centralized S3 data storage (VAST) throughout the platform processing stack 5+ years experience in engineering solutions in a Big Data on-prem or cloud environment. We offer A full-time contract (B2B also possible) Well-defined career path at the European leader in engineering & IT consulting Participation in company conferences, trainings, workshops, integration meetings, etc. Certification and training opportunities Opportunity to relocate and work in different ALTEN Polska branches After completion of the project, opportunity to engage in a subsequent one within the company. Work in company with #GreatPlaceToWork Certificate Benefits Medicover medical care Medicover dental care Medicover Benefits platform / Medicover Sport card Employee referral program E-learning platform Layette for a newborn employee‚Äôs child Group life insurance Pension scheme Do not hesitate and join our team!",[],Database Administration,Database Administration
Full-time,Mid,B2B,Hybrid,298,Cloud Engineer (3 yrs. of exp),emagine Polska,"PROJECT INFORMATION: Start : ASAP Contract : B2B, long term cooperation Work mode : 99% remote work with hybrid on the horizon Project language: English Business trips: to Sealand, Jutland and Denmark when necessary Recruitment process: 2 interviews Summary: The Cloud Engineer role is vital for managing and optimising the Client‚Äôs cloud infrastructure and services. The primary objective is to ensure the cloud environment is scalable, secure, and reliable, facilitating effective operations across various teams. Main Responsibilities: Monitoring system performance and troubleshooting cloud services to ensure high availability. Manage and administer the cloud environment, including provisioning, configuration, policy governance, and security. Automating and optimising cloud operations using tools like Azure DevOps. Providing guidance and support to other teams. Resolving incidents and service requests from cloud development teams. Collaborating closely with team members and stakeholders. Maintain documentation of the cloud environment, processes, and procedures. Key Requirements: A bachelor's degree in computer science or a related field, or equivalent practical experience. At least 3 years of experience in the same role. Significant technical skills in Azure , virtual networks, database administration, virtual machines, and PaaS services. Programming (e.g., SQL, Bicep, PowerShell, Python). Familiarity with DevOps tools and concepts (e.g., Azure DevOps, GitHub). Experience in operating cloud infrastructure, including management and optimisation. Experience with IaC - Bicep and PowerShell. Experience with cloud security, compliance and cloud migrations. Fluent English (written and spoken). Strong problem-solving and communication skills. Nice to Have: Knowledge of Enterprise Scale and the Cloud Adoption Framework. Specific sector knowledge within finance, pensions, energy, or banking. Relevant certifications in Azure or cloud technologies. Curious nature and a desire to share knowledge with others.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,299,Senior GCP Data Engineer (Java + Market Data),ITDS,"GCP Data Engineer Join us, and code the backbone of financial intelligence! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As a GCP Data Engineer , you will be working for our client, a global financial institution developing a cloud-based risk management platform used to generate and deliver risk factor definitions, historical market data, and scenarios for advanced financial modeling. The project involves building and optimizing scalable data pipelines, microservices, and integration layers that process high volumes of real-time and historical market data. You will be joining an international team of engineers focused on innovation, automation, and delivering measurable business value in a highly regulated environment. Your main responsibilities: Translating business requirements into secure, scalable, and performant data solutions Integrating internal systems with an emphasis on fast data processing and cost optimization Developing and documenting data ingestion blueprints for market data pipelines Reviewing data solutions created by other team members Assessing and modernizing existing data pipelines and microservices Collaborating with engineers, analysts, and stakeholders to align technical solutions with business needs Implementing consistent logging, monitoring, error handling, and automated recovery Promoting automated unit and regression testing through test-centric development Designing and implementing performant REST APIs Applying industry-standard integration frameworks and patterns You're ideal for this role if you have: Strong knowledge of Java Solid understanding of software design principles such as KISS, SOLID, and DRY Proficiency with Spring Boot and its ecosystem Experience building performant data processing pipelines Familiarity with Apache Beam or similar technologies Experience working with relational and NoSQL databases, such as PostgreSQL and Bigtable Basic understanding of DevOps practices and CI/CD tools like Jenkins and Groovy Ability to design and implement RESTful APIs Excellent problem-solving and analytical skills Strong communication and team collaboration abilities Experience with GCP services like GKE, Cloud SQL, DataFlow, and BigTable It is a strong plus if you have: Knowledge of monitoring tools such as Open Telemetry, Prometheus, and Grafana Familiarity with Kubernetes and Docker Exposure to Terraform for infrastructure-as-code Experience with messaging and streaming platforms like Kafka We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #6923 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 28000, ""max"": 31080, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,300,üëâ Data Platform Architect,Xebia sp. z o.o.,"üü£ You will be: üü£ Your profile: extensive experience working with Azure cloud provider, üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Technical Interview ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 29800, ""max"": 36800, ""type"": ""Net per month - B2B""}, {""min"": 23900, ""max"": 29700, ""type"": ""Gross per month - Permanent""}]",Data Architecture,Data Architecture
Full-time,Mid,B2B,Remote,301,Informatica Power Center Developer,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: praca w projekcie z obszaru bankowego. Praca ze stosem: IPC, Oracle, SQL, PL SQL, DWH, prowadzenie prac projektowych i implementacyjnych w zakresie rozwoju hurtowni danych oraz system√≥w informacyjnych, projektowanie algorytm√≥w oraz implementacja proces√≥w ETL, udzia≈Ç we wdro≈ºeniu narzƒôdzia Informatica, projektowanie baz danych i mart√≥w raportowo-analitycznych, projektowanie, implementacja i rozw√≥j ≈õrodowisk raportowo-analitycznych w oparciu o ≈õrodowisko klasy BI, kontrola jako≈õci prac wykonanych przez innych - testowanie rozwiƒÖza≈Ñ informatycznych, opracowywanie specyfikacji technicznej oraz dokumentacji projektowej, technicznej i administracyjnej, przygotowywanie pakiet√≥w instalacyjnych oprogramowania, praca zdalna (wizyty w biurze 2x w miesiƒÖcu - ≈Å√≥d≈∫), stawka: do 120z≈Ç/h + VAT. Ta oferta jest dla Ciebie, je≈õli: posiadasz dobrƒÖ znajomo≈õƒá platformy Informatica Power Center w zakresie budowy proces√≥w ETL, posiadasz dobrƒÖ znajomo≈õƒá Oracle 9i / 10g w kontek≈õcie projektowania baz danych i aplikacji, posiadasz bardzo dobrƒÖ znajomo≈õƒá SQL oraz znajomo≈õƒá PL/SQL, posiadasz wiedzƒô z zakresu in≈ºynierii system√≥w informatycznych oraz znajomo≈õƒá metodyk ich wytwarzania, posiadasz minimum 6 miesiƒôcy do≈õwiadczenia w projektowaniu, implementacji i wdra≈ºaniu rozwiƒÖza≈Ñ ETL z u≈ºyciem Informatica Power Center, posiadasz minimum 1 rok do≈õwiadczenia w projektowaniu, implementacji i wdra≈ºaniu system√≥w informacyjnych w ≈õrodowisku Oracle, posiadasz praktyczne do≈õwiadczenie w projektowaniu i programowaniu hurtowni danych oraz system√≥w informacyjnych dla bran≈º takich jak finanse, telekomunikacja lub inne o zbli≈ºonej skali, z≈Ço≈ºono≈õci i wymaganiach technologicznych. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,302,Data Engineer,Heineken,"Digital & Technology Team (D&T) is an integral division of HEINEKEN Global Shared Services Center . We are committed to making Heineken the most connected brewery. That includes digitalizing and integrating our processes, ensuring best-in-class technology, and embedding a data-driven culture. By joining us you will work in one of the most dynamic and innovative teams and have a direct impact on building the future of Heineken ! Would you like to meet the Team, see our office and much more? Visit our website: Heineken ( heineken-dt.pl) As the Data Engineer in the Global Deployment team, you will be a business facing individual responsible for planning, executing, and overseeing the migration of data between source and target systems or platforms, ensuring data accuracy, completeness, and security throughout the process. Your responsibilities would include: Technical Activities: coordinating and executing data migration activities, including data extraction, cleansing, loading, and reconciliation; designing and implementing data mapping, transformation, and validation processes to ensure the accuracy and integrity of migrated data Documentation and Knowledge Transfer: Responsible to create and maintain documentation for data migration processes, procedures, and best practices. Supporting with proper knowledge transfer to service delivery and OpCo teams. Collaboration: colaborating with the Deployment Lead to ensure appropriate data migration process and procedures are in place and followed. Coordinate with OpCo data teams to ensure successful data migrations Learning: being updated on emerging data technologies and trends as it relates to key technologies in our landscape. You are a good candidate if you have: 5+ years of working experience in the similar position responsible for data migration and quality related activities in large multi nationals at least 3-5 years of experience with many of the above-mentioned technologies data models and patterns experience in collaboration with cross-functional teams to ensure successful delivery strong understanding of of data modelling, ETL processes, and data integration techniques strong communication skills with the ability to effectively interact with stakeholders at all levels ability to learn new technologies fast ability to multitask certificates in any ETL or Data Modelling tooling knowledge of technology stack: Jira, ETL Tooling, Excel, SQL Server Data Tools (SSDT) excellent written and verbal English. Good to know: Microsoft Dynamics 365 (CE or F&O or Contact Centre) JavaScript / TypeScript (Node.js) GraphQL / Apollo Loyalty Solutions (i.e. Epsilon) Payment platforms (i.e. process out) CTI Solutions (i.e. Genesys) SAP Middleware solutions (i.e. Boomi or MuleSoft) Azure DevOps. At HEINEKEN Krak√≥w, we take integrity and ethical conduct seriously. If someone has concerns about a possible violation of legal regulations indicated in Polish Whistleblowing Act or our Code of Business Conduct, we encourage them to speak up . Cases can be reported to global team or locally (in line with the local HGSS Whistleblowing procedure) by selecting proper option in this tool or by communicating it on hotline. We Offer: üè† Flexible Work from Home scheme üí∏ Attractive Performance Bonus üöó Parking Space for Employees ‚è∞ Flexible working hours üí≥ Sodexo Card ‚òÇ Life Insurance ‚ûï Employee Referral Programme üåê Job Opportunities within HEINEKEN ü©∫ Private Medical Healthcare ‚≠ê Social Events",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,303,Data Modeller,Antal Sp. z o.o.,"Position: Data Modeller Location: Krak√≥w hybrid Employment Type: Full-time | B2B We are seeking a Data Modeller to design enterprise-grade data models and lead a team of data modellers in shaping our data foundation. You'll collaborate closely with cross-functional teams to ensure consistency, scalability, and data quality across systems. Design and develop conceptual, logical, and physical data models to support business operations and analytics. Work with stakeholders to gather requirements and translate them into scalable, efficient data models. Define and enforce data modeling standards, best practices, and governance policies. Mentor and train the data modelling team in tools, techniques, and methodologies. Collaborate with data architects, engineers, and analysts to align data models with broader data strategies. Contribute to design and architecture discussions to ensure end-to-end data consistency. Stay up to date on data technologies, trends, and tools; evaluate and recommend improvements. Experience in data modeling for both transactional and analytical systems . Strong understanding of metadata, data analysis, and requirement gathering . Ability to clearly communicate complex data modeling concepts to technical and non-technical audiences. Leadership experience , including mentoring and guiding teams. Strong stakeholder management and consulting skills. Solid knowledge of data governance, data quality, and protection practices . Experience with cloud data platforms (AWS, Azure, GCP). Familiarity with Big Data ecosystems (Hadoop, Spark). Knowledge of industry models (BIAN, FSDM, BDW) or experience in designing enterprise-wide data models. To learn more about Antal, please visit www.antal.pl","[{""min"": 130, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,304,Lead Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Lead Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company. This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. This role represents a gradual shift away from hands-on coding towards a more strategic focus on system design, business consultation, and creative problem-solving. It offers an opportunity to engage more deeply with architecture-level decisions, collaborate closely with clients, and contribute to building innovative data-driven solutions from a broader perspective. üöÄ Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Hadoop, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. üéØ What you'll need to succeed in this role: 5+ years of proven commercial experience in implementing, developing, or maintaining Big Data systems. Strong programming skills in Python or Java/Scala : writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity with Big Data technologies like Spark , Cloudera, Airflow , NiFi, Docker , Kubernetes , Iceberg , Trino or Hudi. Proven expertise in implementing and deploying solutions in cloud environments (with a preference for AWS ). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master‚Äôs or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. Fluent English (C1 level) is a must. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn , Instagram ).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,305,Data Engineer,Nexio Management,"Nexio Management to zaufany partner biznesowy w drodze do cyfrowej przysz≈Ço≈õci. Posiadamy prawie 20-letnie do≈õwiadczenie na rynku IT w Polsce i poza jej granicami. Prowadzimy dzia≈Çania w oparciu o transparentne i szczere relacje. Tworzymy innowacyjne rozwiƒÖzania technologiczne, kreujƒÖc przy tym interesujƒÖce i rozwojowe ≈õrodowisko pracy dla naszych ekspert√≥w. Obecnie zatrudniamy 550 konsultant√≥w, kt√≥rzy ≈õwiadczƒÖ us≈Çugi IT dla Klient√≥w na ca≈Çym ≈õwiecie. Nasza g≈Ç√≥wna siedziba mie≈õci siƒô w Warszawie, poza tym mamy biura w Rumunii oraz Wielkiej Brytanii. Posiadamy r√≥wnie≈º w≈Çasne R&D Center, kt√≥re jest miejscem powstawania innowacyjnych projekt√≥w m.in.: w obszarach test√≥w, Big Data, Cloud czy AI.W ramach naszych us≈Çug tworzymy szyte na miarƒô rozwiƒÖzania, utrzymujemy i rozwijamy nawet najbardziej wymagajƒÖce systemy IT. Dzia≈Çamy w takich modelach biznesowych jak managed services, fixed prices oraz wspieramy zespo≈Çy naszych klient√≥w w modelach scale up the team. Naszymi klientami sƒÖ firmy z wielu zr√≥≈ºnicowanych bran≈º, szukajƒÖce wsparcia najwy≈ºszej klasy ekspert√≥w. Poszukujemy osoby na stanowisko Data Platform Engineer do wsp√≥≈Çpracy przy jednym z naszych klient√≥w. Zadania : Projektowanie narzƒôdzi analitycznych wspierajƒÖcych podejmowanie decyzji w oparciu o dane ilo≈õciowe i jako≈õciowe Wsp√≥≈Çpraca z partnerami w celu szybkiej i efektywnej integracji system√≥w i danych Projektowanie i rozwijanie pipelin√≥w Big Data Analiza danych ≈∫r√≥d≈Çowych Tworzenie dashboard√≥w Minimum 3/4-letnie komercyjne do≈õwiadczenie w roli Data Engineer Znajomo≈õƒá Pythona oraz bibliotek do analizy danych ( NumPy , Pandas , Spark, itp.) Umiejƒôtno≈õƒá pracy z SQL w zakresie przetwarzania OLAP Znajomo≈õƒá j. polskiego (min. B2) oraz j.angielskiego (min. C1) Oferujemy: R√≥≈ºnorodne formy wsp√≥≈Çpracy ‚Äì umowa o pracƒô, kontrakt B2B lub umowa zlecenie Stabilno≈õƒá zatrudnienia ‚Äì d≈Çugofalowe projekty, wsp√≥≈Çpracƒô z przodujƒÖcymi firmami z bran≈ºy, mo≈ºliwo≈õci zmiany projektu Benefity: Medicover (rozszerzony o stomatologiƒô) i karta FitProfit Finansowanie certyfikat√≥w technicznych Darmowe lekcje j. angielskiego Udzia≈Ç w r√≥≈ºnorodnych inicjatywach charytatywnych oraz sportowych (np. biegi firmowe, turnieje szachowe, rozgrywki pi≈Çki no≈ºnej)",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,306,Lead Data Architect - Master Data Asset,Bayer Sp. z o.o.,"Lead Data Architect ‚Äì Master Data Asset The Data Assets, Analytics and AI team at Bayer Consumer Health focuses on bringing digital transformation and innovation by creating best in class analytical solutions enabling data-driven decision making and performance optimization for Bayer Consumer Health organization. In this role, you will be part of the Data, Analytics and AI organization with responsibility for master data assets. You will collaborate with business stakeholders, data asset leads,governors, engineers, data scientists, analysts, and fellow architects to implement master data management processes into technical solutions across the system landscape. Your focus will include designing MDM processes that map different technical entities across data domains, with strong emphasis on data quality to support analytics and AI projects with appropriate master data. You will ensure proper data management, ingestion, and pipeline orchestration in a scalable and high-performing manner.If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Your Tasks & Responsibilities: Technically own a central master data asset and be responsible for its solution architecture, data model, lifecycle, access management, stable operations and infrastructure costs Co-Create & implement master data management strategy for Consumer Health IT based on the business requirements from our Consumer Health Division, which allows IT teams, business stakeholders and autonomous AI systems to easily consume master & reference data and decrease time-to-market for new data analytics and AI solutions Manage and integrate master data from multiple sources, including MDM and transactional systems. Collaborate with architects and product owners across various functions to design data flows and business logic throughout the master data process landscape. Partner with data analysts to ensure proper semantic descriptions, creating easily consumable data assets that enable users to effectively join data from different business domains As an individual contributor you are the technical lead of a squad team. You will work closely the lead Data Analyst on timeline of the roadmap and guide data engineers and dev/ops engineers to ensure continuous delivery of the roadmap and priorities and contribute to manage demands addressed by IT and business stakeholders Contribute to the enrichment of our global data and analytics technology stack and its capabilities according to the needs of your core data asset, especially in the area of data quality management and data catalogues You are responsible to technically enable data governance processes to ensure that data governors and data stewards can effectively enrich and correct data sets and provide necessary mapping which is required to integrate data sets from various sources Be responsible to ensure compliance, setup respective processes for data access, manage legal & license data requirements as well as security (e.g. row level security) or data privacy related topics in order to deliver fit-for purpose and scalable analytics products Additionally, you ensure quality by implementing data quality checks and monitoring tools which help to detect, raise, correct and prevent data issues Qualifications & Competencies (education, skills, experience): Master degree in computer science, big data, engineering, data management or a comparable subject 5-8+ years working experience in the field of master data management for data analytics, experience in the area of big data and data science as a data- & solution architect and with experience in CPG/FMCG industry is a plus, but not a must 3-5+ years of proven experience with MDM tools (e.g. Reltio Cloud MDM, Syndigo Enterprise Data Suite) Proven experience in data quality management and related tooling including business rule management systems Solid understanding of data modeling, ETL processing and modern Lakehouse concepts (Medallion architecture, Spark, Databricks, Azure Data Stack) Knowledge of CI/CD processes and tools (GitHub VCS and GitHub Actions) Profound data content knowledge across multiple business functions (Marketing, Sales, R&D, Product Supply, Finance) Experience in Agile methodologies (Scrum, Kanban) Strong problem solving and analytical skills, excellent interpersonal and communication skills, active listening, consulting, challenging, presentation skills Fluent in English, both written & spoken, intercultural awareness and willingness to travel What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring program Wide access to professional development tools, training, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all the criteria we are looking for? That doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL. JEROZOLIMSKIE 158","[{""min"": 24000, ""max"": 34000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Office,307,Oracle Database Administrator,Onwelo Sp. z o.o.,"Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, powiƒôkszy≈Ça zesp√≥≈Ç do 700 os√≥b, a tak≈ºe otworzy≈Ça biura w siedmiu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. O projekcie: Jeste≈õ ekspertem w zakresie baz danych Oracle? Do≈ÇƒÖcz do nas! Dla naszego szwajcarskiego klienta z bran≈ºy bankowej poszukujemy Database Administratora , kt√≥ry bƒôdzie odpowiedzialny za zarzƒÖdzanie, administracjƒô oraz optymalizacjƒô baz danych Oracle. Je≈õli jeste≈õ otwarty/-a na wyjazd i pracƒô w Szwajcarii w siedzibie klienta , zostaw nam swoje CV! Z nami bƒôdziesz: ZarzƒÖdzaƒá i administrowaƒá bazami danych Oracle Monitorowaƒá wydajno≈õƒá i optymalizowaƒá bazy danych Implementowaƒá i zarzƒÖdzaƒá procedurami bezpiecze≈Ñstwa RozwiƒÖzywaƒá problemy i zapewniaƒá wsparcie techniczne Tworzyƒá dokumentacjƒô technicznƒÖ oraz utrzymywaƒá kopie zapasowe Aktualizowaƒá i migrowaƒá bazy danych do nowszych wersji Oracle Przeprowadzaƒá migracjƒô do chmury Oracle (OCI) Czekamy na Ciebie, je≈õli: Masz minimum 5 lat do≈õwiadczenia jako Oracle DBA Bardzo dobrze znasz SQL oraz PL/SQL Masz do≈õwiadczenie w administracji Oracle RAC, Data Guard, ASM Znasz narzƒôdzia do monitorowania i optymalizacji (np. Oracle Enterprise Manager ) Masz wiedzƒô z zakresu system√≥w Unix/Linux Masz znajomo≈õƒá skryptowania w pow≈Çoce shell Znasz Infrastructure as Code (Ansible, Terraform, git) Masz do≈õwiadczenie z CI/CD P≈Çynnie pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie min. B2 Jeste≈õ gotowy/wa do relokacji do Szwajcarii na czas kontraktu Dodatkowym atutem bƒôdzie: Do≈õwiadczenie jako MSSQL DBA Znajomo≈õƒá PostgreSQL DBA Do≈õwiadczenie z Exadata i RAC Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Mo≈ºliwo≈õƒá pracy zdalnej Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºyci","[{""min"": 250, ""max"": 300, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,308,Data Analyst with Tableau,Altimetrik Poland,"2-3 day per week you need to be available until 9: 00 PM for meetings with the US team. Altimetrik Poland is a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are seeking a skilled Data Analyst with Tableau for Airbnb- our customer, an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. Together with them, we are building a world-class payments platform that moves billions of dollars, in 191 countries, with 75 currencies, through a complex ecosystem of payments partners. They are also reinventing how to serve users to improve performance, scalability and extensibility. Responsibilities: Lead data visualization strategy for Policy, maintaining existing dashboards and developing new self-service resources. Optimize for bringing relevant data to the Policy team and bringing data created by the Policy team to cross-functional partners. Build and manage the primary Policy metrics and insights dataset and visualizations. Aggregate critical metrics across lobbying efforts, grassroots advocacy, partnerships, social and advertising campaigns, news articles, research, and regulatory compliance efforts. Architect this data to be flexibly leveraged at many geographic levels, including government administrative boundaries, custom shapes, and business geographic definitions. Partner closely with the Economics and Research Data Science team to ensure the Policy team is consuming all available data and that it meets the highest quality standards. Elevate the legitimacy of Policy Comms data assets to pass our rigorous internal data certification process. . Leverage Minerva, Airbnb‚Äôs in house metrics management tool, to make our data accessible to other data practitioners. Develop experimentation and attribution framework for political advocacy and regulatory compliance marketing initiatives. Support ad hoc analytics needs to support Policy, including supply composition, economic impact, growth and regulatory compliance, etc. Build relationships with local and regional Policy Comms teams around the world to support their local data and analytics needs. Partner with the Comms Data Strategy Taskforce. And if you possess.. 7+ years experience in business intelligence, data analytics, or data science. Expert in SQL and Python languages. Expertise in building data visualization, preferably in Tableau. Experience working with complex geographic data. Experience with knowledge graphs is a plus. Talent for breaking down complex technical concepts into common language and acting as a bridge between technical and departmental stakeholders. Experience working with complex and big data systems across a multitude of relationships and metrics. Ability to apply a creative and nuanced perspective to look beyond common data indicators in order to meet business goals. Expertise designing and running marketing experiments. Ability to self-serve and take the initiative to find answers to technical questions. Experience building and implementing machine learning models is a plus. Experience in survey population sampling and survey response analysis is a plus. Expertise in R, Java, REACT is a plus. ‚Ä¶ then we are looking for you! We work 100% remotely or from our hub in Krak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 20000, ""max"": 24300, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,309,Senior Solutions Architect: ML/AI; Big Data,B2Bnetwork,"Naszym g≈Ç√≥wnym celem jest sprawne wdra≈ºanie nowoczesnych narzƒôdzi w obszarze ryzyka, w tym obni≈ºanie koszt√≥w obs≈Çugi, zwiƒôkszanie poziomu automatyzacji proces√≥w decyzyjnych, zapewnienie aktualno≈õci system√≥w oraz zgodno≈õci z obowiƒÖzujƒÖcymi regulacjami. W zwiƒÖzku z dalszym rozwojem poszukujemy do≈õwiadczonego Senior Solutions Architecta, kt√≥ry do≈ÇƒÖczy do naszego zespo≈Çu IT i bƒôdzie odpowiedzialny za projektowanie oraz wdra≈ºanie rozwiƒÖza≈Ñ opartych na uczeniu maszynowym i sztucznej inteligencji. Zakres obowiƒÖzk√≥w: Projektowanie architektury rozwiƒÖza≈Ñ ML/AI w jƒôzyku Python, z wykorzystaniem bibliotek takich jak scikit-learn, TensorFlow, PyTorch. Tworzenie pipeline‚Äô√≥w danych i modeli w PySpark oraz Spark MLlib. Wdra≈ºanie modeli ML w ≈õrodowiskach produkcyjnych i eksperymentalnych. Integracja modeli ML z backendem aplikacyjnym i systemami bankowymi, przy u≈ºyciu REST API oraz gRPC. Budowa rozwiƒÖza≈Ñ z wykorzystaniem FastAPI lub Flask. Optymalizacja wydajno≈õci system√≥w ML/AI oraz zarzƒÖdzanie cyklem ≈ºycia modeli. Wsp√≥≈Çpraca z zespo≈Çami data science, ML engineering oraz backend development. Dokumentowanie architektury, kodu i proces√≥w wdro≈ºeniowych. Wdra≈ºanie rozwiƒÖza≈Ñ ML/AI w oparciu o praktyki MLOps oraz Continuous Delivery. Udzia≈Ç w projektach prowadzonych zgodnie z metodykƒÖ Agile/Scrum. Profil Kandydata: Wykszta≈Çcenie wy≈ºsze techniczne (informatyka, matematyka, analiza danych, ekonometria lub kierunki pokrewne). Minimum 5 lat do≈õwiadczenia w projektowaniu i wdra≈ºaniu rozwiƒÖza≈Ñ ML/AI w ≈õrodowiskach produkcyjnych. Minimum 3 lata do≈õwiadczenia w pracy z bibliotekami ML/AI w Pythonie (scikit-learn, PyTorch, TensorFlow). Bardzo dobra znajomo≈õƒá jƒôzyka Python oraz bibliotek: Pandas, NumPy, Dask. Do≈õwiadczenie z bibliotekami NLP i LLM (Hugging Face, Transformers, spaCy, LangChain). Praktyczna znajomo≈õƒá narzƒôdzi i ≈õrodowisk MLOps: MLflow, Airflow, Jupyter, GitLab CI/CD, Docker, Kubernetes. Umiejƒôtno≈õƒá tworzenia pipeline‚Äô√≥w ML w PySpark lub Spark MLlib. Do≈õwiadczenie w budowie i integracji API dla modeli ML (REST API, gRPC, FastAPI, Flask). Znajomo≈õƒá architektury system√≥w bankowych i integracji z backendem. Znajomo≈õƒá konteneryzacji oraz wdra≈ºania modeli ML w ≈õrodowiskach produkcyjnych. Do≈õwiadczenie w pracy z narzƒôdziami monitorujƒÖcymi (np. Prometheus, Grafana, MLflow). Umiejƒôtno≈õƒá analizy danych i przek≈Çadania wymaga≈Ñ biznesowych na rozwiƒÖzania ML/AI. Do≈õwiadczenie w pracy w interdyscyplinarnych zespo≈Çach projektowych. Komunikatywno≈õƒá oraz zdolno≈õƒá t≈Çumaczenia z≈Ço≈ºonych kwestii technicznych na jƒôzyk zrozumia≈Çy dla biznesu. Proaktywno≈õƒá, samodzielno≈õƒá i umiejƒôtno≈õƒá podejmowania decyzji architektonicznych. Znajomo≈õƒá jƒôzyka angielskiego na poziomie pozwalajƒÖcym na pracƒô z dokumentacjƒÖ technicznƒÖ. Mile widziane: Do≈õwiadczenie w pracy w ograniczonych ≈õrodowiskach bankowych, gdzie czƒô≈õƒá komponent√≥w wymaga samodzielnego przygotowania. Do≈õwiadczenie z modelami LLM (np. GPT, BERT, Llama), ich trenowaniem i wdra≈ºaniem. Certyfikaty z zakresu ML/AI (np. TensorFlow Developer Certificate, PyTorch, Hugging Face).","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,310,Senior and Mid BI Consultants,emagine Polska,"PROJECT INFORMATION: Industry: Marine infrastructure Client: from Denmark Remote work: yes Project language: English Business trips: no Project length: 12 months contract + option for prolongations Start: Flexible / at latest beginning of October 2025 Assignment type: B2B Remuneration: 160-200 PLN/h for a senior (depending on experience) Headcounts: 4 FTE: 100% Summary: The primary goal of this position is to support digital transformation efforts by migrating from a legacy data platform to a new cloud-based BI platform (from Databricks to Fabric), enabling enhanced data management and analytics capabilities. Responsibilities: Assist in the migration of data from a legacy platform to a new cloud-based BI platform. Implement large-scale enterprise data solutions and centralized cross-domain data models. Apply layered/medallion data architectures and dimensional modeling techniques. Utilize semantic modeling principles and create tabular models for data analysis. Ensure scalability, maintainability, and documentation of data models. Analyze business needs to align technical designs with requirements. Employ DevOps best practices in version control, CI/CD, and monitoring. Manage and execute tasks independently to meet project deadlines. Adopt agile methodologies to break down and track project work. Key Requirements: 5+ years of experience in data warehousing, BI, or data engineering. Experience in implementing large-scale enterprise data solutions. Strong understanding of layered and dimensional modeling techniques. Hands-on experience with semantic and tabular models. Proficiency in DevOps principles, including version control and CI/CD. Experience with Fabric, Power BI, (Py)spark, Azure DevOps, Git. Nice to Have: Familiarity with data normalization techniques. Experience with agile delivery methods. Understanding of best practices in data modeling and documentation. We offer: Long-term cooperation. Transparently built relations based on trust and fair play. Co-financed benefits: Medicover card, Multisport card.","[{""min"": 160, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Hybrid,311,System Engineer,Radpoint,"Poszukujemy In≈ºyniera ds. Wdro≈ºe≈Ñ, kt√≥ry bƒôdzie odpowiedzialny za wdra≈ºanie z≈Ço≈ºonych i zintegrowanych system√≥w informatycznych w przedsiƒôbiorstwach i instytucjach. Oferujemy pracƒô w dynamicznym ≈õrodowisku, w kt√≥rym Twoje umiejƒôtno≈õci i do≈õwiadczenie bƒôdƒÖ doceniane. Je≈õli spe≈Çniasz poni≈ºsze wymagania zachƒôcamy do aplikowania! Wymagania : Co najmniej rok pracy jako In≈ºynier odpowiedzialny za wdro≈ºenia z≈Ço≈ºonych, zintegrowanych system√≥w informatycznych w przedsiƒôbiorstwach lub instytucjach. Umiejƒôtno≈õƒá prowadzenia szkole≈Ñ aplikacyjnych. Praktyczne do≈õwiadczenie w integracji system√≥w informatycznych w tym analiza i tworzenie specyfikacji technicznych Znajomo≈õƒá Excela, umiejƒôtno≈õƒá analizy danych oraz podstawowa znajomo≈õƒá jƒôzyka SQL Jako dodatkowy atut: Znajomo≈õƒá obszaru ochrony zdrowia oraz do≈õwiadczenie w radiologii i diagnostyce obrazowej Znajomo≈õƒá i do≈õwiadczenie w pracy z MIRTH, HL7, DICOM oraz standard√≥w IHE Praktyczna znajomo≈õƒá podstawowych zagadnie≈Ñ zwiƒÖzanych z wdro≈ºeniami w sektorze publicznym- protoko≈Çy i planowanie Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z rozliczeniami NFZ Do≈õwiadczenie w zarzƒÖdzaniu danymi Zadania: Przeprowadzanie szkole≈Ñ aplikacyjnych. Wykonywanie integracji i test√≥w integracyjnych z udzia≈Çem wielu zewnƒôtrznych dostawc√≥w system√≥w IT. Planowanie i prowadzenie test√≥w integracyjnych. Dokonywanie wdro≈ºe≈Ñ w przedsiƒôbiorstwach i instytucjach na terenie Polski. Oferujemy: Stabilne zatrudnienie w nowoczesnym ≈õrodowisku pracy Mo≈ºliwo≈õƒá rozwoju zawodowego i zdobycia do≈õwiadczenia w projektach r√≥≈ºnego rodzaju core hours 9: 00-15: 00 Mo≈ºliwo≈õƒá zdalnego ≈õwiadczenia us≈Çug, jednak chƒôtnie zobaczymy Ciƒô w biurze üòâ Wynagrodzenie adekwatne do umiejƒôtno≈õci i do≈õwiadczenia Jak rekrutujemy? Pierwszy etap to techniczna rozmowa online trwajƒÖca oko≈Ço 1-1,5h Etap drugi to spotkanie w biurze/online sk≈ÇadajƒÖce siƒô ze swobodnej rozmowy i czƒô≈õci negocjacyjnej, gdy ju≈º wiemy, ≈ºe chcemy wsp√≥≈Çpracowaƒá","[{""min"": 6800, ""max"": 9000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Remote,312,Data Analyst,DataFeedWatch,"About Us We Help Merchants Grow. DataFeedWatch is a fast growing tech start-up with roots in Silicon Valley and offices in Krakow, Amsterdam, and Prague. Since March 2022 we are part of Cart.com which brings us numerous new opportunities to grow our business and ourselves. We are a market leader in Feed Marketing Solutions and enable 20,000 online shops on 6 continents to advertise and be successful on shopping channels like Google, Facebook, and Amazon. If you join us, you will be part of a team of nearly 90 people that includes 15 different nationalities. We are a diverse team that enjoys good work-life balance. We don't just work together, we have lunch together and hang out outside work hours. And most importantly, we like people who make their own decisions and want to grow the company and themselves. About the Job We're growing our Data Analytics Team and looking for a curious, technically skilled, and business-minded Data Analyst (mid level) to join us. You‚Äôll play a key role in delivering insights that shape decisions across product, marketing, customer success, and leadership. You'll work closely with stakeholders and Data Analytics team members to build robust data pipelines, automate processes, and create insightful visualizations. You'll also dive deep into our data to uncover trends, deliver impactful insights, and help raise the overall bar for analytics across the company. Data Analytics team is integrated into every part of the business ‚Äî we‚Äôre not just building dashboards; we‚Äôre helping people make better decisions every day. This is a great opportunity for someone looking to apply their technical skills in a fast-paced SaaS environment while continuing to grow and expand their impact. Your main responsibilities: Collaborate with stakeholders across departments to define problems and find data-driven solutions Support and maintain our internal analytics stack using Git, Python (mostly Pandas), SQL and Airflow Design and optimize data transformation processes, automate routine reporting to support scalable, reliable data workflows Build and maintain clear, interactive dashboards using Tableau Analyze structured and unstructured datasets to generate actionable insights Work with the rest of the Data Analytics team to continuously improve our data engineering processes About You You‚Äôre naturally curious, proactive, and passionate about solving real business problems with data. You enjoy collaborating, asking thoughtful questions, and taking ownership of your work. You‚Äôre someone who wants to keep growing, while helping those around you do the same. What We‚Äôre Looking For Must-haves: 2 or more years of experience in a data analyst or data engineer role Proficiency in writing robust Python code, especially using Pandas Familiarity with Git and collaborative version control practices Solid experience with SQL Experience building dashboards in Tableau, Power BI, or similar BI tools Strong analytical thinking and problem-solving skills Excellent communication in English and polish, both written and verbal (C1 level or higher) Ability to work autonomously and collaboratively in a fast-paced environment Nice-to-haves: Understanding of core statistical concepts (e.g. hypothesis testing, distributions, correlation, regression models) Experience working with Airflow Experience working in Agile team, preferably following Scrum framework Previous experience in a SaaS or e-commerce environment Familiarity with tools and concepts like BigQuery, NoSQL databases, Docker, ETL/ELT and data modeling What‚Äôs in it for You? The salary for this position is between 10-13k per month. Self-development budget for courses, training, books and conferences Flexibility ‚Äî remote and hybrid arrangements are both supported Access to healthcare services, sports card, lunch card, life insurance and more Real ownership and the ability to shape the future of our analytics function A collaborative team that values curiosity, initiative, and continuous growth","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 13000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Manager / C-level,B2B,Remote,313,Lead Data Engineer,Upvanta sp. z o.o.,"Kierowanie zespo≈Çami in≈ºynier√≥w danych i budowanie kultury wsp√≥≈Çpracy, innowacji oraz doskona≈Ço≈õci technicznej. Realizacja strategii in≈ºynierii danych we wsp√≥≈Çpracy z interesariuszami. Tworzenie i wdra≈ºanie najlepszych praktyk, standard√≥w, narzƒôdzi i us≈Çug wspierajƒÖcych efektywne zarzƒÖdzanie danymi. Wspieranie wdra≈ºania pipeline‚Äô√≥w danych oraz platform i narzƒôdzi in≈ºynierii danych poprzez automatyzacjƒô i optymalizacjƒô proces√≥w. Zapewnienie dostarczania skalowalnych, wydajnych i wysokiej jako≈õci rozwiƒÖza≈Ñ. Wsp√≥≈Çpraca z zespo≈Çami cross-funkcyjnymi w celu identyfikacji wymaga≈Ñ technicznych i wdra≈ºania usprawnie≈Ñ. Tworzenie proces√≥w zapewniajƒÖcych wysokƒÖ jako≈õƒá danych oraz wydajno≈õƒá system√≥w. Udzia≈Ç w rozwoju zespo≈Çu o wysokiej wydajno≈õci, promujƒÖcego r√≥≈ºnorodno≈õƒá i rozw√≥j zawodowy. Wykszta≈Çcenie wy≈ºsze (magisterskie) w zakresie informatyki, in≈ºynierii danych, zarzƒÖdzania informacjƒÖ lub pokrewnym. Minimum 8 lat do≈õwiadczenia w in≈ºynierii oprogramowania i danych, w tym co najmniej 4 lata w in≈ºynierii pipeline‚Äô√≥w i integracji danych. Praktyczne do≈õwiadczenie w prowadzeniu z≈Ço≈ºonych projekt√≥w data engineering. Bieg≈Ço≈õƒá w pracy z Databricks oraz g≈Çƒôboka znajomo≈õƒá jego funkcji i narzƒôdzi do optymalizacji workflow√≥w. Znajomo≈õƒá Python na poziomie eksperckim ‚Äì automatyzacja, manipulacja danymi, budowanie modeli. Wysoka znajomo≈õƒá SQL ‚Äì tworzenie z≈Ço≈ºonych zapyta≈Ñ, optymalizacja, ekstrakcja danych. Do≈õwiadczenie w pracy z DevOps , CI/CD oraz narzƒôdziami wspierajƒÖcymi ciƒÖg≈ÇƒÖ integracjƒô i wdra≈ºanie kodu. Znajomo≈õƒá zagadnie≈Ñ z zakresu modelowania danych, integracji, zarzƒÖdzania metadanymi i data governance. Co najmniej 4-letnie do≈õwiadczenie w pracy z chmurƒÖ publicznƒÖ ‚Äì Azure lub AWS (certyfikaty mile widziane). Znajomo≈õƒá wymaga≈Ñ regulacyjnych bran≈ºy life sciences bƒôdzie dodatkowym atutem. Do≈õwiadczenie w projektowaniu skalowalnych rozwiƒÖza≈Ñ oraz wsp√≥≈Çpracy z biznesem i zespo≈Çami rozproszonymi. Kontrakt B2B z d≈ÇugoterminowƒÖ perspektywƒÖ. Pracƒô w miƒôdzynarodowym ≈õrodowisku przy projektach z sektora life sciences. Mo≈ºliwo≈õƒá pracy zdalnej Udzia≈Ç w ambitnych projektach w obszarze nowoczesnych rozwiƒÖza≈Ñ data engineering.",[],Data Engineering,Data Engineering
Full-time,Junior,Permanent or B2B,Hybrid,314,Data Center Engineer,Wirtualna Polska Media S.A.,"Cze≈õƒá! W Wirtualnej Polsce wierzymy, ≈ºe nie ma jednej drogi do sukcesu ‚Äì liczy siƒô to, co wniesiesz i jak bƒôdziesz dzia≈Çaƒá. Kreatywno≈õƒá, odwaga i innowacyjno≈õƒá to nasze DNA. Dzia≈Çamy w obszarach medi√≥w, reklamy i e-commerce, ≈ÇƒÖczƒÖc te elementy w spos√≥b, kt√≥ry anga≈ºuje i inspiruje. Tworzymy, testujemy, wprowadzamy zmiany tam, gdzie majƒÖ sens. Budujemy rzeczy od zera i szukamy nowych rozwiƒÖza≈Ñ. Szukamy osoby, kt√≥ra pasjonuje siƒô sprzƒôtem komputerowym, potrafi odr√≥≈ºniƒá bity od bajt√≥w, NVMe od SATA oraz M.2 od U.2. Je≈õli czego≈õ nie wie ‚Äì ma naturalnƒÖ chƒôƒá szukania informacji w sieci. Zale≈ºy nam na kim≈õ, kto nie tylko wykonuje powierzone zadania, ale tak≈ºe wykazuje ciekawo≈õƒá i chƒôƒá zrozumienia ""jak to dzia≈Ça"". Brzmi dobrze? Do≈ÇƒÖcz do nas na stanowisko Data Center Engineer i razem z nami buduj przysz≈Ço≈õƒá Internetu! Jakie dzia≈Çania bƒôdƒÖ nale≈ºeƒá do Ciebie? Dob√≥r konfiguracji sprzƒôtowej serwer√≥w zgodnie z wymaganiami aplikacji Monta≈º, modyfikacja i instalacja serwer√≥w w szafach rack Podstawowy serwis serwer√≥w Instalacja oprogramowania (zapewniamy szkolenie) Podstawowa konfiguracja (zapewniamy szkolenie) Inwentaryzacja zasob√≥w sprzƒôtowych Jakie kompetencje sƒÖ dla nas wa≈ºne? Bardzo dobra znajomo≈õƒá sprzƒôtu IT, umo≈ºliwiajƒÖca dob√≥r odpowiednich podzespo≈Ç√≥w Chƒôƒá rozwoju oraz optymalizacji rozwiƒÖza≈Ñ sprzƒôtowych, np. pod kƒÖtem zu≈ºycia energii Znajomo≈õƒá podstawowych zagadnie≈Ñ sieciowych Prawo jazdy kat. B Mile widziane: Uprawnienia SEP do 1 kV Podstawowa znajomo≈õƒá system√≥w Linux Znajomo≈õƒá sprzƒôtu klasy Data Center Znajomo≈õƒá standard√≥w po≈ÇƒÖcze≈Ñ optycznych (≈õwiat≈Çowod√≥w) Co zyskujesz, do≈ÇƒÖczajƒÖc do WP? Pracƒô u technologicznego lidera i partnera pierwszego wyboru ‚Äì tworzymy tre≈õci, us≈Çugi i rozrywkƒô, kt√≥re towarzyszƒÖ milionom Polak√≥w ka≈ºdego dnia Darmowy dostƒôp do naszych produkt√≥w, takich jak Pilot WP czy Audioteka ‚Äì chcemy, ≈ºeby≈õ czu≈Ç siƒô czƒô≈õciƒÖ tego, co tworzymy Worksmile i elastyczny pakiet benefit√≥w ‚Äì to Ty wybierasz, co jest dla Ciebie wa≈ºne Dostƒôp do bezp≈Çatnych i anonimowych konsultacji psychologicznych Mo≈ºliwo≈õƒá pracy hybrydowej ‚Äì pracujemy 3 dni w biurze i 2 dni z domu Wydarzenia, kt√≥re zbli≈ºajƒÖ ‚Äì od sezonowych akcji (Dzie≈Ñ Dziecka to nasz ulubieniec!) po webinary o zdrowiu i neuror√≥≈ºnorodno≈õci Mo≈ºliwo≈õƒá rozwoju dziƒôki szkoleniom w ramach programu WP Akademia Szansƒô na nowe wyzwania i kierunki w ramach sp√≥≈Çek WP Holding ‚Äì wewnƒôtrzne rekrutacje sƒÖ na wyciƒÖgniƒôcie rƒôki",[],Unclassified,Unclassified
Full-time,Senior,Permanent or B2B,Remote,315,Senior Data Engineer (Azure/Fabric),Onwelo,"üü† Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, a tak≈ºe otworzy≈Ça biura w siedmiu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. üöÄ O projekcie: Do naszego zespo≈Çu Data & Analytics poszukujemy do≈õwiadczonego Azure/Fabric Data Engineera, kt√≥ry bƒôdzie wspiera≈Ç naszych klient√≥w w planowaniu, budowie i wdra≈ºaniu nowoczesnych rozwiƒÖza≈Ñ danych w ≈õrodowisku Microsoft Azure i Fabric. Bƒôdziesz pracowaƒá w zespole z ekspertami od analizy danych, chmury i architektury, w ≈õrodowisku miƒôdzynarodowym i projektach o du≈ºej skali. . üéØ Z nami bƒôdziesz: Projektowaƒá i wdra≈ºaƒá rozwiƒÖzania oparte na Microsoft Fabric ‚Äì nowoczesnej platformie analitycznej ≈ÇƒÖczƒÖcej dane, raportowanie i orkiestracjƒô w jednym ≈õrodowisku Planowaƒá i przeprowadzaƒá orkiestracjƒô danych w ≈õrodowisku Microsoft Azure oraz Fabric Budowaƒá, rozwijaƒá i wdra≈ºaƒá nowoczesnƒÖ hurtowniƒô danych w oparciu o Databricks, Data Vault 2.0, Python i PySpark Tworzyƒá i optymalizowaƒá potoki danych oraz procesy ETL/ELT zasilajƒÖce hurtownie danych Projektowaƒá modele danych wspierajƒÖce analitykƒô biznesowƒÖ i raportowanie w Power BI Wdra≈ºaƒá rozwiƒÖzania z wykorzystaniem Microsoft Fabric, Azure Data Factory, Synapse, Data Lake, Azure SQL Przeprowadzaƒá analizƒô danych i projektowaƒá modele danych wspierajƒÖce cele biznesowe Wspieraƒá innych cz≈Çonk√≥w zespo≈Çu ‚Äì technicznie i merytorycznie Monitorowaƒá jako≈õƒá i efektywno≈õƒá przep≈Çyw√≥w danych oraz optymalizowaƒá je pod kƒÖtem koszt√≥w i wydajno≈õci üòé Czekamy na Ciebie, je≈õli: Masz minimum 5-letnie do≈õwiadczenie jako Data Engineer ‚Äì w projektach zwiƒÖzanych z integracjƒÖ danych, modelowaniem i budowƒÖ hurtowni Masz praktyczne do≈õwiadczenie z Microsoft Fabric lub chcesz rozwijaƒá siƒô w tym obszarze i szybko siƒô uczysz Pracujesz z us≈Çugami chmurowymi Azure, w tym: Azure Data Factory, Azure Databricks, Azure SQL, Data Lake Znasz SQL na poziomie eksperckim Biegle pos≈Çugujesz siƒô jƒôzykami Python i/lub PySpark Rozumiesz architekturƒô nowoczesnych hurtowni danych (np. Data Vault 2.0 ) Masz wy≈ºsze wykszta≈Çcenie techniczne (np. informatyka, matematyka, in≈ºynieria danych) Komunikujesz siƒô po angielsku na poziomie min. B2 (czƒô≈õƒá projekt√≥w i zespo≈Ç√≥w jest miƒôdzynarodowa) ü§ù Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,316,üëâ Senior GCP Data Engineer,Xebia sp. z o.o.,"üü£ You will be: developing and maintaining data pipelines to ensure seamless data flow from the Loyalty system to the data lake and data warehouse, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, ensuring data integrity, consistency, and availability across all data systems, integrating data from various sources, including transactional databases, third-party APIs, and external data sources, into the data lake, implementing ETL processes to transform and load data into the data warehouse for analytics and reporting, working closely with cross-functional teams including Engineering, Business Analytics, Data Science and Product Management to understand data requirements and deliver solutions, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, optimizing data storage and retrieval to improve performance and scalability, monitoring and troubleshooting data pipelines to ensure high reliability and efficiency, implementing and enforcing data governance policies to ensure data security, privacy, and compliance, developing documentation and standards for data processes and procedures. üü£ Your profile: 7+ years in a data engineering role, with hands-on experience in building data processing pipelines, experience in leading the design and implementing of data pipelines and data products, proficiency with GCP services, for large-scale data processing and optimization, extensive experience with Apache Airflow, including DAG creation, triggers, and workflow optimization, knowledge of data partitioning, batch configuration, and performance tuning for terabyte-scale processing, strong Python proficiency, with expertise in modern data libraries and frameworks (e.g., Databricks, Snowflake, Spark, SQL), hands-on experience with ETL tools and processes, practical experience with dbt for data transformation, deep understanding of relational and NoSQL databases, data modelling, and data warehousing concepts, excellent command of oral and written English, Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Information Systems, or a related field. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ üü£ Nice to have: experience with ecommerce systems and their data integration, knowledge of data visualization tools (e.g., Tableau, Looker), understanding of machine learning and data analytics, certification in cloud platforms (AWS Certified Data Analytics, Google Professional Data Engineer, etc.). üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 21500, ""max"": 33000, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Office,317,Data Engineer z AWS,ASTEK Polska,"ASTEK Polska Jeste≈õmy czƒô≈õciƒÖ Grupy ASTEK, kt√≥ra od 1988 roku gromadzi do≈õwiadczenie na ≈õwiatowym rynku us≈Çug konsultingowych i in≈ºynieryjnych. Grupa ASTEK to globalny gracz w bran≈ºy doradztwa in≈ºynieryjnego i technologicznego, obecny na 5 kontynentach. Czego uczymy siƒô od innych podmiot√≥w Grupy w naszej codziennej pracy? Przede wszystkim: inspiracja, cele, dobre praktyki, innowacyjne dzia≈Çania i warto≈õci. W latach 2020, 2021, 2022 i 2023 otrzymali≈õmy certyfikat Great Place to Work i znale≈∫li≈õmy siƒô w gronie 15 Najlepszych Miejsc Pracy w Polsce w kategorii firm miƒôdzynarodowych. Proponujemy: Tr√≥jmiasto - praca 100% z biura w Gda≈Ñsku Praca dla klienta z bra≈ºy bankowej Wynagrodzenie - do 18 000 - 22 000z≈Ç brutto UOP lub do 1500 z≈Ç netto/dziennie B2B. Mo≈ºliwy start od zaraz lub max z 1 miesiƒôcznym okresem wypowiedzenia Projekt: Do≈ÇƒÖcz do projektu budowy, migracji i modernizacji platformy danych docelowo w pe≈Çni opartej na ≈õrodowisku AWS . Projekt realizowany jest dla jednego z wiodƒÖcych bank√≥w , z wysokim naciskiem na bezpiecze≈Ñstwo danych i compliance. Zesp√≥≈Ç bƒôdzie odpowiedzialny za transformacjƒô ≈õrodowiska danych, dlatego szukamy specjalist√≥w z do≈õwiadczeniem w pracy z du≈ºymi zbiorami danych i chmurƒÖ AWS. Zakres obowiƒÖzk√≥w: Wsparcie procesu projektowania, budowy i migracji platformy danych do AWS Tworzenie i rozwijanie pipeline‚Äô√≥w danych, przygotowanie danych do analizy Eksploracja, profilowanie i czyszczenie danych Udzia≈Ç w procesie transformacji danych (ETL/ELT) Weryfikacja sp√≥jno≈õci i jako≈õci danych w ≈õrodowisku migracyjnym ≈öcis≈Ça wsp√≥≈Çpraca z analitykami biznesowymi, in≈ºynierami danych i zespo≈Çami QA ≈öcis≈Ça wsp√≥≈Çpraca z zespo≈Çem projektowym w Gda≈Ñsku Wymagania jƒôzykowe: Upper-lntermediate (B2) Oczekiwania: Do≈õwiadczenie zawodowe jako Data Engineer lub Data Analyst w projektach zwiƒÖzanych z danymi Znajomo≈õƒá ≈õrodowiska AWS ‚Äì mile widziane do≈õwiadczenie z us≈Çugami typu: S3, Glue, Redshift, Athena lub pokrewne Dobra znajomo≈õƒá SQL ‚Äì umiejƒôtno≈õƒá pisania zapyta≈Ñ, analizowania du≈ºych zbior√≥w danych Do≈õwiadczenie w pracy z danymi wra≈ºliwymi / regulowanymi (mile widziane w sektorze bankowym lub finansowym) Umiejƒôtno≈õƒá pracy z du≈ºƒÖ dok≈Çadno≈õciƒÖ i zgodnie z wymaganiami bezpiecze≈Ñstwa danych Komunikatywna znajomo≈õƒá jƒôzyka angielskiego Zapewniamy! D≈ÇugoterminowƒÖ wsp√≥≈Çprace Mo≈ºliwo≈õƒá wyboru preferowanego rodzaju wsp√≥≈Çpracy (umowa o pracƒô ze wszystkimi ≈õwiadczeniami lub elastyczna umowa B2B) Szkolenia techniczne, certyfikaty i podnoszenie kwalifikacji Mentoring Centrum Kompetencyjnego ‚Äì od pierwszego dnia pracy bƒôdziesz cz≈Çonkiem spo≈Çeczno≈õci CC. Bƒôdziesz mia≈Ç szansƒô rozwijaƒá swoje umiejƒôtno≈õci, braƒá udzia≈Ç w r√≥≈ºnorodnych konferencjach oraz dzieliƒá siƒô swojƒÖ wiedzƒÖ i do≈õwiadczeniem z lud≈∫mi, kt√≥rzy w swojej codziennej pracy stajƒÖ przed tymi samymi wyzwaniami JasnƒÖ ≈õcie≈ºkƒô kariery Pakiet ≈õwiadcze≈Ñ pracowniczych PrzyjaznƒÖ atmosferƒô pracy, imprezy integracyjne i spotkania integracyjne üì© Potrzebujesz wiƒôcej informacji? Kontakt: anna.jarecka@astek.net üßßBonus!!! Nie chodzi o Ciebie? Poleƒá nam swojego znajomego i zdobƒÖd≈∫ bonus do 7000 z≈Ç Link: https: //astek.pl/system-rekomendacji/ AO200330","[{""min"": 17000, ""max"": 22000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,318,Data Migration Architect‚Äã,Cognizant Technology Solutions,"Location: Warsaw - Hybrid (minimum 3 days at the office) What we do: At Cognizant, we are dedicated to helping the world's leading companies build stronger businesses ‚Äî helping them go from doing digital to being digital. In Poland, our offices are in Gda≈Ñsk, Wroclaw, and Krak√≥w. However proposed offer is based in the Client office based in Warsaw. With the capacity to support various clients, we offer a world of opportunities for both professionals and graduates. You can expect five-star training, a chance to realize your career goals, and a range of benefits. Be Cognizant! About the role: We're looking for a highly skilled Data Migration Architect to join our dynamic team. In this pivotal role, you'll design and implement comprehensive data migration projects, ensuring a seamless and efficient shift to the cloud-based Snowflake platform. This is an exceptional opportunity to leverage your profound understanding of Snowflake, data warehousing concepts, and cloud-based data architectures to deliver innovative solutions that directly enhance user experience and business operations. About the Team Our team collaborates with all major hyperscalers and leading software vendors in the data and AI space, including AWS, Microsoft, Google, Snowflake, and Databricks. We are committed to leveraging the best tools and technologies to deliver scalable, secure, and efficient solutions. Supported by dedicated Centers of Excellence, we provide specialized expertise and resources to meet unique client needs and drive innovation in AI and data effectively. Our commitment extends to enhancing digital maturity, driving innovation through continuous investment in AI (including our Neuro AI platform), promoting sustainability, and fostering strong partner collaborations. Your Role and Responsibilities: As a Data Migration Architect specializing in Teradata to Snowflake migration, you will be responsible for designing and executing end-to-end data migration projects. This includes: Leading Data Migration Strategies: Design and execute large-scale data migration strategies across cloud and hybrid environments. Architecting Data Pipelines: Create secure, scalable, and high-performance data pipelines to support enterprise data modernization initiatives. Legacy System Assessment: Analyze current data architectures and workloads to design optimal Snowflake migration solutions. Seamless Data Transfer: Develop and execute migration strategies for high-volume data, minimizing downtime and ensuring data integrity. Stakeholder Collaboration: Work closely with data engineers, business analysts, and IT teams to understand requirements and provide guidance on Snowflake best practices. Optimization: Optimize Snowflake workloads, including query performance tuning, data model optimization, and managing storage costs. Data Governance & Security: Enforce data governance policies, ensure compliance with security and regulatory standards, and manage metadata. Best Practices: Implement best practices for data quality, lineage, and overall data lifecycle management. What You'll Bring: Mandatory Skills: Cloud Data Platforms: Deep expertise in Snowflake / Databricks (Delta Lake, Unity Catalog, Lakehouse architecture). Data Migration Tools: Experience with tools like Azure Data Factory, IDMS, or custom migration frameworks. ETL/ELT Development: Designing and orchestrating pipelines using Apache Spark, ADF, or Databricks Workflows. SQL & Performance Tuning: Advanced SQL for data transformation, optimization, and troubleshooting. Data Modeling: Proficiency in dimensional modeling, data vault, and schema design for analytical workloads. Programming/Scripting: Hands-on experience with Python, Scala, or SQL for data engineering and automation. Data Governance & Security: Implementing RBAC, data masking, auditing, and compliance frameworks. Monitoring & Optimization: Experience with tools like Datadog, Azure Monitor, or Snowflake Query Profiler for performance tuning. Proven experience leading end-to-end migration of data and workloads from on-premises data platforms (e.g., Oracle, SQL Server, Teradata) or cloud-based systems to Snowflake. Experience migrating legacy/on-prem ETL/ELT workloads to modern ETL/ELT solutions (e.g., Matillion, Fivetran, dbt). Nice to Have / Your Chance to Grow: Snowflake certification (e.g., SnowPro Core, SnowPro Advanced). Experience with at least one ETL/ELT tool like SSIS, Talend, or Informatica. Knowledge of data modeling and schema design specifically in Snowflake. Familiarity with CI/CD pipelines for data engineering. Enterprise sales/pre-sales/solutions background is a plus. What we offer: Competitive salary with cafeteria benefits and bonuses Opportunity to be part of a rapidly expanding global organization Pleasant and inspiring working atmosphere Professional development and clear career path Training & development opportunities Private healthcare and additional life insurance Employee volunteering programs and opportunities Inclusion and diversity in practice Employee referral program in place For more information about Cognizant, visit https: //www.cognizant.com/en-pl/ If you are looking for another opportunity and are interested in the company, do not hesitate to apply online! We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status.",[],Data Architecture,Data Architecture
Full-time,Senior,Permanent or B2B,Remote,319,Data Architect,Transition Technologies MS,"Work as a part of cross-skilled team to understand, design and build data pipelines in order to maximize value of data in the organization Make sure data is well structured, described by means of metadata and made readily available to the whole organization, treating data as a product - with machine/deep learning as a next step to build insights out of it Research and refine architecture, technology stack and whatever else is needed to achieve the business objectives in the best possible way Help define standards and good practices in regards to Data Architecture Experienced in designing conceptual, logical and physical data models Understands traditional relational data modeling Understands Data Vault 2.0 methodology Understands data processing (ETL/ELT) and harmonization (master data enhancements) on structured and unstructured sources Hands on experience in data engineering Knows data governance and data catalog best practices Understands DBT Understands GIT Excellent communication skills Nice-to-have experience and qualifications: Experienced in Data Vault 2.0 modeling (being certified is huge plus) Experienced in DBT Knows the DIA Commercial Data landscape Understands the fundamentals of distributed data warehousing Interesting and challenging projects Flexible working hours Friendly, non-corporate atmosphere Stable working conditions (CoE or B2B) Possibility for self-development and promotion in the company Rich benefits package Possibility to work remotely We reserve the right to contact the selected candidates.",[],Data Architecture,Data Architecture
Full-time,Mid,Permanent or B2B,Remote,320,Data Engineer (Palantir),Upvanta sp. z o.o.,"Szukamy do≈õwiadczonego Data Engineera, kt√≥ry do≈ÇƒÖczy do naszego zespo≈Çu i pomo≈ºe nam rozwijaƒá rozwiƒÖzania analityczne oparte na platformie Palantir (Foundry). Je≈õli masz do≈õwiadczenie w in≈ºynierii danych, potrafisz pracowaƒá z du≈ºymi zbiorami danych i chcesz pracowaƒá z technologiƒÖ wykorzystywanƒÖ przez najwiƒôksze organizacje na ≈õwiecie ‚Äî ta oferta jest dla Ciebie. ‚úÖ Wymagania: Do≈õwiadczenie w pracy z platformƒÖ Palantir Foundry (lub silna motywacja do szybkiego wdro≈ºenia siƒô) Znajomo≈õƒá proces√≥w przetwarzania i integracji danych Umiejƒôtno≈õƒá modelowania danych oraz pracy z pipeline‚Äôami ETL Dobra znajomo≈õƒá SQL oraz jednego z jƒôzyk√≥w skryptowych (np. Python) Zdolno≈õƒá do analitycznego my≈õlenia i rozwiƒÖzywania problem√≥w Umiejƒôtno≈õƒá pracy w zespole i wsp√≥≈Çpracy z interesariuszami biznesowymi Do≈õwiadczenie z chmurƒÖ (AWS, Azure lub GCP) Znajomo≈õƒá narzƒôdzi CI/CD Znajomo≈õƒá zagadnie≈Ñ Data Governance i Data Quality Tworzenie, rozwijanie i utrzymywanie pipeline‚Äô√≥w danych w ≈õrodowisku Palantir Foundry Modelowanie danych na potrzeby analiz i raportowania Wsp√≥≈Çpraca z zespo≈Çami analitycznymi i biznesowymi przy wdra≈ºaniu rozwiƒÖza≈Ñ opartych na danych Optymalizacja proces√≥w przetwarzania danych Udzia≈Ç w projektach transformacyjnych na poziomie ca≈Çej organizacji Pracƒô w ambitnym, miƒôdzynarodowym zespole przy innowacyjnych projektach Mo≈ºliwo≈õƒá pracy z nowoczesnƒÖ technologiƒÖ Palantir Foundry, wykorzystywanƒÖ globalnie Elastyczne godziny pracy i mo≈ºliwo≈õƒá pracy zdalnej Atrakcyjne wynagrodzenie dopasowane do Twoich kompetencji Wyb√≥r formy zatrudnienia: B2B lub UoP Pakiet benefit√≥w: prywatna opieka medyczna, karta sportowa, sprzƒôt do pracy Kultura pracy oparta na wsp√≥≈Çpracy, zaufaniu i odpowiedzialno≈õc",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,321,Data Analyst,PeakData,"About Peak Data: Peak Data is a Swiss-based startup revolutionizing the pharmaceutical industry through innovative data solutions. Our mission is to empower pharmaceutical companies with actionable insights, driving better decision-making and improving patient outcomes. Join our dynamic team and be a part of a company that is at the forefront of data-driven healthcare innovation. Position Overview: We are seeking a highly skilled and motivated Data Analyst to join our Operations Department. The ideal candidate will have a strong background in data analysis, with a particular focus on the pharmaceutical industry. You will be responsible for analyzing complex data sets, developing insights, and providing recommendations to support our operational strategies. Proficiency in SQL, Python, and AWS is essential for this role. Key Responsibilities: ‚Ä¢ Collect, process, and analyse large datasets to identify trends, patterns, and insights related to pharmaceutical operations. ‚Ä¢ Develop and maintain dashboards and reports to monitor key performance indicators (KPIs) and operational metrics. ‚Ä¢ Collaborate with cross-functional teams to define data requirements and ensure data accuracy and integrity. ‚Ä¢ Utilize SQL to query databases and extract relevant data for analysis. ‚Ä¢ Apply Python for data manipulation, statistical analysis, and automation of data workflows. ‚Ä¢ Leverage AWS services to manage and analyse data in the cloud environment. ‚Ä¢ Provide actionable insights and recommendations to support decision-making processes within the operations department. ‚Ä¢ Stay updated with industry trends, best practices, and emerging technologies in data analysis and the pharmaceutical sector. Qualifications: ‚Ä¢ Bachelor‚Äôs degree in Data Science, Statistics, Computer Science, or a related field. A Master‚Äôs degree is a plus. ‚Ä¢ Proven experience as a Data Analyst, preferably within the pharmaceutical industry. ‚Ä¢ Strong proficiency in SQL for data querying and database management. ‚Ä¢ Advanced skills in Python for data analysis, statistical modelling, and automation. ‚Ä¢ Experience with AWS services, including data storage, processing, and analytics. ‚Ä¢ Solid understanding of pharmaceutical industry operations, regulations, and data requirements. ‚Ä¢ Excellent analytical and problem-solving skills with the ability to interpret complex data sets. ‚Ä¢ Strong communication skills, with the ability to present findings and insights to both technical and non-technical stakeholders. ‚Ä¢ Detail-oriented, with a commitment to accuracy and data integrity. ‚Ä¢ Ability to work independently and collaboratively in a fast-paced startup environment. What We Offer: ‚Ä¢ Competitive salary and benefits package. ‚Ä¢ Opportunity to work with a passionate and innovative team in a growing startup. ‚Ä¢ Flexible working hours and remote work options. ‚Ä¢ Professional development opportunities and support for continuous learning. ‚Ä¢ A dynamic and inclusive work environment that values creativity and diversity.","[{""min"": 6368, ""max"": 12736, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,322,Data Engineer,emagine Polska,"Informacje o projekcie: Bran≈ºa: finanse/po≈ºyczki Lokalizacja: 100% zdalnie Umowa: B2B Stawka: 200 pln/h netto + VAT D≈Çugo≈õƒá projektu: d≈Çugoterminowy Poszukujemy do≈õwiadczonego Data Engineera do zespo≈Çu Data Platform, kt√≥ry bƒôdzie modelowaƒá dane oraz implementowaƒá procesy ELT w chmurze. ObowiƒÖzki: Modelowanie struktur bazodanowych w podej≈õciu DDD oraz tworzenie logicznych i fizycznych modeli danych. Opracowywanie warstwy Data Contracts na podstawie zamodelowanych struktur dla domen danych. Wsp√≥≈Çpraca w procesie ingerencji danych z system√≥w ≈∫r√≥d≈Çowych. Implementacja modeli danych w Data Platform na r√≥≈ºnych warstwach (Bronze, Silver, Gold) w ≈õrodowisku Azure Databricks. Wymagania: Do≈õwiadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejƒôtno≈õƒá implementacji proces√≥w ELT. Mile widziane: Umiejƒôtno≈õƒá tworzenia dokumentacji technicznej. Do≈õwiadczenie w mapowaniu danych ze ≈∫r√≥d≈Çowych do docelowych struktur. Znajomo≈õƒá narzƒôdzi do zarzƒÖdzania metadanymi (np. Azure Purview).","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,323,"Senior Data Software Engineer (Python, GCP)",EPAM Systems,"We are looking for a Senior Data Software Engineer to join the project in the telecommunications sector, focused on creating network infrastructure for high-speed internet access using fiber optic communication. In this role, you will work with an American multinational technology company. The project offers many opportunities to learn as our client is considered one of the Big Five companies in the American information technology industry. This position offers a hybrid model, with 3 days per week working from the client‚Äôs office for candidates from Gdansk, Wroclaw, Warsaw, and Krakow. Responsibilities Provide technical leadership and oversight for the other developers working on the project Design and build ETL/ELT pipelines using Airflow and other technologies on the GCP Provide advice and recommendations (in writing when appropriate) to the Data Warehouse Technical Lead regarding technical options and best practices for the data warehouse and related ETL/ELT pipelines Analyze source data and work with internal data consumers to determine which data is needed and how it should be represented in the output table schemas Write and maintain related technical documentation Perform thorough design and code reviews for other developers Requirements 4+ years of relevant professional experience Solid experience in SQL scripting and Python programming At least 1 year of ETL/ELT to BigQuery experience Experience in developing solutions for the GCP Proficiency with ETL/ETL for data warehousing including data source investigation/analysis, target schema design and data pipeline design/implementation Ability to write clear, concise, and well-reasoned technical explanations and documentation for both engineer and analyst internal audiences (design docs, architecture views/diagrams, etc.) Solid interpersonal skills for working with both upstream teams providing data and downstream analysts consuming data B2+ English level proficiency Nice to have Experience with Apache Airflow Familiarity with tools like IntelliJ, Gradle, Google Cloud Storage, Google Cloud Datastream, Google Cloud Data Catalog, Google Looker, Google Cloud Cortex Google Cloud Platform (GCP) certification We offer We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,324,Senior Data Business Analyst,TQLO SP√ì≈ÅKA Z OGRANICZONƒÑ ODPOWIEDZIALNO≈öCIƒÑ,"Do≈ÇƒÖczysz do strategicznego, globalnego programu transformacji danych, kt√≥rego celem jest ujednolicenie i automatyzacja raportowania finansowego, zarzƒÖdczego i regulacyjnego na poziomie Grupy w Europie, Amerykach i Azji. Projekt opiera siƒô na centralnym Globalnym Centrum Danych (Global Data Hub), korzystajƒÖcym z certyfikowanych ≈∫r√≥de≈Ç danych (Golden Sources) pochodzƒÖcych z Data Lake, co gwarantuje wysokƒÖ jako≈õƒá danych, ich sp√≥jno≈õƒá i zgodno≈õƒá z wymaganiami regulacyjnymi. Obszar PPT (Processes, Payments, Treasury) zapewnia wsparcie dla wszystkich pozosta≈Çych domen: accounting, reporting, payments czyli us≈Çug, bez kt√≥rych bank nie mo≈ºe funkcjonowaƒá. Projekty sƒÖ mocno bazodanowe, obejmujƒÖ implementacjƒô rozwiƒÖza≈Ñ data driven dla regulatory reporting, skalowanie produkt√≥w na r√≥≈ºne kraje, a zakres ich dzia≈Çania jest bardzo szeroki. Praca Hybrydowa - Warszawa - 3 dni z biura G≈Ç√≥wne obowiƒÖzki: Reprezentowanie g≈Çosu klienta oraz podejmowanie decyzji dotyczƒÖcych funkcjonalno≈õci platformy danych. Definiowanie, utrzymywanie i priorytetyzacja backlogu produktowego dla zespo≈Çu in≈ºynierii danych. Przek≈Çadanie potrzeb biznesowych na szczeg√≥≈Çowe user stories, wymagania techniczne i kryteria akceptacji. Wspieranie oceny obecnych modeli operacyjnych oraz wsp√≥≈Çtworzenie modelu docelowego. Zbieranie i przygotowywanie wymaga≈Ñ biznesowych we wsp√≥≈Çpracy z u≈ºytkownikami ko≈Ñcowymi. Wsp√≥≈Çpraca z programistami przy tworzeniu plan√≥w test√≥w i wspieranie test√≥w oraz walidacji. Promowanie ≈õrodowiska wysokiej wydajno≈õci poprzez jasne standardy i ciƒÖg≈Çe doskonalenie. Udzia≈Ç w ceremoniach Agile zgodnie z metodologiƒÖ Scrum. Dopasowanie mo≈ºliwo≈õci platformy do najlepszych praktyk w zakresie ≈Çadu danych, jako≈õci i ≈õledzenia pochodzenia danych. Monitorowanie wydajno≈õci platformy i dostarczania warto≈õci oraz dostosowywanie priorytet√≥w. ≈öledzenie trend√≥w w in≈ºynierii danych, analizie i technologiach chmurowych. Kluczowy kontakt dla interesariuszy z r√≥≈ºnych kraj√≥w i dzia≈Ç√≥w w celu zapewnienia zgodno≈õci i dostarczania warto≈õci. Wymagania: Minimum 7 lat do≈õwiadczenia w analizie biznesowej/danych, najlepiej w r√≥≈ºnych organizacjach. Udokumentowana wsp√≥≈Çpraca z zespo≈Çami deweloperskimi i znajomo≈õƒá cyklu ≈ºycia produktu. Praktyczna znajomo≈õƒá Agile, zw≈Çaszcza SCRUM, z podej≈õciem product ownership. Dobra znajomo≈õƒá platform danych, architektury Data Lake i proces√≥w in≈ºynierii danych. Znajomo≈õƒá przetwarzania, transformacji i modelowania danych (znajomo≈õƒá SQL, Python). Bieg≈Ço≈õƒá w SQL i umiejƒôtno≈õƒá interpretacji modeli danych. Zaawansowana znajomo≈õƒá Excela, tak≈ºe w analizie z≈Ço≈ºonych danych. Silne umiejƒôtno≈õci rozwiƒÖzywania problem√≥w i krytycznego my≈õlenia, podej≈õcie proaktywne. Znajomo≈õƒá sektora bankowego/finansowego, w tym obszar√≥w raportowania regulacyjnego lub us≈Çug wsp√≥lnych. Wykszta≈Çcenie wy≈ºsze (finanse, ekonomia, matematyka, IT lub pokrewne). Umiejƒôtno≈õƒá t≈Çumaczenia z≈Ço≈ºonych koncepcji technicznych dla nietechnicznych interesariuszy. Bardzo dobre umiejƒôtno≈õci komunikacyjne, interpersonalne i negocjacyjne. Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (B2+ lub wy≈ºszy), w mowie i pi≈õmie. Nice to have: Do≈õwiadczenie z platformami danych w chmurze (np. AWS, Azure, Snowflake). Podstawowa znajomo≈õƒá Big Data, Apache Spark i ≈õrodowisk chmurowych. Znajomo≈õƒá framework√≥w ≈Çadu danych (data governance), katalog√≥w danych, kontroli dostƒôpu, ≈õledzenia pochodzenia danych. Znajomo≈õƒá nowoczesnej architektury danych: data mesh, lakehouse, modern data stack. Do≈õwiadczenie w ≈õrodowiskach regulowanych lub z≈Ço≈ºonych (np. finanse, opieka zdrowotna, telekomunikacja). Podstawowa znajomo≈õƒá CI/CD, JIRA, Confluence i narzƒôdzi automatycznych test√≥w. Certyfikaty w obszarze analizy biznesowej lub danych (np. CBAP, PMI-PBA) bƒôdƒÖ atutem. Znajomo≈õƒá jƒôzyka hiszpa≈Ñskiego bƒôdzie dodatkowym atutem. Benefity: KluczowƒÖ rolƒô w globalnej transformacji danych z realnym wp≈Çywem na ca≈ÇƒÖ organizacjƒô. Model pracy hybrydowej oraz elastyczno≈õƒá wspierajƒÖcƒÖ work-life balance. Biuro w centrum Warszawy z nowoczesnym sprzƒôtem. Konkurencyjny pakiet wynagrodzenia. ≈öwietnƒÖ atmosferƒô w miƒôdzynarodowym zespole. Mo≈ºliwo≈õƒá pracy z nowoczesnymi technologiami (cloud, data lake, big data) oraz w strategicznych projektach. Szansa na rozw√≥j technicznych i funkcjonalnych umiejƒôtno≈õci dziƒôki ciƒÖg≈Çemu uczeniu siƒô. Benefity: prywatna opieka medyczna, ubezpieczenie grupowe, karta Multisport.","[{""min"": 22000, ""max"": 28000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent or B2B,Hybrid,325,Administrator Power BI Report Server,Nexio Management,"Nexio Management to zaufany partner biznesowy w drodze do cyfrowej przysz≈Ço≈õci. Posiadamy prawie 20-letnie do≈õwiadczenie na rynku IT w Polsce i poza jej granicami. Prowadzimy dzia≈Çania w oparciu o transparentne i szczere relacje. Tworzymy innowacyjne rozwiƒÖzania technologiczne, kreujƒÖc przy tym interesujƒÖce i rozwojowe ≈õrodowisko pracy dla naszych ekspert√≥w. Obecnie zatrudniamy 550 konsultant√≥w, kt√≥rzy ≈õwiadczƒÖ us≈Çugi IT dla Klient√≥w na ca≈Çym ≈õwiecie. Nasza g≈Ç√≥wna siedziba mie≈õci siƒô w Warszawie, poza tym mamy biura w Rumunii oraz Wielkiej Brytanii. Posiadamy r√≥wnie≈º w≈Çasne R&D Center, kt√≥re jest miejscem powstawania innowacyjnych projekt√≥w m.in.: w obszarach test√≥w, Big Data, Cloud czy AI.W ramach naszych us≈Çug tworzymy szyte na miarƒô rozwiƒÖzania, utrzymujemy i rozwijamy nawet najbardziej wymagajƒÖce systemy IT. Dzia≈Çamy w takich modelach biznesowych jak managed services, fixed prices oraz wspieramy zespo≈Çy naszych klient√≥w w modelach scale up the team. Naszymi klientami sƒÖ firmy z wielu zr√≥≈ºnicowanych bran≈º, szukajƒÖce wsparcia najwy≈ºszej klasy ekspert√≥w. Zakres roli: Udzia≈Ç w pracach projektowych zwiƒÖzanych z wdro≈ºeniem, administracjƒÖ i stabilizacjƒÖ ≈õrodowiska Power BI Report Server w firmie klienta , w tym obs≈Çuga zg≈Çosze≈Ñ u≈ºytkownik√≥w, wsparcie proces√≥w DR, backup oraz aktualizacja dokumentacji. Do obowiƒÖzk√≥w nale≈ºy r√≥wnie≈º przekazanie rozwiƒÖzania do eksploatacji zgodnie ze standardami, wsp√≥≈Çtworzenie bazy wiedzy oraz weryfikacja potrzeb zwiƒÖzanych z capacity planning. Zadania : Udzia≈Ç w projektach wdro≈ºeniowych i administracyjnych zwiƒÖzanych z Power BI Report Server Wsparcie w opracowywaniu oraz aktualizacji dokumentacji zwiƒÖzanych z procedurami reagowania na awarie RozwiƒÖzywanie incydent√≥w i problem√≥w technicznych Ustalanie i koordynowanie okien niedostƒôpno≈õci system√≥w w zwiƒÖzku z releasami, zmianami serwisowymi i innymi pracami technicznymi Wsp√≥≈Çtworzenie i rozwijanie wewnƒôtrznej Bazy Wiedzy Monitorowanie potrzeb wydajno≈õciowych, w kontek≈õcie infrastruktury Power BI Report Server. Obs≈Çugiwanie zg≈Çosze≈Ñ u≈ºytkownik√≥w Tworzenie pe≈Çnej dokumentacji wymaganej do wdro≈ºenia i eksploatacji rozwiƒÖzania, zgodnie z przyjƒôtymi standardami Przekazanie rozwiƒÖzania do u≈ºytkowania z zachowaniem obowiƒÖzujƒÖcych standard√≥w i procedur Przekazanie praw autorskich do wykonanych prac i opracowanej dokumentacji Oczekiwania: Minimum 2 lata do≈õwiadczenia w administracji Power BI Report Server lub podobnymi systemami raportowymi (np. SSRS, Tableau Server) - warunek konieczny Dobra znajomo≈õƒá Power BI Report Server, w tym Mile widziane: Do≈õwiadczenie w integracji Power BI Report Server z systemami klasy ERP/CRM Znajomo≈õƒá jƒôzyka DAX i Power Query Znajomo≈õƒá Power BI w wersji chmurowej (Power BI Service) Do≈õwiadczenie w optymalizacji wydajno≈õci raport√≥w i zapyta≈Ñ Oferujemy: Pracƒô w modelu hybrydowym (min. 1 razy w tygodniu praca w biurze/Wola) lub stacjonarnym D≈ÇugoterminowƒÖ wsp√≥≈Çpracƒô w oparciu o umowƒô B2B lub umowƒô o pracƒô Benefity: Medicover (rozszerzony o stomatologiƒô) i FitProfit Darmowe zdalne lekcje z jƒôzyka angielskiego Udzia≈Ç w r√≥≈ºnorodnych inicjatywach charytatywnych oraz sportowych (np. biegi firmowe, turnieje szachowe, rozgrywki pi≈Çki no≈ºnej) Zapraszamy do aplikowania : )",[],Database Administration,Database Administration
Full-time,Mid,Permanent,Hybrid,326,Data Engineer,ERGO Technology & Services,"ERGO Technology & Services S.A. (ET&S S.A.) was established in January 2021 following the integration of ERGO Digital IT and Atena into one entity, leveraging both companies‚Äô strengths and best practices. As a part of ERGO Technology & Services Management AG, the technology holding of ERGO Group AG, we support millions of internal and external customers with state-of-the-art IT solutions to everyday problems. In October 2022, ET&S S.A. expanded its scope of operations by creating a Business Services unit to contribute in a new way to the growth of ERGO‚Äôs business. Acting as a co-partner and internal consultant, it adds non-IT value and supports the development of the entire ERGO Group, currently offering skills in reporting, analysis, actuarial, and input management. We are committed to fostering innovation and meeting the evolving needs of our clients worldwide. Discover how we implement AI, IoT, Voice Recognition, Big Data science, advanced mobile solutions, and business-related services to anticipate and address our customers‚Äô future needs. We work for the financial industry and you will play a crucial role in a DevOps approach designing, building, and maintaining the infrastructure that underlies data-driven decision-making. Your primary responsibility is to ensure the efficient and reliable flow of data from various sources to support business intelligence, trading and risk management. The tools and frameworks we use: Azure OpenAI, Azure Machine Learning, Cognitive Services, Azure Data Factory, Azure Blob Storage, Azure DevOps, AKS, Azure DevOps, Databricks, Python, Docker, Kubernetes/AKS. Javascript, .Net, MLflow (Databricks). designing and implementing data models and architectures that support financial data management developing and maintaining ETL processes to integrate data from diverse sources, including market data feeds, customer information systems, and transactional databases ensuring data quality, security, and compliance with regulatory requirements collaborating with business owners, data analysts and scientists, SW engineers, and DevOps specialists to support the development of predictive models and business intelligence tools optimizing data processing workflows for performance, scalability, and reliability working in an international Scrum team fluency in English good knowledge of cloud solutions, preferably Azure Databricks strong SQL skills knowledge of ETL processes/programming communicative and open-minded person who is not afraid of challenges great understanding of Python & PySpark, and commercial use of it at least 3 years of experience as a Software Engineer using ML Ops, AI Model Operationalization, Python GenAI working experience with Javascript and .Net Medical package, sports card, and numerous sports sections ‚Äì these are some of the beneÔ¨Åts that help our employees stay in good shape. Work-life balance is a key aspect of a healthy workplace. We offer our employees flexible working hours, a confidential employee assistant program, as well as the possibility of remote working. However, staying at home with our in-office gaming room and dog-friendly office in Warsaw won‚Äôt be easy. We organize numerous workshops and training courses. Thanks to hackathons and meetups, our specialists share their expertise with others. Additionally, we have a wide range of digital learning platforms and language courses. Each year, we participate in several CSR activities, during which, together with our colleagues, we do our best to create a better future. Company-wide bike races and soccer matches, Ô¨Ålm marathons in our cinema room or other engaging team-building activities ‚Äì we got it covered! Every team member is valued, regardless of gender, nationality, religious beliefs, disability, age, and sexual orientation or identity. Your qualiÔ¨Åcations, experience, and mindset are our greatest beneÔ¨Åt!",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,327,Data Scientists with Generative AI (mid/senior),Yosh.AI,"We are excited to announce an open position for: Data Scientists with Generative AI (mid/senior) - Location: Warsaw/remote We are looking for a hands-on generative AI engineer to architect, build, and deploy the next generation of autonomous and agentic AI systems. In this role, you will bridge the gap between rapid prototypes and robust, production-ready solutions that solve complex business challenges for our enterprise clients. You will be at the forefront of developing innovative technologies on a global scale, leveraging the full power of Google's Generative AI stack, LangChain, and other advanced models to create solutions that think, reason, and act. This is a unique opportunity to own end-to-end projects and work with a team of passionate experts, in close collaboration with Google, to drive real-world impact. Key Responsibilities: Architect and build enterprise-grade, agentic AI systems and conversational agents Engineer and deploy scalable AI/ML solutions on Google Cloud Platform (GCP) Develop and maintain serverless systems and containerized Python REST APIs Leverage LLMs to perform deep analysis on structured and unstructured data, enhancing our big data analysis platforms Drive innovation by rapidly prototyping new solutions and championing out-of-the-box thinking Required Experience: Proven experience building and deploying AI/ML models in a production environment Excellent programming skills in Python and experience building and deploying REST APIs Hands-on experience with a major cloud platform (GCP, AWS, or Azure). Practical experience with LLM orchestration frameworks Deep knowledge of Generative AI concepts Understanding of machine learning concepts Proficiency with version control systems (Git) Highly Desirable: Specific, in-depth experience with the Google Cloud Platform (GCP) AI/ML stack Experience designing and building fully autonomous or agentic AI systems Knowledge of technologies related to LLM models (e.g., LangChain, ADK, Vertex AI) Practical experience with Conversational AI platforms (e.g., Google Dialogflow CX/Playbooks) Experience with containerization technologies, specifically Docker Salary Range: 10.000-20.000 PLN gross per month (full time contract equivalent, depending on experience) We offer: Opportunity for professional development in the area of GenAI Private Medical insurance Multisport card Google certification paths Hybrid or remote location - our office is located in the center of Warsaw Cooperation with a great team of energetic and open-minded people","[{""min"": 10000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Hybrid,328,Senior Data Engineer,Winged IT,"Do you want to play a key role in revolutionizing the future of finance? Join our Client - a global leader in deferred payments, with over 85 million active users and 2.5 million transactions processed daily! As a pioneer in modern payment solutions, our Client is developing innovative methods that streamline the shopping experience, enhance transaction security, and expand the availability of purchasing options. We're seeking individuals eager to achieve remarkable results and share their bold vision to redefine the future of payments and fintech. Send us your CV - we can‚Äôt wait to meet you! Your role is: -> To design and implement robust, scalable data pipelines to collect, integrate, and analyze large volumes of data from various sources including Jira, AWS S3, and external websites via APIs; -> To develop and maintain databases and data tables that are optimized for performance and scalability within Klarna‚Äôs cloud environment, leveraging AWS services; -> To collaborate with different teams to understand data needs and deliver solutions that support business objectives; -> To ensure data integrity and compliance with data governance and security policies; -> To provide technical leadership and mentorship to data analysts in the team; Stay current with industry trends and evaluate new technologies for continuous improvements in data architecture and processing; -> To work closely with other teams to leverage Klarna‚Äôs internal systems for optimized data management and operations; -> To assist DevOps or Full Stack engineers, when necessary, to achieve team objectives. Your skills and experiences: -> 5+ years of experience in data engineering, particularly in designing and developing data pipelines; -> Strong programming skills in Python and experience with API integrations; -> Extensive experience with AWS Cloud Services (e.g., S3, EC2, RDS, Lambda, Glue Jobs, EMR) and understanding of best practices in cloud security; -> Experience with Terraform and infrastructure as a code; -> Proficiency in SQL and experience with relational and NoSQL databases; -> Experience with graph and vector databases; -> Capability to handle multiple high-priority tasks simultaneously and meet tight deadlines without sacrificing detail or accuracy; -> Excellent interpersonal skills to engage with colleagues across various teams, contributing to fast-paced projects; -> Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Engineering, or a related field; -> Language proficiency: Advanced English (minimum B2 level). Nice to have: + DevOps experience and skills will be viewed as a huge plus; + FullStack or Development experience and skill will be viewed as a huge plus; + Familiarity with Klarna‚Äôs internal systems, such as C2C and Data Platform services, is highly preferred. Our client offers: + Great opportunity for personal development in a stable and friendly large multinational company; + Start-up mentality, small agile teams; + Global Reach: Impact millions with seamless shopping and payments; + Career growth and additional education. Ôªø","[{""min"": 170, ""max"": 195, ""type"": ""Net per day - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Office,329,Digital Data Analyst,OTCF,"Rok 2024 przyni√≥s≈Ç nam kolejne triumfy sportowe i biznesowe! Na Igrzyskach Olimpijskich w Pary≈ºu ubrali≈õmy 6 Komitet√≥w Olimpijskich oraz lekkoatlet√≥w z 3 kraj√≥w, w tym reprezentant√≥w Polskiego ZwiƒÖzku Lekkiej Atletyki. Z dumƒÖ przygotowujemy profesjonalne stroje m.in. dla Reprezentacji Polski w koszyk√≥wce i ampfutbolu, kilku klub√≥w pi≈Çkarskiej PKO BP Ekstraklasy i siatkarskiej Plus Ligi oraz wszystkich dru≈ºyn koszykarskiej Orlen Basket Ligi i Orlen Basket Ligi Kobiet . Marka 4F jest obecna w 48 krajach na ca≈Çym ≈õwiecie, a nasze logo mo≈ºna zobaczyƒá zar√≥wno w pobliskiej Rumunii, jak i w odleg≈Çym Katarze czy Tajlandii. To dopiero poczƒÖtek naszej globalnej ekspansji! Razem z markƒÖ Under Armour otworzyli≈õmy w Polsce 12 sklep√≥w. Dziƒôki naszej Fundacji 4F Pomaga zbudowali≈õmy ju≈º 5 boisk sportowych, gdzie m≈Çodzi sportowcy mogƒÖ rozwijaƒá swoje talenty. Dzia≈Çamy zgodnie z naszƒÖ strategiƒÖ 4F Change, prowadzƒÖc biznes w spos√≥b zr√≥wnowa≈ºony. Znale≈∫li≈õmy siƒô na li≈õcie Forbes‚Äôa Poland‚Äôs Best Employers 2024, zajmujƒÖc pierwsze miejsce w kategorii handel hurtowy. Nasze sukcesy sƒÖ wynikiem naszych warto≈õci: #BeOneTeam, #BeBetterEveryday i #BeResponsible. Codziennie gramy jako jedna dru≈ºyna, dƒÖ≈ºƒÖc do wsp√≥lnego celu! Determinacja i zaanga≈ºowanie sƒÖ w naszym DNA, dlatego ju≈º pracujemy nad kolejnymi projektami, nie tylko sportowymi. Chcesz do≈ÇƒÖczyƒá do naszej dru≈ºyny? #WeMakeATeam #OTCFteam to ludzie pe≈Çni pasji i zaanga≈ºowania. Przed nami wiele wyzwa≈Ñ i ciekawych projekt√≥w. Jakie wyzwania dla Ciebie przygotowali≈õmy? Analiza danych marketingowych i sprzeda≈ºowych, w tym ruchu na stronie i w aplikacji mobilnej, oraz wyciƒÖganie odpowiednich wniosk√≥w. Opracowywanie rekomendacji kampanii na podstawie przeprowadzonych analiz. Proaktywne identyfikowanie szans i zagro≈ºe≈Ñ biznesowych w dostƒôpnych danych ilo≈õciowych i jako≈õciowych. Samodzielne prowadzenie projekt√≥w analitycznych z zakresu digital, zapewnienie sp√≥jno≈õci i poprawno≈õci danych. Automatyzacja proces√≥w, w tym aktualizacja raport√≥w. Integracja z zewnƒôtrznymi dostawcami danych. Brzmi interesujƒÖco? Digital Data Analyst w naszej dru≈ºynie to zawodnik, kt√≥ry: Minimum 3 lata do≈õwiadczenia na stanowisku Digital Data Analyst . Do≈õwiadczenie w analizie du≈ºych zbior√≥w danych, dobra znajomo≈õƒá SQL (mile widziana znajomo≈õƒá Python ). Do≈õwiadczenie w pracy z narzƒôdziami analitycznymi ( GA4 , Google Data Studio , HotJar, Crazy Egg); znajomo≈õƒá AppsFlyer i BigQuery bƒôdzie atutem. Bieg≈Ça znajomo≈õƒá Google Spreadsheet, Excel, Looker i Power BI. Umiejƒôtno≈õƒá identyfikowania problem√≥w analitycznych , opracowywania rozwiƒÖza≈Ñ i formu≈Çowania rekomendacji biznesowych. Zdolno≈õƒá do przedstawiania z≈Ço≈ºonych danych zar√≥wno technicznym, jak i nietechnicznym odbiorcom. Znajomo≈õƒá angielskiego na poziomie komunikatywnym (min. B2). Dodatkowym atutem bƒôdzie znajomo≈õƒá narzƒôdzi takich jak AirFlow, GitLab i Supermetrics. Je≈õli na powy≈ºsze pytania odpowied≈∫ jest twierdzƒÖca to wyglƒÖda na to, ≈ºe jeste≈õ brakujƒÖcym zawodnikiem naszej dru≈ºyny! Nie czekaj! #JOINOTCFTeam!",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent or B2B,Hybrid,330,Data Architect / Data Warehouse Specialist,Power Media,"Nasz klient kompleksowo realizuje us≈Çugi w zakresie konsultingu, projektowania i zarzƒÖdzania projektami. Specjalizuje siƒô g≈Ç√≥wnie w obszarach e-commerce, data management (Agile Data Warehouse, Data Governance) oraz Content Management. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozw√≥j rozwiƒÖza≈Ñ Data Platform w ≈õrodowisku chmurowym. Kluczowym zadaniem bƒôdzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejs√≥w w z≈Ço≈ºonym krajobrazie systemowym klienta. Projekt obejmuje budowƒô hurtowni danych oraz platform danych od podstaw. Zesp√≥≈Ç sk≈Çada siƒô z 22 os√≥b: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiƒÖzk√≥w: Rozw√≥j i utrzymanie Data Warehouse oraz Data Platform przy u≈ºyciu narzƒôdzi ETL i SQL, Analiza wymaga≈Ñ biznesowych i proponowanie rozwiƒÖza≈Ñ technicznych, Wsp√≥≈Çpraca z zespo≈Çem w celu zapewnienia dostƒôpno≈õci rozwiƒÖza≈Ñ biznesowych, Wykorzystanie swojej wiedzy i do≈õwiadczenia do tworzenia innowacyjnych rozwiƒÖza≈Ñ. G≈Ç√≥wne wymagania: Bardzo dobra znajomo≈õƒá metod i technik projektowania oprogramowania, Ponad 10-letnie do≈õwiadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych , Znajomo≈õƒá ekosystem√≥w Big Data, Znajomo≈õƒá rozwiƒÖza≈Ñ chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejƒôtno≈õci analityczne (analiza i dokumentowanie wymaga≈Ñ biznesowych oraz specyfikacji technicznych) Do≈õwiadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomo≈õƒá SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomo≈õƒá rozwiƒÖza≈Ñ ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (min. B2+/C1) ‚Äì codzienna praca w miƒôdzynarodowym ≈õrodowisku, Gotowo≈õƒá do podr√≥≈ºy s≈Çu≈ºbowych. Mile widziane: Znajomo≈õƒá narzƒôdzi Informatica, Znajomo≈õƒá Machine Learning oraz framework√≥w ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomo≈õƒá jƒôzyk√≥w programowania (Java, Scala, C++, Python). Znajomo≈õƒá jƒôzyka niemieckiego. Firma oferuje: Stabilne, d≈Çugofalowe zatrudnienie w oparciu o B2B lub UoP, Mo≈ºliwo≈õƒá pracy w wiƒôkszo≈õci zdalnej (praca z biura 1 raz w tygodniu, we wtorek) P≈Çaska struktura, antykorporacyjne podej≈õcie do pracy i zespo≈Çu, Ciekawe, miƒôdzynarodowe projekty, Zgrany zesp√≥≈Ç chƒôtnie uczestniczƒÖcy w aktywno≈õciach sportowo ‚Äì rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team ‚Äì building), Dodatkowe zajƒôcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajƒôƒá sportowych, Nowoczesne biuro ze strefƒÖ relaksu, Co roczny 5 dniowy wyjazd firmowy (ca≈Ça firma), warsztaty kulinarne, wyj≈õcia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywno≈õciami ≈öwietna atmosfera, partnerskie podej≈õcie, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa ‚Äûmiƒôkko-techniczna‚Äù. CV w j. angielskim.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,Data Architecture
Full-time,Mid,B2B,Hybrid,331,Data Analyst,Link Group,"Data Analyst Opis stanowiska: Poszukujemy analityka danych ze znajomo≈õciƒÖ ETL oraz zasad zarzƒÖdzania jako≈õciƒÖ danych. Kluczowe bƒôdzie do≈õwiadczenie w budowie przep≈Çyw√≥w danych (pipelines), zapewnianiu sp√≥jno≈õci danych oraz pracy z narzƒôdziem Dataiku. Zakres obowiƒÖzk√≥w: Projektowanie, rozw√≥j i utrzymanie proces√≥w ETL zgodnych z potrzebami raportowymi i biznesowymi Analiza danych kredytowych i identyfikacja problem√≥w jako≈õciowych Monitorowanie jako≈õci danych i wdra≈ºanie zasad governance Pisanie specyfikacji technicznych, przeprowadzanie test√≥w jednostkowych Wdra≈ºanie usprawnie≈Ñ w istniejƒÖcych przep≈Çywach danych Udzia≈Ç w spotkaniach projektowych i wsp√≥≈Çpraca z zespo≈Çami IT/biznesowymi Wymagania: Do≈õwiadczenie z narzƒôdziami ETL (Dataiku, Alteryx, Talend ‚Äì preferowane Dataiku) Bardzo dobra znajomo≈õƒá SQL i pracy z du≈ºymi zbiorami danych Umiejƒôtno≈õƒá pracy w ≈õrodowisku Agile Do≈õwiadczenie w tworzeniu specyfikacji technicznych i test√≥w jednostkowych Podstawowa znajomo≈õƒá Pythona Znajomo≈õƒá zasad zarzƒÖdzania jako≈õciƒÖ danych i kontroli wersji w Dataiku (GIT) Bieg≈Ço≈õƒá w jƒôzyku angielskim (w mowie i pi≈õmie) Otwarto≈õƒá na pracƒô w miƒôdzynarodowym ≈õrodowisku Mile widziane: Znajomo≈õƒá proces√≥w zwiƒÖzanych z kredytem korporacyjnym Do≈õwiadczenie w pracy nad projektami miƒôdzyregionalnymi Umiejƒôtno≈õƒá logicznego my≈õlenia i modelowania danych Technologie: Dataiku 13.2.0 SQL Developer 24.3 360 Suite Jira","[{""min"": 100, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,332,"Senior Data Scientist (Gen AI, LLM)",emagine Polska,"Senior Data Scientist ‚Äì Generative AI & LLM Focus Location: Remote Contract Type: B2B Languages: Polish & English (project communication) About the Role We‚Äôre seeking a highly experienced Senior Data Scientist to join a forward-thinking software company working at the intersection of generative AI, cloud computing, and cutting-edge machine learning. In this role, you‚Äôll apply your expertise in Generative AI and Large Language Models (LLMs) to design impactful, real-world solutions. You‚Äôll work alongside both dynamic startups and enterprise technology teams to deliver data-driven innovation at scale. What You‚Äôll Be Doing Solving practical business problems using advanced machine learning techniques Designing and optimizing NLP or computer vision pipelines Applying deep expertise in Generative AI and LLMs Building scalable ML systems from data ingestion through to deployment and monitoring Using MLOps principles to streamline model lifecycle in cloud environments Extracting insights from large datasets to inform model improvements Clearly communicating technical insights to non-technical teams and stakeholders Collaborating on creative, AI-driven approaches to complex data challenges Your Skills & Experience Must-Haves: 5+ years of experience in data science or a related field Strong Python skills and familiarity with ML libraries (e.g., scikit-learn, TensorFlow, PyTorch) Solid hands-on experience with Generative AI and Large Language Models Strong foundation in statistics and applied mathematics Excellent analytical thinking and problem-solving abilities Fluent English communication skills (written and verbal) Nice to Have: Familiarity with R, SQL, or Java Experience with big data technologies such as Spark, Hadoop, or Kafka Why Join Us? Work on pioneering projects in GenAI and advanced machine learning Collaborate with highly skilled engineers and data scientists across industries Flexible, remote-first work model with occasional in-person collaboration B2B contract with long-term potential",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,333,Senior Data Engineer (2328),N-iX,"About us: N-iX is a software development service company that helps businesses across the globe develop successful software products. Founded in 2002 in Lviv, N-iX has come a long way and increased its presence in eight countries Poland, Ukraine, Sweden, Bulgaria, Malta, the UK, the US, and Colombia. Today, we are a strong community of 2,000+ professionals and a reliable partner for global industry leaders and Fortune 500 companies About the position: Our customer is seeking to expand its Data Engineering team to stand up modern data platform for one of its portfolio companies in the financial services sector. The role requires expertise in Python, SQL (PostgreSQL, MySQL), Airflow, Snowflake, and AWS cloud experience. This project involves managing financial assets owned by the company. It utilizes machine-learning models that operate on data ingested from third-party APIs. The process includes ELT (extract, load, transform), data modeling in Snowflake using DBT, training ML models using AWS SageMaker, running predictions, and storing predictions back in Snowflake. As a Senior Data Engineer, you will design, build, and maintain scalable data pipelines and architectures for our cloud-based analytical platforms. You will collaborate closely with data scientists, analysts, and software engineering teams to deliver robust, high-quality data solutions that drive business decisions. Project involvement plans: Initially 5 months with the possibility of extension. Start in July 2025. Responsibilities: Design, implement, and maintain scalable data pipelines that support business analytics, reporting, and operational needs. Collaborate cross-functionally with analysts, engineers, and product teams to translate data requirements into efficient data models and pipelines. Ensure reliability and performance of data workflows by proactively monitoring, debugging, and optimizing data processes. Drive automation and testing practices in data workflows to maintain high code quality and deployment confidence. Contribute to architectural decisions involving cloud infrastructure, data warehousing strategies, and data governance policies. Required Skills and Qualifications: Key Skills: Python AWS Snowflake Airflow DBT data modeling PostgreSQL, MySQL (or similar) Technical Expertise: Programming Languages: Advanced proficiency in Python for data engineering, data wrangling, and pipeline development. Cloud Platforms: Hands-on experience working with AWS (S3, Glue, Redshift, Lambda, etc.). Data Warehousing: Proven expertise with Snowflake ‚Äì schema design, performance tuning, data ingestion, and security. Workflow Orchestration: Production experience with Apache Airflow (Prefect, Dagster or similar), including authoring DAGs, scheduling workloads and monitoring pipeline execution. Data Modeling: Strong skills in DBT (Data Build Tool), including writing modular SQL transformations, building data models, and maintaining DBT projects. SQL Databases: Extensive experience with PostgreSQL, MySQL (or similar), including schema design, optimization, and complex query development. Additional Competencies: Version Control and CI/CD: Familiarity with Git-based workflows and continuous integration/deployment practices to ensure seamless code integration and deployment processes. Communication Skills: Ability to articulate complex technical concepts to technical and non-technical stakeholders alike. Tools: Git JIRA Confluence Must have: Extensive experience with Python for data analysis Declarative Data Modeling: Experience with modern tools like DBT for streamlined and efficient data modelling. Minimum 5 years of professional experience in production environments, emphasizing performance optimization and code quality. We offer: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits","[{""min"": 18955, ""max"": 24423, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,334,Senior Data Analyst,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. bran≈ºy finansowej. Wykorzystywany stos technologiczny: Python, Power BI, SQL, Excel, wsparcie projektu rozwoju aplikacji mobilnej, optymalizacja aplikacji pod kƒÖtem wydajno≈õci, r√≥wnie≈º przy du≈ºych wolumenach danych, wsp√≥≈Çpraca z zespo≈Çami biznesowymi, technologicznymi oraz dostawcami, rozw√≥j i analiza danych w ≈õrodowisku opartym o Python, SQL i Power BI, rozw√≥j funkcjonalno≈õci w ramach obs≈Çugi funduszy inwestycyjnych, praca w modelu hybrydowym: 1 raz na trzy miesiƒÖce praca z biura w Warszawie lub Poznaniu, stawka do 120z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz minimum 6 lat do≈õwiadczenia w roli Data Analyst, posiadasz wyksztalcenie wy≈ºsze (preferowane kierunki ≈õcis≈Çe), swobodnie pos≈Çugujesz siƒô Pythonem i SQL, pracujesz z Power BI i Excelem, biegle komunikujesz siƒô w jƒôzyku angielskim (min. B2), mile widziane do≈õwiadczeni w projektach dotyczƒÖcych produkt√≥w inwestycyjnych lub funduszy inwestycyjnych. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Hybrid,335,Cloud Data Engineer,PAYBACK,"üîµ PAYBACK is unique worldwide in data-based marketing. With the ability to develop digital business models and implement technical trends, we are pioneers of the data economy. The technical solutions for our 31 million customers relating to the PAYBACK program are designed and operated in-house. To this end, we work with the latest technologies and in a cross-functional and agile way. Your responsibilities: You are responsible for the development, but also for the operation of our services in the cloud You analyze, understand and accompany the requirements from the idea to the end-of-life (DataOps) You contribute to the design of the solution, which includes development, testing and documentation You carry out analyses and research to solve complex business problems You will work as a member of your cross-functional agile team with internal stakeholders from across the organisation to develop best-in-class data products and KPIs using existing data assets Your Profile: You have several years of professional experience as a data engineer or in a similar position You are a clean programmer with an agile mindset and love DataOps You preferably have solid knowledge of Google Cloud Services and Terraform You have experience with Python (Poetry, Pydantic, Google Python APIs, etc.) You are used to dealing with mass data You are characterized by a high level of quality awareness, trustworthiness and interest in new technologies You can work independently in an agile cross-functional team Nice to have: Spark, Oracle PL/SQL, Airflow, Jenkins, OpenShift, Git, Docker, Kubernetes, Java, Kafka TECHSTACK : Python, Google Cloud Services (BigQuery, Cloud Build, Cloud Storage, Pub/Sub, Cloud Composer, DataFlow), Terraform How about? Employment contract? üìù Of course. With us you do not have to worry about stable employment. Benefits? üèãÔ∏è‚Äç‚ôÄÔ∏è We have them! Among other: corporate incentive program, sport card, private medical care. Lunch card? üí≥ With the cooperation extended and permanent contract, you will receive additional funds to use for meal purchases. Working in a hybrid model? üè† Of course! You work with us 3 days a week from the office, 2 days a week from home. Work wherever you want?üå¥ In PAYBACK you have the opportunity. Working 100% remotely, also from European countries for 15 days a year. ‚ÄéFlexible working hours? ‚è∞ Sounds great! We start working between 8 to 10. Trainings? üß† Of course. We provide training to develop hard and soft skills. Convenient location? üè† Sure! We invite you to our new office at Rondo Daszy≈Ñskiego, but we are currently also working remotely. Dress code? üëï We definitely say no. There are no rigid dress code rules in our company, sneakers are more than welcome. Friendly atmosphere at work? ü§ùüèª Yes! In PAYBACK, people are the most important asset‚Äé. ‚ÄéSomething is missing? üñê Open communication is our priority, so dare to ask!‚Äé",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,336,Data Engineer (Azure / Databricks / Spark),Upvanta sp. z o.o.,"Projektowanie, budowa i utrzymanie rozwiƒÖza≈Ñ do przetwarzania danych w chmurze Microsoft Azure Przetwarzanie danych z u≈ºyciem Apache Spark w Azure Databricks Tworzenie i zarzƒÖdzanie pipeline‚Äôami danych w Azure Data Factory Automatyzacja wdro≈ºe≈Ñ z wykorzystaniem Azure DevOps, Terraform, YAML i Azure CLI Testowanie rozwiƒÖza≈Ñ i walidacja danych we wsp√≥≈Çpracy z zespo≈Çami biznesowymi Praca z danymi w czasie rzeczywistym (real-time streaming) Minimum 5 lat do≈õwiadczenia na stanowisku Data Engineer Bardzo dobra znajomo≈õƒá ≈õrodowiska Microsoft Azure, w szczeg√≥lno≈õci: Azure Databricks (Apache Spark) Azure Data Factory Azure Data Storage / Delta Lake Azure DevOps (CI/CD, zarzƒÖdzanie dostƒôpem, automatyzacja) Umiejƒôtno≈õƒá projektowania i wdra≈ºania rozwiƒÖza≈Ñ chmurowych dla zaawansowanych analiz Znajomo≈õƒá i do≈õwiadczenie w obszarze Machine Learning, MLOps, MLflow Znajomo≈õƒá technologii i narzƒôdzi: Python, Terraform, YAML, Parquet, Git, PowerShell, Azure CLI Zorientowanie na jako≈õƒá kodu i wydajno≈õƒá pipeline‚Äô√≥w Umiejƒôtno≈õƒá pracy zespo≈Çowej oraz dobre zdolno≈õci komunikacyjne Pracƒô przy innowacyjnym projekcie opartym o analizƒô danych i Machine Learning Mo≈ºliwo≈õƒá rozwoju w kierunku ML Engineering i MLOps Dostƒôp do najnowszych technologii chmurowych i narzƒôdzi analitycznych Wsp√≥≈Çpracƒô z do≈õwiadczonym i wspierajƒÖcym zespo≈Çem ekspert√≥w Elastyczne podej≈õcie do modelu pracy i formy wsp√≥≈Çpracy",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,337,IT Data Warehouse Analyst - Leader (banking),Primaris,"Forma wsp√≥≈Çpracy : kontrakt B2B Tryb: gotowo≈õƒá do pracy w trybie hybrydowym z Wroc≈Çawia Aktualnie do jednego z projekt√≥w poszukujemy osoby na stanowisko IT Data Warehouse Analyst - Leader , kt√≥ra posiada min. 5 lat komercyjnego do≈õwiadczenia. Dlatego je≈õli szukasz pracy z : budowƒÖ hurtowni danych w bankowo≈õci, SQL, Power BI/ Cognos, projektowaniem baz danych oraz analizƒÖ wymaga≈Ñ chƒôtnie z TobƒÖ porozmawiamy! Projekt kt√≥ry bƒôdziemy gotowi Ci zaproponowaƒá dotyczy : budowy hurtowni danych i systemu raportowego w bankowo≈õci. Do≈õwiadczenie w budowie i rozwoju hurtowni danych w sektorze bankowym ‚Äì minimum 5 lat praktyki, ze zrozumieniem specyfiki danych i proces√≥w w bankowo≈õci Bardzo dobra znajomo≈õƒá SQL ‚Äì umiejƒôtno≈õƒá efektywnego tworzenia zapyta≈Ñ oraz optymalizacji Znajomo≈õƒá narzƒôdzia analityczno-raportowego: praktyczne do≈õwiadczenie z Power BI lub Cognos, w tym definicja struktur raportowych oraz tworzenie dashboard√≥w i wizualizacji Do≈õwiadczenie w zbieraniu i analizie wymaga≈Ñ biznesowych na hurtowniƒô danych i raportowanie ‚Äì umiejƒôtno≈õƒá wsp√≥≈Çpracy z u≈ºytkownikami biznesowymi i t≈Çumaczenia potrzeb na rozwiƒÖzania techniczne Do≈õwiadczenie w projektowaniu i tworzeniu struktur bazodanowych pod hurtowniƒô danych ‚Äì znajomo≈õƒá modelowania danych oraz pracy z du≈ºymi zbiorami danych Znajomo≈õƒá jƒôzyka angielskiego na poziomie B2 Umiejƒôtno≈õƒá skutecznej wsp√≥≈Çpracy i prezentacji wynik√≥w pracy Do≈õwiadczenie z platformƒÖ Teradata oraz narzƒôdziami do integracji i zarzƒÖdzania hurtowniami danych Znajomo≈õƒá narzƒôdzi do przetwarzania danych, np. Ab Initio Praktyka w pracy w ≈õrodowisku chmurowym (np. Azure, AWS, Google Cloud) i wdra≈ºaniu rozwiƒÖza≈Ñ data warehouse w chmurze Zakres obowiƒÖzk√≥w: Budowa i rozw√≥j hurtowni danych w obszarze bankowo≈õci, z uwzglƒôdnieniem specyfiki i wymaga≈Ñ bran≈ºy finansowej Projektowanie i tworzenie struktur bazodanowych dedykowanych hurtowniom danych, w tym modelowanie tabel fakt√≥w i wymiar√≥w oraz relacji miƒôdzy nimi Zbieranie i analiza wymaga≈Ñ biznesowych dotyczƒÖcych hurtowni danych i raportowania w ramach wsp√≥≈Çpracy z r√≥≈ºnymi dzia≈Çami firmy Implementacja oraz optymalizacja proces√≥w ETL do zasilania hurtowni danych, dbanie o jako≈õƒá i sp√≥jno≈õƒá danych Tworzenie i utrzymanie struktur raportowych oraz dashboard√≥w w narzƒôdziach analityczno-raportowych, przede wszystkim w Power BI lub IBM Cognos, zgodnie z wymaganiami u≈ºytkownik√≥w biznesowych Pisanie zaawansowanych zapyta≈Ñ SQL do ekstrakcji, transformacji i analizy danych Wsp√≥≈Çpraca i komunikacja z zespo≈Çami projektowymi i biznesowymi, w tym zbieranie potrzeb i szkolenie u≈ºytkownik√≥w ko≈Ñcowych Utrzymywanie i rozw√≥j ≈õrodowisk hurtowni danych z opcjonalnym wykorzystaniem platform takich jak TeraData czy narzƒôdzi Ab Initio (mile widziane) Dbanie o dokumentacjƒô technicznƒÖ i biznesowƒÖ rozwiƒÖza≈Ñ W naszej firmie bƒôdziesz m√≥g≈Ç/mog≈Ça liczyƒá na: Pracƒô w organizacji z ugruntowanƒÖ pozycjƒÖ rynkowƒÖ Projekty, w kt√≥rych bƒôdziesz mia≈Ç/mia≈Ça wp≈Çyw na ich rozw√≥j Wsp√≥≈Çpracƒô z ciekawymi klientami biznesowymi z r√≥≈ºnych bran≈º ( m.in .: finanse, bankowo≈õƒá, ubezpieczenia, healthcare, robotyzacja, energetyka, media), Permanentny mentoring zar√≥wno techniczny jak i biznesowo-mened≈ºerski, np. podczas naszych cyklicznych szkole≈Ñ ( m.in . Git, Gitflow, Angular, Docker), czy wew. program√≥w rozwojowych (Primaris x TechTalks, Primaris Leadership Academy) oraz zewnƒôtrznych kurs√≥w.Ju≈º na etapie on-boardingu zapewniamy dostƒôp do naszych wewnƒôtrznych szkole≈Ñ, cyklicznych spotka≈Ñ, kt√≥re serializujemy na Confluence oraz platformy e-learning ≈öwietnƒÖ atmosferƒô pracy, w≈õr√≥d zaanga≈ºowanych ludzi z pasjƒÖ w p≈Çaskiej strukturze z prostymi procesami Kompleksowy pakiet benefit√≥w skrojonych na miarƒô - prywatna opieka medyczna dla Ciebie oraz dla Twojej rodziny, Multisport dla Ciebie i os. towarzyszƒÖcej - Ty decydujesz, co wybierasz! Ca≈Çy proces rekrutacyjny oraz onboarding prowadzony jest zdalnie. Proces rekrutacyjny sk≈Çada siƒô z: rozmowy telefonicznej z osobƒÖ z dzia≈Çu Rekrutacji & HR (do 30 min) zdalnej video rozmowy - weryfikacji techniczno-biznesowej z naszym specjalistƒÖ/specjalistkƒÖ (60-90 min) zdalnego spotkania z liderem po stronie klienta projektu (30-60 min) finalnej decyzji dotyczƒÖcej oferty Primaris Services to ponad 250 ekspert√≥w na pok≈Çadzie i 15 lat do≈õwiadczenia w bran≈ºy IT na rynku polskim oraz zagranicznym.Realizujemy ambitne projekty o wysokiej z≈Ço≈ºono≈õci z r√≥≈ºnych obszar√≥w - m.in . bankowo≈õci, ubezpiecze≈Ñ, funduszy inwestycyjnych czy bran≈ºy logistycznej (mamy ponad 40 aktywnych klient√≥w!). Ro≈õniemy w si≈Çƒô oraz ciƒÖgle poszerzamy portfolio zar√≥wno naszych us≈Çug jak i klient√≥w. Zakres naszej dzia≈Çalno≈õci obejmuje budowƒô system√≥w od zera, ich rozw√≥j oraz utrzymanie, wdro≈ºenia produktowe, alokacje ca≈Çych Zespo≈Ç√≥w, a tak≈ºe pojedynczych Ekspert√≥w w strukturach Klienta. Ponadto od kilku lat dzia≈Çamy bardzo intensywnie jako z≈Çoty Partner firmy UiPath (obszar Robotic Process Automation) budujƒÖc roboty i sprzedajƒÖc licencje u naszych Klient√≥w. Co miesiƒÖc do≈ÇƒÖcza do nas 7 nowych os√≥b! Wierzymy, ≈ºe zgrany zesp√≥≈Ç i ludzie z pasjƒÖ to klucz do naszego wsp√≥lnego sukcesu! W≈Ça≈õnie dlatego ciƒÖgle poszukujemy nowych, zdolnych os√≥b, kt√≥re zasilƒÖ nasze szeregi.","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,338,BI Developer,7N,"O projekcie: Projekt realizowany jest dla rozpoznawalnej firmy z bran≈ºy nieruchomo≈õci, dzia≈ÇajƒÖcej na kilku najwiƒôkszych rynkach w Polsce. Zesp√≥≈Ç BI zajmuje siƒô tworzeniem i rozwijaniem raport√≥w finansowych w Power BI oraz pracƒÖ z danymi w MSSQL , w tym budowƒÖ i utrzymaniem modeli semantycznych. To ≈õwietna okazja, by do≈ÇƒÖczyƒá do stabilnej organizacji i pracowaƒá nad nowoczesnymi rozwiƒÖzaniami ‚Äî trwa migracja do Microsoft Fabric, wiƒôc znajomo≈õƒá tej platformy w minimum w podstawowym zakresie jest oczekiwana. Lokalizacja: praca 100% zdalna, z okazjonalnymi wizytami w Warszawie Tworzenie i rozwijanie raport√≥w finansowych w Power BI na potrzeby wewnƒôtrznych dzia≈Ç√≥w organizacji Praca z danymi w MSSQL ‚Äì przygotowanie zapyta≈Ñ, transformacja danych, optymalizacja Budowa oraz utrzymanie modeli semantycznych wykorzystywanych w raportowaniu Wsp√≥≈Çpraca z zespo≈Çem analitycznym oraz dzia≈Çem ksiƒôgowym w zakresie pozyskiwania i interpretacji danych Udzia≈Ç w projektach opartych na Microsoft Fabric ‚Äì tworzenie rozwiƒÖza≈Ñ raportowych z wykorzystaniem Direct Lake Dbanie o jako≈õƒá, sp√≥jno≈õƒá i bezpiecze≈Ñstwo danych wykorzystywanych w raportowaniu Minimum 3‚Äì4 lata do≈õwiadczenia na podobnym stanowisku w obszarze BI Praktyczna znajomo≈õƒá Power BI, Power Query, DAX oraz silnika VertiPaq Bardzo dobra znajomo≈õƒá SQL (T-SQL) i pracy z relacyjnymi bazami danych Do≈õwiadczenie w pracy z Microsoft Fabric (przynajmniej na poziomie podstawowym) Znajomo≈õƒá Power Platform oraz og√≥lne zrozumienie zasad dzia≈Çania ≈õrodowiska Azure (np. Data Lake, Logic Apps, Synapse) Znajomo≈õƒá zagadnie≈Ñ z obszaru finans√≥w lub rachunkowo≈õci bƒôdzie dodatkowym atutem Sta≈Çe wsparcie osobistego agenta , dbajƒÖcego o TwojƒÖ ciƒÖg≈Ço≈õƒá projektowƒÖ, kontakt z klientem, niezbƒôdne formalno≈õci, komfort pracy oraz rozw√≥j, Consultant Development Program ‚Äì doradztwo w planowaniu rozwoju w oparciu o najnowsze trendy i potrzeby rynku IT, obejmujƒÖce m.in . konsultacje z agentami i mentorami rozwoju , Dostƒôp do 7N Learning & Development ‚Äì platformy rozwojowo-edukacyjnej z webinarami, bibliotekƒÖ artyku≈Ç√≥w i raport√≥w bran≈ºowych oraz regularnymi zaproszeniami na jednorazowe i cykliczne wydarzenia rozwojowe ‚Äì techniczne, biznesowe oraz life-stylowe, Spektakularne eventy integracyjne, zar√≥wno dla Ciebie (np. coroczny wyjazd Kick-Off , imprezy ≈õwiƒÖteczne czy sportowe Letnie Igrzyska), jak i dla Twoich bliskich (np. pikniki rodzinne), Rozw√≥j zawodowy nie tylko podczas projektu ‚Äì mo≈ºesz zaanga≈ºowaƒá siƒô w przekazywanie wiedzy innym w ramach oferty 7N Services kierowanej do klient√≥w 7N, Relacje i dostƒôp do wiedzy najbardziej do≈õwiadczonych ekspert√≥w IT na rynku ‚Äì ≈õredni sta≈º zawodowy naszego Konsultanta w Polsce to ponad 10 lat, Pakiet benefit√≥w zaplanowany od A do Z, czyli dofinansowanie do opieki medycznej, ubezpieczenia na ≈ºycie, karty sportowej dla Ciebie i Twoich bliskich, a tak≈ºe zni≈ºki do sklep√≥w w Polsce i za granicƒÖ. O 7N CiƒÖg≈Çe poszukiwanie projekt√≥w, trudne negocjacje stawek, brak wsparcia w rozwoju ‚Äì brzmi znajomo? W 7N zyskujesz nie tylko stabilno≈õƒá kontrakt√≥w, ale te≈º zaanga≈ºowanie osobistego opiekuna dbajƒÖcego o Tw√≥j komfort zawodowy i sta≈Çy dostƒôp do inicjatyw rozwojowych. Naszym celem jest zapewnienie Ci stabilnej i komfortowej wsp√≥≈Çpracy, kt√≥ra przyczyni siƒô do sukcesu Twojego jako eksperta IT oraz naszych klient√≥w. Budujemy trwa≈Çe relacje, bazujƒÖc na skandynawskich warto≈õciach i 30-letnim do≈õwiadczeniu w tworzeniu rozwiƒÖza≈Ñ IT dla ponad 200 organizacji.","[{""min"": 90, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,339,(Senior) Big Data Analyst,InPost,"As the Data & AI department, we are looking for (Senior) Big Data Analyst. The successful candidates will join teams that collaborate with areas such as Marketing, Digital, Sales, Operations, or those focused on the development of specific products like InPost Pay and the Loyalty Program. In this role, you will be responsible for analyzing vast volumes of real-time data from diverse systems, including campaigns data, product and customer analytics, sales performance, logistics processes, and similar topics relative to InPost Group products. This role spans across Poland and extends to 7 international markets, offering a unique opportunity to impact our global operations significantly. You can work remotely. Technologies, you will encounter in your daily work: Apache Spark in Databricks, Python\PySpark, SQL, kafka, Power BI, GitLab, Google BigQuery, dbt. Which skills should you bring to the pitch: min 3 years of experience in an analytical role handling vast volumes of data in (preferably in domains such as Marketing, Logistics, Customer or Sales) experience in data modeling and implementing complex data-driven solutions is a strong plus strong proficiency in Python/PySpark for data analysis, SQL for data processing, bash scripting to manage Git repositories proven ability to pull insightful and actionable conclusions from complex data and communicate recommendations to business stakeholders clearly and concisely comprehensive understanding of the technical aspects of data warehousing, including dimensional data modeling and ETL/ELT processes ability to translate business needs into data models strong understanding of real-time data: ability to request and handle data from both backend and frontend systems, including internal and external platforms self-motivated and self-managing, with the ability to work independently and mange multiple tasks simultenously strong interpersonal skills with the ability to collaborate effectively with cross-functional teams fluent in English and Polish: verbal and written It would be awesome if you have: experience in working with Apache Spark in Databricks familiarity with cloud-based data platforms (e.g. GCP, Azure, AWS) familiarity with modern data building tools like Apache Airflow, DBT familiarity with data visualization tools such as PowerBI/Tableau/Looker knowledge of data governance principles and practices ability to thrive in a highly agile, intensely iterative environment positive and solution-oriented mindset On a daily basis you will: closely support stakeholders in making data-driven decisions work with different InPost departments and business lines gather requirements, harness large-scale, real-time data from various sources, analyze it and, prepare insights and recommendation about business critical areas and processes design, develop, and extend our data model layers that support optimized and scalable calculations and visualizations of successful analytics outcomes craft code that meets our internal standards for style, maintainability, and best practices for a high-scale data environment. Maintain and advocate for these standards through code reviews collaborate with cross-functional teams including data engineers, data scientists, and business analysts to deliver integrated data solutions prototype and coordinate data visualizations",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,340,Data Engineer / Big Data Analyst,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 5-letnie do≈õwiadczenie na stanowisku zwiƒÖzanym z analizƒÖ danych lub analizƒÖ biznesowƒÖ Min. 2-letnie do≈õwiadczenie na stanowisku wymagajƒÖcym przetwarzania i analizy du≈ºych zbior√≥w danych (Big Data) Do≈õwiadczenie projektowe w tworzeniu i optymalizacji zaawansowanych zapyta≈Ñ SQL Do≈õwiadczenie projektowe w programowaniu w jƒôzyku Python Do≈õwiadczenie projektowe w przetwarzaniu i analizie du≈ºych zbior√≥w danych Do≈õwiadczenie projektowe w Data Quality Do≈õwiadczenie w programowaniu w SQL Do≈õwiadczenie w programowaniu w Python Do≈õwiadczenie w programowaniu w PySpark Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z procesami ETL Znajomo≈õƒá relacyjnych baz danych Dobra organizacja pracy w≈Çasnej, orientacja na realizacje cel√≥w Umiejƒôtno≈õci interpersonalne i organizacyjne, planowanie Komunikatywno≈õƒá, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista i dociekliwo≈õƒá Zdolno≈õƒá adaptacji i elastyczno≈õƒá, otwarto≈õƒá na sta≈Çy rozw√≥j i gotowo≈õƒá uczenia siƒô Mile widziane Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Do≈õwiadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Airflow (np. Airflow Fundamentals lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá tworzenia DAG√≥w Airflow (np. Dag Authoring lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL (np.. W3Schools SQL Certificate lub r√≥wnowa≈ºny) Kluczowe zadania Analiza danych biznesowych i technicznych w celu wspierania podejmowania decyzji oraz optymalizacji proces√≥w Przetwarzanie, analiza i interpretacja du≈ºych zbior√≥w danych (Big Data) z wykorzystaniem Python, SQL oraz PySpark Projektowanie, tworzenie i optymalizacja zaawansowanych zapyta≈Ñ SQL w ≈õrodowiskach baz danych Wsp√≥≈Çpraca z zespo≈Çami technicznymi i biznesowymi w celu definiowania potrzeb analitycznych oraz tworzenia rozwiƒÖza≈Ñ opartych na danych Tworzenie i utrzymywanie proces√≥w zwiƒÖzanych z jako≈õciƒÖ danych (Data Quality), w tym ich weryfikacja, czyszczenie i walidacja Programowanie rozwiƒÖza≈Ñ analitycznych i integracyjnych w jƒôzyku Python (w tym PySpark) w ≈õrodowiskach przetwarzania danych Praca z relacyjnymi bazami danych oraz narzƒôdziami ETL w celu ekstrakcji, transformacji i za≈Çadunku danych Wspieranie inicjatyw zwiƒÖzanych z automatyzacjƒÖ raport√≥w i analiz, w oparciu o du≈ºe zbiory danych i zapytania Utrzymywanie wysokiej jako≈õci dokumentacji technicznej oraz przekazywanie wniosk√≥w i rekomendacji interesariuszom Monitorowanie integralno≈õci danych oraz proponowanie i wdra≈ºanie usprawnie≈Ñ w procesach analitycznych","[{""min"": 150, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Hybrid,341,DATA Engineer,PAYBACK,"üîµ PAYBACK is the world‚Äôs leading multi-partner loyalty program. As an international player, we operate in German, Italian and Austrian markets. More than 10 million active customers already use the German PAYBACK app and mobile PAYBACK services via their smartphones. PAYBACK is a top player in data-driven marketing. With the ability to develop digital business models and implement technical trends, we are one of the leading companies in the data economy. All technical solutions of the PAYBACK program are developed, invented and implemented in Germany. Therefore, we provide the latest technologies and have cross-functional and agile teams. You want to take responsibility and collaborate your ideas to our unique product? You are ambitious and eager to and move things further? Then you are exactly right with PAYBACK. We are looking forward to getting to know you. TECHSTACK: Git, Python, Airflow, SQL, NoSQL, Spark, MapR, Google Cloud Services (BigQuery, Cloud Build, Cloud Storage, Pub/Sub, Cloud Composer, DataFlow). Description: Within the Global DWH Engineering Team you can work with a huge amount of data and help us to improve by using it in the right way in Germany, Poland, Mexico, Italy and Austria. We‚Äôre not only dealing with data, but also heading towards Continuous Integration and Continuous Delivery and constantly improve and converge our systems to become a truly Global Data Warehouse by migrating our services into the public cloud. You will be responsible for development, automation and the maintain of our services on-premises and within the cloud. Your responsibilities: Within the Data Engineering Team you can work with a huge amount of data and help us with its usage in an efficient way in Germany, Poland, Mexico, Italy and Austria. We strive towards a modern cloud-based data platform on GCP leveraging bleeding edge services and in parallel we keep existing processes running on-premise until they are fully migrated. You will not only be responsible for development but also for the operation of our services, both on-premise and within the cloud. You support operations and take responsibility of productive processes (you build it, you run it). You analyze, understand, and accompany the requirements from the idea until end-of-life (DataOps) You contribute to the design of our solution, execute development of design, test and document your implementation. In order to solve complex business problems, you perform comprehensive data analysis and research. You work together with internal stakeholders across the organization to produce high-quality KPIs out of raw data. You work in close collaboration within your cross-functional agile team on-side. Your Profile: You have 4+ years of work experience as a Data Engineer with huge datasets or in a similar position. You are a clean coder with an agile mindset. Mass data doesn't scare you. You have experienced with/you are eager to learn Kubernetes/OpenShift, data related GCP services or MapR Cluster. You are characterized by a high-quality awareness, your trustworthiness and interest in new technologies. You can work independently in an agile team. You have very good English skills. How about? Employment contract? üìù Of course. With us you do not have to worry about stable employment. Great location üè¨ Still like! We invite you to our new office at Rondo Daszy≈Ñskiego metro station. Currently we also work from home. Working in a hybrid model? üè† Of course! You work with us 3 days a week from the office, 2 days a week from home. Work wherever you want?üå¥ In PAYBACK you have the opportunity. Working 100% remotely, also from European countries for 15 days a year.e Friendly atmosphere at work? ü§ùüèª Yes! In PAYBACK, people are the most important asset‚Äé. Work wherever you want? üå¥ In PAYBACK you have the opportunity. Working 100% remotely, also from European countries for 10 days a year. Dress code? üëï We definitely say no. There are no rigid dress code rules in our company, sneakers are more than welcome. Trainings? üß† Of course. We provide training to develop hard and soft skills. ‚ÄéSomething is missing? üñê Open communication is our priority, so dare to ask!‚Äé",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,342,Senior Data Engineer Market Data Asset,Bayer Sp. z o.o.,"Senior Data Engineer Market Data Asset The Data Assets, Analytics & AI organization at Bayer Consumer Health focuses on driving digital transformation and innovation by creating best-in-class analytical solutions that enable data-driven decision making and performance optimization for Bayer Consumer Health. You will be part of the Data Assets, Analytics & AI organization and will be responsible for building data assets. You partner with business stakeholders, data analysts, data architects, data scientists, analytics leads as well as other engineers. You will build data pipelines, data models and provision data for Analytics products and data scientists. You will also make sure that proper development processes are followed within the team, enhance implementation frameworks and guide other team members in building scalable, secure and well performing data products. If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Key Tasks & Responsibilities: Implement efficient data pipelines that integrate data from different sources and business domains to develop globally harmonized data models and KPI calculations Contribute to define data management and data quality standards. Ensure that data is well-managed to build stable, reusable and quality assured data assets. Collaborate with other IT functions (enabling functions data asset teams, analytics teams, platform product managers & integration architects) to ensure the aforementioned activities are executed effectively. Ensure that data products adhere to the data protection and compliance standards. Implement effective data access management policies, data privacy policies and secure data provisioning based on corporate guidelines. Continuously enhance implementation frameworks based on the needs of the analytics products consuming the data assets in your responsibility. Guide other data engineers in your team (internal or external engineers) and ensure that all engineers apply same design principles and assure code quality. Break down bigger work packages into manageable tasks. Together with the assigned data architect, ensure that cost and time estimations are accurate, quality of delivery is assured, and deliverables are properly tested, documented and handed over to the operations team. Qualifications & Competencies (education, skills, experience): Bachelor/Master‚Äôs degree in Computer Science, Engineering, or a related field. 5+ years of working experience in the field of Data & Analytics, preferably in the CPG industry 5+ years of proficient coding experience with Python for data engineering, including SQL and PySpark (DataFrame API, Spark SQL, MLlib), with hands-on experience in various databases (SQL/NoSQL), key libraries (e.g., pandas, SQLAlchemy), parallel processing, and advanced data transformation and performance optimization techniques. Ability to develop, and maintain data engineering applications using Python classes and PySpark, ensuring code modularity, reusability, and maintainability. Excellent data engineering & technology knowledge (Azure Data Lake Gen2, Azure Data Factory and Databricks as well as data management knowhow (data cataloguing, data quality management) Solid understanding of data modeling, ETL processing and lakehouse concepts. Profound knowledge of CI/CD processes and tools (GitHub VCS, GitHub Actions, Azure DevOps Pipelines)) Profound data content knowledge of market data (Sell-out, Sell-through) for the CPG or health care industry which is licensable from data providers like IQVIA, Nielsen and IRI. Advanced understanding of data governance processes to support mapping of data from multiple sources. Experience in leading developer teams and breaking down work into smaller work packages, experience in Agile methodologies (Scrum, Kanban) in order to manage the work. Strong problem solving and analytical skills. Excellent interpersonal and communication skills, team collaboration, active listening, consulting, challenging in a constructive way. Fluent in English, both written & spoken, intercultural awareness and willingness to travel What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,343,Revalize CPQ Product Support Specialist,Link Group,"Codzienne wsparcie u≈ºytkownik√≥w i obs≈Çuga zg≈Çosze≈Ñ w ramach CPQ (BAU) Diagnozowanie i rozwiƒÖzywanie incydent√≥w technicznych Wprowadzanie usprawnie≈Ñ i modyfikacji funkcjonalno≈õci systemu Konfiguracja i utrzymanie proces√≥w ofertowania w CPQ Wsp√≥≈Çpraca z zespo≈Çami technicznymi i biznesowymi po stronie klienta Dokumentowanie zmian i konfiguracji Minimum 2 lata do≈õwiadczenia w pracy z platformami CPQ Praktyczna znajomo≈õƒá przynajmniej jednej z platform: Revalize CPQ Oracle CPQ (BigMachines) Infor CPQ Tacton CPQ Do≈õwiadczenie w obs≈Çudze zg≈Çosze≈Ñ BAU, incydent√≥w oraz konfiguracji i optymalizacji CPQ Umiejƒôtno≈õƒá pracy w ≈õrodowisku miƒôdzynarodowym (projekt prowadzony po angielsku)","[{""min"": 85, ""max"": 100, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,344,Data Vault Expert with Snowflake,Experis Manpower Group,Main Responsibilities: Assess the current platform and propose a path forward Stakeholder and change management Define platform strategy and vision Coordinate delivery and planning Required Skills: Min. 5 years of experience Experience with Data Vault 2.0 Proficiency in Snowflake Hands-on experience with DBT Familiarity with AWS What We Offer: B2B contract via Experis Sports card Life insurance Private medical care Fully remote work - candidate must be located in Poland,"[{""min"": 28560, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,345,Data Architect / Data Officer_L4,Experis Manpower Group,"Responsibilities: Building solutions that combine data from multiple sources to support our client's processes using the latest Azure technology stack. Supporting team members with technical issues, platform administration, and access rights. Understanding business targets, challenging choices and roadmaps, prioritizing needs based on technical capabilities, and designing target architectures. Acting as a data and cloud expert, sharing your knowledge with other architects, data engineers/data team members, business stakeholders, and platform experts. Developing data architecture with a solid understanding of data domain, data modeling, quality, and sensitive data protection topics. Documenting technical and architectural specifications and providing architectural advice in collaboration with business analysts. Act as an architect in data projects. Assist in creating Business Use Cases for several reporting projects. Define architecture of data management and advanced analytics solutions in the Azure cloud. Ensure solution compliance with client‚Äôs frameworks and standards. Develop documentation of system components following obligatory standards. Support IT team in technical implementation, testing, application deployment, and knowledge transfer to the IT support. Collaborate closely with business stakeholders to ensure the high quality of the final product. Code review and quality assurance Maintenance of Azure Data Relevant resources Requirements: Master‚Äôs degree in Computer Science or related field 5+ years of experience in Data Architecture & Engineering Experience with business intelligence, data analysis, and reporting in enterprise environments Experience in design, development, and implementation of solutions architecture on the Microsoft Azure platform Good understanding of data security topics, from networking layer to the end user interface Working experience with Azure Databricks (administration and data engineering) and Azure Data Factory Good understanding of Azure services (SQL database, administration, IAM, Data Lake) Experience with agile project delivery methodologies (Scrum, Kanban) SQL/Python programming skills Soft Skills: Fluent in English (C1) Hands-on mentality Self-managed Take end-to-end task ownership Excellent communication skills Ability to understand business stakeholders and the business landscape Ability to challenge decisions made by the platform or other architects Strong analytical, organizational, problem-solving, and time-management skills Comfortable working in a diverse, complex, and fast-changing landscape of data sources Proactive problem solver with innovative thinking and a strong team player Our offer: 100% remote work model Multisport card Private healthcare system Life insurance","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,346,Team Lead - Data Engineer,Clouds on Mars Sp. z o.o.,"We are seeking a Team Lead - Data Engineer with proven expertise in leading consulting projects for external clients across various industries, delivering advanced data solutions using Microsoft Azure, Fabric, Databricks, and modern data engineering technologies. In this role, you will work directly with external clients on modern data platform implementations, while also supporting the development, performance, and engagement of a team of engineers working across multiple projects. We are currently looking for Team Lead - Data Engineer with experience in AI/ML or a strong interest in developing their skills in this area. Candidates with relevant expertise or growth potential in this field will be given priority during the selection process. Responsibilities: Design and deliver scalable ETL/ELT pipelines, data models, and reporting solutions using technologies such as Azure Data Factory, Synapse, Fabric, Databricks, and Power BI. Architect modern data platforms (data warehouse, data lake, or lakehouse) with performance, scalability, and reliability in mind. Collaborate with technical stakeholders to translate business requirements into robust, production-ready data solutions. Contribute to CI/CD processes, code quality standards, automated testing, and documentation. Act as a senior technical contributor across one or more client projects in a consulting setting. Act as direct people manager for a team of data engineers, responsible for performance reviews, development planning, and career coaching. Conduct regular 1: 1s, provide feedback, and support skills growth through mentoring and learning opportunities. Balance and allocate team members across multiple projects, ensuring workload clarity and capacity planning. Foster a healthy, collaborative team culture by promoting open communication, continuous improvement, and psychological safety. Monitor team morale and resolve delivery blockers that affect individual or team performance. Work closely with Project Managers and Technical Leads to align team priorities and remove conflicts or inefficiencies. Ensure consistency in delivery standards, team engagement, and cross-project coordination. Support team members in navigating overlapping priorities or challenges across multiple engagements. Colaborate with HR on implementing people management and development practices Requirements Minimum 5+ years of experience delivering data engineering solutions in a consulting or multi-client project environment. Strong technical expertise with the Microsoft Azure ecosystem (Data Factory, Synapse, Fabric), Databricks, and Power BI. Solid background in ETL/ELT design, data integration, data modeling, and performance optimization. Proven people management experience, including performance evaluation, coaching, and career development. Excellent communication skills and ability to engage with both technical and business stakeholders. Fluency in English and Polish. We offer Free UNUM health insurance ‚Äì up to 350 000PLN (advanced 500 000PLN) Co-fundating for the PZU, Medicover or LuxMed medical package Multisport card Access to training platforms (MS certificates, SQL BI, Udemy, internal Delivery Excellence Workshops) Possibility: 100% remotely work/ hybrid work / work in office (we meet at the office once a month) Flexible working hours Simple breakfasts, fruit and snacks in our office Always chilled beverages in our office Program ""It's good to stay with us"" - a voucher of PLN 2 500 for an employee with 3-year seniority for any purpose Paid referral system - get up to PLN 8000 for referring an employee A small gift on each anniversary of work at Clouds on Mars Quarterly budgets for integration","[{""min"": 150, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,347,Data Architect with AWS,Link Group,"About the Role: We are looking for a skilled and experienced Data Architect with strong knowledge of AWS cloud services to join our team. This role involves designing scalable, secure, and high-performance data solutions in a cloud-native environment. You'll work closely with data engineers, analysts, and business stakeholders to create modern data platforms that support analytics, reporting, and AI/ML initiatives. Design and implement cloud-native data architecture using AWS services Define data models, architecture standards, and best practices for data pipelines, storage, and security Collaborate with stakeholders to understand business and technical requirements Guide development of data lakes, data warehouses, and real-time data streaming systems Ensure compliance with data governance, security, and privacy standards Evaluate and recommend appropriate AWS services and tools Support the development and review of ETL/ELT processes Provide technical leadership to data engineering teams Proven experience as a Data Architect , Cloud Architect , or Senior Data Engineer with architectural responsibilities Expertise in AWS cloud services (e.g., S3, Redshift, Glue, Lambda, EMR, Athena, Kinesis, Lake Formation) Strong knowledge of data modeling , ETL/ELT , and data warehousing concepts Experience designing and implementing data lakes and modern data platforms Familiarity with infrastructure-as-code (e.g., Terraform, CloudFormation) Experience with SQL and at least one scripting/programming language (e.g., Python) Understanding of data governance , compliance , and security standards Excellent communication skills in English Experience with Snowflake , Databricks , or similar platforms Knowledge of machine learning workflows and MLOps Familiarity with data mesh or event-driven architectures","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,Permanent or B2B,Remote,348,Middle/Senior Data Engineer,N-iX,"#3682 We are seeking a Middle/Senior Data Engineer with proven expertise in AWS, Snowflake, and dbt to design and build scalable data pipelines and modern data infrastructure. You'll play a key role in shaping the data ecosystem, ensuring data availability, quality, and performance across business units. 4+ years of experience in Data Engineering roles. Experience with the AWS cloud platform. Proven experience with Snowflake in production environments. Hands-on experience building data pipelines using dbt . Python skills for data processing and orchestration. Deep understanding of data modeling and ELT best practices. Experience with CI/CD and version control systems (e.g., Git). Strong communication and collaboration skills. Strong experience with Snowflake (e.g., performance tuning, storage layers, cost management) Production-level proficiency with dbt (modular development, testing, deployment).. Experience developing Python data pipelines. Proficiency in SQL (analytical queries, performance optimization). Experience with orchestration tools like Airflow, Prefect, or Dagster. Familiarity with cloud platforms (e.g., GCP, or Azure). Knowledge of data governance, lineage, and catalog tools. Experience in working in Agile teams and CI/CD deployment pipelines. Exposure to BI tools like Tableau or Power BI. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 109, ""max"": 193, ""type"": ""Net per hour - B2B""}, {""min"": 80, ""max"": 160, ""type"": ""Gross per hour - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,349,Senior Consultant SAP CDC,INFOPLUS TECHNOLOGIES,"Key Responsibilities: ‚Ä¢ Architect and lead the implementation of secure, scalable CIAM solutions using SAP CDC, aligned with Coustomers digital strategy ‚Ä¢ Serve as a consultative partner to business and technology teams, translating business needs into secure, user-friendly identity experiences. ‚Ä¢ Drive the strategic integration of CIAM with digital platforms and applications, ensuring interoperability and performance across ecosystems. ‚Ä¢ Champion compliance and data privacy by designing solutions that meet global regulatory standards (GDPR, CCPA, etc.) and internal governance requirements. ‚Ä¢ Provide technical leadership and mentoring to developers and platform engineers, fostering a high-performing and knowledge-sharing team culture. ‚Ä¢ Oversee CIAM operations and continuous improvement, ensuring system reliability, security posture, and proactive incident resolution. ‚Ä¢ Engage in cross-functional program planning, contributing to timelines, resource strategies, and stakeholder alignment in an agile delivery environment. ‚Ä¢ Stay ahead of emerging technologies and trends in CIAM, identity protocols, and cybersecurity‚Äîproactively proposing enhancements to future-proof the platform. Your Profile: ‚Ä¢ 6+ years of experience in identity and access management, with a strong track record in delivering customer-centric solutions. ‚Ä¢ Deep hands-on expertise with SAP Customer Data Cloud (Gigya) and associated CIAM capabilities. ‚Ä¢ Strong understanding of identity standards and protocols such as SAML, OAuth 2.0, OpenID Connect, and their secure implementation. ‚Ä¢ Experience designing and managing privacy-first architectures, with knowledge of GDPR, CCPA, and industry data protection frameworks. ‚Ä¢ Backend experience with platforms such as Java, .NET, PHP, and front-end skills using JavaScript, HTML/CSS, REST APIs. ‚Ä¢ Proven ability to lead technical initiatives, influence architectural decisions, and collaborate effectively with both technical and business teams. ‚Ä¢ Professional certifications (SAP CDC, CIAM, CISSP, CISM, ITIL, etc.) are advantageous. ‚Ä¢ Excellent communication and stakeholder engagement skills‚Äîable to convey complex ideas in a clear and actionable way. ‚Ä¢ Agile working experience (Scrum, SAFe, etc.) and a mindset of continuous learning and adaptability.","[{""min"": 300000, ""max"": 325000, ""type"": ""Gross per year - Permanent""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Hybrid,350,Database Administrator (Db2 for z/OS),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: projektowanie i wdra≈ºanie rozwiƒÖza≈Ñ opartych na IBM Db2 for z/OS oraz ich bie≈ºƒÖca administracja, monitorowanie dostƒôpno≈õci i wydajno≈õci systemu oraz szybkie diagnozowanie i usuwanie usterek, reorganizacja struktur bazodanowych, backupy i odzyskiwanie danych, planowanie rozbudowy zasob√≥w w celu zapewnienia skalowalno≈õci, wsparcie specjalist√≥w z zakresu system√≥w z/OS przy instalacji poprawek Db2 (MSS) oraz migracji ≈õrodowiska, integracja globalnych rozwiƒÖza≈Ñ z istniejƒÖcƒÖ infrastrukturƒÖ IBM z/OS, wsparcie techniczne dla zespo≈Ç√≥w wewnƒôtrznych i u≈ºytkownik√≥w ko≈Ñcowych, tworzenie dokumentacji technicznej, prowadzenie szkole≈Ñ i dzielenie siƒô wiedzƒÖ na temat najlepszych praktyk w obszarze Db2 for z/OS, praca w modelu hybrydowym: 2-3 dni w tygodniu praca z biura w Krakowie. Ta oferta jest dla Ciebie, je≈õli: masz minimum 5 lat do≈õwiadczenia z IBM Db2 for z/OS, masz do≈õwiadczenie w diagnozowaniu usterek w ≈õrodowisku mainframe i optymalizacji wydajno≈õci SQL, znasz procedurƒô BIND i zarzƒÖdzanie planami wykonawczymi, masz do≈õwiadczenie w tworzeniu i przywracaniu kopii zapasowych na poziomie systemowym, masz do≈õwiadczenie w zarzƒÖdzaniu uprawnieniami i rolami u≈ºytkownik√≥w w bazach danych, znasz JCL i podstawy REXX oraz IBM Workload Scheduler (IWS), pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie B2/C1. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!",[],Database Administration,Database Administration
Full-time,Senior,Permanent,Hybrid,351,Senior Data Engineer,HSBC Service Delivery,"Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity Our technology team builds innovative digital solutions rapidly and at scale to deliver the next generation of banking services for our customers around the world. You‚Äôll be helping us be digital-first when developing new products and services, as well as enhancing existing ones by providing software development and support to each of our Global Businesses and Global Functions. Using best-in-class technologies, you'll deliver end-to-end execution of all technology services consumed across the HSBC landscape, including change programmes and applications in production. What you‚Äôll do Work on engaging projects with one of the largest banks in the world, on projects that will transform the financial services industry. Develop new and enhance existing financial and data solutions, having the opportunity to work on exciting greenfield projects as well as on established Tier1 bank applications adopted by millions of users. Work on automating and optimising data engineering processes, develop robust and fault tolerant data solutions both on cloud and on-premise deployments. You‚Äôll be involved in digital and data transformation processes through a continuous delivery model. Learn and work with specialised data and cloud technologies to widen the skill set. Have an opportunity to learn and work with specialised data and cloud technologies to widen the skill set. What you need to have to succeed in this role Excellent experience in the Data Engineering Lifecycle. You will have created data pipelines which take data through all layers from generation, ingestion, transformation and serving. Senior stakeholder management skills. Experience of modern Software Engineering principles and experience of creating well tested and clean applications. Strong experience in Hadoop on premises distribution especially Cloudera. Strong experience in one or more cloud service provider. AWS, Azure or GCP (GCP Experience is preferable.) Extensive experience using Python, Pyspark and the Python Ecosystem with good exposure to python libraries. Strong experience in SQL and building Data Analytics. Proven experience building robust production data pipelines using Airflow preferred. Nice to have Experience developing in other languages e.g. Scala/Java. What we offer Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery discounts Financial support with trainings and education Social fund Flexible working hours Free parking If your CV meets our criteria, you should expect the following steps in the recruitment process: Online behavioural test Telephone screen Zoom interview with the hiring manager We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,352,Principal Software Engineer,dotLinkers,"Salary: up to 49 000 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Principal Software Engineer you will serve as the key strategic and technical leader shaping the next generation of compute infrastructure. You‚Äôll design scalable execution platforms supporting microservices, batch processing, streaming data pipelines, and long-running workflows‚Äîleveraging Azure technologies including AKS (Kubernetes), KEDA, Temporal, and Apache Spark. You‚Äôll help define the broader platform architecture, influencing areas such as compute, storage, monitoring, and developer enablement. Working closely with engineering leaders and platform stakeholders, you‚Äôll lead the evolution toward a scalable, cloud-native, and developer-friendly compute environment that supports a variety of workloads across the organization. Responsibilities: Develop and refine the technical roadmap for compute infrastructure, enabling scalability, flexibility, and reliability across multiple workload types. Design core cloud-native execution frameworks for workflows, stream processing, and high-volume batch processing. Lead the adoption of Kubernetes, KEDA, and Temporal to orchestrate compute workloads with strong observability and fault tolerance. Integrate advanced data processing tools like Apache Spark, Azure Stream Analytics, and event-driven architectures across product services. Guide multi-team transformations of legacy systems into scalable microservices, containerized workloads, or serverless solutions. Align compute architecture with business priorities in collaboration with product and platform teams, ensuring compliance and operational reliability. Mentor Staff and Lead Engineers, promoting best practices in scalable architecture, cloud infrastructure, and modern compute strategies. Participate in architectural reviews, contribute to design documentation, and support long-term technical planning. Advocate for platform quality, security, and a streamlined developer experience. Required Qualifications: 10+ years of experience in software engineering, infrastructure, or platform development with demonstrated leadership in architecture. Hands-on experience running large-scale, production-grade Kubernetes-based compute platforms. Strong knowledge of orchestration technologies like KEDA, Temporal, or similar workflow/job orchestration engines. Practical experience with both batch (e.g., Spark) and streaming (e.g., Kafka, Azure Event Hubs) data processing systems. Proficiency in multiple programming languages (Go, Python, C#, Rust) and infrastructure-as-code tools (Terraform, Pulumi). In-depth understanding of distributed systems, autoscaling strategies, and compute security standards. Proven ability to work cross-functionally with multiple engineering teams and contribute to broader platform strategy. Preferred Qualifications: Background in building internal developer platforms or compute services offered as products. Knowledge of Azure serverless technologies such as Azure Functions or Azure Container Apps. Familiarity with tools like Dapr, KEDA Scalers, and modern runtime technologies such as Wasm, Nomad, or OpenFaaS. Contributions to open-source projects within the cloud-native or CNCF ecosystems. Experience designing systems that are multi-region, multi-tenant, and support zero-downtime deployments. Leadership Expectations: Define the long-term vision for compute infrastructure, ensuring alignment with business and growth objectives. Provide architectural leadership and technical mentorship across engineering and product teams. Drive complex, multi-disciplinary initiatives covering compute, data, security, and reliability. Cultivate a culture of technical excellence, innovation, and continuous improvement. Mentor senior engineers and encourage platform thinking and service-oriented design. Core Skills: Visionary Architecture: Ability to craft and communicate future-proof compute strategies. Technical Authority: Expertise in modern compute technologies and the ability to resolve complex technical challenges. Cross-Functional Leadership: Comfortable working with technical leaders across infrastructure, platform, and product domains. Cloud-Native Expertise: Mastery of compute orchestration, workload management, and event-driven architectures on Azure. Balanced Innovation: Ability to combine cutting-edge technologies with pragmatic, reliable solutions. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 39000, ""max"": 49000, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Part-time,Mid,Permanent,Remote,353,Power BI Developer (Part-time),Webdoctor,About this role You should - Have excellent spoken and written English. - Be able to work Monday to Friday within our core office hours of 10: 00 to 16: 00 Irish Summer Time / GMT. *Candidate must be based in the EEA (European Economic Area),[],Data Analysis & BI,Data Analysis & BI
Full-time,Manager / C-level,B2B,Remote,354,AI/ML Data Practice Lead,Knowit Poland sp. z o.o.,"Role Overview: As the AI/ML & Data Practice Lead, you will be responsible for designing, implementing, and delivering AI/ML solutions that drive value for our clients. You will also play a key and active role in pre-sales activities, defining company offering, lead discovery workshops, and provide hands-on consulting. Your expertise will help establish and grow our AI/ML and Data Management practice, positioning Knowit Poland as a leader in the field. Key Responsibilities: Lead the development and delivery of AI/ML and Data Management solutions. Engage in pre-sales activities, including participation in defining company offer, client consultations, discovery workshops, and inspiration events. Design and implement AI/ML and Data solutions based on public cloud infrastructure (preferably Azure). Establish and grow Knowit Poland's AI/ML and Data practices. Serve as the technical and professional lead in AI/ML and Data areas.","[{""min"": 150, ""max"": 200, ""type"": ""Net per day - B2B""}]",Data Science,Data Science
Full-time,Senior,Permanent,Hybrid,355,Senior PL SQL Developer,BNP Paribas SA oddzia≈Ç w Polsce,"Description As a part of the FA IT team within Securities Services cluster you will contribute to providing new solutions for operational teams and establishing the link between international stakeholders and development teams. This position will give you the unique opportunity to cooperate with dynamic, cross location squads while you will shape the product vision, manage the backlog, including transversal topics, to ensure continuity and guarantee the quality and sustainability of the application. Senior PL SQL Developer Responsibilities: Organizes development of software that meets development and security standards to support underlying business function. Drives optimization of existing code and determines techniques to improve its maintainability to continuously improve the quality of delivered solutions. Selects software packages and their configuration to lower the cost of delivered solutions Selects and configures product deployment tools to minimize impact on business continuity Drives implementation and creates the test strategy to ensure quality of delivered solutions Clarifies and qualifies the needs and carries out the studies before project launch to assess feasibility Requirements: 7-10+ years of professional experience Extensive knowledge of IT development standards, methods, and tools: Dev tools ‚Äì PL/SQL, SQL Developers OS: Cloud oriented platform AIX is an advantage Oracle feature Database ‚Äì performance tuning, code review, new feature analysis CI/CD DEVOPS Test automation Communication and presentation skills to audiences of different levels of seniority and different culture Awareness of IT quality and security standards, functional and technical architecture rules Knowledge of Fund Admin domain or familiarity with MultiFonds would be an advantage. Proactiveness, accuracy and ability to work under time pressure. Fluency in English as working language. What we offer: Hybrid work mode, 60% working from home within a month Equivalent for remote work expenses (120 PLN per month) Stable employment in the international company Fully paid private medical care for employee Pre-paid lunch card Employee Pension Plan Co-financed Multisport Card MyBenefit Cafeteria Platform Life insurance Car parking availability in the office building Trainings and development opportunities",[],Database Administration,Database Administration
Full-time,Senior,B2B,Remote,356,Senior Data Scientist,Code21 sp. z o.o,"Senior Data Scientist üìç Remote | üïí Core working hours: Polish time | üåç US-based clients Code21 is a boutique AI & software development company with a proven track record of delivering cutting-edge data solutions across energy, mobility, and fintech sectors. We're scaling up our AI division and looking for a Senior Data Scientist to join our growing remote team. Our clients are primarily based in the United States , and our delivery teams are spread between Europe and North America . We operate flexibly with core hours aligned to Polish time , with only minor time overlap with US stakeholders when needed. Build and deploy production-grade ML and AI models for real-world applications Work with structured and unstructured data (sensor data, NLP, tabular data) Collaborate with engineers, analysts, and product teams on feature development Participate in architecture decisions and technical strategy Support model lifecycle management (monitoring, retraining, optimization) 5+ years of hands-on experience in Data Science / Machine Learning Proficiency in Python , Pandas , NumPy , Scikit-learn , TensorFlow or PyTorch Experience with ML Ops tools (e.g. MLflow, DVC, Docker, Airflow) Solid knowledge of SQL and working with modern data warehouses (BigQuery, Snowflake) Proven experience with NLP , Time Series Forecasting , or Computer Vision Comfortable working in an agile and remote environment Strong communication skills in English (written and spoken) Experience working with US-based clients or in distributed teams Knowledge of cloud platforms (GCP, AWS, or Azure) Experience with deep learning models in production Familiarity with data privacy and compliance standards Competitive compensation based on experience 100% remote work with flexible hours (core hours: 12: 00‚Äì17: 00 CET) Exposure to innovative projects and US-based customers Opportunity to work with a senior, talented, and diverse team Modern tech stack, clean architecture, and freedom to make engineering decisions Flat structure and founder access ‚Äî your voice matters","[{""min"": 150, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Mid,Any,Remote,357,Platform Engineer (Oracle),Billennium,"Billennium is a global technology company with over 20 years of experience, committed to innovation and empowering businesses. As an employer, we offer a supportive, growth-focused environment where collaboration and creativity thrive. Join us to shape the future of technology together! We are looking for an experienced Platform Developer to join our client‚Äôs integration team and support the development and maintenance of enterprise-level solutions based on Oracle technologies. If you're comfortable working in a SOA environment and have hands-on experience with system integration ‚Äì we‚Äôd love to hear from you! ‚úÖ Your responsibilities: Design and implement integration processes Develop and maintain SOA/BPEL services Support and optimize existing integrations Work with XML/XSLT for data transformation Collaborate with IT and business teams to ensure seamless system communication ‚úÖ Requirements: Proven experience with Oracle SOA Suite and ODI Knowledge of WebLogic, XML/XSLT, BPELExperience with JDeveloper Nice to have: understanding of ETL processes and enterprise system integration ‚úÖ Perks and benefits: Comprehensive benefits - enjoy Udemy for Business, private medical care, Multisport card, veterinary package, language lessons, and shopping vouchers. Flexibility - adaptable working hours and remote/hybrid work options to suit your lifestyle & location. Career growth - access opportunities for professional development and learning, including perks related to our official partnerships with global IT giants: Microsoft, AWS, Snowflake, Salesforce & more. Global collaboration - work with a diverse, international team. Innovative environment - be part of a forward-thinking and growth-oriented workplace. Engaging community ‚Äì Work with passionate professionals and participate in team-building events, hackathons, and CSR initiatives to make an impact beyond work. Team-building events including our company tradition (annual company event in Mazury ). A pleasant surprise to start your journey with us in the form of a welcome pack. Recruitment process: HR call Technical Interview Interview with the dedicated Client Final decision/ Feedback Sounds interesting? Click ""Apply"" and have a chance to hear more! üìû",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,358,"Director, Data Engineering & Analytics",Optiveum,"Director, Data Engineering & Analytics Location: Warsaw (remote for now, hybrid soon) Salary: up to 33 000 PLN/month Contract type: Employment contract (UoP only) Our client is a US-based technology company headquartered in New York City, delivering innovative digital solutions and cloud-based platforms for private capital markets. With offices in multiple countries, the company is now investing in a new engineering centre in Warsaw, highly valuing the technical expertise and strong work ethic of software engineers in Poland. Currently, our client is building a new team in Warsaw. While the work is fully remote for now, within a few months, the role will require working from the Warsaw office 3 days per week. About the role As Director of Data Engineering & Analytics , you will lead a high-performing team of data engineers and be responsible for building and maintaining scalable data platforms and architectures. You will play a key role in shaping data-driven strategies and empowering analytics and data science teams with robust tools and pipelines. What you will do Lead, recruit, mentor, and develop a world-class data engineering team Design and maintain scalable data pipelines and architectures Optimize infrastructure for data extraction, transformation, and loading (ETL) Model data for optimal performance and insights Build and implement tools and systems that empower analytics and data science functions What we are looking for 8+ years of experience in data engineering or cloud/web engineering 5+ years in leadership roles with direct client communication Hands-on experience with ETL tools (AWS Glue, Azure Data Factory) Knowledge of orchestration tools (Airflow, Prefect) Proficiency with DBT, SQL, and data modeling techniques Experience with data lakes, data warehouses, and data mesh architecture Programming skills in Python or R Knowledge of reporting best practices and tools (Power BI, Tableau, Redash, etc.) Experience working with cloud platforms like AWS, Azure, or GCP Bachelor‚Äôs degree in Computer Science or a related field what is offered Competitive salary (up to 33 000 PLN/month) Employment contract (UoP only) Flexible work arrangements Great paid time off policy Comprehensive benefits package Opportunities for professional growth and leadership Supportive, inclusive, and fast-paced work environment","[{""min"": 28000, ""max"": 33000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Junior,Permanent,Office,359,Sta≈ºysta/ka ds. Wsparcia Dzia≈Çu Launch Management,Zak≈Çady Farmaceutyczne POLPHARMA S.A.,"Osoba na tym stanowisku bƒôdzie odpowiedzialna za wsparcie operacyjne i administracyjne zespo≈Çu Launch Management, ze szczeg√≥lnym naciskiem na zarzƒÖdzanie danymi projektowymi, dokumentacjƒÖ oraz narzƒôdziami wspierajƒÖcymi wdro≈ºenia produkt√≥w. Tw√≥j zakres obowiƒÖzk√≥w: Nadz√≥r nad bazami danych projektowych - aktualizacja, weryfikacja poprawno≈õci i sp√≥jno≈õci danych. Utrzymywanie porzƒÖdku w repozytorium projektowym - MS Project Online. Inicjowanie i monitorowanie wniosk√≥w zwiƒÖzanych z procesami wdro≈ºeniowymi (np. dostƒôp√≥w, zmian, zg≈Çosze≈Ñ). Wsp√≥≈Çpraca z zespo≈Çami projektowymi w zakresie harmonogram√≥w, status√≥w i dokumentacji. Tworzenie i aktualizacja raport√≥w oraz dashboard√≥w wspierajƒÖcych decyzje operacyjne. Wsparcie w przygotowywaniu materia≈Ç√≥w projektowych i prezentacji. Wymagania: Bardzo dobra znajomo≈õƒá Microsoft Excel (w tym tabele przestawne, formu≈Çy, Power Query). Znajomo≈õƒá Microsoft Project - planowanie i ≈õledzenie harmonogram√≥w projektowych. Mile widziana znajomo≈õƒá Power Query oraz Power BI - tworzenie raport√≥w i wizualizacji danych. Umiejƒôtno≈õƒá pracy z du≈ºƒÖ ilo≈õciƒÖ danych i dba≈Ço≈õƒá o szczeg√≥≈Çy. Samodzielno≈õƒá, dobra organizacja pracy i proaktywno≈õƒá w dzia≈Çaniu. Komunikatywno≈õƒá i umiejƒôtno≈õƒá wsp√≥≈Çpracy z r√≥≈ºnymi zespo≈Çami. Oferujemy: Pracƒô w oparciu o umowƒô zlecenie na okres roku Pracƒô w dynamicznym ≈õrodowisku projektowym. Mo≈ºliwo≈õƒá rozwoju kompetencji w obszarze zarzƒÖdzania projektami i analizy danych. Dostƒôp do nowoczesnych narzƒôdzi i technologii wspierajƒÖcych pracƒô zespo≈Çu.",[],Unclassified,Unclassified
Full-time,Manager / C-level,Permanent,Hybrid,360,Data Science Director,Procter & Gamble,"As the Director of Data Science, you'll be at the forefront of solving billion-dollar Data Science challenges across trillions of data points. You'll lead the design and development of scalable Data Science algorithms across functions and Business Units, using P&G‚Äôs cutting-edge global tech stack. This role will focus on building and improving Machine Learning, genAI / LLM and Optimization models at the enterprise level across operational, research, and strategic decisions. As a hands-on leader, you'll oversee and develop a team of Data Scientists, leveraging your expertise to help them succeed and grow to the next level. Key Responsibilities: Define and align strategy for the functioning of Data Science in the organization, ensuring operational excellence and scalability. Lead technical innovation by designing new algorithms that transform business processes. Manage and develop a team of 4-5 direct reports, growing to 9-12 direct reports, fostering a culture of learning and development. Collaborate with Data and AI Engineering teams to productionize algorithms and ensure their impact on business development. Quantify improvements in business areas resulting from algorithm use, influencing business development opportunities. Ensure algorithms adhere to production quality coding standards and follow corporate Data Science Development principles. Job Qualifications A Master‚Äôs degree in a quantitative field (Operation Research, Computer Science, Engineering, Applied Math, Statistics, Physics, Analytics, etc.) Experience in applying AI methodologies such as Machine Learning, Optimization, and Simulation to real-world problems. Proven leadership in applying and scaling Analytic and Machine Learning techniques to deliver insights from data. Knowledge of scientific computing languages such as Python, R, or Scala. Experience with large datasets and cloud computing platforms like GCP and Azure. What we offer Industry Certifications (e.g. ITIL, DevOps, Azure, and many others). Competitive salary and benefits program (private health care, life insurance, P&G stock options, saving plans, lunch subsidy, sports cards, in-office fitness center). Internal coaching programs & training. Flexible working arrangements. A long-term career with development and growth opportunities. At Procter & Gamble, we embrace a hybrid work model that combines the flexibility of remote work with the collaborative benefits of in-office engagement. Employees can enjoy the option to work from home two days a week while also spending time in the office to foster teamwork and enhance communication.",[],Data Science,Data Science
Full-time,Mid,B2B,Remote,361,Data Engineer,DataMass,"About the Role: We are looking for an experienced Data Engineer to join our delivery team working for a leading financial client in the UK . This is an exciting opportunity to contribute to the development of a scalable and secure data platform leveraging Databricks , Apache Spark , and Azure . The ideal candidate will be comfortable working in regulated environments and building robust pipelines that support critical analytics and reporting needs. Build, optimize, and maintain data pipelines using Apache Spark and Databricks Develop and deploy ETL workflows using PySpark and Python Write clean, optimized SQL queries for complex data transformations Work closely with data analysts, data scientists, and business users to deliver well-structured, reliable datasets Implement data modeling best practices for both batch and streaming data Leverage Azure services to manage data storage, orchestration, and compute workloads Ensure data quality, lineage, and compliance within a financial services context Contribute to solution architecture and participate in code reviews and Agile ceremonies Strong experience with Apache Spark (Advanced) Proficient in Databricks , PySpark , Python , and SQL (Regular) Hands-on experience working with Azure cloud services Solid understanding of data modeling and data warehousing principles Experience working with financial or highly regulated datasets (preferred) Strong communication and stakeholder collaboration skills Experience with Delta Lake , Unity Catalog , or Azure Synapse Familiarity with data governance, security, and compliance standards (e.g., GDPR, FCA) Exposure to CI/CD , Git , or workflow/orchestration tools (e.g., Azure Data Factory) Knowledge of data quality frameworks (e.g., Great Expectations) Opportunity to work on a high-impact project in the financial domain Access to cutting-edge technologies and cloud infrastructure Collaborative, diverse, and inclusive team environment Flexible working model (remote)","[{""min"": 20300, ""max"": 25700, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,362,Data Scientist,INFOPLUS TECHNOLOGIES,"üöÄ Hiring Now: ETRM Data Scientist (Remote | B2B Contract) Are you a Data Scientist with deep expertise in time-series forecasting , machine learning , and Azure ML ecosystems ? Do you thrive in complex energy trading environments and love building scalable, production-ready ML systems ? We have the perfect opportunity for you! üîç Role : ETRM Data Scientist üìç Location : Remote üìÑ Type : B2B Contract üéØ What You‚Äôll Do Design and implement cutting-edge forecasting models using ARIMA, LSTM, Prophet, and more. Develop scalable and reusable machine learning pipelines using Azure ML, PySpark , and Python-based MLOps frameworks . Deploy, monitor, and fine-tune models in operational environments to ensure long-term accuracy and performance. Work on large-scale datasets and high-performance ML workflows using Azure Databricks and Data Lake . Apply ensemble methods (stacking, boosting, bagging) for robust and accurate predictions. Collaborate in a dynamic energy trading environment to optimize decision-making using advanced data science techniques. ‚úÖ What We‚Äôre Looking For Education üéì Master‚Äôs in Mathematics, Statistics, Data Science, or related fields (Ph.D. preferred but not mandatory) Mandatory Skills Advanced Time-Series Forecasting and Predictive Modelling Deep understanding of ML frameworks: scikit-learn, XGBoost, TensorFlow, PyTorch, Darts Strong coding skills in Python and PySpark Solid hands-on with Azure Machine Learning SDK , Azure Databricks , and Azure Data Lake Expertise in MLOps and model lifecycle automation Proficiency in data handling libraries : Pandas, NumPy Preferred Skills K-Means Clustering , Bottom-Up Forecasting Experience with Azure Data Factory and pipeline orchestration Knowledge of Power/Energy Trading concepts Exposure to Generative AI (GenAI) and large language models like GPT üåü Why Join Us? Work with cutting-edge technology in the ETRM and energy analytics domain 100% remote ‚Äì collaborate with global experts from the comfort of your space Competitive B2B contract with flexible terms High-impact role with real-world business value üì© Interested? Apply now and let‚Äôs transform energy trading with data-driven intelligence! üîó [Apply Now] or send your CV to siva.s@infoplusltd.co.uk #ETRM #DataScience #MachineLearning #AzureML #MLOps #EnergyTrading #TimeSeriesForecasting #RemoteJobs #PythonJobs #DeepLearning #AzureDatabricks #ContractJobs #PySpark #GenAI #PowerTrading #B2BJobs #Contract #Hiring #Remotejobs","[{""min"": 31500, ""max"": 37800, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,B2B,Remote,363,Data Migration Specialist with German,in4ge sp. z o.o.,"Key Responsibilities: Performing data migration from Dynamics 365 system. Analyzing data migration requirements. Creating a comprehensive data migration plan. Preparing and cleaning data (data remediation) to ensure high data quality. Testing and verifying migrated data. Executing and monitoring the end-to-end data migration process. Minimum 1 year of experience working with Dynamics 365, especially in the area of data migration. Strong knowledge of SQL and Microsoft Excel. Good understanding of business processes. Fluent English and German (C1) and ability to work effectively in a distributed, international team. Fully remote work with flexible working hours - EMEA Timezone. Long-term collaboration on B2B contract. Opportunity to work on complex cloud projects for international clients. Professional growth in a highly skilled and supportive team. Collaborative and open working culture.","[{""min"": 18000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,Permanent or B2B,Remote,364,In≈ºynier Danych,Nexio Management,"Nexio Management to polska firma technologiczna z blisko 20-letnim do≈õwiadczeniem na rynku. Obecnie w naszych centrach kompetencyjnych zatrudniamy ponad 400 konsultant√≥w, kt√≥rzy ≈õwiadczƒÖ us≈Çugi IT dla Klient√≥w na ca≈Çym ≈õwiecie. Siedziba Nexio Management mie≈õci siƒô w Warszawie, a Nasze biura zlokalizowane sƒÖ we Wroc≈Çawiu, Rumunii oraz Wielkiej Brytanii. Posiadamy r√≥wnie≈º w≈Çasne R&D Center, kt√≥re jest miejscem powstawania innowacyjnych projekt√≥w m.in.: w obszarach Big Data, Cloud, CRM, Biling, czy AI. W ramach naszego portfolio tworzymy szyte na miarƒô rozwiƒÖzania, utrzymujemy i rozwijamy nawet najbardziej wymagajƒÖce systemy IT. Dostarczamy w≈Çasne produkty oraz wspieramy zespo≈Çy naszych klient√≥w w modelach scale up the team. Naszymi klientami sƒÖ firmy z ca≈Çego ≈õwiata szukajƒÖce wsparcia najwy≈ºszej klasy ekspert√≥w. Dla klienta z sektora publicznego, poszukujemy In≈ºyniera danych do d≈Çugofalowej wsp√≥≈Çpracy. Miejsce pracy: 100% zdalnie G≈Ç√≥wne obowiƒÖzki: Projektowanie, rozwijanie i utrzymywanie wydajnych i skalowalnych potok√≥w danych (ETL/ELT). Zapewnienie jako≈õci, integralno≈õci i bezpiecze≈Ñstwa danych. Optymalizacjƒô baz danych i system√≥w przechowywania danych. Wsp√≥≈Çpracƒô z analitykami danych i zespo≈Çami biznesowymi w celu zrozumienia ich potrzeb. Wdra≈ºanie i zarzƒÖdzanie infrastrukturƒÖ danych w chmurze (np. AWS, Azure, GCP) lub on-premise. Wymagania: Min. 4 lata do≈õwiadczenia jako In≈ºynier Danych lub na podobnym stanowisku. Bieg≈Ça znajomo≈õƒá SQL i do≈õwiadczenie z relacyjnymi oraz nierelacyjnymi bazami danych. Do≈õwiadczenie w pracy z platformami big data (np. Spark, Hadoop) oraz narzƒôdziami do orkiestracji (np. Apache Airflow). Umiejƒôtno≈õƒá programowania w Pythonie (lub innym jƒôzyku skryptowym do przetwarzania danych). Znajomo≈õƒá narzƒôdzi do zarzƒÖdzania wersjƒÖ kodu (Git). Mile widziane do≈õwiadczenie z technologiami chmurowymi i kontenerowymi (Docker, Kubernetes). Oferujemy: Umowƒô B2B lub umowƒô o pracƒô Pakiet benefit√≥w: Opieka medyczna, karta sportowa, lektoraty jƒôzyka angielskiego, Stabilne zatrudnienie PrzyjaznƒÖ atmosferƒô pracy",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,365,Azure Data Engineer z kompetencjami Devopsowymi,UNIVIO,"Jeste≈õmy polskƒÖ firmƒÖ technologicznƒÖ z ponad 25-letnim do≈õwiadczeniem jako partner cyfrowej transformacji handlu. Realizujemy miƒôdzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocze≈õnie lu≈∫nƒÖ, niezobowiƒÖzujƒÖcƒÖ atmosferƒô. Nasza organizacja opiera siƒô na kulturze otwarto≈õci i dzielenia siƒô wiedzƒÖ. Dowiedz siƒô kogo szukamy i zaaplikuj, je≈õli spe≈Çniamy Twoje oczekiwania üòâ","[{""min"": 16800, ""max"": 23520, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 17300, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,366,Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Development and maintenance of a large platform for processing automotive data. A significant amount of data is processed in both streaming and batch modes. The technology stack includes Spark, Cloudera, Airflow, Iceberg, Python, and AWS. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Centralized reporting platform for a growing US telecommunications company. This project involves implementing BigQuery and Looker as the central platform for data reporting. It focuses on centralizing data, integrating various CRMs, and building executive reporting solutions to support decision-making and business growth. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. üöÄ Your main responsibilities: Develop and maintain a high-performance data processing platform for automotive data, ensuring scalability and reliability. Design and implement data pipelines that process large volumes of data in both streaming and batch modes. Optimize data workflows to ensure efficient data ingestion, processing, and storage using technologies such as Spark, Cloudera, and Airflow. Work with data lake technologies (e.g., Iceberg) to manage structured and unstructured data efficiently. Collaborate with cross-functional teams to understand data requirements and ensure seamless integration of data sources. Monitor and troubleshoot the platform, ensuring high availability, performance, and accuracy of data processing. Leverage cloud services (AWS) for infrastructure management and scaling of processing workloads. Write and maintain high-quality Python (or Java/Scala) code for data processing tasks and automation. üéØ What you'll need to succeed in this role: At least 3 years of commercial experience implementing, developing, or maintaining Big Data systems, data governance and data management processes. Strong programming skills in Python (or Java/Scala): writing a clean code, OOP design. Hands-on with Big Data technologies like Spark , Cloudera, Data Platform, Airflow, NiFi, Docker, Kubernetes, Iceberg, Hive, Trino or Hudi. Excellent understanding of dimensional data and data modeling techniques. Experience implementing and deploying solutions in cloud environments. Consulting experience with excellent communication and client management skills, including prior experience directly interacting with clients as a consultant. Ability to work independently and take ownership of project deliverables. Fluent in English (at least C1 level). Bachelor‚Äôs degree in technical or mathematical studies. ‚ûï Nice to have: Experience with an MLOps framework such as Kubeflow or MLFlow. Familiarity with Databricks, dbt or Kafka. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn ).","[{""min"": 15120, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,367,BI Consultant (Power BI),Onwelo Sp. z o.o.,"Poznaj Onwelo: Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Bƒôdziesz pe≈Çniƒá zar√≥wno rolƒô konsultanta, jak i developera, pomagajƒÖc organizacjom w optymalizacji proces√≥w raportowania i podejmowaniu ≈õwiadomych decyzji opartych na danych. Do≈ÇƒÖcz do zespo≈Çu Data & Analytics w Onwelo, kt√≥ry realizuje projekty dla r√≥≈ºnych klient√≥w ‚Äì zar√≥wno polskich, jak i zagranicznych. W zale≈ºno≈õci od Twoich kompetencji i dostƒôpno≈õci bƒôdziesz mieƒá mo≈ºliwo≈õƒá pracy nad wdro≈ºeniami Power BI do r√≥≈ºnych organizacji, migracjƒÖ narzƒôdzi BI (np.Tableau) do Power BI, wsparciem biznesu w definiowaniu potrzeb analitycznych oraz budowƒô nowoczesnych hurtowni danych. Tworzyƒá i rozwijaƒá raporty oraz dashboardy w Power BI zgodnie z wymaganiami biznesowymi. Modelowaƒá dane, tworzyƒá zapytania DAX oraz optymalizowaƒá wydajno≈õƒá raport√≥w. Projektowaƒá i wdra≈ºaƒá procesy ETL do przekszta≈Çcania oraz ≈Çadowania danych. Integrowaƒá dane z r√≥≈ºnych ≈∫r√≥de≈Ç ‚Äì baz danych SQL, plik√≥w Excel, us≈Çug chmurowych i system√≥w ERP/CRM stanie siƒô czƒô≈õciƒÖ Twojej codziennej pracy. Wsp√≥≈Çpracowaƒá z zespo≈Çami biznesowymi i technicznymi, aby zapewniƒá efektywne raportowanie i analizƒô danych . Utrzymywaƒá i optymalizowaƒá istniejƒÖce rozwiƒÖzania BI , dbajƒÖc o ich wydajno≈õƒá oraz skalowalno≈õƒá. Pracowaƒá nad migracjƒÖ danych i system√≥w BI , np. przenoszeniem raport√≥w z Tableau lub Qlik do Power BI . Braƒá udzia≈Ç w budowie i rozwoju hurtowni danych np. Snowflake , modelujƒÖc i integrujƒÖc dane. Doradzaƒá w zakresie najlepszych praktyk BI oraz wspieraƒá klient√≥w w podejmowaniu ≈õwiadomych decyzji opartych na danych . Masz min. 3 lata do≈õwiadczenia w pracy z Power BI ‚Äì tworzysz raporty, dashboardy i modelujesz dane. Znasz DAX oraz Power Query i potrafisz je wykorzystaƒá do analizy oraz transformacji danych. Swobodnie modelujesz dane i projektujesz optymalne struktury raportowe . Posiadasz do≈õwiadczenie w pracy z bazami danych SQL ‚Äì tworzysz zapytania, optymalizujesz je i integrujesz dane. Znasz procesy ETL i masz praktyczne do≈õwiadczenie w przekszta≈Çcaniu oraz ≈ÇƒÖczeniu danych z r√≥≈ºnych ≈∫r√≥de≈Ç. Rozumiesz potrzeby biznesowe i potrafisz przek≈Çadaƒá je na konkretne rozwiƒÖzania analityczne . Pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie min. B2 , co pozwala Ci pracowaƒá w miƒôdzynarodowym ≈õrodowisku. Dodatkowym atutem bƒôdzie, je≈õli: Masz do≈õwiadczenie w migracji narzƒôdzi BI (np. Tableau, Qlik ) do Power BI. Znasz Dynamic 365 Praca ze Snowflake nie jest Ci obca Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 800, ""max"": 950, ""type"": ""Net per day - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Hybrid,368,G≈Ç√≥wny Specjalista ds. Baz Danych Oracle i DevOps¬†‚Äã(k/m),Polskie Sieci Elektroenergetyczne S.A.,"Polskie Sieci Elektroenergetyczne S.A. poszukujƒÖ kandydata / kandydatki na stanowisko: G≈Ç√≥wny Specjalista ds. Baz Danych Oracle i DevOps ‚Äã(k/m) Podstawowy zakres obowiƒÖzk√≥w: zarzƒÖdzanie i administracja bazami danych Oracle w ≈õrodowiskach produkcyjnych i testowo-rozwojowych (w tym backupy, recovery, monitorowanie, optymalizacja wydajno≈õci) instalacja, konfiguracja i utrzymanie instancji Oracle (w tym RAC, Data Guard) instalacja i konfiguracja komponent√≥w aplikacyjnych na bazach danych, serwerach aplikacyjnych oraz na platformach do konteneryzacji tworzenie i implementacja polityk bezpiecze≈Ñstwa baz danych planowanie i przeprowadzanie aktualizacji oraz migracji baz danych monitorowanie i rozwiƒÖzywanie problem√≥w zwiƒÖzanych z wydajno≈õciƒÖ i dostƒôpno≈õciƒÖ baz danych automatyzacja proces√≥w administracyjnych wsp√≥≈Çpraca z zespo≈Çami deweloperskimi, infrastrukturalnymi i ds. architektury udzia≈Ç w projektach wdro≈ºeniowych i rozwojowych rozwiƒÖzywanie zg≈Çosze≈Ñ serwisowych dotyczƒÖcych zarzƒÖdzanej warstwy system√≥w tworzenie i utrzymywanie dokumentacji technicznej opiniowanie dokumentacji projektowej uczestnictwo w dy≈ºurach (wsparcie 24/7) Wymagania: wykszta≈Çcenie wy≈ºsze minimum 5 lat do≈õwiadczenia w podobnym obszarze bardzo dobra znajomo≈õƒá architektury i administracji Oracle Database 12c, 19c praktyczne do≈õwiadczenie z Oracle RAC (Real Application Clusters) do≈õwiadczenie w pracy z Oracle Data Guard/Active Data Guard do≈õwiadczenia w tworzeniu i zarzƒÖdzaniu backupami oraz odtwarzaniu danych (RMAN) umiejƒôtno≈õƒá optymalizacji zapyta≈Ñ SQL oraz strojenia wydajno≈õci baz danych. do≈õwiadczenie w pracy z serwerami aplikacyjnymi Oracle WebLogic praktyczna znajomo≈õƒá system√≥w operacyjnych Linux/Unix (shell scripting) znajomo≈õƒá narzƒôdzi do monitorowania baz danych Oracle (OEM) umiejƒôtno≈õƒá rozwiƒÖzywania z≈Ço≈ºonych problem√≥w (troubleshooting) do≈õwiadczenie z narzƒôdziami do automatyzacji (Ansible) do≈õwiadczenie w korzystaniu z narzƒôdzi Jira i Confluence samodzielno≈õƒá i odpowiedzialno≈õƒá za powierzone zadania umiejƒôtno≈õci komunikacyjne i pracy w zespole Mile widziane: do≈õwiadczenie w pracy z innymi bazami danych (np. PostgreSQL, MySQL, IBM DB2) do≈õwiadczenie w platformach do konteneryzacji (HashiCorp, OpenShift) do≈õwiadczenie w pracy z innymi serwerami aplikacyjnymi (JBoss/Wildfly, Tomcat, IBM WAS) do≈õwiadczenie w pracy z Apache Kafka Oferujemy: zatrudnienie w oparciu o umowƒô o pracƒô atrakcyjny system wynagrodze≈Ñ uwzglƒôdniajƒÖcy czytelny system premiowy szkolenia podnoszƒÖce kwalifikacje i zapewniajƒÖce rozw√≥j zawodowy pakiet benefit√≥w pozap≈Çacowych, m.in . prywatna opieka medyczna, ubezpieczenie na ≈ºycie, karta multisport, dodatkowe ≈õwiadczenia w ramach ZF≈öS dodatkowy dzie≈Ñ wolny z okazji Dnia Energetyka - 14 sierpnia Osoby zainteresowane ofertƒÖ prosimy o nadsy≈Çanie dokument√≥w aplikacyjnych. Gwarantujemy pe≈ÇnƒÖ dyskrecjƒô w trakcie i po zako≈Ñczeniu procesu rekrutacyjnego. PSE S.A. zastrzega sobie prawo do kontaktowania siƒô tylko z wybranymi osobami.",[],Database Administration,Database Administration
Full-time,Senior,B2B,Remote,369,Data Scientist,Ework Group,"Ework Group - founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking for Senior Data Scientist ‚Äì Medical Image Analysis - remote work üîπ ‚úîÔ∏èThe Position This position focuses on exploring new insights and developing innovative technologies in the field of medical image analysis. Key responsibilities include the collection, curation, and processing of medical image data, as well as applying software engineering principles within the context of medical imaging and computer vision. You will also collaborate with team members, as well as clinical scientists from other disciplines to design, code, train, test, deploy, and iterate on machine learning systems. ‚úîÔ∏èMain responsibilities include: Research, develop and implement medical image processing pipelines utilizing state of the art computer-vision-based approaches in the medical image analysis space. Implement and maintain local and cloud-based data and computational environments and platforms to enable the work. Build data pipelines (data curation, preparation, cleaning, and consolidation) as an integral part of data science activity. Utilize AI/ML (Artificial Intelligence/Machine Learning) to develop complex methodologies and analyses, all around various medical imaging modalities. Perform informed semi-automated quality assessments of the acquired imaging data and of the derived measures. Align with clinical experts on the requirements, constraints and deliverables of a project. ‚úîÔ∏èQualifications: In this role, it is necessary to have either a PhD degree with 3+ years‚Äô relevant direct non-academic professional experience or PhD degree with a strong post-doc experience in the field of medical image analysis. Degree within computer science, mathematics, (medical-)engineering, physics, statistics, or a related quantitative discipline is preferred. ‚úîÔ∏èFurthermore, you must have: Hands-on experience with deep learning for computer vision using deep/machine learning frameworks, mainly PyTorch, Sklearn, and with conventional image processing techniques. Strong practical knowledge in medical image analysis in multiple medical imaging modalities (X-ray, US, MRI, CT, Digital Pathology) across different therapies (cardiology, obesity, diabetes etc.) with a proven high impact academic publication record Solid understanding of deep learning theory: CNNs, Transformers, RNN, GenAI, etc., and its various applications in semantic segmentation, classification, object detection and more. The ability to develop and validate algorithms against clinical data, ensuring appropriateness of fit for data science processes The ability to do quick prototyping/proofs of concept up to production ready models following best practices. The ability to perform in-depth data analysis and present results and conclusions to engineering and leadership teams. Appetite for research work Ideally, you have hands on knowledge in imaging biomarkers in pharmaceutical industry, healthcare industry, medical device development or in another regulated field. Experience with modern software development toolset (CI/CD, AWS, Azure Machine Learning, git, JIRA) which includes writing high quality code processing vision/imaging data using Python is highly preferred and so are excellent written and oral communication skills. ‚úîÔ∏èDepartment Overview The Imaging Analytics department part of AI and Analytics Department within Data Science division, where we apply state of the art algorithms and machine learning techniques to some of the hardest problems in the discovery and development of new healthcare solutions, focusing on medical imaging in the clinical trials space. By leveraging a blend of scientific, problem-solving, and quantitative skills, we provide superior data insights that empower us to further develop and deliver life-changing treatments. We work in multidisciplinary teams with strong collaboration across all areas of the organization and engage in external collaborations to ensure access to cutting edge research and technology. ‚úîÔ∏èJoin the team The organisation values flexibility in ways of working to support various life situations. Employees are recognised for their unique qualities and skills, and the environment fosters development and collaboration. The broader mission includes improving the lives of millions of patients globally through innovation and dedication to chronic disease care. There is a commitment to becoming not just the best company in the world, but the best company for the world. This vision can only be achieved through the contributions of talented employees with diverse backgrounds and perspectives. An inclusive culture is fostered that celebrates diversity across employees, patients, and communities. ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 161, ""max"": 178, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,370,Senior Data Engineer,emagine Polska,"Project information: Industry : Insurance and IT services Rate: up to 175 z≈Ç/h netto + VAT Location : remote work with occasional meetings (Warsaw) Project language : English Summary: Join a newly forming team responsible for building the next Data Hub in a large-scale enterprise environment. You‚Äôll have a direct impact on the architecture and development of a modern data platform based on a proven internal framework (Databricks + Azure). The platform supports a variety of use cases, from analytics and reporting to operational systems and Generative AI. We‚Äôre looking for an experienced Senior Data Engineer to shape the foundation of this new initiative and bring deep expertise in scalable data engineering solutions. Your Responsibilities: Data Hub Development: Implement a scalable Data Hub platform based on a company-wide framework using Azure and Databricks. Data Engineering: Build and optimize both batch and streaming data pipelines using Python and PySpark. Architectural Collaboration: Work closely with the Data & Solution Architect to implement enterprise-grade architecture. Technical Standards & Mentorship: Help define best practices, participate in code reviews, and support knowledge sharing within the team. Automation & CI/CD: Automate deployment and data operations using tools like Azure DevOps, Terraform, Docker, and Kubernetes. Data Quality & Monitoring: Ensure data validation, anomaly detection, and pipeline monitoring. Cross-Team Collaboration: Collaborate with multidisciplinary teams to align technical solutions with business goals. Documentation: Produce high-quality technical documentation in line with corporate standards. Must-Have Qualifications At least 6 years of experience as a Data Engineer in enterprise-scale environments. Proficiency in Python and PySpark . Solid hands-on experience with Microsoft Azure. Familiarity with Azure DevOps and automation workflows. Practical knowledge of Databricks . Comfortable working in cross-functional teams (e.g., architects, DevOps, analysts) within complex organizational structures. Fluent in English (minimum B2 level ). Nice to Have Hands-on experience with dbt (Data Build Tool) and DLT pipelines in Databricks. Understanding of medallion architecture in a production context. Background in large enterprises or consulting firms , ideally with exposure to complex data ecosystems. Experience working in Agile/Scrum development environments. Experience with infrastructure as code (Terraform), containerization (Docker), orchestration (Kubernetes).","[{""min"": 160, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Any,Remote,371,Business Data Analyst,Billennium,"More about us Billennium is a global technology company with over 20 years of experience, committed to innovation and empowering businesses. As an employer, we offer a supportive, growth-focused environment where collaboration and creativity thrive. Join us to shape the future of technology together! About the Client Project is based within our client ‚Äì a global leader in the healthcare and pharmaceutical sector, known for pioneering solutions in disease research, clinical development, and data-driven innovation. You will join an international, cross-functional team supporting an enterprise-grade data analytics platform. What you will do ‚Ä¢ Collaborate with business stakeholders to gather and clarify requirements for a cloud-based analytics platform ‚Ä¢ Translate business needs into functional specifications and break them into actionable tasks for developers ‚Ä¢ Conduct data analysis across multiple sources and support solution design in collaboration with architects and developers ‚Ä¢ Support solution delivery by preparing documentation (e.g. User Requirements, Functional Specs), UATs and demos ‚Ä¢ Ensure continuous alignment of deliverables with project objectives and regulatory standards What we are looking for ‚Ä¢ Solid experience in data-driven projects and/or integration platforms (e.g. DBT, Talend, Airflow) ‚Ä¢ Strong data analysis skills, including querying databases using SQL; knowledge of Snowflake is a plus ‚Ä¢ Background in pharma, clinical trials or healthcare domain is a strong advantage ‚Ä¢ Ability to clearly communicate technical concepts to non-technical stakeholders and create comprehensive documentation ‚Ä¢ Agile mindset, ability to work independently, and experience in a multicultural, global environment Perks and benefits Comprehensive benefits - enjoy Udemy for Business, private medical care, Multisport card, veterinary package, language lessons, and shopping vouchers. Flexibility - adaptable working hours and remote/hybrid work options to suit your lifestyle & location. Career growth - access opportunities for professional development and learning, including perks related to our official partnerships with global IT giants: Microsoft, AWS, Snowflake, Salesforce & more. Global collaboration - work with a diverse, international team. Innovative environment - be part of a forward-thinking and growth-oriented workplace. Engaging community ‚Äì Work with passionate professionals and participate in team-building events, hackathons, and CSR initiatives to make an impact beyond work. Team-building events including our company tradition (annual company event in Mazury . A pleasant surprise to start your journey with us in the form of a welcome pack. Recruitment process: HR call Technical Interview Interview with the dedicated Client Final decision/ Feedback Sounds interesting? Click ""Apply"" and have a chance to hear more!",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,372,Data Engineering Analyst,TechTorch,"About Us TechTorch is a fast-growing consultancy at the intersection of enterprise tech, AI, and private equity. We partner with top-tier PE funds and their portfolio companies to deliver AI-powered platforms, data accelerators, and digital transformation projects that generate measurable value ‚Äî fast. Founded by former Bain consultants, CIOs, and enterprise tech leaders, we blend startup agility with big-firm experience. We‚Äôre not a typical startup ‚Äî we were built to deliver. As a Data Analyst , you‚Äôll be a key contributor to building a modern Azure-native data platform for one of our private equity-backed clients. You‚Äôll analyze current data sources and legacy reports, reverse-engineer business logic, and translate complex reporting requirements into structured data specifications. You‚Äôll also support data profiling, validation, and the design of a Common Data Model (Bronze/Silver/Gold). Your work will directly influence dbt model development, data pipelines, and reporting layers ‚Äî all serving as the organization‚Äôs single source of truth. SQL (advanced querying, troubleshooting) dbt, ETL, data modeling (dimensional/relational) Data profiling, validation, quality checks Azure Data Services (preferred), AWS/GCP also welcome Familiarity with Medallion architecture Bonus: Python (Pandas), Salesforce data Analyze raw and semi-structured datasets to understand key metrics and usage Reverse-engineer legacy SQL, stored procedures, reports Perform data profiling and assess quality issues Translate business needs into clear data specs, rules, and mappings Support Common Data Model design (Bronze/Silver/Gold) Define data validation, error handling, and quality checks Collaborate closely with Data Architects and engineers Support testing and debugging through validation reports and business logic guidance Engage with business stakeholders to clarify reporting needs 5+ years of experience in enterprise data analysis Strong SQL skills (querying, analysis, debugging) Deep experience in data profiling and validation Familiarity with relational and dimensional modeling Ability to understand and document transformation logic Experience working across technical and business teams Knowledge of modern data architectures (Medallion, warehouse design) Clear, structured communication skills English: fluent spoken and written Python (Pandas) for exploratory analysis Experience with Salesforce data Cloud experience (Azure preferred, AWS/GCP okay) Projects with high-impact PE-backed companies Work with ex-Bain, top-tier CIOs, and data leaders Real ownership and visibility in project delivery Remote-first culture built on speed, clarity, and results A fast-paced environment for high performers who want to grow Client First ‚Äì Value and outcomes over slide decks We, Not Me ‚Äì We move faster as a team Get Stuff Done ‚Äì Execution over bureaucracy AI First ‚Äì AI is embedded into what we build Own It ‚Äì We take full responsibility Agile Mindset ‚Äì We adapt fast and improve constantly","[{""min"": 16000, ""max"": 23500, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Hybrid,373,Azure Technical Customer Success Senior Analyst with Polish and English,Accenture,"Accenture is a leading global professional services company that helps the world‚Äôs leading businesses, governments and other organizations build their digital core, optimize their operations, accelerate revenue growth and enhance citizen services. We offer solutions and assets across Strategy & Consulting, Technology, Operations , Industry X and Accenture Song. Operations drives business value and outcomes to advance our clients on their journey to intelligent operations through our innovative operating engine ‚Äî SynOps. We transform business processes to achieve sustainable growth by optimizing people, technology, data and intelligence. Azure is the most comprehensive, innovative and flexible cloud platform today and talented professionals are needed to drive customer cloud adoption within the most important companies in the market. Lead, drive and manage engagements for repeatable achievement of revenue and consumption targets. Leverage best practices to guide customer strategy and future growth by cultivating customer affinity with client programs/solutions that drive impact for the customer. Provide feedback on customer development needs, customer blockers, or mitigation strategies. Conduct analyses into what customers are using versus needs. Drive greater consumption (cloud and support) with customers based on analysis of both usage and needs. Leverage insights to provide guidance and recommendations to customers; drive, retain, and optimize customer consumption. Apply technical knowledge and customer insights to create a modernization roadmap. Architect solutions to meet business and IT needs, ensuring technical viability of new projects and successful deployments, while orchestrating key resources and infusing key Infrastructure technologies. Run Architectural Design Sessions to build plans for implementing solutions which align to customer business goals and technical environments. Collaborate and orchestrate with other Cloud Solution Architects and stakeholders, including FastTrack, Solution Assessments, etc., in developing cloud solutions on Azure. Proficiency in English and Polish (C1 level required) 3+ years‚Äô experience in technical architect, technical consulting, design and implementation, and/or technical sales. OR Bachelor's Degree in Computer Science, Information Technology, Engineering, or related field and 4+ years‚Äô experience in technical architect, technical consulting, design and implementation, and/or technical sales. OR Relevant certifications from Microsoft or competitive platforms AND 3+ years‚Äô experience in technical architect, technical consulting, design and implementation, and/or technical sales. OR equivalent experience. Preferred Qualifications : Bachelor's Degree in Computer Science, Information Technology, Engineering, or related field AND 8+ years‚Äô experience in technical architect, consulting, design and implementation, and/or sales. 3+ years‚Äô experience with cloud-based solution designs, migrations and management. MSFT Ceritificates: AZ-305, AZ400, AI-102, DP-100 or DP-500. Research indicates that some candidates, especially the most diverse ones, may hesitate to apply for positions if they don't meet all requirements. If you believe you possess the necessary skills, even if not meeting every requirement, we wholeheartedly encourage you to submit your application. 6-month employment contract with the possibility of extension for 1 year or an indefinite employment contract. Hybrid work model: 2 days remote, 3 days in-office.‚ÄãUsing foreign language and new technology solutions daily, cooperating with various global Clients. Individual support of a People Lead and a specific path of professional development, as well as the possibility of a session with a Coach. A wide training package (soft, technical, and language training offer, access to the e-learning platforms, Gallup test, GenAI training, possibility of co-financing courses, and certification). Employee Assistance Program - legal, financial, and psychological consultations. Accenture employees eligible for the Employee share purchase plan automatically become eligible for quarterly dividends if they own company shares. Paid employee referral program. Private medical care, life insurance. Access to the MyBenefit platform (possibility of using a wide range of products and services, including the Multisport card). Accenture does not discriminate employment candidates on the basis of race, religion, color, sex, age, disability, national origin, political beliefs, trade union membership, ethnicity, denomination, sexual orientation or any other basis impermissible under Polish law. All our leaders are committed to building a better, stronger and more durable company for future generations to create positive, long-lasting change. Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and creative, which helps us better serve our clients and our communities. Our position as partner to many of the world‚Äôs leading businesses, organizations and governments affords us both an extraordinary opportunity and a tremendous responsibility to make a difference. Sustainability is one of our greatest responsibilities, which we embed it into everything we do and for everyone we work with.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Hybrid,374,Programista PL/SQL,GS Services,"Programista PL/SQL poszukiwany! üïµüèº‚Äç‚ôÄÔ∏è Je≈ºeli chcia≈Çby≈õ do≈ÇƒÖczyƒá do jednego z globalnych gigant√≥w technologicznych, znanego z tworzenia wiodƒÖcych rozwiƒÖza≈Ñ bazodanowych oraz kompleksowych us≈Çug chmurowych ‚Äî firmy, kt√≥rej produkt√≥w u≈ºywajƒÖ miliony przedsiƒôbiorstw na ca≈Çym ≈õwiecie, a jej technologie sƒÖ standardem w bran≈ºy IT ‚Äî to ta oferta jest dla Ciebie! Do≈ÇƒÖczajƒÖc do zespo≈Çu, bƒôdziesz pracowaƒá z najnowszymi rozwiƒÖzaniami jednego z najbardziej rozpoznawalnych lider√≥w technologii na rynku. Wymagania: Bardzo dobra znajomo≈õƒá SQL i PL/SQL Do≈õwiadczenie w pracy z du≈ºymi zbiorami danych Nastawienie na rozw√≥j w technologii Oracle i jƒôzyku PL/SQL w obszarze integracyjnym Umiejƒôtno≈õƒá optymalizacji zapyta≈Ñ w bazie danych Oracle Kompetencje w projektowaniu oraz implementacji interfejs√≥w us≈Çug w architekturze SOA Praktyczne do≈õwiadczenie w wykorzystywaniu SOAP, REST, XQuery, XSLT w projektach integracyjnych Minimum 5-letnie do≈õwiadczenie na podobnym stanowisku MIle widziane: Certyfikaty Oracle PL/SQL Znajomo≈õƒá Oracle Service Bus, Oracle SOA Suite, Oracle Weblogic Server Opis projektu: Projekt obejmuje modelowanie i tworzenie struktur danych na potrzeby integracji system√≥w w architekturze SOA. Realizujƒô rozw√≥j oraz optymalizacjƒô kodu PL/SQL, skupiajƒÖc siƒô na wydajnych zapytaniach, procedurach i interfejsach us≈Çug. Przeprowadzam analizƒô oraz tuning zapyta≈Ñ SQL na du≈ºych zbiorach danych. Pracujƒô zgodnie z wymaganiami SLA, zapewniajƒÖc wysokƒÖ dostƒôpno≈õƒá i niezawodno≈õƒá system√≥w",[],Database Administration,Database Administration
Full-time,Senior,B2B,Remote,375,Senior MS Data Engineer (Azure SQL DB / ETL / PySpark),1dea,"Poszukujemy do≈õwiadczonego Senior MS Data Engineer‚Äôa z pasjƒÖ do tworzenia innowacyjnych rozwiƒÖza≈Ñ. Idealny kandydat powinien posiadaƒá do≈õwiadczenie z Azure SQL , silne umiejƒôtno≈õci w optymalizacji zapyta≈Ñ oraz zdolno≈õƒá proponowania nowych implementacji w odpowiedzi na zg≈Çaszane problemy. Dodatkowo, kluczowe jest solidne zrozumienie architektury , powiniene≈õ byƒá w stanie sugerowaƒá zmiany, takie jak wdro≈ºenie wymiany danych niemal w czasie rzeczywistym . Informacje organizacyjne: Bran≈ºa: IT Consulting Wakaty: 2 Lokalizacja: praca w 100% zdalnie Start: ASAP (max 2msc okresu wypowiedzenia) Stawka: do ustalenia w zakresie 155 - 170 PLN netto + VAT / h Warunki zaanga≈ºowania: B2B (outsourcing przez 1dea), full-time, long-term Proces rekrutacyjny (w pe≈Çni zdalny): Kr√≥tka rozmowa telefoniczna z rekruterem 1dea - o projekcie i warunkach zaanga≈ºowania: (~10 minut) Przedstawienie Twojej kandydatury Klientowi Rozmowa techniczno-projektowa z Klientem (wideo) (~1-1.5 h) (Opcjonalnie) rozmowa podsumowujƒÖca proces z Managerem Technicznym ze strony naszego Klienta (~30 minut) Podjƒôcie decyzji o wsp√≥≈Çpracy Projektowanie i wdra≈ºanie zaawansowanych rozwiƒÖza≈Ñ in≈ºynierii danych na platformie Azure Optymalizacja zapyta≈Ñ Proponowanie nowych implementacji w odpowiedzi na zg≈Çaszane problemy Sugerowanie zmian, takich jak wdro≈ºenie wymiany danych niemal w czasie rzeczywistym Optymalizacja i utrzymanie istniejƒÖcych proces√≥w ETL Budowa skalowalnych i wydajnych pipeline‚Äô√≥w danych Wsp√≥≈Çpraca z zespo≈Çami biznesowymi w celu zrozumienia wymaga≈Ñ i prze≈Ço≈ºenia ich na rozwiƒÖzania techniczne Zapewnienie wysokiej jako≈õci, bezpiecze≈Ñstwa i zgodno≈õci danych z obowiƒÖzujƒÖcymi przepisami Udzia≈Ç w projektach zwiƒÖzanych z rozwojem platformy danych Minimum 3 lata do≈õwiadczenia w in≈ºynierii danych, z naciskiem na platformƒô Azure Solidna znajomo≈õƒá Azure SQL Databases, Databricks oraz innych kluczowych us≈Çug Azure Praktyczna umiejƒôtno≈õƒá wykorzystania Power Query, Python i PySpark do transformacji i manipulacji danymi Umiejƒôtno≈õƒá modelowania danych, projektowania i optymalizacji proces√≥w ETL Silne umiejƒôtno≈õci analityczne i zdolno≈õƒá do rozwiƒÖzywania problem√≥w Umiejƒôtno≈õƒá pracy w metodyce Agile Znajomo≈õƒá jƒôzyka angielskiego pozwalajƒÖca na swobodnƒÖ komunikacjƒô w mowie i pi≈õmie (B2+) Mile widziane: Certyfikaty Microsoft Data Engineering (DP 600, DP 203) Do≈õwiadczenie w pracy z narzƒôdziami BI, takimi jak Power BI Znajomo≈õƒá praktyk DevOps D≈Çugotrwa≈Çy kontrakt B2B (od razu podpisujemy umowƒô na czas nieokre≈õlony / bezterminowo - Klient nastawia siƒô tylko i wy≈ÇƒÖcznie na d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô) Zosta≈Ñ czƒô≈õciƒÖ firmy o silnej pozycji na rynku Nowoczesny sprzƒôt i oprogramowanie (zapewnia nasz Klient) Elastyczne godziny pracy: ciesz siƒô swobodƒÖ efektywnego zarzƒÖdzania swoim czasem Pracƒô w 100% zdalnie Kultura wsp√≥≈Çpracy: Cenimy pracƒô zespo≈ÇowƒÖ, otwarto≈õƒá, szacunek i wzajemne wsparcie w rozwoju umiejƒôtno≈õci. Kreatywno≈õƒá mile widziana: Twoje pomys≈Çy i sugestie w realizacji projektu bƒôdƒÖ uwzglƒôdniane i brane pod uwagƒô : -)","[{""min"": 155, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Mid,B2B,Remote,376,Data Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients‚Äô systems, departments and sites. We provide an open technology platform that‚Äôs shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience. Want to be part of it? Job Summary: As a Data Engineer at Keyloop you will work within the Analytics Engineering team and be responsible for maintaining and developing our Data Lake and existing Data Pipelines, as well as continually exploring, analysing and proposing improvements to existing processes and tooling. You will also be required to contribute to scoping and discovery exercises being conducted elsewhere in the business, leveraging your expert knowledge of the data and business intelligence offering and the associated data requirements. You will therefore be working in a busy and multi-functional team, planning and prioritising a variable workload and delivering to deadlines. You will report to the Lead Analytics Engineer, and you may provide mentorship to other analysts and data engineers in Analytics Engineering where appropriate to support their knowledge and skills growth. Key Responsibilities: ‚Ä¢ Design, build and maintain scalable and robust ETL/ELT pipelines using various data integration tools and programming languages (e.g. Python, SQL) ‚Ä¢ Collaborate with Analytics Engineers to design and implement optimised data models within our data warehouse, ensuring data quality, consistency and ease of use for analytics ‚Ä¢ Manage and optimise our data warehouse including cost optimisation and ensuring data governance best practices ‚Ä¢ Implement robust data validation, monitoring and alerting mechanisms to ensure accuracy and completeness of our data ‚Ä¢ Work closely with Analytics Engineers to understand their data requirements, provide technical guidance and ensure the efficient delivery of data products ‚Ä¢ Pre and post release communications where necessary to relevant stakeholders ‚Ä¢ Documenting processes (or SOPs) for commonly performed tasks to assist with the training of other Data Engineers, or to aid business continuity as a general theme ‚Ä¢ Fully understanding the data landscape in databases, analytics and applications and all the front-end and back-end products in the global portfolio ‚Ä¢ Understanding of data compliance and laws, as well as the full data collection to output technology stack Technical Competencies: ‚Ä¢ Experience with Amazon Web Services cloud platform and with knowledge in particular of data related services they offer ‚Ä¢ SQL for data manipulation, transformation and query optimisation ‚Ä¢ Experience in Python for data engineering tasks ‚Ä¢ Hands-on experience with a modern data warehouse platform (e.g. Amazon Redshift) ‚Ä¢ Solid understanding of data warehousing concepts, dimensional modelling (star schema approach), and ETL/ELT principles ‚Ä¢ Experience with data pipeline orchestration tools ‚Ä¢ Familiarity with version control systems (e.g. Git) ‚Ä¢ Understanding of data governance principles and tools Behavioural & Personality Competencies: ‚Ä¢ Analytical and logical mindset ‚Ä¢ Time management skills ‚Ä¢ High standard of problem-solving skills and attention to detail ‚Ä¢ Good communication and listening skills ‚Ä¢ Team player and collaborative ‚Ä¢ Ability to manage multiple different projects ¬∑ Organised & self-sufficient ¬∑ Logical, methodical approach to problem and issue solving ¬∑ Numerate, innovative and critical thinking Desirable Skills: ‚Ä¢ Familiarity with JIRA software ‚Ä¢ Experience, or an interest in, the automotive industry ‚Ä¢ Experience of Agile/SCRUM delivery environment Why join us? We‚Äôre on a journey to become market leaders in our space ‚Äì and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We‚Äôre committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles ‚Äì not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn‚Äôt require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply .","[{""min"": 18000, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,377,Data Engineer - Delivery Experience,Allegro,"About the team The salary range for this position is (contract of employment): 14 200 - 19 690 PLN in gross terms A hybrid work model requires 1 day a week in the office In the area of Delivery Experience, we are building technology that makes Allegro's deliveries easy, cost-effective, fast and predictable. Our team takes care of critical services along the Allegro shopping journey, responsible for predicting delivery times using statistical algorithms and machine learning, selecting the best delivery methods tailored to customers, and integrating with carrier companies. Delivery Experience is also one of the fastest-growing areas where we undertake new, complex projects to enhance logistics and warehousing processes. We are looking for a Mid/Senior Data Engineer with a focus on the data processing and preparation, deployment and maintenance of our data projects. Join our team to enhance your skills related to deploying data-based processes, data ops approaches and share the skills within the team. Your main responsibilities: - You will be actively responsible for developing and maintaining processes for handling large volumes of data - You will be streamlining and developing the data architecture that powers analytical products and work along a team of experienced analysts - You will be monitoring and enhancing quality and integrity of the data - You will manage and optimize costs related to our data infrastructure and data processing on GCP This is the right job for you if: - You have at least 3 years of experience as Data Engineer and working with large datasets. - You have experience with cloud providers (GCP preferred). - You are highly proficient in SQL. - You have strong understanding of data modeling and cloud DWH architecture. - You have experience in designing and maintaining ETL/ELT processes. - You are capable of optimizing cost and efficiency of data processing. - You are proficient in Python for working with large data sets (using PySpark or Airflow). - You use good practices (clean code, code review, CI/CD). - You have a high degree of autonomy and take responsibility for developed solutions. - You have English proficiency on at least B2 level. - You like to share knowledge with other team members. Nice to have: - Experience with Azure and cross-cloud data transfers and multi-cloud architecture What we offer: - Big Data is not an empty slogan for us, but a reality - you will be working on really big datasets (petabytes of data). - You will have a real impact on the direction of product development and technology choices. We utilize the latest and best available technologies, as we select them according to our own needs. - Our tech stack includes: GCP, BigQuery, (Py)Spark, Airflow. - We are a close -knit team where we work well together. - You will have the opportunity to work within a team of experienced engineers and big data specialists who are eager to share their knowledge, including publicly through allegro.tech Apply to Allegro and see why it is #dobrzetubyƒá (#goodtobehere)","[{""min"": 14200, ""max"": 19690, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,378,BI Developer,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: projektowanie, budowa i utrzymanie kompleksowych przep≈Çyw√≥w danych oraz proces√≥w wzbogacania danych, wsp√≥≈Çpraca z Product Managerami, analitykami biznesowymi, Data Scientistami oraz interesariuszami na r√≥≈ºnych poziomach organizacji w celu przekszta≈Çcania wymaga≈Ñ biznesowych w konkretne rozwiƒÖzania BI, projektowanie i rozw√≥j hurtowni danych oraz struktur analitycznych (data marts) wspierajƒÖcych raportowanie i analizƒô danych, tworzenie i utrzymywanie dashboard√≥w w Power BI oraz rozw√≥j modeli semantycznych, zarzƒÖdzanie transformacjami danych w ramach proces√≥w ELT przy u≈ºyciu DBT, utrzymanie wysokiej wydajno≈õci i skalowalno≈õci rozwiƒÖza≈Ñ BI, wsparcie i szkolenie u≈ºytkownik√≥w ko≈Ñcowych w zakresie korzystania z system√≥w BI, praca zdalna w zespole miƒôdzynarodowym (+1h r√≥≈ºnicy wzglƒôdem polskiej strefy czasowej), stawka do 130 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Aktualnie posiadamy dwa osobne wakaty, na kt√≥re aktualnie rekrutujemy!1) Ta oferta jest dla Ciebie, je≈õli: posiadasz minimum 3 lata do≈õwiadczenia na podobnym stanowisku, masz bardzo dobrƒÖ znajomo≈õƒá SQL posiadasz praktyczne do≈õwiadczenie z Lookerem, w tym rozw√≥j LookML, tworzenie dashboard√≥w oraz administracja Lookerem i zarzƒÖdzanie uprawnieniami, posiadasz do≈õwiadczenie z DBT w kontek≈õcie transformacji danych w pipeline'ach ELT, znajomo≈õƒá angielskiego na poziomie min. B2, posiadasz znajomo≈õƒá Snowflake, lub innych nowoczesnych baz danych analitycznych, 2) Ta oferta jest dla Ciebie, je≈õli: posiadasz minimum 4 lata do≈õwiadczenia na podobnym stanowisku, posiadasz bardzo dobrƒÖ znajomo≈õƒá SQL oraz modelowania danych, posiadasz do≈õwiadczenie w pracy z Power BI (dashboardy, modele danych, DAX), posiadasz praktycznƒÖ znajomo≈õƒá DBT i architektury ELT, posiadasz znajomo≈õƒá hurtowni danych - szczeg√≥lnie Snowflake i/lub BigQuery, posiadasz do≈õwiadczenie w pracy z systemami. znajomo≈õƒá angielskiego na poziomie min. B2, mile widziane do≈õwiadczenie z Lookerem oraz znajomo≈õƒá platform chmurowych (Azure, AWS, GCP). Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 18480, ""max"": 21840, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent or B2B,Remote,379,Data Analyst,Sigma Software,"It is an international technology company that specializes in developing high-load platforms for data processing and analytics. Its core product helps businesses manage large volumes of data, build models, and gain actionable insights. The company operates globally and serves clients primarily in the marketing and advertising sectors, and focuses on modern technologies, microservices architecture, and cloud-based solutions. The project focuses on working with audiences and targeting from the advertisers‚Äô side (such as marketing agencies and similar structures). This is an independent project that is part of a larger AdTech platform. 4+ years of experience solving business problems through data analysis and extracting actionable insights from large datasets Proficiency in SQL, Python, and data visualization tools (e.g., Tableau, Power BI, or similar) Strong attention to detail with a proven track record of ensuring data integrity and accuracy Experience working within Agile frameworks and collaborating with cross-functional teams Excellent time management skills and the ability to prioritize tasks effectively to meet deadlines Demonstrated capability to manage multiple competing priorities in a fast-paced, dynamic environment At least an Upper-Intermediate level of English Strong written and verbal communication skills Fast learner with a proactive mindset and eagerness to explore new tools and technologies Analyze and evaluate datasets to unlock their value for product improvements Ensure data quality and address inconsistencies Conduct exploratory analysis to identify trends and opportunities Provide insights to support strategic decisions and partnerships Create and maintain dashboards and reports for clear communication Collaborate with cross-functional teams to define analytics needs Build deep domain knowledge to refine use cases and enhance features",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,380,Senior Data Engineer,7N,"O projekcie: Poszukujemy do≈õwiadczonych Senior Data Engineer'√≥w, kt√≥rzy do≈ÇƒÖczƒÖ do nowo tworzonego w Polsce zespo≈Çu specjalizujƒÖcego siƒô w migracji danych do chmury. Projekt realizowany jest dla miƒôdzynarodowej du≈Ñskiej organizacji z sektora finansowego. Kluczowym zadaniem bƒôdzie zapewnienie p≈Çynnej transformacji istniejƒÖcych rozwiƒÖza≈Ñ oraz stworzenie trwa≈Çych fundament√≥w pod rozw√≥j nowoczesnej platformy danych opartej na architekturze Lakehouse. Praca w 100% zdalna, wymagana otwarto≈õƒá na podr√≥≈º do Danii na poczƒÖtku projektu. Zakres obowiƒÖzk√≥w Migracja istniejƒÖcych rozwiƒÖza≈Ñ danych do ≈õrodowiska chmurowego Projektowanie oraz implementacja proces√≥w ETL Modelowanie danych oraz tworzenie transformacji danych w oparciu o wymagania u≈ºytkownik√≥w ko≈Ñcowych Implementacja kontrakt√≥w danych i domen danych (np. Umowy, Polisy, Klienci, Roszczenia) Wsp√≥≈Çpraca z interesariuszami w celu zrozumienia wymaga≈Ñ biznesowych Przeprowadzanie test√≥w jako≈õci danych, preferencyjnie automatycznych, i tworzenie mechanizm√≥w kontrolnych zapewniajƒÖcych sp√≥jno≈õƒá danych Tworzenie i utrzymywanie dokumentacji technicznej i biznesowej Udzia≈Ç w planowaniu prac z wykorzystaniem Azure DevOps Udzia≈Ç w analizach potrzeb biznesowych i wsp√≥≈Çpraca z lokalnymi przedstawicielami klienta w celu okre≈õlenia wymaga≈Ñ funkcjonalnych i niefunkcjonalnych. Dbanie o jako≈õƒá i sp√≥jno≈õƒá kodu ≈∫r√≥d≈Çowego, w tym przestrzeganie dobrych praktyk programistycznych i standard√≥w kodowania przy pracy z SQL i Pythonem. Udzia≈Ç w rozwoju i utrzymaniu fundament√≥w danych wspierajƒÖcych produkty biznesowe Wsp√≥≈Çpraca z zespo≈Çami miƒôdzynarodowymi Oczekiwania Minimum 5 lat do≈õwiadczenia komercyjnego w roli Data Engineera Do≈õwiadczenie w migracji danych i projektowaniu rozwiƒÖza≈Ñ chmurowych Bardzo dobra znajomo≈õƒá jƒôzyk√≥w SQL i Python, w tym umiejƒôtno≈õƒá tworzenia skalowalnych i czytelnych skrypt√≥w oraz test√≥w automatycznych Praktyczna znajomo≈õƒá narzƒôdzi z ekosystemu Azure, w szczeg√≥lno≈õci: - Azure Data Factory (ADF) -Databricks - Azure DevOps (zarzƒÖdzanie backlogiem i planowaniem pracy) - (mile widziana znajomo≈õƒá: Microsoft Purview w zakresie rozumienia, niekoniecznie rozwoju) Do≈õwiadczenie w budowaniu proces√≥w ETL oraz tworzeniu i implementacji data pipelines Zrozumienie zasad tworzenia data contracts i domen danych Wiedza z zakresu testowania jako≈õci danych w procesach przetwarzania danych (ETL/ELT), np. Great Expectations ‚Äì mile widziane Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z migracjƒÖ danych i integracjƒÖ danych legacy Umiejƒôtno≈õƒá pracy w miƒôdzynarodowym zespole oraz komunikacji z interesariuszami - zdolno≈õƒá zrozumienia ich potrzeb i przekszta≈Çcania ich w wymagania techniczne Mile widziane certyfikaty: Databricks Certified Data Engineer Associate Microsoft Azure Data Fundamentals (DP-900) Mile widziana wiedza domenowa z zakresu ubezpiecze≈Ñ (w szczeg√≥lno≈õci um√≥w, os√≥b, polis, roszcze≈Ñ itp.) Bardzo dobra komunikacja w jƒôzyku angielskim Oferujemy Sta≈Çe wsparcie osobistego agenta , dbajƒÖcego o TwojƒÖ ciƒÖg≈Ço≈õƒá projektowƒÖ, kontakt z klientem, niezbƒôdne formalno≈õci, komfort pracy oraz rozw√≥j, Consultant Development Program ‚Äì doradztwo w planowaniu rozwoju w oparciu o najnowsze trendy i potrzeby rynku IT, obejmujƒÖce m.in. konsultacje z agentami i mentorami rozwoju , Dostƒôp do 7N Learning & Development ‚Äì platformy rozwojowo-edukacyjnej z webinarami, bibliotekƒÖ artyku≈Ç√≥w i raport√≥w bran≈ºowych oraz regularnymi zaproszeniami na jednorazowe i cykliczne wydarzenia rozwojowe ‚Äì techniczne, biznesowe oraz life-stylowe, Spektakularne eventy integracyjne, zar√≥wno dla Ciebie (np. coroczny wyjazd Kick-Off , imprezy ≈õwiƒÖteczne czy sportowe Letnie Igrzyska), jak i dla Twoich bliskich (np. pikniki rodzinne), Rozw√≥j zawodowy nie tylko podczas projektu ‚Äì mo≈ºesz zaanga≈ºowaƒá siƒô w przekazywanie wiedzy innym w ramach oferty 7N Services kierowanej do klient√≥w 7N, Relacje i dostƒôp do wiedzy najbardziej do≈õwiadczonych ekspert√≥w IT na rynku ‚Äì ≈õredni sta≈º zawodowy naszego Konsultanta w Polsce to ponad 10 lat, Pakiet benefit√≥w zaplanowany od A do Z, czyli dofinansowanie do opieki medycznej, ubezpieczenia na ≈ºycie, karty sportowej dla Ciebie i Twoich bliskich, a tak≈ºe zni≈ºki do sklep√≥w w Polsce i za granicƒÖ. O 7N CiƒÖg≈Çe poszukiwanie projekt√≥w, trudne negocjacje stawek, brak wsparcia w rozwoju ‚Äì brzmi znajomo? W 7N zyskujesz nie tylko stabilno≈õƒá kontrakt√≥w, ale te≈º zaanga≈ºowanie osobistego opiekuna dbajƒÖcego o Tw√≥j komfort zawodowy i sta≈Çy dostƒôp do inicjatyw rozwojowych. Naszym celem jest zapewnienie Ci stabilnej i komfortowej wsp√≥≈Çpracy, kt√≥ra przyczyni siƒô do sukcesu Twojego jako eksperta IT oraz naszych klient√≥w. Budujemy trwa≈Çe relacje, bazujƒÖc na skandynawskich warto≈õciach i 30-letnim do≈õwiadczeniu w tworzeniu rozwiƒÖza≈Ñ IT dla ponad 200 organizacji.","[{""min"": 160, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,381,Data Engineer - Allegro Pay,Allegro,"About the team The salary range for this position is (contract of employment): 14 200 - 19 690 PLN in gross terms A hybrid work model requires 1 day a week in the office. Allegro Pay is the largest fintech in Central Europe ‚Äì we are growing fast and need engineers who want to learn and develop, while at the same time solving problems related to serving thousands of RPSs. If, like us, you like flexing your mental muscles to solve complex problems and you would be happy to co-create the infrastructure which underpins our solutions, make sure you apply! In this role, you will be a contributor, helping us expand our modern cloud-based analytical solutions. We embrace challenging and interesting projects and take quality very seriously. Depending on your preference, your position may be more business-oriented or platform-oriented. Your main responsibilities: - You will be actively responsible for developing and maintaining processes for handling large volumes of data. - You will be streamlining and developing the data architecture that powers analytical products and work along a team of experienced analysts. - You will be monitoring and enhancing quality and integrity of the data. - You will manage and optimize costs related to our data infrastructure and data processing on GCP. This is the right job for you if: - You have at least 3 years of experience as Data Engineer and working with large datasets. - You have experience with cloud providers (GCP preferred). - You are highly proficient in SQL. - You have strong understanding of data modeling and cloud DWH architecture. - You have experience in designing and maintaining ETL/ELT processes. - You are capable of optimizing cost and efficiency of data processing. - You are proficient in Python for working with large data sets (using PySpark or Airflow). - You use good practices (clean code, code review, CI/CD). - You have a high degree of autonomy and take responsibility for developed solutions. - You have English proficiency on at least B2 level. - You like to share knowledge with other team members. What we offer: - Big Data is not an empty slogan for us, but a reality - you will be working on really big datasets (petabytes of data). - You will have a real impact on the direction of product development and technology choices. We utilize the latest and best available technologies, as we select them according to our own needs. - Our tech stack includes: GCP, BigQuery, (Py)Spark, Airflow. - We are a close -knit team where we work well together. - You will have the opportunity to work within a team of experienced engineers and big data specialists who are eager to share their knowledge, including publicly through allegro.tech Apply to Allegro and see why it is #dobrzetubyƒá (#goodtobehere)","[{""min"": 14200, ""max"": 19690, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Remote,382,Adoption Engineer,Link Group,"We are looking for a skilled Adoption Engineer with strong hands-on experience in dbt (Core & Cloud) , DevOps , Infrastructure as Code , and Airflow . This role is critical to support the successful rollout, scaling, and enablement of modern data transformation platforms within enterprise environments. As an Adoption Engineer, you will serve as a technical bridge between platform engineering and data users, ensuring smooth adoption of tools and best practices across teams. You will be responsible for building robust, reusable infrastructure components, supporting automation, and guiding users through optimal implementation patterns. Act as a subject matter expert (SME) in dbt Core and Cloud implementation, configuration, and adoption strategy Design and build scalable and reusable infrastructure components using IaC tools (Terraform, CloudFormation, etc.) Enable and support Airflow DAG creation and scheduling for orchestration of dbt models and other workflows Collaborate with data teams to improve workflows, CI/CD pipelines, and DevOps processes Define and enforce best practices in data modeling , code versioning , and deployment automation Provide onboarding and enablement support to teams migrating to dbt Cloud Support observability, testing, and documentation standards for all data transformation processes Ensure security, scalability, and operational excellence across the data transformation stack Proven experience with dbt Core and dbt Cloud (advanced user-level, admin or platform support experience is a plus) Strong proficiency in SQL and data modeling principles Experience with DevOps tooling and CI/CD pipelines Hands-on experience with Airflow and workflow orchestration Experience with Infrastructure as Code (Terraform preferred, but others acceptable) Cloud platform familiarity (AWS, Azure or GCP) Strong communication skills, with the ability to support teams across different technical maturity levels Experience in data platforms such as Snowflake , BigQuery , Databricks , or Redshift Familiarity with monitoring & observability tools for data workflows Understanding of data governance and access control concepts Background in platform evangelism, internal consulting, or enablement is a plus","[{""min"": 85, ""max"": 105, ""type"": ""Gross per hour - Permanent""}]",Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,383,Senior Big Data Engineer (AI Enthusiast),The Stepstone Group Polska,"At The Stepstone Group, we have a simple yet very important mission: The right job for everyone. Using our data, platform, and technology, we create opportunities for job seekers and companies around the world to find a perfect match, in a fair and equitable way. With over 20 brands across 30+ countries, we strive for fair and unbiased hiring, and we are particularly focused on building a diverse, inclusive workforce. At our Tech Hub, located near Wilanowska Metro, we are over 300 passionate and driven specialists who work on the development of our IT products. We are proud to be part of The Stepstone Group, a global leader in job-tech platforms and e-recruiting. Join our team of 4,000+ employees and help us reshape the labour market. Together, we will create a future where everyone‚Äîregardless of background‚Äîhas equal opportunities in the job market. As a Senior Big Data Engineer and AI Enthusiast, you will join the Marketplace Enablement department, working inside a cross-functional team to design and implement cutting-edge, high-performance central streaming data products that power our recruitment AI applications, including deploying advanced machine learning models. Your expertise will help create a more inclusive and efficient recruitment experience globally, directly supporting our mission to reshape the labour market for everyone. You will have a strong sense of ownership and proactively provide solutions before a situation deteriorates. Your Responsibilities Lead the design and build of modern big data platforms and AI-powered tools that promote fair and inclusive hiring. Champion code quality by applying best practices like clean code, automated testing, CI/CD, observability, security, and Federated Governance. Design, implement, and optimize high-performance data pipelines and real-time applications using tools like Apache Kafka and Apache Spark. Manage and optimize complex ETL/ELT processes and data orchestration workflows. Continuously evaluate, improve, and scale our data infrastructure, advocating for new technologies and architectural patterns. Collaborate and mentor in an international, agile, cross-functional team, helping others grow and succeed. Contribute to building an inclusive team culture where every voice is valued. Work in a collaborative environment that values diverse perspectives and inclusive decision-making. Your Skills & Qualifications Excellent communication skills in spoken and written English (B2+ level). Genuine passion for leveraging data and AI to solve complex problems and make a tangible impact. Deep expertise and proven track record in Python for building large-scale data and backend applications (5+ years professional experience). Strong experience with AWS cloud services (e.g., EMR, Glue, ECS with Docker, Redshift, DynamoDB, S3) and understanding of cloud-native architectures (3+ years). Professional experience in Big Data architecture, Data Warehousing, designing complex ETL/ELT pipelines, and data pipeline orchestration (2+ years). Solid experience with Apache Kafka (MSK) for real-time data ingestion and processing (1+ years). Familiarity with Infrastructure as Code (Terraform) and Huggingface Transformers (considered a plus). We value different career paths ‚Äî if you have taken career breaks, switched disciplines, or balanced work with family life, we still encourage you to apply. If you don‚Äôt meet every single requirement but are excited about this role, we still would love to see you applying : ) We believe in supporting our team members both inside and outside the office. We strive to ensure that your job enriches your life. Here are just some of the benefits we offer: Premium medical and dental care Life insurance Flex Benefits - Worksmile Cafeteria System (Multisport, vouchers, tickets, etc.) Employee Referral Program Hackathons, Knowledge Sharing Hours, In-house projects Tech and sport communities Events and integration parties Charity initiatives, including 2 extra volunteer days English/German language classes A supportive and inclusive workplace that promotes professional and personal growth for everyone Flexible working hours and hybrid work model to support work‚Äìlife balance. Our commitment At The Stepstone Group, diversity and inclusion are key to our success. We encourage applications from everyone, regardless of gender identity, sexual orientation, disability status, ethnicity, belief, age, family or parental status, or any other characteristic. We are particularly passionate about attracting more women into tech and leadership roles and provide an environment that supports your growth. We offer mentoring and support networks to help women and underrepresented colleagues grow into senior and leadership roles.","[{""min"": 16000, ""max"": 23000, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,384,Senior Cloud Data Engineer (Azure and Databricks),Future Processing,"Do naszej linii biznesowej Data Solutions poszukujemy osoby na stanowisko Senior Cloud Data Engineer ze znajomo≈õciƒÖ Azure. Szukamy Ciebie, je≈õli: masz min. 5 lat do≈õwiadczenia w IT, w tym min. 3,5 roku w pracy z danymi w chmurze Azure (potwierdzone projektami komercyjnymi wdro≈ºonymi na produkcje), masz komercyjne do≈õwiadczenie w przetwarzaniu sporych danych przy u≈ºyciu Databricks, korzystasz z SQL na poziomie zaawansowanym i wykorzystujesz go na rozwiƒÖzaniach technologicznych MS i nie tylko, znasz metodyki i stosujesz biegle Git oraz CI/CD , masz do≈õwiadczenie w pracy z platformƒÖ Microsoft Fabric , tworzysz i optymalizujesz rozwiƒÖzania przetwarzajƒÖce dane (ETL, ELT, itp.) poprzedzone projektem technicznym oraz alternatywami rozwiƒÖza≈Ñ, monitoring, diagnostyka oraz rozwiƒÖzywanie problem√≥w w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanowaƒá infrastrukturƒô oraz obliczyƒá jej koszt, koncepcje Delta Lake i Data Lakehouse sƒÖ Ci znane, znasz architekturƒô SMP oraz MPP wraz z przyk≈Çadami rozwiƒÖza≈Ñ opartych o te architektury, masz wiedzƒô na temat migracji rozwiƒÖza≈Ñ on-premise do chmury oraz znasz podstawowe typy migracji, masz wiedzƒô na temat stosowania mechanizm√≥w zwiƒÖzanych z bezpiecznym przechowywaniem i przetwarzaniem danych w chmurze, bardzo dobrze znasz us≈Çugi zwiƒÖzanie z przechowywaniem i przetwarzaniem danych, oferowanych przez dostawcƒô chmury Azure, masz do≈õwiadczenie w bezpo≈õredniej wsp√≥≈Çpracy z klientem, pos≈Çugujesz siƒô j. angielskim na poziomie ≈õredniozaawansowanym (min. B2). Praca na tym stanowisku w naszej firmie oznacza: odpowiedzialno≈õƒá za ca≈Ço≈õƒá rozwiƒÖza≈Ñ wsp√≥≈Çtworzonych wraz z zespo≈Çem, tworzenie lub modyfikowanie rozwiƒÖza≈Ñ do przetwarzania danych w chmurze, tworzenie i modyfikowanie dokumentacji, analizowanie i optymalizowanie rozwiƒÖza≈Ñ w zakresie dzia≈ÇajƒÖcego lub projektowanego systemu, analizowanie wymaga≈Ñ klienta pod kƒÖtem dostarczenia optymalnego rozwiƒÖzania jego potrzeby biznesowej, analizowanie potencjalnych zagro≈ºe≈Ñ, dostosowywanie rozwiƒÖza≈Ñ wzglƒôdem wymaga≈Ñ biznesowych.","[{""min"": 135, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,385,üëâ Azure Data Governance Specialist,Xebia sp. z o.o.,"üü£ You will be: Define, data classification, quality, ownership models, Implement governance policies using Microsoft Purview, Create golden record processed with MDM, Work closely with stakeholders to understand and recommend approaches for data governance implementation of all required data sources, Proven experience in implementing end-to-end data governance using Microsoft Purview, Define metadata, business glossary, data stewards, owner etc., Set up automated data quality checks, Manage cataloging and discoverability of internal and external data fields, Identify and implement critical data elements, Develop automated data classification and labeling mechanisms using MS Purview, Knowledge of data governance lifecycle and frameworks such as DAMA-DMBOK, Exposure to regulatory frameworks like GDPR, ISO 8000; üü£ Your profile: Proven experience implementing end-to-end data governance, including cataloging, classification, lineage, business glossary, and data stewardship setup, Skilled in defining metadata standards, setting up rule-based and automated data quality checks, and applying sensitivity labels using Microsoft Purview and Azure-native tools, Hands-on experience with golden record creation, entity resolution, and master data lifecycle using tools like Azure Data Factory, Profisee, or Microsoft MDS, Solid understanding of data governance frameworks (e.g., DAMA-DMBOK) and regulatory standards such as GDPR and ISO 8000, Comfortable working with Azure Data Factory, Azure Synapse, Azure SQL, and Azure Data Lake; familiarity with Power BI for lineage and reporting, Able to collaborate with business and technical stakeholders, define ownership models, and influence adoption of governance policies across teams; üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,386,Data Engineer,Square One,"The Clinical Stage Cell Therapy Process & Analytical Development product provides capabilities required to support the analytical development of CGT clinical stage, including experimentation, Clinical assessment, PLM, Material Science. We are looking for 2 resources with a combination of Data Engineering/Visualization and Data Analysis with extensive hands-on experience with designing and implementing solutions to manage data, setting up data pipelines and building data solutions to establish data products in our Cell and Gene Therapy space. Requirements: 4+ years of experience The candidate needs to have extensive experience with building complex solutions using cloud technologies ( AWS, Snowflake ) and workflow orchestration and data management tools. Experience with data visualization tools such as Tableau or Thoughtspot is a must. The candidate needs to have practical and proven experience with setting up CI/CD pipelines , workflow automation and developing data pipelines using Python, Airflow, dbt. The candidate needs to be able to converse pro-actively with customers to understand their needs and collect requirements.","[{""min"": 150, ""max"": 165, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,387,Data Engineer,Link Group,"We are currently looking for an experienced Data Engineer to join a project focused on building modern data solutions using Snowflake and cloud-based data architecture approaches. Your responsibilities: Building and developing scalable analytics solutions based on Snowflake. Working with DBT and implementing data models using the Data Vault approach. Designing data architectures aligned with modern concepts such as Data Mesh. Close collaboration with technical and business teams to optimize data processing workflows. Requirements: Minimum of 4 years of experience in data engineering. Hands-on experience with Snowflake and Data Warehousing . Experience with DBT , Data Vault , and Data Mesh is a strong plus. Proficiency in both English and Polish at C1 level. Project tech stack: Snowflake, DBT, Data Vault, Data Mesh, Data Warehousing","[{""min"": 115, ""max"": 135, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Any,Remote,388,Senior Business Intelligence Specialist,Billennium,"More about us Billennium is a global technology company with over 20 years of experience, committed to innovation and empowering businesses. As an employer, we offer a supportive, growth-focused environment where collaboration and creativity thrive. Join us to shape the future of technology together! About the Client The project is delivered for a global leader in the healthcare and life sciences industry. The organization is committed to transforming patient care through data-driven innovation and cutting-edge technology solutions. You‚Äôll be part of a long-term digital transformation program with significant impact on business intelligence and analytics capabilities. What you will do: ‚Ä¢ Design and prototype BI solutions based on business needs and available tools ‚Ä¢ Develop and maintain advanced reports using Tableau, SAP BO, or other BI tools ‚Ä¢ Collaborate with stakeholders to define requirements and propose optimal reporting strategies ‚Ä¢ Estimate work and provide input on architecture and testing strategies ‚Ä¢ Support and defend BI solution architecture in alignment with infrastructure standards What we are looking for: ‚Ä¢ 5+ years of experience with Tableau, SAP BO, Tibco Spotfire, or SSRS ‚Ä¢ Strong knowledge of data modeling, DWH concepts, and SQL ‚Ä¢ Experience with ETL tools and BI usability design ‚Ä¢ Fluent English and excellent communication skills Perks and benefits: ‚Ä¢ Comprehensive benefits - enjoy Udemy for Business, private medical care, Multisport card, veterinary package, language lessons, and shopping vouchers ‚Ä¢ Flexibility - adaptable working hours and remote/hybrid work options to suit your lifestyle & location ‚Ä¢ Career growth - access opportunities for professional development and learning, including perks related to our official partnerships with global IT giants: Microsoft, AWS, Snowflake, Salesforce & more ‚Ä¢ Global collaboration - work with a diverse, international team ‚Ä¢ Innovative environment - be part of a forward-thinking and growth-oriented workplace ‚Ä¢ Engaging community ‚Äì work with passionate professionals and participate in team-building events, hackathons, and CSR initiatives to make an impact beyond work ‚Ä¢ Team-building events including our company tradition (annual company event in Mazury) ‚Ä¢ A pleasant surprise to start your journey with us in the form of a welcome pack Recruitment process: ‚Ä¢ HR call ‚Ä¢ Technical Interview ‚Ä¢ Interview with the dedicated Client ‚Ä¢ Final decision / Feedback Sounds interesting? Click ""Apply"" and have a chance to hear more!",[],Data Analysis & BI,Data Analysis & BI
Full-time,Manager / C-level,Permanent,Hybrid,389,Big Data Staff Engineer,Relativity,"Job Overview At Relativity, we make software to help users organize data, discover the truth, and act on it. Our e-discovery platform is used by more than 13,000 organizations around the world to manage large volumes of data and quickly identify key issues during litigation, internal investigations, and compliance projects. We are seeking a Staff Data Engineer to join our Relativity Data Services organization, a team dedicated to developing data and AI infrastructure that powers AI-driven applications in support of our mission to drive pursuit of justice. Relativity‚Äôs scale and breadth provide significant opportunities for rich data exploration and insights. Our market position and advanced products ensure that our latest models and insights can quickly benefit our users. Great insights stem from excellent data, and the best insights arise from substantial data. Our data infrastructure and engineering guarantee that Relativity's vast data is accessible for insights, confidential data remains secure, and data protection is always upheld. We are making substantial investments in data pipeline and data lake technology for the future. In this role, you will partner with teams across the Data Services organization to scale and optimize our data platforms, advance tooling for large-scale distributed processing, and enable critical use cases such as reporting, analytics, and audit. Additionally, you will lead efforts to redefine and strengthen our approach to audit and behavioral analytics. Role overview: Staff Engineer serves as a technical liaison between his or her teams and other internal and external development teams to identify and resolve dependencies, to identify, improve, and apply software engineering best practices and processes, and to identify and mitigate risks to the on-time delivery of software. Staff Engineer ‚Äì thinks what to buy or what to build, designs the architecture to serve the user needs and support the system scale. Understands the trade-offs to be made and that there are no silver bullets. But ultimately builds systems that work, deliver value in time and are predictable to operate and extend. Staff Engineer serves as a mentor to other team members to improve technical and process expertise and promotes collaboration. Job Description and Requirements Responsibilities Lead design of software using abstraction, low coupling and high cohesion, modularization, encapsulation and information hiding, and separation of concerns. Lead implementation of software using practical application of algorithms, defensive programming and exception handling, fault tolerance, design patterns. Be pragmatic ‚Äì in using object-oriented principles, applying SOLID principles and design patterns in a variety of languages. Build systems that are low maintenance but not overengineered ‚Äì balancing security, observability and extensibility with time-to-market and user value. Specify non-functional software requirements and analyze all requirements to determine design feasibility within time and cost constraints. Test and lead test of software emphasizing the practice of Test-Driven Design and the use of autonomous frameworks and Continuous Integration. Identify and offer solutions to reduce technical debt. Display an ownership mindset; be accountable for and beyond the features your team and larger organization develops. Provide solutions to varied and ambiguous issues, utilizing judgment to select methods and techniques for obtaining solutions. Offer coaching to ensure the team stays focused and delivers against the goals, adapting to changing business requirements. Advocate for and ensure adherence to best practices in coding standards, quality assurance, and security, while aligning solution with company-wide architectural principles. Your Skills 10+ years of professional software development experience on commercial-grade systems and applications with a proven track record of building and shipping successful software. 6+ years of hands-on experience with large-scale data infrastructure and cloud-native distributed systems. Proven proficiency in multiple programming languages, with a strong aptitude for rapidly learning and adapting to new technologies. Experience with at least two of the following is required: Java, Python, Scala, Rust and C#. Experience building and optimizing data pipelines using Apache Spark for large-scale batch workloads. Familiarity with deploying and managing data workloads on Kubernetes, including containerization and orchestration best practices. Extensive knowledge of and adherence to SDLC (Software Development Life Cycle) standards and best practices. Ability to consistently identify and deliver technical improvement feedback to team members in a supportive and constructive manner, to achieve demonstrable results over time. Excellent problem solving-solving skills, with a clear ability to present trade-offs, make informed decisions, and drive strategic execution. Excellent verbal and written communication to clearly, succinctly, and completely communicate intent (both technical and non-technical) in interactions with team members and management. Nice to have: Experience working with Data Lake and Lakehouse architectures on cloud storage platforms like ADLS. Nice to have: Hands-on experience or practical understanding of machine learning systems and their integration into production environments. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law. #LI-MM5 Relativity is committed to competitive, fair, and equitable compensation practices. This position is eligible for total compensation which includes a competitive base salary, an annual performance bonus, and long-term incentives. The expected salary range for this role is between following values: 300 000 and 450 000PLNThe final offered salary will be based on several factors, including but not limited to the candidate's depth of experience, skill set, qualifications, and internal pay equity. Hiring at the top end of the range would not be typical, to allow for future meaningful salary growth in this position.","[{""min"": 300000, ""max"": 450000, ""type"": ""Gross per year - Permanent""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,390,Solution Architect,Lingaro,"Tasks: Leading delivery of Data Integration and Analytics solutions for our customers provided within Microsoft Azure Cloud Platform. Preparing a complete solution design. Taking responsibility for critical technological decisions. Providing continuous architectural and technical design support to the application development team. Leading application development at all lifecycle stages ‚Äì POC, MVP, full-scale PROD solutions. Providing mid- and long-term application development/growth strategy. Proactively identifying and proposing solutions to mitigate project risks. Directly facing our customers for ongoing project and presales matters. Cooperating with internal and client teams in international setup. Defining development standards and frameworks. Driving technology innovation, continuously improving technical knowledge, staying up-to-date with latest trends. Participating in projects staffing and planning (as a consulting Architect). Supporting pre-sales initiatives from a technological standpoint. Contributing to internal tech communities. At least 10 years of overall experience in IT commercial projects, including at least 4 years in the role of a Solution Architect. Expertise in the area of Azure Cloud, especially related to Data Engineering / Data Analytics solutions. Deep understanding and hands-on experience with Azure data engineering resources, including preferably: Databricks, Synapse, Azure Data Factory, Storage Account. Expertise in Data Integration/Engineering and Analytics concepts, including knowledge of current design and technological trends. CI/CD setup and resource provisioning (preferably incl. Terraform) skills. Experience in customer facing roles (incl. presales activities) and working in international team setup. Good organizational and priority management skills. Leadership and team player mindset. Higher degree in Computer Science, Engineering or a related field. Excellent English communication skills (written and spoken). Microsoft Fabric. Data governance principles. Azure services not related strictly to data engineering, especially: Machine Learning, GenAI, web application/services development. Technical certifications (espcially Microsoft Azure related). Project or team management experience. Missing one or two of these qualifications? We still want to hear from you! If you bring a positive mindset, we'll provide an environment where you feel valued and empowered to learn and grow. Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,391,Senior Database Administrator,Link Group,"Job Summary: We are seeking a skilled and experienced Senior Database Administrator (DBA) to join our IT team. In this role, you will be responsible for the installation, configuration, maintenance, and optimization of our database systems. You will collaborate closely with development teams to design and fine-tune database structures, support users, and ensure data security and integrity. Install, configure, and maintain database management systems (DBMS) to ensure high availability and optimal performance. Monitor database performance and proactively implement tuning and optimization measures. Ensure data integrity and security through robust access controls and best practices. Perform regular backups and manage recovery strategies to protect business-critical data. Collaborate with software developers to design, implement, and optimize database schemas and queries. Diagnose and resolve database issues, providing expert-level technical support to end-users and internal teams. Maintain accurate and up-to-date documentation for database architecture, configurations, and procedures. Stay current with new database technologies, tools, and industry trends to recommend and implement improvements. Bachelor‚Äôs degree in Computer Science, Information Technology, or a related field. Proven experience as a Database Administrator or similar role, ideally at a senior level. Strong proficiency with major database systems such as Oracle, SQL Server, or MySQL. Solid understanding of database design principles, data modeling, and performance tuning. Knowledge of backup, restore, and disaster recovery procedures. Excellent problem-solving skills and keen attention to detail. Strong communication and collaboration skills to work effectively within cross-functional teams. Experience with cloud-based database platforms (e.g., AWS, Azure). Familiarity with data warehousing concepts and ETL processes. Understanding of database security best practices and compliance requirements.","[{""min"": 100, ""max"": 135, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,392,MOCNY GCP Data Engineer,Link Group,"Poszukujemy do≈õwiadczonego GCP Data Engineera do projektu realizowanego dla globalnej firmy konsultingowej w obszarze data & analytics. Praca dotyczy budowy i optymalizacji zaawansowanych pipeline‚Äô√≥w danych w ≈õrodowisku Google Cloud Platform (BigQuery, Airflow, DBT). Kluczowe bƒôdƒÖ umiejƒôtno≈õci projektowania architektury danych, integracji danych z r√≥≈ºnych ≈∫r√≥de≈Ç oraz optymalizacji wydajno≈õci system√≥w. Wymagania: Minimum 4 lata do≈õwiadczenia jako Data Engineer, w tym 3 lata z GCP Bardzo dobra znajomo≈õƒá BigQuery, Python, SQL Do≈õwiadczenie z Airflow, DBT/Dataform Znajomo≈õƒá zasad modelowania danych, optymalizacji zapyta≈Ñ Angielski B2/C1 Mile widziane: Certyfikaty GCP, do≈õwiadczenie z BI (Power BI, Tableau), znajomo≈õƒá Azure Informacje organizacyjne: üìç 100% zdalnie üóì Start: sierpie≈Ñ üíº Proces: HR + techniczne + opcjonalne spotkanie z klientem","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,393,"Senior Data Software Engineer (Python, GCP)",EPAM Systems,"We are looking for a Senior Data Software Engineer to join the project in the telecommunications sector, focused on creating network infrastructure for high-speed internet access using fiber optic communication. In this role, you will work with an American multinational technology company. The project offers many opportunities to learn as our client is considered one of the Big Five companies in the American information technology industry. This position offers a hybrid model, with 3 days per week working from the client‚Äôs office for candidates from Gdansk, Wroclaw, Warsaw, and Krakow. Responsibilities Provide technical leadership and oversight for the other developers working on the project Design and build ETL/ELT pipelines using Airflow and other technologies on the GCP Provide advice and recommendations (in writing when appropriate) to the Data Warehouse Technical Lead regarding technical options and best practices for the data warehouse and related ETL/ELT pipelines Analyze source data and work with internal data consumers to determine which data is needed and how it should be represented in the output table schemas Write and maintain related technical documentation Perform thorough design and code reviews for other developers Requirements 4+ years of relevant professional experience Solid experience in SQL scripting and Python programming At least 1 year of ETL/ELT to BigQuery experience Experience in developing solutions for the GCP Proficiency with ETL/ETL for data warehousing including data source investigation/analysis, target schema design and data pipeline design/implementation Ability to write clear, concise, and well-reasoned technical explanations and documentation for both engineer and analyst internal audiences (design docs, architecture views/diagrams, etc.) Solid interpersonal skills for working with both upstream teams providing data and downstream analysts consuming data B2+ English level proficiency Nice to have Experience with Apache Airflow Familiarity with tools like IntelliJ, Gradle, Google Cloud Storage, Google Cloud Datastream, Google Cloud Data Catalog, Google Looker, Google Cloud Cortex Google Cloud Platform (GCP) certification We offer We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Unclassified,Unclassified
Full-time,Mid,B2B,Remote,394,Data Scientist/ML Developer,B2Bnetwork,"ObowiƒÖzki: Projektowanie, budowa i wdra≈ºanie zaawansowanych modeli uczenia maszynowego (ML), dostosowanych do potrzeb biznesowych i produkcyjnych. Programowanie i rozw√≥j rozwiƒÖza≈Ñ ML w jƒôzyku Python, z wykorzystaniem bibliotek takich jak scikit-learn, MLLib, XGBoost oraz PySpark. Przetwarzanie i analiza du≈ºych zbior√≥w danych z u≈ºyciem narzƒôdzi takich jak Apache Spark i Hadoop. Tworzenie i zarzƒÖdzanie procesami orkiestracji zada≈Ñ ETL/ML za pomocƒÖ Apache Airflow, w tym definiowanie i optymalizacja DAG√≥w. Monitorowanie, ocena jako≈õci i tuning modeli ML, w tym wykorzystanie MLFlow do zarzƒÖdzania cyklem ≈ºycia modeli. Wdra≈ºanie rozwiƒÖza≈Ñ ML do ≈õrodowisk produkcyjnych, zapewniajƒÖc stabilno≈õƒá i skalowalno≈õƒá. Wsp√≥≈Çpraca z zespo≈Çami projektowymi oraz interesariuszami, planowanie prac, definiowanie wymaga≈Ñ oraz monitorowanie realizacji cel√≥w. Analiza i implementacja rozwiƒÖza≈Ñ w obszarze ochrony zdrowia, ze szczeg√≥lnym uwzglƒôdnieniem specyfiki proces√≥w biznesowych i rozlicze≈Ñ ≈õwiadcze≈Ñ medycznych. Dba≈Ço≈õƒá o jako≈õƒá pracy, efektywnƒÖ komunikacjƒô, kreatywno≈õƒá oraz samodzielno≈õƒá w dzia≈Çaniu, z zachowaniem wysokiej kultury osobistej i odporno≈õci na stres. Sta≈Çy rozw√≥j kompetencji technicznych i metodycznych, aktywne uczenie siƒô i adaptacja do zmieniajƒÖcych siƒô technologii oraz wymaga≈Ñ biznesowych. Wymagania: Do≈õwiadczenie zawodowe na stanowisku Programisty Machine Learning lub na stanowisku Data Scientist, minimum 3 lata Do≈õwiadczenie projektowe w zaawansowanym modelowaniu opartym o ML, minimum 1 projekt Do≈õwiadczenie projektowe w programowaniu w jƒôzyku Python, minimum 1 projekt Do≈õwiadczenie projektowe w przetwarzaniu i analizie du≈ºych zbior√≥w danych, minimum 1 projekt Do≈õwiadczenie we wdra≈ºaniu rozwiƒÖza≈Ñ opartych o ML na ≈õrodowisko produkcyjne, minimum 1 projekt Znajomo≈õƒá bibliotek uczenia maszynowego (scikit-learn, MLLib, XGBoost) Znajomo≈õƒá PySpark Znajomo≈õƒá Airflow Znajomo≈õƒá MLFlow",[],Data Science,Data Science
Full-time,Senior,Permanent,Hybrid,395,Senior Data Analyst - Customer Data,Bayer Sp. z o.o.,"Senior Data Analyst Customer Data Are you ready to make a significant impact in the world of data analytics and data management? We are seeking a talented Data Analyst to become a vital part of our dynamic Data Assets, Analytics, and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in building and maintaining our core data assets across various domains, ensuring their completeness, semantics and quality. You will work closely with data owners, product managers, data engineers, data architects, data stewards, data governors and data scientist to enable our data analytics solutions, enhance the strategic value of our data assets and enable cutting-edge AI solutions and support data-driven decision-making. If you‚Äôre passionate about transforming data into actionable insights and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Collaborate with data owners, architects, engineers, stewards, governors, scientists and product managers to understand objectives and requirements for data assets. Serve as a liaison between technical teams and business stakeholders to ensure data assets meet both business requirements and technical standards. Take ownership of a data asset roadmap. Co-develop data strategy and governance frameworks. Proactively identify new datasets from across the organization and collaborate with data architects and data engineers to integrate them into the core data assets Define transformation logic and enrich data to create valuable KPIs and features in the consumption layer. Develop and maintain clear semantics and metadata for data assets, ensuring that all data is well-documented and easily understandable. Ensure data quality, availability, and completeness through implementation of quality checks, validation processes, and continuous monitoring in collaboration with data architects and data engineers. Serve as the primary point of contact for users of data assets, providing guidance and support to help them understand and utilize the data effectively. Analyze complex datasets to extract actionable insights that streamline the development of analytics and AI solutions. Become a go-to expert for customer data. Qualifications & Competencies: Master's degree in Statistics, Computer Science, Data Management, Data Science or a related field. 5+ years of experience as a Data Analyst or Data Steward, preferably within the consumer-packaged goods, FMCG, pharmaceutical or healthcare industry. Strong knowledge of data management principles, data quality frameworks, and metadata management practices and tools. Understanding of data lineage and data cataloging concepts. Business acumen in the area of customer & sales analysis. Familiarity with CRM systems. Experience with data manipulation and analysis using Azure Databricks, SQL and Python. Familiarity with relational databases (PostgreSQL, MSSQL) and data modelling. Excellent analytical and problem-solving skills with a keen attention to detail. Strong understanding about data compliance & security standards such as data privacy regulations (GPDR, HIPAA), EU AI Act, management of confidential data, and experience with measures to mitigate data risks. Strong communication skills, with the ability to present complex data in a clear and understandable manner. Interest and experience with AI tools supporting data analysis and stewardship is a plus. Experience with preparing data for AI solutions (e.g. traditional machine learning models, AI Agents) is a plus. Experience in IT product management is a plus. Ability to work collaboratively in a team-oriented environment. Fluent in English, both written and spoken. What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent or B2B,Hybrid,396,Data Privacy and Compliance Specialist,OChK,"Data Privacy and Compliance Specialist Miejsce pracy: Warszawa / hybrydowo Poziom stanowiska: Intermediate 10.000 - 13.000 brutto UoP lub umowa B2B Tw√≥j zakres obowiƒÖzk√≥w: udzia≈Ç w negocjacjach i opiniowaniu um√≥w z klientami oraz dostawcami, w zakresie zapis√≥w dotyczƒÖcych ochrony danych i bezpiecze≈Ñstwa informacji, wsparcie w ocenie i monitorowaniu bezpiecze≈Ñstwa ≈Ça≈Ñcucha dostaw, ze szczeg√≥lnym uwzglƒôdnieniem aspekt√≥w zwiƒÖzanych z przetwarzaniem danych osobowych, udzia≈Ç w kontrolach zgodno≈õci, audytach wewnƒôtrznych i zewnƒôtrznych oraz wdra≈ºanie dzia≈Ça≈Ñ korygujƒÖcych i zapobiegawczych, przygotowywanie, aktualizacja i rozw√≥j dokumentacji z zakresu bezpiecze≈Ñstwa informacji (m.in. polityki, procedury, klauzule, rejestry), wsparcie zespo≈Ç√≥w biznesowych w analizie ryzyka oraz ocenie i zapewnianiu zgodno≈õci planowanych dzia≈Ça≈Ñ z obowiƒÖzujƒÖcymi regulacjami, weryfikacja zgodno≈õci przetwarzania danych osobowych w procesach biznesowych i systemach IT, prowadzenie szkole≈Ñ i dzia≈Ça≈Ñ edukacyjnych dla pracownik√≥w w obszarze bezpiecze≈Ñstwa informacji, w tym ochrony danych, monitorowanie zmian w przepisach prawa i regulacjach oraz inicjowanie i wdra≈ºanie niezbƒôdnych dzia≈Ça≈Ñ dostosowawczych. Nasze wymagania: wykszta≈Çcenie wy≈ºsze (preferowane: prawnicze, cyberbezpiecze≈Ñstwo), doskona≈Ça znajomo≈õƒá przepis√≥w oraz norm i standard√≥w dotyczƒÖcych bezpiecze≈Ñstwa informacji oraz ochrony danych osobowych, z uwzglƒôdnieniem zagadnie≈Ñ dotyczƒÖcych chmury obliczeniowej oraz sztucznej inteligencji, znajomo≈õƒá jƒôzyka angielskiego na poziomie minimum B2, co najmniej 3-letnie do≈õwiadczenie na stanowisku zwiƒÖzanym z bezpiecze≈Ñstwem informacji/ochronƒÖ danych osobowych, certyfikaty po≈õwiadczajƒÖce wiedzƒô z zakresu bezpiecze≈Ñstwa informacji oraz ochrony danych osobowych ‚Äì mile widziane. W OChK: pracujemy zadaniowo w trybie hybrydowym (nowoczesne biuro przy ul. Grzybowskiej), dzia≈Çamy w zwinnym ≈õrodowisku, z wykorzystaniem aplikacji zwiƒôkszajƒÖcych efektywno≈õƒá (m. in. Google Workspace, Slack, GitHub, Jira), inwestujemy w Tw√≥j rozw√≥j poprzez finansowanie szkole≈Ñ i cert√≥w, a od pierwszego dnia pracy udostƒôpniamy platformy edukacyjne Google i Microsoft, oferujemy prywatne ubezpieczenie medyczne, preferencyjne warunki ubezpieczenia grupowego oraz kartƒô Multisport organizujemy i wsp√≥≈Çfinansujemy naukƒô jƒôzyka angielskiego, udostƒôpniamy program polece≈Ñ, dziƒôki kt√≥remu pracownicy zyskujƒÖ dodatkowe bonusy za skutecznƒÖ rekomendacjƒô kandydat√≥w do pracy, cenimy proaktywno≈õƒá i inicjatywƒô w≈ÇasnƒÖ, dlatego wspieramy autonomiƒô w podejmowaniu decyzji, budujemy kulturƒô organizacyjnƒÖ na warto≈õciach takich jak profesjonalizm, wsp√≥≈Çodpowiedzialno≈õƒá i wzajemny szacunek, przyk≈Çadamy du≈ºƒÖ wagƒô do efektywnego onboardingu, podczas kt√≥rego w lu≈∫nej atmosferze i ze wsparciem Twojego CloudBuddiego poznajesz zesp√≥≈Ç, firmƒô i swoje obowiƒÖzki, stawiamy na integracjƒô zespo≈Ç√≥w podczas r√≥≈ºnorodnych inicjatyw, zar√≥wno firmowych jak i oddolnych, kt√≥re pomagajƒÖ nam lepiej siƒô poznawaƒá oraz budowaƒá i utrzymywaƒá dobrƒÖ atmosferƒô wsp√≥≈Çpracy.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 13000, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,397,Data Engineer,Ework Group,"üíª Ework Group - founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking for Data Engineer üîπ ‚úîÔ∏è VCE team is looking for a skilled Data Engineer. We process data coming from machines and factories in order to provide customized data products to our customers around the world. Our mission is to expose value of the data and support clients to become fully operational data driven company. At our company, data practitioners have the opportunity to work with various types of data coming through diverse channels with different frequency. Preferred hybrid work from the Wroc≈Çaw office, but candidates from outside Wroc≈Çaw will also be considered. ‚úîÔ∏è Competences and skills : Informatica Power Center Azure Databricks (Pyspark, Spark SQL, Unity Catalog, Jobs/Workflows) Advanced SQL Practical experience with at least one relational DBMS (SQL Server / Oracle / PostgreSQL) Azure DevOps (Repos, Pipelines, YAML) Azure Key Vault Azure Data Factory (optional) DBT (optional) ‚úîÔ∏è Soft skills: Open-minded Engaged and flexible Driver of topics - ready to find his/her way to develop or proceed with topics Good at collaboration with stakeholders from IT and business side Working in the past in the data mash environment (as a bonus) ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 120, ""max"": 128, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,398,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for a Data Engineer who can help us to provide solutions for customers for making informed decisions in volatile short-term markets. You will design, implement, and maintain data pipelines and storages, become a data manager of our model inputs, and create insightful and powerful analysis and visualization for day-ahead, intraday and balancing data. In our day-to-day work, we include pair programming, joint learning sessions, and recurring hacking days to explore new ideas. We have very much an agile and digital way of working with fast feedback loops and embracing a culture of learning and personal growth. What you will be doing to make a difference? Thrive in an empowered, self-driven team where you take ownership across the entire data lifecycle: Work together with in-house analysts to understand the domain and the data in question. Design, build and maintain flexible and scalable end-to-end data pipelines together with software engineers. Monitor quality and reliability of data and implement required tooling in coordination with data scientists. Visualize data in a meaningful way for in-house analysis and together with product manager and UX designer, for customer facing dashboards. What do you need to succeed in the role? A Bachelor‚Äôs or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. A good understanding of database systems. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Why will you love being part of our team? Supportive Onboarding : Begin your journey with a thorough introduction and a steep learning curve. Room to Grow : Shape and develop your role with a large degree of influence. Mission-Driven Culture : Join one of Europe‚Äôs most exciting green tech companies and contribute to building a more sustainable future. Inclusive Environment : Work in an innovative, international, and supportive atmosphere. Competitive Benefits : Enjoy salaries that reflect your professional experience, flexible working hours, and a hybrid work model that fits your lifestyle. Team Spirit : Collaborate with talented, inspiring colleagues who believe in succeeding together. Attractive Perks : Benefit from our referral program and other employee-focused initiatives. We are looking to hire for Volue office in Gda≈Ñsk but will be ready to consider other locations for the right candidate. In Volue, we cherish each employee‚Äôs competence, ideas and personality. Let your skills and talent be a part of our team ‚Äì and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,399,IPC Developer,ITDS,"Drive Innovation in Data Warehousing: IPC Developer wanted! ≈Å√≥d≈∫ based opportunity with remote work model (2 days in the office/month). As an IPC Developer , you will be working for our client, a leading player in the online banking sector, on the development and optimization of data warehouse solutions. This includes designing and implementing ETL processes, enhancing data architecture, and supporting the business intelligence environment to ensure effective data reporting and analysis. The project involves using cutting-edge technologies like Informatica Power Center and Oracle-based systems to support complex financial systems. You‚Äôll be part of a dynamic IT team that ensures data integrity, performance, and scalability of large-scale data systems. Your main responsibilities: Design and implement ETL processes using Informatica Power Center Develop and maintain data warehouses and reporting data marts Participate in the implementation and integration of Informatica tools Design, implement and develop BI-class analytical environments Create and maintain Oracle databases and applications Define and document technical specifications and administrative documentation Test and validate software solutions created by others Prepare software installation packages Support the software release and handover to production teams You're ideal for this role if you have: Strong knowledge of Informatica Power Center for ETL process development Solid understanding of RDBMS Oracle 9i/10g and database design Excellent command of SQL and good knowledge of PL/SQL Understanding of information systems engineering and development methodologies At least 6 months of experience designing and implementing ETL solutions At least 1 year of experience with Oracle-based systems in information environments Practical experience designing data warehouses for large-scale institutions Ability to read and write technical documentation in English Strong analytical thinking and problem-solving skills Team-oriented mindset with attention to quality and detail Nice to have: Theoretical knowledge and practical experience with Big Data, Python, and Spark Familiarity with Business Intelligence (BI) concepts Ability to work using Agile methodologies (Scrum, Kanban) Knowledge of tools such as SQL Developer, SVN, GitHub, JIRA, and Confluence We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here . Ref. number 7154","[{""min"": 16800, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,400,Senior Data Engineer,SIX,"Senior Data Engineer Warsaw | working from home up to 40% | Reference 7218 Are you passionate about the stock market and cutting-edge technology? At SIX, we operate one of the most advanced, innovative, and stable stock exchanges in the world ‚Äî and we‚Äôre looking for a Senior Data Engineer to join our Data Management team. If you have a strong background in Python, SQL, databases, and cloud-based analytics, this is your opportunity to design and deliver high-impact data products and pipelines that power critical financial services. design, develop, and maintain our cloud-based data platform (Azure) for the Swiss Stock Exchange, aligned with our architecture standards lead the migration of legacy data solutions to modern cloud-based infrastructure conceptualize and implement transformation projects in the DWH and Big Data ecosystems collaborate with and support team members in building data-centric solutions, including direct involvement in solution design work cross-functionally with developers, testers, product owners, and business stakeholders in an agile setup (SCRUM & SAFe) 4+ years of experience building data pipelines on public cloud platforms such as Microsoft Azure, or with Hadoop (e.g., Hortonworks, Cloudera) proficiency in relational databases and SQL/NoSQL datastores; hands-on experience with data modeling, Spark, and preferably Databricks familiarity with agile methodologies (SCRUM, Kanban) and collaboration tools like Jira and Confluence a proactive, open mindset with a strong drive to learn and adapt to new technologies; excellent teamwork and communication skills strong customer focus with the ability to work independently and collaborate across diverse teams and cultures; fluency in English (German is a plus) sharing the costs of sports activities private medical care & life insurance sharing the costs of foreign language classes sharing the costs of professional training & courses remote work opportunities &flexible working time integration events &charity initiatives fruits and popcorn in the office video games at work, no dress code & leisure zone extra social benefits & holiday funds (Christmas/Easter gifts) meal and transportation allowance employee referral program Employee Assistance Program Day for U (Day for Medical Checkup) My Benefit Cafeteria Udemy for Business days for remote work from abroad If you have any questions, please call Gabriela Swiatek at +48 22 104 67 70. For this vacancy we only accept direct applications in English . Diversity is important to us. Therefore, we are looking to receiving applications regardless of any personal background.","[{""min"": 19000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,401,Senior AI & Data Scientist (m/f/d),PAYBACK,"You develop and implement innovative data and AI products using classical machine learning methods and generative AI (GenAI) You are responsible for the entire lifecycle of ML/GenAI models on the Google Cloud Platform (GCP), from conception and deployment to monitoring and optimization As a subject matter expert, you are responsible for the technical leadership of an interdisciplinary team and establish best practices in the areas of data science, machine learning and MLOps You work closely with business stakeholders to identify and evaluate potential for AI products and implement concrete solutions You ensure the smooth operation of our AI systems, optimize existing applications and always stay up to date with the latest technology You have several years of professional experience in the development and operationalization of data and AI products, ideally with practical know-how in the field of GenAI You have in-depth knowledge of Python and SQL (especially for data analysis, data engineering and ML/GenAI model development) and also practical experience with GCP services (BigQuery, Vertex AI, Cloud Storage, Cloud Run, ...) and Terraform You posess a deep understanding and practical experience in managing ML models across the entire lifecycle (development, deployment, monitoring, optimization) using modern MLOps practices You enjoy leading a team technically, motivating its members and promoting a collaborative way of workingStrong analytical skills, a structured way of working and a proactive approach to solving complex problems characterize you Very good communication skills and fluency in English and ideally German round off your profile You are open for hybrid work: 3 days from the office in Warsaw Employment contract? Of course. With us you do not have to worry about stable employment. Benefits? We have them! Among other: corporate incentive program, sport card, private medical care. Lunch card? With the cooperation extended and permanent contract, you will receive additional funds to use for meal purchases. Working in a hybrid model? üè† Of course! You work with us 3 days a week from the office, 2 days a week from home. Work wherever you want? üå¥ In PAYBACK you have the opportunity. Working 100% remotely, also from European countries for 15 days a year. ‚ÄéFlexible working hours? Sounds great! We start working between 7 to 10. Trainings? Of course. We provide training to develop hard and soft skills. Convenient location? Sure! We invite you to our new office at Rondo Daszy≈Ñskiego, but we are currently also working remotely. Dress code? We definitely say no. There are no rigid dress code rules in our company, sneakers are more than welcome. Friendly atmosphere at work? Yes! In PAYBACK, people are the most important asset‚Äé. Something is missing? Open communication is our priority, so dare to ask!‚Äé",[],Data Science,Data Science
Part-time,Mid,B2B,Remote,402,BI Consultant (part-time),Upvanta sp. z o.o.,"Do≈ÇƒÖczysz do zespo≈Çu, kt√≥ry odpowiada za rozw√≥j nowoczesnej platformy analitycznej wspierajƒÖcej kluczowe procesy finansowe, podatkowe oraz operacyjne w organizacji. Celem projektu jest zbudowanie skalowalnego i zautomatyzowanego ≈õrodowiska raportowego i decyzyjnego, kt√≥re zastƒÖpi rozproszone modele w Excelu oraz przyspieszy i upro≈õci codziennƒÖ pracƒô zespo≈Ç√≥w biznesowych. W projekcie wykorzystujemy najnowsze rozwiƒÖzania chmurowe (Azure), narzƒôdzia klasy enterprise (DataBricks, Power BI, PowerApps) oraz tworzymy dedykowane aplikacje i modele danych w oparciu o realne potrzeby u≈ºytkownik√≥w ko≈Ñcowych. Automatyczna walidacja i przetwarzanie danych w DataBricks ‚Äì projektowanie oraz wdra≈ºanie wydajnych pipeline‚Äô√≥w ETL/ELT. Budowa aplikacji biznesowych w ≈õrodowisku Microsoft 365 ‚Äì wykorzystanie Dataverse i PowerApps do tworzenia customowych rozwiƒÖza≈Ñ wspierajƒÖcych decyzje operacyjne. Rozw√≥j modeli podatkowych ‚Äì udzia≈Ç w wyborze i implementacji nowej technologii (alternatywa dla Excela) do zarzƒÖdzania modelami podatkowymi; pierwsze warsztaty planujemy ok. miesiƒÖc po Twoim onboardingu. Projektowanie hurtowni danych oraz tworzenie raport√≥w i dashboard√≥w (Power BI). Wsp√≥≈Çpraca z dzia≈Çami finans√≥w, podatk√≥w i IT w celu prze≈Ço≈ºenia wymaga≈Ñ biznesowych na rozwiƒÖzania analityczne. Min. 5 lat do≈õwiadczenia jako BI Consultant Bardzo dobra znajomo≈õƒá SQL oraz praktyczne do≈õwiadczenie z narzƒôdziami chmurowymi (Azure lub AWS/GCP). Umiejƒôtno≈õƒá pracy w DataBricks lub gotowo≈õƒá do szybkiego jej opanowania. Do≈õwiadczenie z Power BI, PowerApps, Dataverse albo pokrewnymi platformami low‚Äëcode. Samodzielno≈õƒá w prowadzeniu projekt√≥w i komunikatywna znajomo≈õƒá jƒôzyka angielskiego (min. B2). Znajomo≈õƒá zagadnie≈Ñ podatkowych lub finansowych (np. CIT, VAT, IFRS). Do≈õwiadczenie z CI/CD (Azure DevOps, GitHub Actions) i Python / PySpark. Stabilne zatrudnienie w oparciu o umowƒô B2B (part-time) Realny wp≈Çyw na architekturƒô rozwiƒÖza≈Ñ ‚Äì bƒôdziesz g≈Ç√≥wnƒÖ osobƒÖ odpowiedzialnƒÖ za DataBricks i za dob√≥r technologii do modeli podatkowych Prywatna opieka medyczna (Medicover), pakiet sportowy (Multisport), ubezpieczenie grupowe. Przyjazna kultura organizacyjna ‚Äì bez dress‚Äëcode‚Äôu, z kr√≥tkimi ≈õcie≈ºkami decyzyjnymi i wsparciem Tech Lead√≥w.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,403,Senior Analytics Engineer,Link Group,"Zakres obowiƒÖzk√≥w Projektowanie, budowa i utrzymanie proces√≥w ETL/ELT w DBT Cloud z wykorzystaniem r√≥≈ºnych ≈∫r√≥de≈Ç danych. Migracja ≈õrodowisk Snowflake i Qlik do nowej instancji EUCAN CDP . Implementacja framework√≥w kontroli jako≈õci danych (DBT tests, freshness checks, anomaly detection). Udzia≈Ç w procesach RUN Management: monitorowanie, ponowne uruchomienia, analiza przyczyn i dzia≈Çania prewencyjne. Dokumentowanie logiki modeli, transformacji i struktur w architekturze CDW_CORE ‚Üí CDW_STAGE ‚Üí CDW_PROD . Bliska wsp√≥≈Çpraca z zespo≈Çami produktowymi, architektami i interesariuszami w celu dostarczania rozwiƒÖza≈Ñ zgodnych z potrzebami biznesu. Mentoring m≈Çodszych in≈ºynier√≥w, wsparcie techniczne i rozwiƒÖzywanie z≈Ço≈ºonych problem√≥w. ≈örodowisko technologiczne DBT Cloud ‚Äì g≈Ç√≥wny silnik ELT Snowflake ‚Äì hurtownia danych AWS S3 ‚Äìdata lake / strefa ingestu GitHub , Azure DevOps ≈πr√≥d≈Ça danych: IQVIA OCE, Salesforce Marketing Cloud, Adobe AEM/Analytics, Shopify, Qualtrics, Accutics, Facebook, LinkedIn, Instagram Wymagania Min. 5 lat do≈õwiadczenia jako Data Engineer w obszarze chmurowych proces√≥w ELT. Zaawansowane umiejƒôtno≈õci w zakresie SQL i modelowania w DBT . G≈Çƒôboka znajomo≈õƒá koncepcji hurtowni danych (modelowanie wymiarowe, ≈Çadowania przyrostowe, optymalizacja wydajno≈õci). Do≈õwiadczenie z Snowflake i architekturƒÖ danych w chmurze. Doskona≈Çe umiejƒôtno≈õci komunikacyjne i dokumentacyjne w pracy z rozproszonymi zespo≈Çami. Proaktywne podej≈õcie, orientacja na jako≈õƒá danych, szybkie reagowanie na incydenty i terminowo≈õƒá w dostarczaniu rozwiƒÖza≈Ñ.","[{""min"": 130, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,404,Data Engineer,emagine Polska,"Location: Cracow (hybrid - 1 time per month in the office) Contract: b2b The Data Engineer role involves the development of critical data pipelines and services integral to our Generative AI models. Successful candidates will be proficient in Python and familiar with both on-premises Unix/Linux environments and Azure Cloud. The position is ideal for detail-oriented individuals who thrive in dynamic settings and possess a strong foundation in data processes. Main Responsibilities: Key responsibilities include: Creation of agents for data sourcing from various systems. Building data transfer pipelines. Developing microservices synchronized with Generative AI solutions. Key Requirements: Candidates must possess the following skills and qualifications: Experienced in Python and Unix/Linux environment on-premises. Experience with time-series/analytics databases such as Elasticsearch. Experience in Azure Cloud. Experience in RESTful APIs using Python FastAPI. Experience with Generative AI APIs. Experience in building data models and pipelines for Retrieval-Augmented Generation. Experience in building microservices. Familiarity with industry-standard version control tools (Git, GitHub) and deployment tools (Ansible & Jenkins). Basic shell-scripting proficiency. Understanding of big data modeling techniques using relational and non-relational approaches. Self-starter, proactive, and team-oriented. Willingness to learn and adapt to changing requirements. Experience and understanding of the Software Development Lifecycle (SDLC). Nice to Have: Preferred skills and qualifications include: Understanding or experience of Cloud design patterns. Great communication skills.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,405,"Senior Analytics Engineer (DBT Cloud, Snowflake)",Upvanta sp. z o.o.,"Design, build, and maintain efficient ETL/ELT pipelines in DBT Cloud , ingesting data from sources such as IQVIA OCE, Adobe Analytics, Qualtrics, Salesforce CIAM , and others. Contribute to the migration of legacy Snowflake and Qlik environments into the unified EUCAN CDP platform. Implement robust data quality validation frameworks (including DBT tests, freshness checks, anomaly detection ). Support and enhance the RUN Management process: monitoring, job reruns, root cause analysis (RCA), and preventive action implementation. Maintain and document model ownership, schema logic, and transformation processes within the CDW_CORE ‚Üí CDW_STAGE ‚Üí CDW_PROD architecture. Collaborate closely with the Product Owner, Data Architect , and affiliate stakeholders to deliver trusted, scalable, and well-governed data products. Mentor junior data engineers and serve as a point of escalation for complex data-related issues. DBT Cloud ‚Äì core ELT engine Snowflake ‚Äì data warehouse AWS S3 ‚Äì data lake / ingestion zone GitHub, Azure DevOps ‚Äì version control & CI/CD Data sources: IQVIA OCE, Salesforce Marketing Cloud, Adobe AEM/Analytics, Shopify, Qualtrics, Accutics, Facebook, LinkedIn, Instagram 5+ years of experience as a Data Engineer (or similar role) working with cloud-based ELT pipelines. Expert-level SQL and DBT data modeling skills. Deep understanding of data warehousing concepts: dimensional modeling, incremental loading, performance optimization. Hands-on experience with Snowflake and modern cloud data architectures. Strong communication and documentation skills ‚Äì proven ability to work effectively in distributed teams. Proactive mindset with ownership over data quality, incident resolution, and disciplined delivery.","[{""min"": 1200, ""max"": 1500, ""type"": ""Net per day - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Office,406,"Cloud Data Engineer, Customer Solutions, Global Service Delivery",Google,"Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Wroc≈Çaw, Poland; Warsaw, Poland . Bachelor's degree in Science, Technology, Engineering, Mathematics, or equivalent practical experience. Experience writing code in one or more programming languages (e.g., Python, Java). Experience in solution engineering and in stakeholder management, professional services, or technical consulting. Experience with Oracle, MySQL and MongoDB. Master‚Äôs degree in Engineering, Computer Science, Business, or a related field. Experience in an analytical role such as business intelligence, data analytics, or statistics. Experience working with database technologies (e.g., SQL, NoSQL). Experience with cloud technologies such as architecting, developing, or maintaining cloud solutions in virtualized environments or cloud data engineering. Knowledge and experience in Google Cloud Native Databases like Cloud Spanner, Cloud BigTable and AlloyDB. Proficiency in database design principles, data modeling, and the ability to translate business requirements into efficient database structures. The Google Cloud Platform team helps customers transform and build what's next for their business ‚Äî all with technology built in the cloud. Our products are developed for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. Our teams are dedicated to helping our customers ‚Äî developers, small and large businesses, educational institutions and government agencies ‚Äî see the benefits of our technology come to life. As part of an entrepreneurial team in this rapidly growing business, you will play a key role in understanding the needs of our customers and help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. As a Strategic Cloud Data Engineer, you'll work with customers and partner teams to design, develop, deploy and manage highly scalable and reliable databases on the Google Cloud Platform meeting the Organization‚Äôs storage demands. You will work on data migrations and modernization projects incorporating best practices of data governance and security controls. You will travel to customer sites to deploy solutions and deliver workshops to educate and empower customer teams to adhere to best practices in Database design and operations. Additionally, you'll work closely with Product Management and Product Engineering teams to build and constantly drive excellence in our products that solve the biggest customer challenges. Google Cloud accelerates every organization‚Äôs ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google‚Äôs cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems. Work with clients to understand their objectives and challenges, identify technical gaps, and surface opportunities for solution reuse or innovations. Design and implement solutions that meet client needs and are compliant with data and legal policies. Understand the nuances of clients within the industry and develop subject matter expertise in trending spaces. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,407,ETL Developer ‚Äì Informatica BDM/DEI,Calimala.ai,"Calimala.ai is seeking an experienced ETL Developer with expertise in Informatica BDM/DEI to join our innovative team in the telecom sector. In this capacity, you will be at the forefront of designing, developing, and optimizing scalable ETL pipelines that integrate data from diverse sources. This role requires a deep technical foundation in ETL development and a passion for turning complex data challenges into reliable, high-performance solutions. Responsibilities As an ETL Developer, you will work closely with data architects, analysts, and other team members to understand mapping specifications and implement efficient data workflows. Key responsibilities include: Design and develop complex ETL pipelines using Informatica BDM/DEI. Integrate data from various sources, including big data environments like Hive and Spark. Optimize mappings and workflows to ensure high performance and reliability. Collaborate with cross-functional teams to align data integration strategies with business needs. Document and maintain best practices in ETL and data governance. Requirements Candidates must bring at least five years of hands-on experience in ETL development and demonstrable expertise using Informatica BDM/DEI. A strong technical background in SQL, Hive, and Spark is essential along with proven experience in performance tuning and data integration. Additionally, familiarity with the telecom industry will serve as a significant advantage. What We Offer At Calimala.ai , we provide a dynamic work environment where innovation meets real-world data challenges. In addition to a competitive salary we offer opportunities for professional growth and learning. Join our team to play a pivotal role in transforming data integration processes and driving business success.","[{""min"": 12000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,408,Senior Data Modeller,GS Services,Poszukujemy superbohater√≥w ‚Äì Senior Data Modeller‚Äô√≥w! ü¶∏‚Äç‚ôÄÔ∏èü¶∏‚Äç‚ôÇÔ∏è,"[{""min"": 180, ""max"": 210, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Senior,Permanent or B2B,Hybrid,409,Developer Hurtowni Danych,ALTEN Polska,"DEVELOPER HURTOWNI DANYCH WARSZAWA/HYBRYDA KIM JESTE≈öMY? ALTEN ‚Äì ≈öwiatowy lider us≈Çug in≈ºynieryjnych i IT Grupa ALTE N, za≈Ço≈ºona w 1988 roku we Francji, jest wiodƒÖcym partnerem technologicznym, obs≈ÇugujƒÖcym najwiƒôkszych klient√≥w na ≈õwiecie. W 2023 roku osiƒÖgnƒô≈Ça przychody przekraczajƒÖce 4,07 miliarda euro i zatrudnia ponad 57 000 pracownik√≥w, w tym 88% in≈ºynier√≥w i ekspert√≥w IT . ALTEN Polsk a dzia≈Ça od 13 lat, wspierajƒÖc innowacje, badania i rozw√≥j oraz systemy IT swoich klient√≥w. Posiada biura w Krakowie, Warszawie, Wroc≈Çawiu, Gda≈Ñsku i Poznaniu. Obs≈Çuguje kluczowe bran≈ºe, takie jak lotnicza, kosmiczna, przemys≈Çowa, motoryzacyjna, kolejowa, energetyczna, finansowa, telekomunikacyjna i sprzeda≈ºowa. Zakres ekspertyz y: analiza biznesowa, cyberbezpiecze≈Ñstwo, sztuczna inteligencja, analiza danych i rozwiƒÖzania chmurowe, rozw√≥j aplikacji, testowanie oprogramowania, zarzƒÖdzanie projektami, oprogramowanie wbudowane, zarzƒÖdzanie jako≈õciƒÖ, industrializacja, transformacja cyfrowa oraz in≈ºynieria przemys≈Çowa . Wiƒôcej informacji na http: //www.altenpolska.p l KLUCZOWE ZADANIA: Budowa narzƒôdzi wspomagajƒÖcych pracƒô developer√≥w Hurtowni Danych - m.in . wsp√≥lne re-u≈ºywalne komponenty (SAS/SAS Viya/Oracle/inne technologie) Budowa API ≈ÇƒÖczƒÖcego wykorzystywane w HD technologie (SAS/SAS Viya/Oracle/Office365/REST API/ inne technologie) Budowa narzƒôdzi wspomagajƒÖcych wymianƒô danych pomiƒôdzy systemami informatycznymi m.in . Kafka POC nowych narzƒôdzi/technologii do potencjalnego wdro≈ºenia w HD m.in . SAS Viya & Exadata w chmurze Azure Udzia≈Ç w pracach optymalizujƒÖcych przetwarzania HD, poprzez budowƒô wspierajƒÖcych komponent√≥w a tak≈ºe punktowe analizy i optymalizacje przetwarza≈Ñ Budowa technicznych data mart-√≥w oraz raport√≥w BI ( m.in . w SAS Viya lub Power BI) WYMAGANIA: Do≈õwiadczenie w projektowaniu rozwiƒÖza≈Ñ z zakresu Hurtowni Danych i Business Intelligence Do≈õwiadczenie w pracy na ≈õrodowiskach/narzƒôdziach SAS i SAS Viya (DI Studio, Enterprise Guide, SAS Studio, Visual Analytics) Do≈õwiadczenie w pracy na ≈õrodowiskach/narzƒôdziach Oracle (mile widziane do≈õwiadczenie z Oracle Exadata i PL/SQL) Znajomo≈õƒá zagadnie≈Ñ i technologii CI/CD (Git / Bitbucket / Bamboo / JIRA) CO MO≈ªEMY CI ZAOFEROWAƒÜ? DNA ALTEN ≈ÇƒÖczy w sobie ludzkie warto≈õci, kulturƒô doskona≈Ço≈õci i do≈õwiadczenie w s≈Çu≈ºbie swoim klientom. Oferujemy indywidualne podej≈õcie do ka≈ºdego pracownika w duchu work-life balance. K≈Çadziemy nacisk na odpowiedzialny, zr√≥wnowa≈ºony rozw√≥j i proekologiczne rozwiƒÖzania. Oferujemy: ‚ñ™ Umowƒô na pe≈Çen etat z mo≈ºliwo≈õciƒÖ wyboru formy zatrudnienia (UoP/B2B) ‚ñ™ Udzia≈Ç w konferencjach bran≈ºowych, szkoleniach i warsztatach oraz spotkaniach integracyjnych ‚ñ™ Mo≈ºliwo≈õƒá odbycia kurs√≥w i zdobycia certyfikacji ‚ñ™ Mo≈ºliwo≈õƒá relokacji w ramach lokalnych oddzia≈Ç√≥w ALTEN Polska BENEFITY ‚ñ™ Prywatna opieka medyczna Medicover ‚ñ™ Prywatna opieka dentystyczna Medicover ‚ñ™ Platforma zakupowa Medicover Benefits / karta sportowa Medicover ‚ñ™ Program polece≈Ñ pracownik√≥w ALTEN Talenty ‚ñ™ Wyprawka dla nowonarodzonego dziecka pracownika ‚ñ™ Ubezpieczenie grupowe na ≈ºycie ‚ñ™ Mo≈ºliwo≈õƒá udzia≈Çu w programie emerytalnym Do≈ÇƒÖcz do naszego zespo≈Çu i razem z nami tw√≥rz przysz≈Ço≈õƒá technologii ju≈º dzi≈õ! KlikajƒÖc w przycisk ‚Äû Wy≈õlij ‚Äù zgadzasz siƒô na przetwarzanie przez Alten Polska sp. z o.o. z siedzibƒÖ w Warszawie Twoich danych osobowych zawartych w zg≈Çoszeniu rekrutacyjnym w celu prowadzenia rekrutacji na stanowisko wskazane w og≈Çoszeniu. W ka≈ºdym momencie mo≈ºesz cofnƒÖƒá zgodƒô, kontaktujƒÖc siƒô z nami pod adresem ALTEN_PL_RODO@alten.com , co oznacza zako≈Ñczenie rekrutacji i nieuwzglƒôdnienie Twojej kandydatury przy przysz≈Çych rekrutacjach. Pe≈ÇnƒÖ informacjƒô odno≈õnie przetwarzania Twoich danych osobowych znajdziesz https: //docs.altenpolska.pl/do_pobrania/Klauzula_informacyjna_dla_potencjalnych_pracownikow.pdf UWAGA: Uprzejmie informujemy, ≈ºe skontaktujemy siƒô z wybranymi kandydatami",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,410,Data Engineer with Hadoop,Antal Sp. z o.o.,"Develop automation tools and integrate existing solutions within a complex platform ecosystem Provide technical support and design for Hadoop Big Data platforms (Cloudera preferred) Manage user access and security (Kerberos, Ranger, Knox, TLS, etc.) Implement and maintain CI/CD pipelines using Jenkins and Ansible Perform capacity planning, performance tuning, and system monitoring Collaborate with architects and developers to design scalable and resilient solutions Deliver operational support and improve engineering tooling for platform management Analyze existing processes and design improvements to reduce complexity and manual work Building scalable automation in a diverse ecosystem of tools and frameworks Enhancing service resilience and reducing operational toil Supporting the adoption of AI agents and real-time data capabilities Integrating with corporate identity, CI/CD, and service management tools Collaborating with cross-functional teams in a global environment Minimum 5 years of experience in engineering Big Data environments (on-prem or cloud) Strong understanding of Hadoop ecosystem: Hive, Spark, HDFS, Kafka, YARN, Zookeeper Hands-on experience with Cloudera distribution setup, upgrades, and performance tuning Proven experience with scripting (Shell, Linux utilities) and Hadoop system management Knowledge of security protocols: Apache Ranger, Kerberos, Knox, TLS, encryption Experience in large-scale data processing and optimizing Apache Spark jobs Familiarity with CI/CD tools like Jenkins and Ansible for infrastructure automation Experience working in Agile or hybrid development environments (Agile, Kanban) Ability to work independently and collaboratively in globally distributed teams To learn more about Antal, please visit www.antal.pl","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,411,Starszy Specjalista ds. Bada≈Ñ i Rozwoju,PSE Innowacje sp. z o.o.,"PSE Innowacje jest sp√≥≈ÇkƒÖ powsta≈ÇƒÖ w 2012 na zlecenie operatora systemu przesy≈Çowego - PSE S.A. Od 2012 roku realizujemy takie zadania jak: prowadzenie analiz i bada≈Ñ, w tym analiz techniczno-ekonomicznych, prowadzenie prac badawczo-rozwojowych, budowa nowych oraz rozw√≥j i modernizacja istniejƒÖcych system√≥w informatycznych wspierajƒÖcych prowadzenie ruchu sieciowego. NaszƒÖ misjƒÖ jest dba≈Ço≈õƒá o niezawodnƒÖ i efektywnƒÖ pracƒô systemu elektroenergetycznego w Polsce oraz jego sta≈Çy rozw√≥j. Jeste≈õmy jednostkƒÖ do innowacyjnych zada≈Ñ specjalnych w bran≈ºy elektroenergetycznej. Poszukujemy do naszego Centrum Kompetencji Badania i Rozw√≥j w Warszawie lub Katowicach: Starszego Specjalist(k)ƒô ds. Bada≈Ñ i Rozwoju - Obszar kompetencji: Big Data, Prognozowanie i Optymalizacja üìç Miejsce pracy: - Warszawa lub Katowice; - Praca hybrydowa. üéØ Opis stanowiska: Osoba na tym stanowisku do≈ÇƒÖczy do obszaru merytorycznego Big Data, Prognozowanie i Optymalizacja i bƒôdzie odpowiedzialna przede wszystkim za projektowanie i budowanie modeli optymalizacyjnych oraz ich utrzymywanie, jak r√≥wnie≈º bƒôdzie uczestniczy≈Ça w pracach zwiƒÖzanych z wytwarzaniem i utrzymywaniem modeli prognostycznych w obszarze zarzƒÖdzania systemem elektroenergetycznym. üõ† Podstawowy zakres obowiƒÖzk√≥w na stanowisku: Aktywny udzia≈Ç w warsztatach z PSE S.A. dotyczƒÖcych okre≈õlania i dyskusji wymaga≈Ñ biznesowych stawianych dla projektowanych modeli optymalizacyjnych lub prognostycznych, w tym ich krytyczna analiza; Projektowanie r√≥wna≈Ñ matematycznych modeli optymalizacyjnych na podstawie wymaga≈Ñ biznesowych i ich implementacja z wykorzystaniem MathProg lub gurobipy; Tworzenie modeli prognostycznych z obszaru elektroenergetyki o r√≥≈ºnych horyzontach czasowych, od modeli ultra-kr√≥tkoterminowych po modele d≈Çugoterminowe; Testowanie modeli optymalizacyjnych; Testowanie modeli prognostycznych; Budowanie framework√≥w obliczeniowych dla zagadnie≈Ñ optymalizacyjnych i prognostycznych w jƒôzyku R/Python, zgodnie z przyjƒôtym w Firmie standardem kodowania; Aktywny udzia≈Ç w procesie wdra≈ºania wypracowanych rozwiƒÖza≈Ñ prototypowych do ≈õrodowisk produkcyjnych; Analizowanie du≈ºych, r√≥≈ºnorodnych zbior√≥w danych na potrzeby prowadzonych prac analitycznych i badawczych; Przygotowywanie raport√≥w, prezentacji i dokumentacji projektowej z uzyskanych wynik√≥w prac wraz z wnioskami i rekomendacjami; Praca w zespo≈Çach zarzƒÖdzanych w metodyce AgilePM; ≈öwiadczenie us≈Çug serwisowych dla wytwarzanych produkt√≥w. üìù Od kandydat√≥w oczekujemy: Wykszta≈Çcenia wy≈ºszego kierunkowego (mile widziana: matematyka, ekonometria, statystyka, energetyka); Min. 3-letniego do≈õwiadczenia w budowaniu modeli optymalizacyjnych produkcyjnych lub na potrzeby bada≈Ñ naukowych; Dobrej znajomo≈õƒá zagadnie≈Ñ Programowania Liniowego (LP); Praktycznej znajomo≈õci przynajmniej jednego z silnik√≥w optymalizacyjnych (Gurobi, CPLEX, GLPK, CBC, FICO Xpress, itp.); Praktycznej znajomo≈õci przynajmniej jednego wysokopoziomowego jƒôzyka do budowania modeli optymalizacyjnych: MathProg, AMPL, MOSEL, itp. ; Min. 2-letniego do≈õwiadczenia w budowaniu modeli matematycznych (preferowane modelowanie zagadnie≈Ñ z obszaru rynku energii elektrycznej, gazu ziemnego, itp.) ; Znajomo≈õci zagadnie≈Ñ dot. modelowania szereg√≥w czasowych; Praktycznej i bieg≈Çej znajomo≈õƒá jƒôzyka Python; Do≈õwiadczenia w pracy z jƒôzykiem R; Znajomo≈õci zagadnie≈Ñ zwiƒÖzanych z rynkiem elektroenergetycznym; Umiejƒôtno≈õci analitycznego my≈õlenia oraz formu≈Çowania ocen i wniosk√≥w; Znajomo≈õci systemu kontroli wersji Git/GitLab do zarzƒÖdzania kodem i wsp√≥≈Çpracy zespo≈Çowej; Komunikatywno≈õci i umiejƒôtno≈õci pracy w zespole; Dobrej organizacji pracy; Znajomo≈õci jƒôzyka angielskiego na poziomie min. B2; Chƒôci do podnoszenia kwalifikacji zawodowych. üíé Mile widziane: Do≈õwiadczenie w budowaniu aplikacji zgodnie z dobrymi praktykami in≈ºynierii oprogramowania; Do≈õwiadczenie w pracy z zakresu Data Science; Znajomo≈õƒá jƒôzyka SQL; Znajomo≈õƒá praktyk CI/CD; Kreatywno≈õƒá i aktywno≈õƒá w podejmowaniu inicjatywy. üöÄ Oferujemy: Udzia≈Ç w ciekawych projektach majƒÖcych strategiczny wp≈Çyw na sektor energetyczny w Polsce i na ≈õwiecie; Pracƒô w przyjaznej atmosferze i wsparcie zespo≈Çu nastawionego na dzielenie siƒô wiedzƒÖ oraz do≈õwiadczeniami; KlarownƒÖ ≈õcie≈ºkƒô rozwoju zawodowego oraz szkolenia; Wewnƒôtrzny program mentoringowy wspierajƒÖcy zar√≥wno nowych jak i obecnych pracownik√≥w; Bogaty pakiet benefit√≥w: opieka medyczna, karta sportowa, ubezpieczenie na ≈ºycie; Program onboardingowy pozwalajƒÖcy na szybkƒÖ i przyjaznƒÖ adaptacje do pracy; Chcesz do≈ÇƒÖczyƒá do naszego zespo≈Çu? Aplikuj ju≈º teraz üíª",[],Unclassified,Unclassified
Full-time,Senior,B2B,Remote,412,Senior/ Lead Data Science Engineer,N-iX,"We are looking for a Data Scientist with a strong analytical mindset and a passion for solving real-world supply chain problems. You will join a cross-functional team focused on optimizing the flow of units from warehouses to stores. This is a high-impact role where your insights and models will directly influence key business operations. Responsibilities: Prepare and clean datasets to enable reliable experimentation Develop and fine-tune machine learning models and algorithms Test models, analyze outcomes, and generate actionable insights Communicate findings to stakeholders through reports and visualizations Propose data-driven solutions and optimization strategies Requirements: Proficiency in Python and/or R Experience with SQL Data Platforms & Tools Hands-on experience with Azure Databricks or Snowflake Familiarity with building and deploying machine learning pipelines is a strong advantage Java experience is a plus Education - Bachelor's or Master's degree in Computer Science, Mathematics, Engineering, or a related field Mathematics & Statistics: Solid understanding of multivariable calculus and linear algebra Applied knowledge of statistical distributions and hypothesis testing Machine Learning Practical knowledge of ML techniques: kNN, decision forests, regressions, MLE, time series Understanding of model evaluation metrics and performance tuning Data Handling & Visualization Skilled in managing missing or inconsistent data Experience with tools like matplotlib, seaborn, or ggplot2 Nice to Have: Experience with deploying ML models into production environments Previous work in supply chain or logistics domains","[{""min"": 23330, ""max"": 28069, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,Any,Hybrid,413,Data Engineer,Heineken,"Digital & Technology Team (D&T) is an integral division of HEINEKEN Global Shared Services Center . We are committed to making Heineken the most connected brewery. That includes digitalizing and integrating our processes, ensuring best-in-class technology, and embedding a data-driven culture. By joining us you will work in one of the most dynamic and innovative teams and have a direct impact on building the future of Heineken ! Would you like to meet the Team, see our office and much more? Visit our website: Heineken ( heineken-dt.pl ) As the Data Engineer in the Global Deployment team, you will be a business facing individual responsible for planning, executing, and overseeing the migration of data between source and target systems or platforms, ensuring data accuracy, completeness, and security throughout the process. Your responsibilities would include: Technical Activities: coordinating and executing data migration activities, including data extraction, cleansing, loading, and reconciliation; designing and implementing data mapping, transformation, and validation processes to ensure the accuracy and integrity of migrated data Documentation and Knowledge Transfer: Responsible to create and maintain documentation for data migration processes, procedures, and best practices. Supporting with proper knowledge transfer to service delivery and OpCo teams. Collaboration: colaborating with the Deployment Lead to ensure appropriate data migration process and procedures are in place and followed. Coordinate with OpCo data teams to ensure successful data migrations Learning: being updated on emerging data technologies and trends as it relates to key technologies in our landscape. You are a good candidate if you have: 5+ years of working experience in the similar position responsible for data migration and quality related activities in large multi nationals at least 3-5 years of experience with many of the above-mentioned technologies data models and patterns experience in collaboration with cross-functional teams to ensure successful delivery strong understanding of of data modelling, ETL processes, and data integration techniques strong communication skills with the ability to effectively interact with stakeholders at all levels ability to learn new technologies fast ability to multitask certificates in any ETL or Data Modelling tooling knowledge of technology stack: Jira, ETL Tooling, Excel, SQL Server Data Tools (SSDT) excellent written and verbal English. Good to know: Microsoft Dynamics 365 (CE or F&O or Contact Centre) JavaScript / TypeScript (Node.js) GraphQL / Apollo Loyalty Solutions (i.e. Epsilon) Payment platforms (i.e. process out) CTI Solutions (i.e. Genesys) SAP Middleware solutions (i.e. Boomi or MuleSoft) Azure DevOps. At HEINEKEN Krak√≥w, we take integrity and ethical conduct seriously. If someone has concerns about a possible violation of legal regulations indicated in Polish Whistleblowing Act or our Code of Business Conduct, we encourage them to speak up . Cases can be reported to global team or locally (in line with the local HGSS Whistleblowing procedure) by selecting proper option in this tool or by communicating it on hotline. We Offer: üè† Flexible Work from Home scheme üí∏ Attractive Performance Bonus üöó Parking Space for Employees ‚è∞ Flexible working hours üí≥ Sodexo Card ‚òÇ Life Insurance ‚ûï Employee Referral Programme üåê Job Opportunities within HEINEKEN ü©∫ Private Medical Healthcare ‚≠ê Social Events",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,414,Cloud Data Engineer ‚Äì sektor finansowy,Sii,"Do≈ÇƒÖcz do projektu realizowanego w obszarze p≈Çatno≈õci jako Data Engineer. Zesp√≥≈Ç odpowiada za tworzenie, rozwijanie i utrzymanie globalnej platformy danych, integrujƒÖcej informacje o p≈Çatno≈õciach i terminalach z r√≥≈ºnych system√≥w ≈∫r√≥d≈Çowych na ca≈Çym ≈õwiecie. Specjali≈õci zajmujƒÖ siƒô ekstrakcjƒÖ, transformacjƒÖ oraz dostarczaniem danych u≈ºytkownikom ko≈Ñcowym w okre≈õlonych formatach. Kluczowym elementem pracy jest komunikacja z u≈ºytkownikami biznesowymi, w≈Ça≈õcicielami danych oraz interesariuszami w celu doprecyzowania wymaga≈Ñ dotyczƒÖcych danych, ich formatu i metod dostarczania. Je≈õli chcesz pracowaƒá z pasjonatami technologii i mieƒá wp≈Çyw na tworzenie prze≈Çomowych rozwiƒÖza≈Ñ, aplikuj do nas ju≈º dzi≈õ! Budowanie i projektowanie rozwiƒÖza≈Ñ Data Lake i DWH Automatyzowanie oraz optymalizowanie proces√≥w przetwarzania danych ≈ÅƒÖczenie danych z r√≥≈ºnych system√≥w ≈∫r√≥d≈Çowych (bazy danych, pliki, API) w jedno sp√≥jne ≈õrodowisko analityczne Rozwijanie proces√≥w ekstrakcji, transformacji oraz ≈Çadowania danych (ETL) z r√≥≈ºnych ≈∫r√≥de≈Ç ≈öcis≈Ça wsp√≥≈Çpraca z interesariuszami biznesowymi Rozwijanie i optymalizacja ≈õrodowiska chmurowego pod kƒÖtem wydajno≈õci i niezawodno≈õci Minimum 5 lat do≈õwiadczenia na podobnym stanowisku Bardzo dobra znajomo≈õƒá jƒôzyk√≥w SQL i Python oraz narzƒôdzia Spark/PySpark Do≈õwiadczenie w budowaniu skalowalnych, dzia≈ÇajƒÖcych w czasie rzeczywistym i wysokowydajnych rozwiƒÖza≈Ñ typu Data Lake Umiejƒôtno≈õƒá rozwijania procesu ETL w r√≥≈ºnych narzƒôdziach (SSIS, ADF, IPC) Znajomo≈õƒá rozwiƒÖza≈Ñ chmurowych (Azure, AWS, GCP) Do≈õwiadczenie w projektach zwiƒÖzanych z migracjƒÖ danych z on-premise do chmury (bazy danych: Oracle, PostgreSQL, MS SQL) Swobodna komunikacja w jƒôzyku angielskim Tytu≈Ç Great Place to Work od 2015 roku - to dziƒôki opiniom pracownik√≥w otrzymujemy tytu≈Ç i wdra≈ºamy nowe pomys≈Çy Stabilno≈õƒá zatrudnienia ‚Äì 2,1 MLD PLN przychodu, brak d≈Çug√≥w, od 2006 roku na rynku Dzielimy siƒô zyskiem z pracownikami - od 2022 roku przeznaczyli≈õmy na ten cel ju≈º ponad 60 milion√≥w PLN Bogaty pakiet benefit√≥w - prywatna opieka zdrowotna, platforma kafeteryjna, zni≈ºki na samochody i wiƒôcej Komfortowe miejsce pracy - pracuj w naszych biurach klasy A lub zdalnie DziesiƒÖtki fascynujƒÖcych projekt√≥w dla presti≈ºowych marek z ca≈Çego ≈õwiata ‚Äì mo≈ºesz je zmieniaƒá dziƒôki aplikacji Job Changer 1 000 000 PLN rocznie na Twoje pomys≈Çy - takƒÖ kwotƒÖ wspieramy pasje i akcje wolontariackie naszych pracownik√≥w Stawiamy na Tw√≥j rozw√≥j - meetupy, webinary, platforma szkoleniowa i blog technologiczny ‚Äì Ty wybierasz Fantastyczna atmosfera stworzona przez wszystkich Sii Power People 1 Wy≈õlij do nas swoje CV 2 We≈∫ udzia≈Ç w rozmowie rekrutacyjnej 3 Poznaj projekty dopasowane do Twoich potrzeb 4 Podejmij decyzjƒô i zacznij swojƒÖ przygodƒô z Sii! Sii to czo≈Çowy dostawca doradztwa technologicznego, transformacji cyfrowej oraz us≈Çug in≈ºynieryjnych i biznesowych w Polsce.Zatrudniamy ju≈º ponad 7 500 specjalist√≥w i realizujemy projekty z r√≥≈ºnorodnych bran≈º dla klient√≥w z wielu kraj√≥w na ca≈Çym ≈õwiecie. Tytu≈Ç Great Place to Work zdobyty 10 razy z rzƒôdu to dow√≥d na to, ≈ºe w Sii tworzymy przyjazne ≈õrodowisko pracy. W badaniu a≈º 90% naszych pracownik√≥w odpowiedzia≈Ço, ≈ºe Sii jest ≈õwietnym miejscem pracy, a 95% z nich sƒÖdzi, ≈ºe panuje tu ≈õwietna atmosfera.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,415,Database Administrator MS SQL,emagine Polska,"INFORMACJE O PROJEKCIE: Bran≈ºa : Bankowo≈õƒá Lokalizacja : hybryda (1 x w tygodniu w biurze Krak√≥w lub Warszawa) Stawka: do 120 PLN/h netto + VAT (B2B) Szukamy do≈õwiadczonego administratora baz danych MS SQL, kt√≥ry posiada 3-5 letnie do≈õwiadczenie w tej roli. Idealny kandydat powinien wykazaƒá siƒô solidnymi umiejƒôtno≈õciami w administracji baz danych oraz dostosowywaniu system√≥w operacyjnych i silnika baz danych do z≈Ço≈ºonych wymaga≈Ñ. Administrator baz danych MS SQL bƒôdzie odpowiedzialny za zapewnienie ciƒÖg≈Ço≈õci dzia≈Çania baz danych oraz wsparcie podczas incydent√≥w i awarii. OBOWIƒÑZKI: Wdra≈ºanie i utrzymywanie system√≥w baz danych MS SQL Server. Wykonywanie zada≈Ñ administracyjnych, takich jak instalacja, konfiguracja i uaktualnienia. Monitorowanie wydajno≈õci baz danych oraz identyfikowanie problem√≥w. Wdra≈ºanie i utrzymywanie bezpiecze≈Ñstwa baz danych. Wsp√≥≈Çpraca z zespo≈Çami rozwoju i operacji. Planowanie i wykonywanie procedur tworzenia kopii zapasowych. Opracowywanie plan√≥w odzyskiwania po awarii. Automatyzowanie zada≈Ñ przy u≈ºyciu jƒôzyk√≥w skryptowych. Tworzenie dokumentacji operacyjnej i technicznej. WYMAGANIA: 3+ lat do≈õwiadczenia jako administrator bazy danych. Solidne do≈õwiadczenie z bazami danych MS SQL Server. Udokumentowane do≈õwiadczenie w implementacji i utrzymywaniu baz danych . Do≈õwiadczenie w dostrajaniu wydajno≈õci baz danych. Znajomo≈õƒá najlepszych praktyk bezpiecze≈Ñstwa baz danych. Do≈õwiadczenie w procedurach tworzenia kopii zapasowych. Znajomo≈õƒá jƒôzyk√≥w skryptowych, takich jak PowerShell. Silne umiejƒôtno≈õci analityczne i rozwiƒÖzania problem√≥w. Doskona≈Çe umiejƒôtno≈õci komunikacyjne. Umiejƒôtno≈õƒá pracy w zespole. Do≈õwiadczenie z s ystemem operacyjnym Windows Server. Znajomo≈õƒá bankowo≈õci bƒôdzie atutem, ale nie jest obowiƒÖzkowa. Komunikatywna znajomo≈õƒá jƒôz. angielskiego. MILE WIDZIANE: Do≈õwiadczenie z us≈Çugami baz danych w chmurze. Do≈õwiadczenie z bazami danych PostgreSQL. Do≈õwiadczenie z systemem operacyjnym Linux. Do≈õwiadczenie z IBM InfoSphere Data Replication.","[{""min"": 100, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,Permanent,Hybrid,416,Ekspert ds. Platform Danych i Analityki w Biurze Transformacji Cyfrowej i AI,ORLEN S.A.,"Rozw√≥j i zarzƒÖdzanie nowoczesnymi platformami danych, w tym r√≥wnie≈º w ≈õrodowisku chmurowym Koordynowanie zespo≈Ç√≥w projektowych, wsp√≥≈Çtworzenie strategii danych Koncernu oraz nadzorowanie realizacji du≈ºych projekt√≥w analitycznych i architektonicznych Koordynowanie prac zespo≈Çu specjalist√≥w ds. danych i analityki (architekci, in≈ºynierowie danych, analitycy) Projektowanie i rozw√≥j architektury platform danych w ≈õrodowiskach chmurowych (Snowflake, Databricks, BigQuery, Microsoft Fabric, Redshift) Nadz√≥r nad realizacjƒÖ projekt√≥w z obszaru danych i analityki ‚Äì od koncepcji po wdro≈ºenie Wsp√≥≈Çpraca z interesariuszami biznesowymi w celu zrozumienia potrzeb analitycznych i przek≈Çadania ich na rozwiƒÖzania technologiczne Tworzenie i wdra≈ºanie strategii zarzƒÖdzania danymi, w tym jako≈õci danych, bezpiecze≈Ñstwa i zgodno≈õci z regulacjami Monitorowanie trend√≥w rynkowych i rekomendowanie nowych rozwiƒÖza≈Ñ technologicznych Wykszta≈Çcenie wy≈ºsze informatyczne lub pokrewne Kilkuletnie do≈õwiadczenie w pracy z technologiami danych Do≈õwiadczenie w projektowaniu architektury danych i wdra≈ºaniu du≈ºych projekt√≥w analitycznych Do≈õwiadczenie we wsp√≥≈Çpracy z wieloma jednostkami organizacyjnymi w ORLEN oraz ze sp√≥≈Çkami wchodzƒÖcymi w sk≈Çad grupy kapita≈Çowej, zar√≥wno po stronie biznesu, jak i IT Do≈õwiadczenie w koordynowaniu prac zespo≈Ç√≥w interdyscyplinarnych, ≈ÇƒÖczƒÖcych kompetencje techniczne (IT, in≈ºynieria danych) z biznesowymi (analityka, strategia, rozw√≥j produkt√≥w) Do≈õwiadczenie i umiejƒôtno≈õƒá zarzƒÖdzania bud≈ºetami projekt√≥w inwestycyjnych Do≈õwiadczenie i umiejƒôtno≈õƒá oceny ryzyk i specyficznych uwarunkowa≈Ñ (technicznych, ekonomicznych, ≈õrodowiskowych, regulacyjnych) zwiƒÖzanych z dostƒôpnymi technologiami, danymi itd. Praktyczna znajomo≈õƒá r√≥≈ºnych rozwiƒÖza≈Ñ i technologii, np. Snowflake, Databricks, Google BigQuery, Microsoft Fabric, Amazon Redshift, Terradata, Qlik, Informatica Znajomo≈õƒá jƒôzyka angielskiego na poziomie umo≈ºliwiajƒÖcym swobodnƒÖ komunikacjƒô technicznƒÖ i biznesowƒÖ Skuteczno≈õƒá w dzia≈Çaniu i odpowiedzialno≈õƒá biznesowa Nastawienie na ciƒÖg≈Çy rozw√≥j i umiejƒôtno≈õƒá czerpania z r√≥≈ºnych perspektyw Do≈õwiadczenie na stanowisku kierowniczym, w zarzƒÖdzaniu zespo≈Çami i projektami, w tym w ≈õrodowisku zwinnym (Agile/Scrum) Znajomo≈õƒá przepis√≥w i standard√≥w dotyczƒÖcych zarzƒÖdzania danymi (np. GDPR, ISO/IEC 27001, DAMA-DMBOK) oraz do≈õwiadczenie w obszarze data governance Znajomo≈õƒá uznanych metodyk zarzƒÖdzania projektami, takich jak PMI/PMP, PRINCE2, Agile, Scrum oraz umiejƒôtno≈õƒá ich praktycznego zastosowania w ≈õrodowiskach z≈Ço≈ºonych technologicznie i organizacyjnie Stabilne zatrudnienie i atrakcyjne wynagrodzenie (w tym system premiowy) Praca w nowoczesnym, miƒôdzynarodowym koncernie Mo≈ºliwo≈õƒá pracy z ekspertami z r√≥≈ºnych dziedzin Wyzwania zawodowe na ≈õwiatowym poziomie Uczestnictwo w du≈ºych, nowatorskich projektach Dostƒôp do nowoczesnych narzƒôdzi i metod pracy Mobilno≈õƒá wewnƒÖtrz Grupy ORLEN i mo≈ºliwo≈õci rozwoju w zagranicznych sp√≥≈Çkach Mentoring pracowniczy wspierajƒÖcy Tw√≥j rozw√≥j zawodowy Programy well-beingowe wspierajƒÖce dobrostan Program wspierajƒÖcy rodzinƒô Dostƒôp do platformy kafeteryjnej",[],Data Science,Data Science
Practice / Internship,Junior,Internship,Remote,417,Data Analytics Engineering Trainee,EPAM Systems,"Striving to gain market-oriented knowledge and skills to jumpstart your career in IT? Apply for this program and shape your professional path with EPAM experts. Details If you are interested in creating data products and exploring the power of data to turn raw information into valuable insights for business growth, then this training program is what you need. By participating, you will have the opportunity to acquire new skills or expand existing knowledge through hands-on experience in three major areas: Data Integration ‚Äì development and support of a wide range of data transformations and migrations Data Visualization ‚Äì creation of interactive and complex data visualizations and analysis tools Data Quality ‚Äì data validation and transformation at every stage of the project After successfully completing all program stages, you will gain market-oriented soft and hard skills, which you may further apply at EPAM or elsewhere in the IT industry. What do we offer? Industry-based education. As a leading software engineering company, we will help you explore emerging technologies and best practices that the market demands. Top-notch learning materials. Our Data & Analytics specialists with extensive project experience have designed and tested the educational content in numerous training runs. Practice-oriented approach. This comprehensive program focuses on providing you with hands-on experience and practical application of the concepts learned. Deep dive into the specialization. Our graduates become highly skilled specialists ready to face complex technical challenges and work with the world's leading customers. Support from experienced mentors. We will guide you at all training stages, covering your open questions and sharing feedback on assigned tasks. Career advancement. Upon successful completion of both Fundamentals and Specialization stages, we will consider you for open positions based on your demonstrated skills and available opportunities at EPAM. Training process The program consists of two stages: Fundamentals stage It will last 3 months and require ~15 hours of weekly engagement. You will explore self-study materials, complete one assigned task each week, ensure its approval by mentors and discuss your questions during weekly group sessions led by experts. Additionally, you will participate in 2-3 individual assessments with mentors, which will involve a theoretical review and live coding exercises. If you show good results and successfully pass a technical interview, we will invite you to the next stage. Specialization stage It will last 4 months and require ~20 hours of weekly engagement. At this stage, learning will become more intensive, involving daily assignments, daily group Q&A sessions and exploration of self-study materials. What is required for training: English level from B2 (Upper-Intermediate) and higher Basic knowledge of Relational Database Management System (DBMS) theory Understanding of Structured Query Language (SQL) Familiarity with Python basics Nice to have: Degree from a technical university or other educational institution with a technical specialization Experience in banking and technical spheres Please read this info before registration This program is for citizens of Poland and people who have relocated to this country for a permanent stay. The program start date may change, so the selection period may be adjusted accordingly. Please regularly check for updates on this page and via email. Considering the limited number of places in this program's group, the selection results will be decisive factors in enrollment, with the applicants with the highest scores being processed first. We strive to keep the registration and testing process fair for everyone. If we notice any cheating, we will have to reject your application. If you are interested in applying while enrolled in another EPAM Campus program or employed at EPAM, please discuss it with your Training Coordinator or Resource Manager first.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,418,Data Modeler / Data Engineer,emagine Polska,"Informacje o projekcie: Bran≈ºa: finanse/po≈ºyczki Lokalizacja: zdalnie Umowa: B2B Stawka: 200 pln/h netto + VAT D≈Çugo≈õƒá projektu: d≈Çugoterminowy Poszukujemy do≈õwiadczonego Data Engineera do zespo≈Çu Data Platform, kt√≥ry bƒôdzie modelowaƒá dane oraz implementowaƒá procesy ELT w chmurze. ObowiƒÖzki: Modelowanie struktur bazodanowych w podej≈õciu DDD oraz tworzenie logicznych i fizycznych modeli danych. Data Mapping. Przygotowywanie warstwy Data Contracts (wymaga≈Ñ HD do system√≥w ≈∫r√≥d≈Çowych pod merytorycznƒÖ p≈Çaszczyznƒô kontraktu na dane) na podstawie zamodelowanych uprzednio struktur dla poszczeg√≥lnych domen danych. Wsp√≥≈Çpraca w procesie ingerencji danych z system√≥w ≈∫r√≥d≈Çowych.Implementacja modeli danych dla poszczeg√≥lnych domen w Data Platform (warstwa Bronze, Silver i Gold) w podej≈õciu ELT w ≈õrodowisku Azure Databricks. Wymagania: Do≈õwiadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejƒôtno≈õƒá implementacji proces√≥w ELT. Umiejƒôtno≈õƒá tworzenia dokumentacji technicznej. Do≈õwiadczenie w mapowaniu danych ze ≈∫r√≥d≈Çowych do docelowych struktur w DWH. Umiejƒôtno≈õƒá interpretacji fizycznego/logicznego modelu danych (ERD, modele relacyjne) i synchronizacji struktur tych≈ºe modeli z wymaganiami ≈õwiata analityki. Do≈õwiadczenie w podej≈õciu do projektowania s≈Çownik√≥w i zarzƒÖdzania nimi (masterdata). Znajomo≈õƒá narzƒôdzia dbdiagram.io. Wiedza na temat zagadnie≈Ñ Data Quality, Data Lineage i zasad zarzƒÖdzania danymi.","[{""min"": 160, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Senior,Permanent or B2B,Remote,419,Senior Data Engineer (Microsoft),Onwelo,"üü† Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, a tak≈ºe otworzy≈Ça biura w siedmiu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. üöÄ O projekcie: Szukamy do≈õwiadczonego Microsoft Data Engineera , kt√≥ry poprowadzi techniczne aspekty projekt√≥w i bƒôdzie kluczowym ≈ÇƒÖcznikiem miƒôdzy IT a biznesem. PracujƒÖc z nami, bƒôdziesz mieƒá realny wp≈Çyw na spos√≥b, w jaki przetwarzane sƒÖ dane kluczowe dla biznesu i klient√≥w. Je≈õli masz do≈õwiadczenie w hurtowniach danych i lubisz wyzwania zwiƒÖzane z presales oraz definiowaniem wymaga≈Ñ , do≈ÇƒÖcz do nas i wsp√≥≈Çtw√≥rz rozwiƒÖzania. üéØ Z nami bƒôdziesz: Projektowaƒá, rozwijaƒá i optymalizowaƒá hurtownie danych opartych na technologii Microsoft SQL Server Implementowaƒá i utrzymywaƒá procesy ETL przy u≈ºyciu SSIS Tworzyƒá raporty i analizy z wykorzystaniem SSRS oraz modele wielowymiarowe i tabelaryczne w SSAS Integrowaƒá dane z r√≥≈ºnych ≈∫r√≥de≈Ç, w tym Oracle Optymalizowaƒá wydajno≈õci zapyta≈Ñ SQL oraz proces√≥w przetwarzania danych Aktywnie wsp√≥≈Çpracowaƒá z klientami w celu zbierania i definiowania wymaga≈Ñ biznesowych oraz ich przek≈Çadania na rozwiƒÖzania techniczne Prowadziƒá dzia≈Çania presales ‚Äì przygotowywanie ofert, udzia≈Ç w spotkaniach z klientami, doradztwo w zakresie architektury danych Koordynowaƒá i nadzorowaƒá pracƒô zespo≈Çu developerskiego, pe≈Çniƒá rolƒô lidera technicznego üòé Czekamy na Ciebie, je≈õli: Masz m in. 5 lat do≈õwiadczenia w pracy z hurtowniami danych oraz rozwiƒÖzaniami Microsoft BI Bardzo dobra znasz SQL Server , w tym mechanizm√≥w przechowywania i przetwarzania danych Posiadasz Do≈õwiadczenie w pracy z SSIS, SSRS, SSAS oraz umiejƒôtno≈õƒá efektywnego wykorzystywania tych narzƒôdzi. Pracujesz r√≥wnie≈º z innymi ≈∫r√≥d≈Çami danych, w szczeg√≥lno≈õci Azure , Oracle Potrafisz prowadziƒá projekty i wsp√≥≈Çpracowaƒá z biznesem ‚Äì umiejƒôtno≈õƒá definiowania wymaga≈Ñ, rekomendowania rozwiƒÖza≈Ñ oraz prezentowania wynik√≥w. Masz do≈õwiadczenie w dzia≈Çaniach presales , tworzeniu ofert i doradztwie technologicznym. Mo≈ºesz pochwaliƒá siƒô umiejƒôtno≈õciƒÖ zarzƒÖdzania zespo≈Çem oraz mentoringu m≈Çodszych cz≈Çonk√≥w zespo≈Çu. Znasz jƒôzyka angielskiego na poziomie min. B2 ü§ù Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Junior,Permanent,Hybrid,420,Data Specialist,Macrobond Financial,"About Us Macrobond is a leading provider of global economic and financial data and technology for investment professionals. Our customers include over 900 firms spanning the buyside, sell side, corporate and academic sectors. Our platform, rich in intellectual property and supported by a rapidly expanding global team, ensures we remain at the forefront of our industry. With the backing of Francisco Partners, a prominent global tech investment firm, we operate as a truly international company. Our headquarters are in Malm√∂, Sweden, and we have key offices in Gothenburg, London, Poland, Lisbon, Hong Kong, and New York."" Job Overview Our growth as a company means that we need to grow our support team as well, which is why we‚Äôre looking forward to having you on board of our team as a Data Specialist. You‚Äôll be responsible for maintaining and expanding macroeconomic content in the Macrobond application. Your work will include sourcing data from national, international & private sources, performing updates, database administration, and more. Job Responsibilities: Performing timely data updates with high focus on quality Filtering & scrubbing data to identify data patterns and correct data problems Communicating with sources to ensure that data is maintained at a high standard Finding & defining opportunities to improve work processes (data automation & structuring) Cooperating with other team members & your team manager Required Qualifications and experience: University degree, preferably in economics, statistics, finance or similar Experience working with macroeconomic/financial/other data is a big asset Advanced knowledge of English; additional languages are also valuable (especially Ukrainian/Russian/French), but are not mandatory Good practical knowledge of MS Excel High attention to detail and an analytical mind Well-developed online research skills What do we offer: Stable position in a growing business with a ""people first"" atmosphere Private health care and sport benefits Cafeteria platform Language learning platform Internal, external or online (e.g. Udemy) training possibilities Hybrid work model A modern office in the city center with an amazing chillout space. Our commitment to Diversity Diversity, equity, and inclusion are core values at Macrobond. We believe that diverse backgrounds and perspectives strengthen our organization and foster innovation. By joining us, you‚Äôll be part of a team that values and celebrates individuality while driving success together. Apply today and become part of our exciting journey!",[],Unclassified,Unclassified
Full-time,Senior,B2B,Hybrid,421,Data Analyst,emagine Polska,"Project information: Area: banking Location: Krak√≥w or other locations (1-2 days per month in the office) Start: ASAP/to determinate Cooperation on B2B Duration: long-term The main function of this role is to enhance the quality of data and manage data modeling within the Time Series Data Programme, which is focused on regulatory compliance and effective data management. As a Data Analyst, you will be responsible for leading data quality initiatives and collaborating across teams to ensure data integrity and compliance. Lead a pod or manage different models within a pod. Excel in stakeholder management. Independently collate, test, and assess sourced data for robustness and fitness for purpose. Learn and comprehend source systems, data models, and data standards. Collaborate with model development and monitoring teams to address data quality issues. Create ETL pipelines for the Wholesale Credit Risk data platform. Contribute to the design of data, application, and business process architecture roadmaps. Prepare and disseminate effective material to stakeholders at all levels. Manage relationships across key functional teams. Work with management to prioritize business and information needs. Identify and drive opportunities for process improvement. Solid experience in a similar role. Educational background: Bachelor's degree in IT or a related field; Master‚Äôs degree preferred. Strong programming skills, especially in SAS, Python, PySpark, SQL, or Hadoop Big Data. Business analysis experience including requirements gathering. Ability to understand complex data architecture. Strong organizational and problem-solving skills. Familiarity with JIRA, Confluence, and Monday.com . Experience with Prophecy. Knowledge of banking risk data, systems, and risk models.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Remote,422,Senior Data Engineer,N-iX,"#3204 Join our team to work on enhancing a robust data pipeline that powers our SaaS product, ensuring seamless contextualization, validation, and ingestion of customer data. Collabd engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Client was established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client‚Äôs platform rates with product teams to unlock new user experiences by leveraging data insights. Engage with domain experts to analyze real-world empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renowned Scandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities : Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements : Programming: Minimum of 3-4 years as a data engineer, or in a relevant field Python Proficiency: Advanced experience in Python, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights Cloud: Familiarity with cloud platforms (preferably Azure) Data Platforms: Experience with Databricks, Snowflake, or similar data platforms Database Skills: Knowledge of relational databases, with proficiency in SQL. Big Data: Experience using Apache Spark Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability Version Control: Experience with Gitlab or equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have : Experience with Docker and Kubernetes Experience with document and graph databases Ability to travel abroad twice a year for an on-site workshops","[{""min"": 18117, ""max"": 30584, ""type"": ""Net per month - B2B""}, {""min"": 14581, ""max"": 25881, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Junior,Permanent,Hybrid,423,Junior Data Analyst,CRIF,"DESCRIPTION: Prepare and deliver daily, weekly, monthly and quarterly reports, operational metrics, and other relevant business data, ensuring accuracy, consistency, and timeliness. Develop a deep understanding of the business context surrounding the data, ensuring that the analysis is aligned with business goals and priorities. Collaborate with key stakeholders to ensure that reports meet their specific needs and provide actionable insights. Proactively research and investigate emerging data trends, patterns, or anomalies that could impact business operations or market products. Identify opportunities to enrich CRIF products offered on the market by uncovering insights or patterns that can inform product development or optimization. Assist senior analysts in enhancing reporting processes, focusing on improving data visualization, accessibility and reporting efficiency. Document reporting processes, methodologies, and data sources for reference, ensuring clear communication and knowledge sharing within the team. Basic analytical skills ‚Äì understanding of methodologies and ability to work with data Familiarity with SQL, Python, and MS Excel ‚Äì comfortable with basic queries, scripting, and data manipulation Basic knowledge of data visualization tools (e.g., Plotly) and data analysis libraries (e.g., Pandas) ‚Äì eagerness to learn and apply these tools effectively Awareness of personal data protection issues Good communication and presentation skills in Polish and English languages No need to worry if you're lacking experience in some areas ‚Äì we'll provide training to help you get up to speed! A stable job, with performance development plan in a fast growing environment Benefit package (private medical care, Multisport, meal/restaurants discounts etc.) Fresh different fruits in the office every day Language classes (English) Free car and bike parking Trainings package (soft-skills and tech-topics) Knowledge-exchange environment (Lunch & Learn Initiatives) Team-building and charity initiatives",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent or B2B,Remote,424,Middle Configuration Engineer,GR8 Tech,"GR8 Tech is a leading B2B provider of iGaming solutions that empowers operators to grow, lead, and win. We deliver high-impact, full-cycle tech solutions designed to scale. From seamless integration and expert consulting to long-term operational support, our platform powers millions of active players and drives real business growth. It‚Äôs more than just a product ‚Äî it‚Äôs the iGaming Platform for Champions, built for those who play to lead. We know the game and how to take it to the next level. With 1000+ talented professionals on board, we don't just build tech ‚Äî we build success stories for iGaming operators all over the world. Our ambition drives us, our people make it real. Join us and be part of building champion-level success! Contributing to the technical launch of new brands and operators by configuring components required to implement new iGaming platform solutions; Analyzing and implementing configuration change requests and handling support inquiries; Collaborating closely with Product teams and Quality Assurance engineers to ensure timely and high-quality brand launches; Performing configuration updates across all parts of the Product Platform (backend, web, and native applications); Executing testing of new brands, components, and product features to validate readiness for production; Developing expertise in configuration management by learning and applying new setup processes and platform updates; Providing consultation and guidance to team members and customer representatives on platform configuration topics; Reporting progress regularly to the Brand Launch Coordinator or Client Delivery Manager. At least 2 years of proven experience in manual software testing; Hands-on experience in mobile application testing; Experience in API testing using tools like Postman; Solid understanding of JSON and XML data formats; Practical experience with SQL for data validation and analysis; Familiarity with Postman or similar REST API client tools; Working knowledge of Git/GitLab and basic understanding of CI/CD processes; Experience using JIRA and Confluence in a project environment; Ability to gather and analyze logs using tools such as Kibana or OpenSearch; Strong communication skills and the ability to work cross-functionally; High attention to detail and strong sense of responsibility; Ability to work effectively in a multitasking environment; English proficiency at Intermediate+ level or higher; Proficiency in Ukrainian or Russian. Experience with configuration management tools (e.g., GrowthBook, Firebase) and content management systems (e.g., Strapi); Understanding of cloud computing concepts; Ability to read and interpret software code (JavaScript or C#). Benefits Cafeteria An annual fixed budget that you can use based on your needs and lifestyle. You decide how to allocate it: Sports ‚Äì gym, yoga, or any activity to keep you active; Medical ‚Äì insurance and wellness services; Mental health‚Äì therapy or coaching support; Home office ‚Äì ergonomic furniture, gadgets, and tools; Languages ‚Äì courses to improve or learn new skills. Work-life Parental support with paid maternity/paternity leave and monthly childcare allowance; 20+ vacation days, unlimited sick leave, and emergency time off; Remote-first setup with full tech support and coworking compensation; Regular team events ‚Äì online, offline, and offsite; Learning culture with internal courses, career development programs, and real growth opportunities. Our Culture & Core Values GR8 Tech culture is how we win. Behind every bold idea and breakthrough is a foundation of trust, ownership, and a growth mindset. We move fast, stay curious, and always keep it real, with open feedback, room to experiment, and a team that‚Äôs got your back. FUELLED BY TRUST: we‚Äôre open, honest, and have each other‚Äôs backs; OWN YOUR GAME: we take initiative and own what we do; ACCELER8: we move fast, focus smart, and keep it simple; CHALLENGE ACCEPTED: we grow through challenges and stay curious; BULLETPROOF: we‚Äôre resilient, ready, and always have a plan. To keep things efficient, please apply only for roles that closely match your experience.",[],Unclassified,Unclassified
Full-time,Mid,B2B,Remote,425,Data Engineer (Azure & Databricks),in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Dla naszego Klienta poszukujemy do≈õwiadczonych Data Engineer√≥w, kt√≥rzy chcƒÖ pracowaƒá w ≈õrodowisku opartym o Azure i Databricks. Szukamy os√≥b, kt√≥re potrafiƒÖ projektowaƒá wydajne modele danych i majƒÖ do≈õwiadczenie w przetwarzaniu du≈ºych zbior√≥w danych. Zakres roli: Projektowanie, implementacja i utrzymanie skalowalnych proces√≥w przetwarzania i integracji danych (ETL/ELT) w ≈õrodowisku Azure. Praca z du≈ºymi zbiorami danych z wykorzystaniem Apache Spark i PySpark w Azure Databricks. Tworzenie i rozw√≥j warstw danych w oparciu o Delta Lake ‚Äì z uwzglƒôdnieniem dobrych praktyk modelowania danych i kontroli jako≈õci. Udzia≈Ç w projektowaniu struktur danych pod kƒÖtem wydajno≈õci, przejrzysto≈õci i u≈ºyteczno≈õci biznesowej. Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç z wykorzystaniem Azure Data Factory i Azure Functions. Optymalizacja kodu oraz monitorowanie i rozwiƒÖzywanie problem√≥w wydajno≈õciowych. Praca z zespo≈Çami analitycznymi, data science i biznesowymi w celu zapewnienia dostƒôpno≈õci danych w odpowiedniej formie i czasie. Oczekujemy: Minimum 3 lata do≈õwiadczenia w roli Data Engineera lub pokrewnej. Bardzo dobra znajomo≈õƒá Azure Databricks, Spark i PySpark. Do≈õwiadczenie z Delta Lake oraz modelowaniem danych i tuningiem wydajno≈õci. Znajomo≈õƒá ADF, Azure Functions. Umiejƒôtno≈õƒá pracy z formatami danych: Parquet, Avro, JSON. Praktyczna znajomo≈õƒá Python i SQL. Do≈õwiadczenie w pracy z Git. Umiejƒôtno≈õƒá pracy z du≈ºymi wolumenami danych i do≈õwiadczenie w tworzeniu wydajnych rozwiƒÖza≈Ñ w ≈õrodowiskach chmurowych. Zrozumienie architektury przetwarzania danych i zasad projektowania system√≥w odpornych i ≈Çatwo skalowalnych. Mile widziane: Do≈õwiadczenie w pracy w oparciu o metodyki Agile (np. Scrum). Znajomo≈õƒá narzƒôdzi do monitorowania i automatyzacji proces√≥w (CI/CD) Praktyka w pracy z danymi wra≈ºliwymi lub w ≈õrodowiskach regulowanych (np. sektor finansowy, medyczny) Proponujemy: Rozw√≥j kariery w miƒôdzynarodowych projektach, z wykorzystaniem nowoczesnych narzƒôdzi i technologii. Elastyczny model pracy: mo≈ºliwo≈õƒá 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejƒôtno≈õci i do≈õwiadczenia. Wsp√≥≈Çpracƒô w zgranym zespole, kt√≥ry ceni wymianƒô wiedzy oraz otwartƒÖ komunikacjƒô. Realny wp≈Çyw na projekty oraz wdra≈ºane rozwiƒÖzania. Wynagrodzenie na poziomie 20 000 - 30 000 PLN/miesiƒôcznie w zale≈ºno≈õci od do≈õwiadczenia. üí°Nie przegap dopasowanych ofert! Mamy wiele rekrutacji, a nowe projekty pojawiajƒÖ siƒô na bie≈ºƒÖco. Pamiƒôtaj, ≈ºe zaznaczajƒÖc zgodƒô na przetwarzanie danych w celu przysz≈Çych proces√≥w , bƒôdziemy mogli zaprosiƒá Ciƒô do udzia≈Çu w kolejnych procesach, dopasowanych do Twojego do≈õwiadczenia i oczekiwa≈Ñ! PS Zamierzamy kontaktowaƒá siƒô z TobƒÖ wy≈ÇƒÖcznie wtedy, kiedy bƒôdziemy dla Ciebie ciekawe projekty, bez tej zgody nie bƒôdzie to mo≈ºliwe. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 17000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,426,Senior DBA Consultant,Lumicode Sp. z o.o. (Pentacomp Group),"Kim jeste≈õmy Lumicode Sp. z o.o. nale≈ºy do Grupy Pentacomp , kt√≥ra jest producentem rozwiƒÖza≈Ñ informatycznych i dostawcƒÖ profesjonalnych us≈Çug IT dla du≈ºych przedsiƒôbiorstw i sektora publicznego. Jako Pentacomp tworzymy rozwiƒÖzania IT ≈ÇƒÖczƒÖce innowacyjno≈õƒá z latami do≈õwiadcze≈Ñ - a ich mamy ca≈Çkiem sporo. Istniejemy na rynku prawie 30 lat i mo≈ºemy pochwaliƒá siƒô wieloma zrealizowanymi projektami Aktualnie do projektu naszego Klienta poszukujemy Senior DBA Consultanta. Oferujemy: Praca hybrydowa (Warszawa, 2 razy w tygodniu w biurze) Stawka do 190 pln/h netto + VAT B2B ; Dofinansowanie do karty sportowej oraz mo≈ºliwo≈õƒá skorzystania z prywatnej opieki zdrowotnej; 10+ lat do≈õwiadczenia w zarzƒÖdzaniu bazƒÖ danych Oracle w wersji od 12c do 19c (RAC, Data guard) Do≈õwiadczenie z systemem operacyjnym Linux, Narzƒôdzia do zarzƒÖdzania Oracle (Data Guard, RMAN, Data pump), Znajomo≈õƒá zasad projektowania architektury, Obs≈Çuga r√≥≈ºnych typ√≥w problem√≥w Data Guard, Praktyczne do≈õwiadczenie ORACLE w produkcyjnym ≈õrodowisku RAC (ASM) (19c), Dobra znajomo≈õƒá Oracle Golden gate - jasne zrozumienie przep≈Çywu procesu, bazy danych Downstream, OGG start stop, Dobra znajomo≈õƒá Storage, Dobra znajomo≈õƒá rozwiƒÖzywania problem√≥w FSFO Dobra znajomo≈õƒá procesu - tj. zarzƒÖdzanie zmianƒÖ, zarzƒÖdzanie incydentami, Wiedza na temat obs≈Çugi Oracle SR, Wiedza na temat architektury OEM i zarzƒÖdzania macierzami OEM. Przygotowanie ≈õrodowiska prePROD w celu w≈ÇƒÖczenia FSFO Synchronizacja DataGuard w przypadku jakichkolwiek problem√≥w / b≈Çƒôd√≥w w DG, Przebudowa Standby zgodnie z potrzebami, Migracja starego PDB do nowego kontenera Aktywacja FSFO RozwiƒÖzywanie problem√≥w z FSFO Wykonywanie test√≥w na prePROD: Switchover i Failover","[{""min"": 140, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,427,MicroStrategy Developer,Calimala.ai,"Calimala.ai is seeking a highly skilled MicroStrategy Developer with over 5 years of proven experience. In this role, you'll play a pivotal part in designing, developing, and optimizing advanced MicroStrategy reports, dashboards, and datasets. Your expertise will drive business insights for our clients, particularly within the Telecom industry, ensuring that our reporting solutions are both efficient and scalable. Responsibilities Develop and optimize a wide range of MicroStrategy reports, dashboards, and datasets. Collaborate with data engineers and analysts to translate business requirements into actionable insights. Design and implement robust schema structures to support high-performance reporting. Integrate ETL processes to ensure comprehensive and reliable data feeds. Manage performance tuning and remediation efforts to maintain system efficiency. Requirements Candidates must have a demonstrated background in business intelligence with specific expertise in MicroStrategy BI and related technologies. The ideal candidate will combine technical proficiency with strong problem-solving and communication skills. Experience in the Telecom industry is highly preferred, as is a commitment to continuous improvement and excellence in data-driven environments. Benefits Competitive salary package with performance-based incentives. The position is fully remote Opportunities for career advancement within a rapidly growing company. Continuous learning and professional development resources. A collaborative work culture that fosters innovation and teamwork. ÔªøWhy Apply? This is an excellent opportunity to contribute to a dynamic team in a company that values technical innovation and professional growth. If you thrive in a challenging environment and are excited to apply your skills to impactful projects, we invite you to apply today.","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,Permanent or B2B,Remote,428,Senior Data Engineer,Transition Technologies MS,"Your responsibilities: Data processing, Data Quality Assurance, building database pipeline, Implementing Business Logic on the Snowflake pipes and streams Data Quality checks with DBT Exploratory Data Analysis Statistical Methods, decision Trees Models Cross-Validation We are looking for you, if you have: Snowflake ‚Äì Must DBT -Must Python (at least one of libs: Pandas, Matplotlib, Scikit) ‚Äì Must Jupyter ‚Äì Should 4+ years of working with programming language focused on data pipelines,eg. Python or R 4+ years of experience working with SQL 3+ years of experience in data pipelines maintenance 3+ years of experience with different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.) 3+ years of experience in working in data architecture concepts (in any of following areas data modeling, metadata mng., workflow management, ETL/ELT, real-time streaming, data quality, distributed systems) 3+ years of cloud technologies with emphasis on data pipelines (Airflow, Glue, Dataflow ‚Äì but also other smart solutions of handling data in the cloud ‚Äì elastic, redshift, bigquery, lambda, s3, EBS etc.) 4+ years of experience working with SQL 1+ years of experience in Java and/or Scala Very good knowledge of relational databases (optional) Very good knowledge of data serialization languages such as JSON, XML, YAML Excellent knowledge of Git, Gitflow and DevOps tools (e.g. Docker, Bamboo, Jenkins, Terraform Capability to conduct performance analysis, troubleshooting and remediation (optional) Excellent knowledge of Unix Pharma data formats is a big plus (SDTM) Other Needs for Resource Consideration: Ability to Travel to Warsaw or Pozna≈Ñ 3 days per Quartier Agile Mindset and experience in the Scrum Teams We offer: Interesting and challenging projects Flexible working hours Friendly, non-corporate atmosphere Stable working conditions (CoE or B2B) Possibility for self-development and promotion in the company Rich benefits package We reserve the right to contact the selected candidates.",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,429,Senior Data Engineer with Databricks,Crestt,"Cze≈õƒá! Poszukujemy osoby na stanowisko Senior Data Engineer , kt√≥ra do≈ÇƒÖczy do zespo≈Çu naszego Klienta zajmujƒÖcego siƒô projektowaniem i rozwijaniem nowoczesnych rozwiƒÖza≈Ñ w obszarze Data Lakehouse, Business Intelligence oraz zaawansowanej analityki w chmurze. Wsp√≥≈ÇpracujƒÖc w miƒôdzynarodowym ≈õrodowisku, bƒôdziesz mia≈Ç(a) okazjƒô pracowaƒá z najnowszymi technologiami w tej dziedzinie. Lokalizacja : praca g≈Ç√≥wnie zdalna (1x w miesiƒÖcu spotkanie w biurze w Warszawie) Wide≈Çki: B2B 160-200 pln netto+vat/h UoP 26-30 tys. brutto/mies. Wymagania: Bieg≈Ço≈õƒá w SQL oraz Pythonie (min. 5 lat do≈õwiadczenia) Co najmniej 2-letnie do≈õwiadczenie w pracy z Databricks Do≈õwiadczenie w pracy w ≈õrodowisku chmurowym (preferowany Azure) Minimum 5-letnie do≈õwiadczenie w projektowaniu oraz implementacji rozwiƒÖza≈Ñ klasy BI, ETL/ELT, Data Warehouse, Data Lake, Big Data oraz OLAP Praktyczna znajomo≈õƒá zar√≥wno relacyjnych, jak i nierelacyjnych Do≈õwiadczenie z narzƒôdziami typu Apache Airflow, dbt, Apache Kafka, Flink, Azure Data Factory, Hadoop/CDP Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z zarzƒÖdzaniem danymi, jako≈õciƒÖ danych oraz przetwarzaniem wsadowym i strumieniowym Umiejƒôtno≈õƒá stosowania wzorc√≥w architektonicznych w obszarze danych (Data Mesh, Data Vault, Modelowanie wymiarowe, Medallion Architecture, Lambda/Kappa Architectures) Praktyczna znajomo≈õƒá system√≥w kontroli wersji (Bitbucket, GitHub, GitLab) Wysoko rozwiniƒôte umiejƒôtno≈õci komunikacyjne, otwarto≈õƒá na bezpo≈õredni kontakt z Klientem ko≈Ñcowym Certyfikaty z Databricks lub Azure bƒôdƒÖ dodatkowym atutem Zakres obowiƒÖzk√≥w: Projektowanie i wdra≈ºanie nowych rozwiƒÖza≈Ñ oraz wprowadzanie usprawnie≈Ñ w istniejƒÖcych platformach danych Udzia≈Ç w rozwoju platform danych i proces√≥w ETL/ELT, optymalizacja przetwarzania du≈ºych zbior√≥w danych zgodnie z najlepszymi praktykami in≈ºynierii danych Standaryzacja i usprawnianie proces√≥w technicznych ‚Äì implementacja standard√≥w kodowania, testowania i zarzƒÖdzania dokumentacjƒÖ Dbanie o jako≈õƒá kodu i zgodno≈õƒá z przyjƒôtymi standardami ‚Äì przeprowadzanie regularnych code review Aktywna wsp√≥≈Çpraca z innymi ekspertami technologicznymi, w celu doskonalenia proces√≥w oraz identyfikacji nowych wyzwa≈Ñ technologicznych Mentoring i wsparcie zespo≈Çu w zakresie projektowania rozwiƒÖza≈Ñ, optymalizacji proces√≥w i wdra≈ºania najlepszych praktyk Klient oferuje: Udzia≈Ç w miƒôdzynarodowych projektach opartych na najnowocze≈õniejszych technologiach chmurowych Pokrycie koszt√≥w certyfikacji (Microsoft, AWS, Databricks) 60 p≈Çatnych godzin rocznie na naukƒô i rozw√≥j Mo≈ºliwo≈õƒá wyboru miƒôdzy pracƒÖ zdalnƒÖ a spotkaniami w biurze Indywidualnie dopasowane benefity: prywatna opieka medyczna, dofinansowanie karty sportowej, kursy jƒôzykowe, premie roczne i medialne oraz bonus za polecenie nowego pracownika (do 15 000 PLN)","[{""min"": 25600, ""max"": 32000, ""type"": ""Net per month - B2B""}, {""min"": 26000, ""max"": 30000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,430,Senior Power Platform Developer,Onwelo Sp. z o.o.,"Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, powiƒôkszy≈Ça zesp√≥≈Ç do kilkaset os√≥b, a tak≈ºe posiada biura w sze≈õciu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. O projekcie: Projekt dotyczy rozwoju i implementacji rozwiƒÖza≈Ñ opartych na Microsoft Power Platform oraz us≈Çugach Azure. Celem jest stworzenie zintegrowanego ≈õrodowiska wspierajƒÖcego procesy wewnƒôtrzne klienta, z naciskiem na automatyzacjƒô, raportowanie i poprawƒô efektywno≈õci operacyjnej. Z nami bƒôdziesz: Tworzyƒá rozwiƒÖzania w Power Platform Pracowaƒá w ≈õrodowisku Azure Projektowaƒá architekturƒô i szacowaƒá czas realizacji Wsp√≥≈Çpracowaƒá z biznesem Doradzaƒá technologicznie i wspieraƒá rozw√≥j zespo≈Çu Czekamy na Ciebie, je≈õli: Masz min. 3 lata do≈õwiadczenia w pracy z Power Platform Znasz Power BI, Power Apps, Power Automate, Copilot Umiesz analizowaƒá wymagania biznesowe Czujesz odpowiedzialno≈õƒá za realizowane zadania Mile widziane: JavaScript/TypeScript, C#, React) Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Potrzebujesz pracowaƒá hybrydowo? Mo≈ºemy zaproponowaƒá 4 dni pracy zdalnej i 1 dzie≈Ñ w tygodniu pracy z biura w Warszawie Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 800, ""max"": 1100, ""type"": ""Net per day - B2B""}]",Unclassified,Unclassified
Full-time,Senior,B2B,Remote,431,Data Engineer,emagine Polska,"Information about the project: Rate: depending on expectations Location: Cracow - hybrid/remote Industry: banking We are seeking a Data Engineer to join our PSM Engineering team. The ideal candidate will be innovative and possess a strong desire for continuous improvement in engineering best practices. You will have deep technical expertise in various technologies and a passion for learning. Your experience in delivering software/technology projects using Agile methodologies is crucial. Candidates should demonstrate their contributions to critical business applications, ideally customer-facing, and effectively communicate complex ideas to non-expert audiences. Additionally, familiarity with emerging technologies in finance will be highly regarded. Main Responsibilities: You will be responsible for creating data pipelines and supporting the data engineering lifecycle effectively. Key responsibilities include: ‚Ä¢ Develop and maintain robust data pipelines for data ingestion, transformation, and serving.‚Ä¢ Apply modern software engineering principles to deliver clean, tested applications.‚Ä¢ Collaborate with cross-functional teams to identify and solve engineering problems.‚Ä¢ Migrate on-premise solutions to cloud ecosystems as required.‚Ä¢ Utilize strong programming skills in Python and related technologies.‚Ä¢ Ensure effective data modeling and schema design practices.‚Ä¢ Manage CI/CD pipelines using tools like Jenkins and GitHub Actions.‚Ä¢ Experiment with emerging technologies and methodologies in a fast-paced environment. Key Requirements: ‚Ä¢ Extensive experience in the Data Engineering Lifecycle.‚Ä¢ Strong proficiency in Hadoop and Cloudera.‚Ä¢ Solid experience with AWS, Azure, or GCP, with a preference for GCP.‚Ä¢ Proficient in Python and Pyspark, along with other languages like Scala/Java.‚Ä¢ Familiar with Big Data technologies (Hadoop, HDFS, HIVE, Spark, etc.).‚Ä¢ Knowledge in data lake formation and data warehousing principles.‚Ä¢ Understanding of file formats such as Parquet, ORC, and Avro.‚Ä¢ Experience with SQL and building data analytics.‚Ä¢ Proven ability to use version control systems like Git.‚Ä¢ Understanding of CI/CD principles. Nice to Have: ‚Ä¢ Experience developing near real-time event streaming pipelines using Kafka or similar tools.‚Ä¢ Familiarity with MLOps and maintaining ML models.‚Ä¢ Understanding of NoSQL databases and their trade-offs compared to SQL.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,432,Data Engineer (Azure),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. bran≈ºy automotive. Wykorzystywany stos technologiczny: Azure, Databricks, PySpark, Azure Data Factory, Azure Synapse, Power BI, Spark, Python, MongoDB, Azure Functions, SQL Server, rozw√≥j nowych funkcjonalno≈õci oraz utrzymanie aplikacji, wsparcie u≈ºytkownik√≥w wewnƒôtrznych oraz proces√≥w biznesowych poprzez rozwiƒÖzywanie problem√≥w zwiƒÖzanych z dzia≈Çaniem system√≥w i jako≈õciƒÖ danych, zapewnienie wydajnego i niezawodnego dzia≈Çania system√≥w, stawka do 140z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz minimum 3 lata do≈õwiadczenia w pracy z danymi i in≈ºynieriƒÖ oprogramowania, posiadasz tytu≈Ç magistra z informatyki, in≈ºynierii danych lub pokrewnej dziedziny, pracujesz z us≈Çugami Azure, takimi jak Azure Data Factory, Azure Synapse, Azure Functions, masz praktyczne do≈õwiadczenie z Databricks i PySpark, bardzo dobrze znasz bazy danych (np. SQL Server, Netezza, MongoDB), znasz narzƒôdzia i technologie: Power BI, Spark, Python, biegle komunikujesz siƒô w jƒôzyku angielskim (min. B2). Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 21840, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,433,Data Modeller,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie z bran≈ºy finansowej/windykacyjnej, kt√≥rego celem jest rozbudowa platformy danych i transformacja proces√≥w danych w organizacji. Bƒôdziesz mieƒá realny wp≈Çyw na rozw√≥j nowoczesnej platformy danych , wspierajƒÖcej analitykƒô i zarzƒÖdzanie danymi w ca≈Çej organizacji, wykorzystywany stos technologiczny w projekcie: Azure (ADF, Databricks), SQL, dbdiagram.io, Azure Purview , modelowanie danych w podej≈õciu Domain-Driven Design (DDD) ‚Äì od warstwy logicznej po fizycznƒÖ, tworzenie i rozw√≥j modeli danych hurtowni (DWH) w chmurze, opracowanie i dokumentowanie kontrakt√≥w danych ‚Äì definiowanie wymaga≈Ñ danych wobec system√≥w ≈∫r√≥d≈Çowych, praca z danymi z r√≥≈ºnych ≈∫r√≥de≈Ç: bazy danych (querying), API, event streaming, projektowanie i implementacja proces√≥w ELT w ≈õrodowisku Azure / Databricks (Bronze/Silver/Gold), wsp√≥≈Çpraca z zespo≈Çami domenowymi, w≈Ça≈õcicielami proces√≥w, architektami i ≈∫r√≥d≈Çowymi zespo≈Çami IT, udzia≈Ç w budowie sp√≥jnego glosariusza danych oraz integracji z Data Governance i narzƒôdziami klasy Purview, praca 100% remote, stawka do 220z≈Ç/h NET + VAT Ta oferta jest dla Ciebie, je≈õli: posiadasz min. 4‚Äì5 lat do≈õwiadczenia w obszarze modelowania danych / DWH / Data Engineeringu, masz praktycznƒÖ znajomo≈õƒá podej≈õcia DDD oraz umiejƒôtno≈õƒá tworzenia logicznych i fizycznych modeli danych, znasz rozwiƒÖzania chmurowe ‚Äì Azure (ADF, Databricks) lub pokrewne (GCP, AWS), bardzo dobrze znasz SQL i masz do≈õwiadczenie w procesach ELT/ETL, potrafisz tworzyƒá dokumentacjƒô: glosariusze danych, specyfikacje techniczne, wymagania kontraktowe, masz do≈õwiadczenie w pracy z danymi z system√≥w ≈∫r√≥d≈Çowych i ich mapowaniu do struktur docelowych, znasz zasady Data Governance, Data Lineage i Data Quality, sprawnie komunikujesz siƒô w j. angielskim (min. B2), dodatkowo mile widziane: znajomo≈õƒá Azure Purview, dbdiagram.io, Power BI, SSIS, Python/PySpark. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 33600, ""max"": 36960, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,Any,Remote,434,Data Quality & BI Tester,Billennium,"About us Billennium is a global technology company with more than 20 years of experience. We foster innovation, encourage growth, and value collaboration. Join us and help shape the future of technology. About the client Our client is a leading retail organization operating across EMEA. As part of their digital-transformation and data-modernization strategy, we support their Business Intelligence team with advanced data validation and reporting. Tech stack SQL / T-SQL | Power BI & DAX | Azure Synapse & Databricks | Microsoft SQL Server (SSIS) | ETL / ELT tools | JIRA Your role Design and run data test cases across data-warehouse layers and BI reports Validate Power BI dashboards, semantic models, and DAX logic Write SQL scripts to verify data consistency, calculations, and ETL logic Identify, document, and track defects while collaborating with BI and engineering teams Safeguard data quality during report migrations (e.g., SSRS ‚Üí Power BI) What we‚Äôre looking for 3 + years in data testing, QA, or BI environments Strong SQL skills and experience with data warehouses and ETL processes Good grasp of Power BI (DAX, visuals, model validation) Attention to detail and solid test-plan documentation skills Business-level English and clear communication Nice to have Experience with ETL test-automation frameworks Knowledge of Azure tools (Databricks, Synapse) and SSIS Familiarity with Agile methods and JIRA Basic scripting for automated data tests A proactive, quality-first mindset Perks & benefits Udemy for Business, private medical care, Multisport, veterinary package, language lessons, shopping vouchers Flexible hours and remote/hybrid work options Career-growth paths via our partnerships with Microsoft, AWS, Snowflake, Salesforce & more Diverse, international team and innovative environment Hackathons, CSR initiatives, and team events (incl. annual Mazury retreat) Welcome pack on day one Recruitment process HR call Technical interview Client interview Offer / feedback Sounds interesting? Click ‚ÄúApply‚Äù and let‚Äôs talk! üìû",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,435,Senior Data Scientist,hubQuest,"We are a team of tech enthusiasts on a mission to bring together the best minds in IT services and analytics. Our goal? To create cutting-edge IT and Analytical Hubs that empower our partners to become truly data-driven organizations. Tackle real-world challenges with advanced analytics and machine learning Lead impactful projects that influence decision-making across global markets Flexible remote or hybrid work model (with a modern office in Warsaw) Join a diverse and experienced international team No red tape ‚Äì just real tech challenges, autonomy, and ownership Enjoy private medical care, Multisport, access to online learning platforms and certifications Supportive, collaborative, and relaxed work culture About the Team What You‚Äôll Do Take technical ownership of an end-to-end data science project ‚Äì from business understanding to deployment Develop predictive and prescriptive models that support strategic and operational goals Design, test, and implement scalable data pipelines and modeling workflows (Python, PySpark) Work closely with stakeholders to identify opportunities for impact through data science Lead experimentation, model validation, and performance monitoring in production Contribute to the design of robust and maintainable analytics architecture using Azure cloud services What We‚Äôre Looking For Hands-on experience with the Azure ecosystem (e.g., Azure Databricks, Azure ML, Azure Data Factory, Azure DevOps) Advanced Python and PySpark skills, including writing production-level code Experience with large-scale data processing and analytics Good understanding of CI/CD pipelines and deployment automation Strong background in building and deploying machine learning models 5+ years of data science experience in production environments 2+ years in senior or lead roles with proven project ownership Strong portfolio of deployed ML models used in production at scale Ability to independently lead projects from conception to launch Excellent communication and collaboration skills across technical and non-technical teams Adaptability to new tools, processes, and environments Self-driven learner with a proactive approach to skill development Ready to take ownership and build data science solutions that drive global decisions? Join us and help shape the future of data-driven transformation. Please add to your CV the following clause: "" I hereby agree to the processing of my personal data included in my job offer by hubQuest sp√≥≈Çka z ograniczonƒÖ odpowiedzialno≈õciƒÖ located in Warsaw for the purpose of the current recruitment process.‚Äù If you want to be considered in the future recruitment processes please add the following statement: "" I also agree to the processing of my personal data for the purpose of future recruitment processes.‚Äù","[{""min"": 28500, ""max"": 37000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Hybrid,436,Senior Data Engineer (sta≈Çe zatrudnienie - sektor bankowy),DEVTALENTS Sp. z o.o.,"Senior Data Engineer | Tworzenie skalowalnych rozwiƒÖza≈Ñ chmurowych O DEVTALENTS oraz model zatrudnienia W DEVTALENTS ≈ÇƒÖczymy wybitnych specjalist√≥w IT z ambitnymi projektami, stosujƒÖc nasz unikalny model wsp√≥≈Çpracy ‚ÄûBuild-Operate-Transfer‚Äù. Jako cz≈Çonek zespo≈Çu DEVTALENTS bƒôdziesz pracowaƒá nad innowacyjnymi rozwiƒÖzaniami dla naszych klient√≥w, majƒÖc jasno okre≈õlonƒÖ ≈õcie≈ºkƒô prowadzƒÖcƒÖ do bezpo≈õredniego zatrudnienia u klienta. Prowadzenie projektowania, rozwoju i utrzymania potok√≥w danych oraz proces√≥w ETL/ELT obs≈ÇugujƒÖcych du≈ºe, zr√≥≈ºnicowane zbiory danych. Optymalizacja proces√≥w pobierania, transformacji i dostarczania danych z wykorzystaniem SQL, PySpark i Pythona. Wykorzystywanie framework√≥w takich jak Apache Airflow, AWS Glue, Kafka i Redshift w celu zapewnienia wydajnej orkiestracji danych, przetwarzania wsadowego/strumieniowego i wysokiej wydajno≈õci analiz. Wdra≈ºanie najlepszych praktyk w zakresie kontroli wersji (Git), infrastruktury jako kodu (Terraform, Ansible) oraz pipeline‚Äô√≥w CI/CD, aby zapewniƒá solidne, powtarzalne i skalowalne wdro≈ºenia. ≈öcis≈Ça wsp√≥≈Çpraca z zespo≈Çami Data Science, Analityki i Product Management przy projektowaniu modeli danych i architektur wspierajƒÖcych cele biznesowe. Monitorowanie, debugowanie i optymalizacja potok√≥w ETL, zapewnianie wysokiej niezawodno≈õci, niskich op√≥≈∫nie≈Ñ i efektywno≈õci kosztowej. Mentoring in≈ºynier√≥w na poziomie ≈õrednim i juniorskim oraz budowanie kultury dzielenia siƒô wiedzƒÖ, ciƒÖg≈Çego doskonalenia i innowacji. Du≈ºa bieg≈Ço≈õƒá w SQL, PySpark i Pythonie w zakresie transformacji danych oraz tworzenia skalowalnych potok√≥w danych (minimum 6 lat do≈õwiadczenia komercyjnego). Praktyczne do≈õwiadczenie w pracy z Apache Airflow, AWS Glue, Kafka i Redshift. Znajomo≈õƒá pracy z du≈ºymi wolumenami danych strukturalnych i czƒô≈õciowo strukturalnych. Mile widziane do≈õwiadczenie z DBT. Bieg≈Ço≈õƒá w korzystaniu z Gita do kontroli wersji. Airflow jest kluczowy do orkiestracji proces√≥w. Solidne do≈õwiadczenie w pracy z AWS (Lambda, S3, CloudWatch, SNS/SQS, Kinesis) oraz znajomo≈õƒá architektur serverless. Do≈õwiadczenie w automatyzacji i zarzƒÖdzaniu infrastrukturƒÖ za pomocƒÖ Terraform i Ansible. Umiejƒôtno≈õci w zakresie monitorowania potok√≥w ETL, rozwiƒÖzywania problem√≥w z wydajno≈õciƒÖ oraz utrzymywania wysokiej niezawodno≈õci operacyjnej. Znajomo≈õƒá proces√≥w CI/CD w celu automatyzacji test√≥w, wdro≈ºe≈Ñ i wersjonowania potok√≥w danych. Umiejƒôtno≈õƒá projektowania rozproszonych system√≥w, kt√≥re skalujƒÖ siƒô horyzontalnie dla du≈ºych wolumen√≥w danych. Wiedza o architekturach przetwarzania w czasie rzeczywistym (Lambda) i wsadowym (Kappa) bƒôdzie dodatkowym atutem. Do≈õwiadczenie w tworzeniu API (REST, GraphQL, OpenAPI, FastAPI) do wymiany danych. Znajomo≈õƒá zasad Data Mesh i narzƒôdzi self-service do danych bƒôdzie du≈ºym plusem. Wcze≈õniejsze do≈õwiadczenie w budowaniu skalowalnych platform danych i przetwarzaniu du≈ºych zbior√≥w danych jest wysoko cenione. Wy≈ºsze wykszta≈Çcenie w zakresie informatyki lub kierunk√≥w pokrewnych. Znajomo≈õƒá jƒôzyka angielskiego na poziomie co najmniej B2. Proaktywne podej≈õcie do rozwiƒÖzywania problem√≥w, pasja do podejmowania decyzji w oparciu o dane i nieustannego doskonalenia. Doskona≈Çe umiejƒôtno≈õci komunikacyjne pozwalajƒÖce przek≈Çadaƒá z≈Ço≈ºone koncepcje in≈ºynierii danych na zrozumia≈Çy jƒôzyk dla odbiorc√≥w technicznych i nietechnicznych. Umiejƒôtno≈õƒá wsp√≥≈Çpracy w ≈õrodowisku wielofunkcyjnym i zwinnym oraz gotowo≈õƒá do wspierania i mentorowania cz≈Çonk√≥w zespo≈Çu. Chƒôƒá ≈õledzenia trend√≥w bran≈ºowych, eksperymentowania z nowymi technologiami i wdra≈ºania innowacji w praktykach in≈ºynierii danych.","[{""min"": 25000, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,437,Data Scientist,Onwelo Sp. z o.o.,"Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, specjalizujƒÖca siƒô w budowie innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z r√≥≈ºnych sektor√≥w na ca≈Çym ≈õwiecie. Firma oferuje kompleksowe us≈Çugi z zakresu tworzenia, rozwoju i utrzymania oprogramowania, oraz silne wsparcie kompetencyjne. Do naszego zespo≈Çu Data & Analytics poszukujemy Data Scientista, kt√≥ry bƒôdzie pracowa≈Ç nad budowƒÖ i rozwojem modeli analitycznych oraz uczenia maszynowego dla klient√≥w z r√≥≈ºnych bran≈º ‚Äì zar√≥wno z Polski, jak i z rynk√≥w zagranicznych. Bƒôdziesz wsp√≥≈Çpracowaƒá z zespo≈Çami analitycznymi i biznesowymi, wspieraƒá podejmowanie decyzji na podstawie danych i tworzyƒá rozwiƒÖzania, kt√≥re realnie wp≈ÇywajƒÖ na dzia≈Çalno≈õƒá naszych klient√≥w. Przeprowadzaƒá eksploracyjnƒÖ analizƒô danych (EDA) Poszukiwaƒá zale≈ºno≈õci, wzorc√≥w i insight√≥w w danych biznesowych Budowaƒá modele klasyfikacyjne, regresyjne i klasteryzacyjne Przeprowadzaƒá feature engineering i przygotowywaƒá dane do modelowania Wsp√≥≈Çpracowaƒá z zespo≈Çami analitycznymi, technologicznymi i biznesowymi Wizualizowaƒá wyniki analiz i przygotowywaƒá raporty oraz prezentacje Masz minimum 2-letnie do≈õwiadczenie w pracy jako Data Scientist lub na podobnym stanowisku Bardzo dobrze znasz Python a i pracowa≈Çe≈õ z bibliotekami: pandas, scikit-learn, numpy, matplotlib, seaborn Swobodnie pracujesz z danymi tabelarycznymi i znasz techniki EDA Potrafisz wyciƒÖgaƒá trafne wnioski z danych i prezentowaƒá je w przystƒôpny spos√≥b Znasz SQL i masz do≈õwiadczenie z du≈ºymi zbiorami danych Dodatkowo docenimy, je≈õli : Korzysta≈Çe≈õ z narzƒôdzi B I i znasz metody interpretacji modeli Masz do≈õwiadczenie w automatyzacji proces√≥w analitycznych Znasz system kontroli wersji GIT i technologie konteneryzacji ( Docker ) Pracowa≈Çe≈õ z rozwiƒÖzaniami w chmurze obliczeniowej (Azure, AWS lub GCP) Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Wydarzenia firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu","[{""min"": 15750, ""max"": 21000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 15000, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Senior,Permanent,Hybrid,438,Data analyst (senior) - Nykredit,BEC Financial Technologies,"We are searching for a Senior data analyst - Nykredit This is a temporary project-based position for a duration of 1 year. You will join our Data & Analytics team, working on advanced data modeling and architecture for business-critical solutions. The role focuses on migrating one of Nykredit‚Äôs critical business solutions from legacy mainframe technologies to a modern cloud data warehouse environment in Azure. This position is based at our friendly office in Warsaw. At BEC, we prefer to collaborate often in the office, but we also keep the opportunity to work remotely up to 8 days per month. Your direct manager will be Esben Torz Pedersen. Primary task and responsibilities include: Data modeling, information architecture, and data warehousing, with a particular focus on Data Vault 2.0 architecture Analyzing and translating business requirements into concrete, value-creating solutions. Design and implementation of advanced data models based on Data Vault 1.0 and 2.0 methodologies to support business-critical needs. Development of robust data warehouse architectures that ensure a compliant, scalable, and efficient data foundation. Utilization of tools such as SQL for effective data processing and analysis to support business objectives. Acting as a liaison between technical and non-technical stakeholders, capable of conveying complex concepts in a simple and understandable manner. Problem-solving through analysis of complex data challenges and development of effective, data-driven business solutions. To succeed you will have: 5+ years of experience in data analysis. Experience in gathering business requirements. Experience in designing data solutions architecture from source systems to user-facing solutions. Experience with data warehouse concepts and ETL processes. Experience in data modeling, including methodologies such as Data Vault 1.0 and 2.0. Possesses technical skills and can use SQL. Experience working on a Cloud Data Platform; familiarity/experience with Azure Data Lake, Azure Synapse, and Databricks is a plus. Strong communication skills ‚Äì both technical and business-oriented ‚Äì and is proactive. Comfortable communicating in English at a professional level, as the role involves collaboration with technical teams including colleagues in Poland. Has previously been part of a development team and understands how to apply Scrum and agile methodologies. It‚Äôs nice-to-have: Knowledge of the financial sector and experience with data governance is an advantage Be your best self with BEC‚Äôs benefits! We offer a diverse range of benefits for our employees. Here are just a few of them. Flexible working hours Mental health support Free lunch at the office Professional development Referral bonus up to PLN 10,000 PLN 600 on a benefit platform a month Passion clubs and social events (Tennis, salsa dancing, board games, family picnics and more!) What does the recruitment process look like? Send us your CV: We want to get to know you Screening call: Let‚Äôs chat and see if we‚Äôre a match Technical test : Show off your knowledge! Meet our talent partner and technical experts : Learn more about the job, BEC Poland, and tell us more about your skills and experience. Time to sign the contract : We‚Äôre ready to welcome you to BEC! If you have any questions related to the position, please contact Natalia Cendrowska ( Natalia.cendrowska@bec.dk ). Make us aware of your talent We are an equal opportunities employer. We hire top talent regardless of race, religion, color, national origin, sexual orientation, gender identity, and age. We encourage all qualified candidates to apply. See our full list of vacancies at https: //www.bec.dk/en/vacancies/ You can also learn more about BEC by browsing our company culture book: wearebec.pdf",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Hybrid,439,Data Engineer,Aristocrat Interactive,"We are seeking a Senior Data Engineer to lead the design, implementation, and optimization of our modern data stack. You'll work with tools like Snowflake, dbt, Airflow, and Terraform to build scalable, reliable, and modular data systems. This role will have a strong focus on enabling analytics through clean data modelling, automation, and observability - empowering domain teams with trusted, self-serve data products. What You'll Do Modern Data Platform Development Design and expand a Snowflake-based data platform, incorporating modular design principles. Orchestrate complex workflows using Apache Airflow and containerized jobs on Docker and Kubernetes. Use Terraform to define infrastructure as code for consistent, version-controlled deployments. Pipeline Engineering & Automation Design robust ELT pipelines using Python, Spark, and dbt, processing structured and semi-structured data (e.g., JSON, Parquet). Automate ingestion and transformation layers while enforcing data contracts, quality rules, and schema validation. Build reusable, testable modules to reduce development effort and improve standardization. Data Modelling & Analytics Enablement Own the semantic layer using dbt for transformation and modelling in Snowflake, including SCD and dimensional designs. Develop curated, documented, and test-covered data models that serve as trusted sources for analytics and reporting. Enable self-service analytics by partnering with analysts and product teams to understand data needs and deliver scalable solutions. DevOps & Observability Establish CI/CD pipelines using GitHub Actions, integrating testing, linting, and automated deployment for dbt and data pipelines. Set up monitoring and alerting for pipeline health, SLA breaches, and data quality metrics. Implement practices for lineage tracking, documentation, and incident resolution. Collaboration & Technical Leadership Partner with data analysts, scientists, and engineers to design efficient data flows and interfaces. Contribute to architectural decisions, roadmap planning, and tool evaluations. Mentor team members and champion engineering excellence through peer reviews, documentation, and shared learning. What We're Looking for Must-Have 8+ years of experience in data engineering. Expert-level experience with Snowflake and dbt in production-scale environments. Strong programming skills in Python and experience with Spark for large-scale transformations. Hands-on experience with Apache Airflow, Terraform, Docker, and Kubernetes. Solid grasp of dimensional modelling, modular transformation pipelines, and analytics enablement best practices. Nice-to-Have Familiarity with real-time data streaming tools (e.g., Kafka, Spark Streaming). Experience with observability tools like Grafana, Plotly Dash, or lineage systems. Exposure to data mesh or federated data ownership models. What We offer High-level compensation on an employment contract and regular performance based salary and career development reviews; Medical insurance (health), employee assistance program; Multisport Card; English classes with native speakers, trainings, conferences participation; Referral program; Team buildings, corporate events. Aristocrat Interactive Aristocrat Interactive is Aristocrat Leisure Limited‚Äôs (ASX: ALL) regulated online Real Money Gaming (RMG) business and was formed in 2024 when the Anaxi and NeoGames businesses (Anaxi, NeoGames, Aspire Global, BtoBet, and Pariplay) came together. The business is an industry leader in content and technology solutions for online RMG, with a full-service offering that includes content, proprietary technology platforms and a range of value added services across iLottery, iGaming and Online Sports Betting (OSB). About Aristocrat Aristocrat Leisure Limited (ASX: ALL) is a leading gaming content creation company powered by technology to deliver industry-leading casino games together with mobile games and online real money games, collectively entertaining millions of players worldwide, every day. Headquartered in Sydney, Australia, Aristocrat has three operating business units, spanning regulated land-based gaming (Aristocrat Gaming), social casino (Product Madness) and regulated online real money (Aristocrat Interactive). Our team of over 8,500 people across the globe are united by our company mission to bring joy to life through the power of play . Aristocrat is proud to be an equal opportunity employer. We celebrate diversity and do not discriminate based on gender, race, religion, color, national origin, sex, sexual orientation, age, veteran status, disability status, or any other applicable characteristics protected by law. Diversity and Inclusion are integral to our values of Talent Unleashed, Collective Brilliance, Good Business, Good Citizen, and It‚Äôs All About the Player.","[{""min"": 30000, ""max"": 32000, ""type"": ""Net per month - B2B""}, {""min"": 24500, ""max"": 25800, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,440,Senior Application Manager Microsoft M365,Simon-Kucher CBS,"Become part of a unique entrepreneurial team. Think independently, use your initiative, and take some risks. Entrepreneurship is a powerful force that drives the growth not only of our firm but our clients and people. Unlock the power of opportunity. Advance your career in a thriving company that creates positive impact. We invest in your professional development every step of the way. Enjoy balance and flexible working. Be empowered to do your best work ‚Äì we offer flexible and hybrid working, sabbaticals, and paid time off. Prioritize your health and wellbeing. No matter where you live, we offer a competitive suite of health benefits to help keep you and your loved ones safe. Work in a values-driven culture. At Simon-Kucher, our vision is to become the world's leading growth specialist. Our values guide the way we do business and communicate our distinctiveness. They sum up what we stand for, influence our culture, and drive how and why we do things. Be responsible for the planning, development, implementation and operation of Microsoft M365 tools (SharePoint, Teams) and Power Platform applications (Power Apps, Power Automate, etc.). Collaborate with and consult stakeholders to understand business needs, recommend application enhancements, and drive the adoption of best practices. Participate in strategic planning to align Microsoft services with business goals, staying abreast of industry trends and emerging technologies. Define operational level agreements within the IT organization and ensure compliance with them. Oversee configuration and maintenance of the Microsoft applications in close collaboration with the Simon-Kucher MS operations team. Ensure advanced technical support to end-users, addressing and resolving service-related issues according to agreed service level agreements. Liaise with software vendors for support, updates, enhancements, and developments, ensuring service level agreements are met. Support and enhance knowledge management and digital collaboration based on Microsoft tools and solutions. Develop and deliver training sessions for end-users (in collaboration with our Learning & Skill Development team), create detailed documentation, and maintain up-to-date knowledge bases. Manage incident resolution and problem-solving processes, conducting root cause analysis and implementing preventive measures. Apprenticeship as IT specialist or bachelor‚Äôs degree in information systems, computer science, IT, or a related field. Around 5 years of experience in Microsoft application management or a similar role. Proven experience in developing and customizing applications within the Microsoft 365 environment, including SharePoint, Power Apps, and Power Automate. Understanding of web technologies such as HTML5, CSS, and JavaScript. Experience with SAAS enterprise software solutions and cloud technologies. Familiarity with integration scenarios and process automation scenarios as well as API management. Advanced communication skills to effectively convey technical information to non-technical stakeholders and collaborate with cross-functional teams. A keen focus on user satisfaction, understanding their needs, and ensuring applications meet their expectations. Strong analytical, problem-solving, and organizational skills. Proficient in both spoken and written English to effectively communicate with international teams, vendors, and stakeholders.","[{""min"": 25000, ""max"": 28000, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,441,BI Developer,HAYS Poland,"B2B | BI DEVELOPER | TABLEAU | ETL | POLAND Hays IT Contracting is a cooperation based on B2B rules. We connect IT specialists with the most interesting, technological projects on the market. Join the group of 500 satisfied Contractors working for Hays‚Äô clients! For our Client we are currently looking for Candidates for the position of: BI DEVELOPER 100% remote work Job type: B2B (with Hays Poland) Length: long-term cooperation What You will be doing: Develop and maintain Data blends and Tableau dashboards to visualize Service Data, ensuring accuracy, performance, and usability. Implement business requirements into long-term, scalable, and standardized dashboard solutions. Ensure quality assurance by implementing best practices in Tableau development, including performance optimization and consistency across dashboards. Blend multiple data sources efficiently, understanding underlying data models to create accurate and meaningful reports. Write and maintain comprehensive documentation for dashboards, including data source used, logic, and visualization standards. Perform bug fixing and ongoing maintenance to ensure dashboards remain functional and aligned with evolving business needs. Standardize visualization approaches across all dashboards to enhance readability and consistency. What we expect from you: Proficiency in Tableau (dashboard creation, advanced calculations, data blending, performance optimization). Strong SQL skills for querying and blending multiple data sources. Experience with Alteryx or other data preparation and ETL tools is a plus. Experience with additional data visualization tools such as Power BI is a plus. Solid understanding of data modeling and ability to work with different data structures (e.g., relational databases, APIs, and flat files). Experience in dashboard standardization and establishing visualization best practices. Attention to detail with a focus on data accuracy and dashboard performance. Ability to troubleshoot and debug issues within Tableau dashboards. Excellent documentation skills to ensure dashboards are well-documented and easy to maintain What will you get: Long-term cooperation with the client implementing projects for the largest players in the banking, insurance, telco and more sectors Standard benefits - preferential rates for LuxMed and Multisport packages When you choose to work via Hays, you also get the opportunity to work for many of Hays' other leading clients in the future What will the recruitment process look like: Your CV will be verified by Hays Recruiter Recruiter will contact you by phone - a 15-minute conversation about the project and your experience Technical conversation with the client - online meeting (1h) Offer Welcome to the project! Hays Poland sp. z o.o. is an employment agency registered in a registry kept by Marshal of the Mazowieckie Voivodeship under the number 361.","[{""min"": 120, ""max"": 130, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Hybrid,442,Sr Data Engineer - Product Supply Analytics,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Senior Data Engineer - Product Supply Analytics The PS Data & Analytics team at Bayer Consumer Health focuses on driving digital transformation and innovation by creating best-in-class analytical solutions that enable data-driven decision making and performance optimization for Bayer Consumer Health‚Äôs Supply Chain organization. You will be part of the data & analytics organization and will be responsible for building data products in the area of product supply and supply chain. You partner with business stakeholders, data architects, data scientists, analytics leads as well as other engineers. You will build data pipelines, data models and provision data for front end developers and data scientists. You will also make sure that proper development processes are followed within the team, enhance implementation frameworks and guide other team members in building scalable, secure and well performing data products. If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Key Tasks & Responsibilities: Integrate data from different sources (e.g. supply chain planning data, financial data, quality data, distributor and transportation data, market data (sell-in, sell-out)) to develop globally harmonized data models & KPIs. Continuously enhance implementation frameworks based on the needs of the analytics products in your responsibility. Ensure that product supply data products adhere to the analytics data architecture guidance and deliver fit-for purpose & scalable analytical solutions. Guide other data engineers in your team and ensure that all engineers apply same design principles. Ensure that data is well-managed to build stable, reusable and quality assured data assets. Collaborate with other IT functions (enabling functions data asset teams, analytics teams, platform product managers & integration architects) to ensure the aforementioned activities are executed effectively. Together with the assigned data architect, ensure that cost and time estimations are accurate, quality of delivery is assured, and deliverables are properly handed over to the operations team Qualifications & Competencies (education, skills, experience): Bachelor/Master‚Äôs degree in Computer Science, Engineering, or a related field. 5+ years of working experience in the field of Data & Analytics, preferably in the area of product supply and the CPG industry Excellent data engineering & technology knowledge (Azure Data Lake Gen2, Azure Synapse, Databricks, Snowflake, potentially also legacy stack SAP Hana, SQL, Python as well as data management knowhow (data cataloguing, data quality management) Knowledge of CI/CD processes and tools (GitHub, Azure DevOps Pipelines) Profound data content knowledge (Supply Chain, Logistics, Quality Management) and Product Supply process knowhow Experience in Agile methodologies (Scrum, Kanban) Strong problem solving and analytical skills, combined with impeccable business judgment. Excellent interpersonal and communication skills, active listening, consulting, challenging, presentation skills. Fluent in English, both written & spoken, intercultural awareness and willingness to travel What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Office,443,Data Analyst - Monopoly World,Reality Games,"Monopoly World is going global, and we‚Äôre looking for a talented Data Analyst to help us turn data into actionable insights that will shape the future of gaming. This is your chance to be part of an exciting, high-impact project that will reach players worldwide. This role will report to the Head o f Analytics. This is a full-time, in-office position based in the iconic railway station building in Krakow. If you have a knack for solving complex problems, love working with data, and want to be part of a team that‚Äôs pushing the boundaries of mobile gaming, this is the opportunity for you. Join us in creating something truly innovative and make your mark on one of the most iconic games of all time. Come build something incredible with us! Check out our trailer and see what‚Äôs in store! üöÄüé• Key Responsibilities: Analyze and model data to provide actionable insights for game design and monetization strategies Develop and maintain data pipelines, and work with large datasets to derive meaningful patterns Collaborate with game designers and engineers to create data-driven solutions Perform web scraping and interact with REST APIs to collect and consume data Work with BigQuery SQL to query and manipulate game data for analysis Design and run simulations to support the development of Free-to-Play mobile games Continuously monitor key performance indicators and suggest improvements based on data analysis Job requirements Proficiency in at least one programming language, such as Python, JavaScript, Java, R, or Scala Experience with web scraping or consuming data from REST APIs Experience with BigQuery SQL Strong analytical thinking skills and the ability to interpret complex data At least 2 years of experience in data modeling and simulations for Free-to-Play mobile games Very good command of English Why Join Us? Leader‚Äôs support ‚Äì Get mentorship, feedback, and career growth guidance Knowledge sharing ‚Äì We invest in employee development Fast-paced career ‚Äì Clear growth opportunities with performance reviews Top-notch equipment ‚Äì Get the right tools for your job Beautiful office ‚Äì Spacious, creative workspace in the city center Flexible hours ‚Äì Whether you prefer early mornings or late evenings, we accommodate your schedule Open kitchen ‚Äì Enjoy coffee, juices, fruits, a stocked fridge, and more Team gatherings ‚Äì Connect with colleagues over pizza, games, and great company If you are passionate about data analytics and eager to apply your expertise to a groundbreaking project, we want to hear from you. Join us and contribute to an exciting and fast-growing industry. Apply now!","[{""min"": 90, ""max"": 110, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Hybrid,444,Qlik Developer,ITDS,"Qlik Developer Join us, and turn data into powerful business decisions! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As a Qlik Developer, you will be working for our client, a leading global financial institution undergoing a major digital transformation to enhance its data analytics capabilities. You will be part of a dynamic team focused on developing and maintaining scalable Qlik dashboards and reports, aimed at improving decision-making across various business units. The project involves managing complex data sets, implementing infrastructure best practices, and ensuring compliance within a fast-paced, highly regulated environment. Your role plays a key part in optimizing how data is shared, visualized, and utilized to deliver impactful business insights. Developing advanced dashboards and reports using QlikSense Ensuring data integrity and managing updates to data sources Managing project sites and content on Qlik Server Documenting data sources, processes, and dashboards clearly Analyzing data sharing policies and promoting compliance best practices Collaborating with cross-functional teams and stakeholders at all levels Supporting Qlik deployment by following infrastructure best practices Enhancing dashboard performance for large data sets Influencing stakeholders through thoughtful data presentations Following established internal control standards and audit requirements Proficiency in Qlik with strong dashboard development experience 3+ years of experience in data analysis roles Practical knowledge of QlikSense and Qlik architecture Ability to work with large data sets while maintaining performance High level of mathematical and analytical skills Proven experience in stakeholder management and communication Strong collaboration skills within cross-functional teams Experience documenting technical processes and data sources Ability to adapt quickly in fast-changing environments Understanding of regulatory requirements for data sharing Experience with Qlik administration Familiarity with financial services or highly regulated industries Knowledge of best practices for offshore project coordination Prior involvement in change or transformation projects Awareness of risk management and internal control standards We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7326 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 25200, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,445,Data Engineer,edrone,"We're a hard-working, fun-loving, get-things-done type of team dedicated to providing unique marketing automation solutions for clients. We understand the challenges of eCommerce and the importance of seamless customer service and satisfaction. We roll our sleeves up, act fast, and learn together. We're looking for a Data Engineer who will do the same! üöÄ Who are we? Edrone is a SaaS-based product that helps thousands of small and medium-sized businesses compete with major brands. Our mission is to provide simple solutions to big challenges in eCommerce. We achieve results through a strong feedback culture and clearly defined, transparent expectations. Currently, we work with nearly 2,000 online stores ‚Äî primarily in Poland and Brazil. Brands such as Bielenda , Mosquito , 2005 , or Lilou have placed their trust in our product. If you want to learn more about our culture and what it‚Äôs like to work at edrone, check it out here. Our social media: LinkedIn , Instagram , YouTube . Sounds great? Keep on reading! ‚ú® What‚Äôs in it for you: Be part of a small, fast-paced team that values innovation, efficiency, and a positive work culture. We thrive on challenges, embrace change, and keep things moving. We value initiative and ownership‚Äîif something makes sense, we act on it quickly and take full responsibility for delivering it. Direct responsibility for projects, regular 1: 1s with your leader with a blameless postmortems, code reviews B2B contract (15-20k) & covering all the costs of accounting services Hybrid or remote work or a modern, well-equipped office - whatever you prefer! 26 paid days off so you can relax properly! Benefits - MultiSport card, LuxMed medical package, group accident insurance, English classes, and Hedepy - a portal for mental health and development üöÄ How you will spend your time: Backend System Development Design, build, and maintain robust Python-based services and microservices Develop and optimize RESTful APIs and background services supporting core business logic and integrations Ensure code quality, reusability, and scalability through modular design and adherence to best practices Cloud-Native Application Engineering Develop serverless and containerized applications using AWS Lambda , ECS , and other cloud-native tools Leverage AWS services (S3, RDS, DynamoDB, Step Functions, etc.) to support backend operations and workflows Collaborate with DevOps to provision, deploy, and monitor cloud infrastructure Automation and Task Orchestration Automate recurring tasks, background jobs, and workflows using Python scripts and AWS orchestration tools Build and maintain task schedulers and asynchronous workers for time-sensitive operations Implement monitoring, logging, and alerting systems for observability and proactive issue resolution Data Access and Integration Build data access layers and connectors for interfacing with relational and NoSQL databases Develop data integration scripts or services to move and sync data between systems when needed Write efficient, production-grade SQL and Python code to support internal tools and services Contribute to Innovation and Excellence Stay informed on modern Python practices, libraries, and AWS developments Take initiative in proposing improvements and new ideas to enhance our platform üëÄWho you are: 3+ years of experience as a Data Engineer. Hands-on experience with schema design, complex SQL/query optimization, and running data pipelines in production. Experienced with AWS services (Redshift, Aurora, DynamoDB, S3, Glue, Lambda, Step Functions, etc.) to build data pipelines and scalable cloud-native applications. dbt experience (or strong SQL/ELT background and eagerness to learn dbt quickly). Familiarity with data orchestration tools (e.g., Airflow, Step Functions) ‚Äî scheduling, monitoring, and troubleshooting data pipelines. Ability to build and maintain RESTful APIs/microservices in Python (e.g., FastAPI/Flask) and understand basic backend architecture. üëÄ It‚Äôs nice if you have: Experience in Java is a plus. üìù How does the recruitment process look like: A 30-minute phone interview with the recruiter - Milena Micor , where we aim to get to know you a little better! A technical online interview with the Team Lead Krystian Andruszek and another panelist A short call with our CTO ‚Äì Maciej Mendrela ‚Äì where we‚Äôll share more about the direction of our organization and how we see this role evolving Decision regarding the offer and welcome on board! ‚≠ê After each stage, you will always receive feedback regarding your candidacy.","[{""min"": 15000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,446,Database Administrator/Developer,Britenet,"A project carried out for a client in the lottery industry. Our expectations: In-depth knowledge of Microsoft SQL Server features, performance tuning, and best practices Familiarity with other SQL databases like PostgreSQL and its functionalities Proven track record of creating databases and systems from scratch Demonstrated experience in migrating databases, particularly from MongoDB to Microsoft SQL Knowledge of Database CI/CD processes, DACPAC, Flyway etc. Experience working in a microservices architecture Experience in migrating and modernizing legacy database systems Proactive and self-motivated with the ability to manage work independently and prioritize tasks effectively Excellent communication and collaboration skills Strong analytical and problem-solving abilities Strong attention to detail and commitment to quality Welcome Skills Proficiency in one or more coding languages such as C#, C++, Java, or Python is a plus Key tasks Design, develop, and optimize databases (primarily Microsoft SQL Server) Create database structures, stored procedures, indexes, and other database objects Administer databases ‚Äì monitor performance, perform tuning, manage backups, and ensure security Migrate data and systems, including from MongoDB to SQL Server, and modernize legacy database solutions Collaborate with other teams in a microservices architecture to ensure effective database integration Maintain and automate CI/CD processes for databases (e.g., DACPAC, Flyway) Work with various database engines, including PostgreSQL Ensure data quality, consistency, and system integrity Take a proactive approach to problem-solving and manage tasks and priorities independently Create and maintain technical documentation","[{""min"": 140, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Mid,Permanent or B2B,Office,447,Specjalista/Specjalistka ds. AirTable,Wy≈ºsza Szko≈Ça Kszta≈Çcenia Zawodowego,"W WSKZ dajemy szansƒô na zdobywanie wiedzy i kwalifikacji zawodowych poprzez studia pierwszego i drugiego stopnia oraz podyplomowe, r√≥wnie≈º w formie online. Praca w naszej Uczelni to nie tylko mo≈ºliwo≈õƒá kariery w dynamicznie rozwijajƒÖcej siƒô instytucji, ale tak≈ºe szansa na bycie czƒô≈õciƒÖ zgranego zespo≈Çu. Wierzymy, ≈ºe sukces organizacji zale≈ºy od silnej dru≈ºyny, dlatego dbamy o rozw√≥j i zadowolenie naszych pracownik√≥w. Nasz zesp√≥≈Ç to specjali≈õci z r√≥≈ºnych dziedzin, takich jak IT, obs≈Çuga studenta, marketing, ksiƒôgowo≈õƒá, administracja i wiele innych. Szukamy osoby, kt√≥ra pomo≈ºe nam projektowaƒá, rozwijaƒá i zarzƒÖdzaƒá systemami opartymi na rozwiƒÖzaniu Airtable kt√≥re bƒôdzie wspomagaƒá obszar KIP√≥w, procesu produkcji materia≈Ç√≥w video oraz baz danych wyk≈Çadowc√≥w. Je≈õli lubisz porzƒÖdkowaƒá dane, usprawniaƒá procesy i korzystaƒá z narzƒôdzi no-code do automatyzacji powtarzalnych zada≈Ñ ‚Äî ta rola jest dla Ciebie! W tej roli bƒôdziesz odpowiedzialny/a za: Tworzenie i optymalizacjƒô baz danych w Airtable (widoki, relacje, formularze). Automatyzacjƒô proces√≥w z u≈ºyciem Airtable Automations. Wsp√≥≈Çpracƒô z zespo≈Çami wewnƒôtrznymi w celu mapowania i usprawniania przep≈Çyw√≥w pracy. Tworzenie dashboard√≥w i raport√≥w. Szkolenie innych cz≈Çonk√≥w zespo≈Çu w zakresie korzystania z Airtable. Monitorowanie i raportowanie wydajno≈õci systemu Airtable. Nasze wymagania: Praktyczna znajomo≈õƒá Airtable ‚Äî najlepiej z do≈õwiadczeniem w realnym projekcie. W przypadku braku praktycznego do≈õwiadczenia w pracy z AirTable umiejƒôtno≈õƒá sprawnego wdro≈ºenia siƒô filozofiƒô AirTable czyli ‚Äúbazodanowego power excela z mo≈ºliwo≈õciƒÖ tworzenia GUI‚Äù. Chƒôci oraz motywacja do nabycia bieg≈Ço≈õci w Airtable. Umiejƒôtno≈õƒá logicznego my≈õlenia i projektowania struktury danych. Do≈õwiadczenie z narzƒôdziami no-code (np. Zapier, Make, Notion, Google Workspace). Chƒôci oraz motywacja do nabycia bieg≈Ço≈õci w pracy z narzƒôdziami no-code (np. Zapier, Make, Notion, Google Workspace). Samodzielno≈õƒá, proaktywno≈õƒá i dobra organizacja pracy. Umiejƒôtno≈õƒá szybkiego uczenia siƒô i adaptacji do nowych narzƒôdzi i proces√≥w. Dobre umiejƒôtno≈õci komunikacyjne i wsp√≥≈Çpracy w zespole. Zdolno≈õƒá do pracy pod presjƒÖ czasu i w dynamicznym ≈õrodowisku. To oferujemy: Stabilne zatrudnienie w formie umowy o pracƒô. Cykliczne imprezy integracyjne. Atrakcyjny system kafeteryjny (dostƒôp do setek benefit√≥w, od opieki medycznej, poprzez vouchery do allegro/pyszne.pl lub subskrypcje np. Legimi). Solidny pakiet rozwojowy: dofinansowanie do szkole≈Ñ zewnƒôtrznych. Pe≈Çny wymiar urlopu (26 dni) niezale≈ºnie od sta≈ºu pracy/rodzaju umowy. Do≈ÇƒÖcz do nas i wsp√≥lnie realizujmy misjƒô nowoczesnego kszta≈Çcenia! ü§ùüèª",[],Unclassified,Unclassified
Full-time,Senior,Any,Remote,448,Senior GCP Data Engineer,Lingaro,"Growth through diversity, equity, and inclusion. As an ethical business, we do what is right ‚Äî including ensuring equal opportunities and fostering a safe, respectful workplace for each of us. We believe diversity fuels both personal and business growth. We're committed to building an inclusive community where all our people thrive regardless of their backgrounds, identities, or other personal characteristics. Tasks: You will be a part of the team accountable for design, model and development of whole GCP data ecosystem for one of our Client‚Äôs (Cloud Storage, Cloud Functions, BigQuery). Involvement throughout the whole process starting with the gathering, analyzing, modelling, and documenting business/technical requirements will be needed. The role will include direct contact with clients. Modelling the data from various sources and technologies. Troubleshooting and supporting the most complex and high impact problems, to deliver new features and functionalities. Designing and optimizing data storage architectures, including data lakes, data warehouses, or distributed file systems. Implementing techniques like partitioning, compression, or indexing to optimize data storage and retrieval. Identifying and resolving bottlenecks, tuning queries, and implementing caching strategies to enhance data retrieval speed and overall system efficiency. Identifying and resolving issues related to data processing, storage, or infrastructure. Monitoring system performance, identifying anomalies, and conducting root cause analysis to ensure smooth and uninterrupted data operations. Train and mentor less experienced data engineers, providing guidance and knowledge transfer. What We're Looking For: At least 6 years of experience as a Data Engineer, including min. 4 years of experience working with GCP cloud-based infrastructure & systems. Deep knowledge of Google Cloud Platform and cloud computing services. Extensive experience in design, build, and deploy data pipelines in the cloud, to ingest data from various sources like databases, APIs or streaming platforms. Proficient in database management systems such as SQL (Big Query is a must), NoSQL. Candidate should be able to design, configure, and manage databases to ensure optimal performance and reliability. Programming skills (SQL, Python, other scripting). Proficient in data modeling techniques and database optimization. Knowledge of query optimization, indexing, and performance tuning is necessary for efficient data retrieval and processing. Knowledge of at least one orchestration and scheduling tool (Airflow is a must). Experience with data integration tools and techniques, such as ETL and ELT Candidate should be able to integrate data from multiple sources and transform it into a format that is suitable for analysis. Knowledge of modern data transformation tools (such as DBT, Dataform). Excellent communication skills to effectively collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders. Ability to convey technical concepts to non-technical stakeholders in a clear and concise manner. Ability to actively participate/lead discussions with clients to identify and assess concrete and ambitious avenues for improvement. Tools knowledge: Git, Jira, Confluence, etc. Open to learn new technologies and solutions. Experience in multinational environment and distributed teams. What Will Set You Apart: Certifications in big data technologies or/and cloud platforms. Experience with BI solutions (e.g. Looker, Power BI, Tableau). Experience with ETL tools: e.g. Talend, Alteryx Experience with Apache Spark, especially in GCP environment. Experience with Databricks. Experience with Azure cloud-based infrastructure & systems. Missing one or two of these qualifications? We still want to hear from you! If you bring a positive mindset, we'll provide an environment where you feel valued and empowered to learn and grow. We offer: Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,449,PowerBI Developer,Spyrosoft,"Project description: Our Partner is a prestigious Saudi Arabian conglomerate that stands today as one of the Middle East‚Äôs most influential family businesses, blending commercial expansion, global partnerships, and impactful philanthropy. Over eight decades, it has diversified into seven major sectors, including automotive, energy and financial services, operating in over 30 countries. Together, we are looking for a skilled and proactive Power BI Data Engineers to join a cross-functional analytics team supporting a client from the automotive and mobility industry. This is a delivery-focused role requiring fast turnaround, strong attention to UX and quality, and close collaboration with both technical teams and non-technical stakeholders across various countries and cultures. Main responsibilities: Designing and delivering intuitive dashboards and reports that consolidate data from multiple business systems to provide critical insights for company leadership, Play a key part in helping decision-makers track performance across sales, revenue, margins, and regional brand performance. Must-Have Skills and Experience: Advanced Power BI skills, including data modeling, DAX, and dashboard/report creation Proficient in SQL and experience working with relational databases Experience integrating data from APIs into reporting solutions Ability to design clean, user-friendly dashboards for both desktop and mobile formats Strong understanding of KPIs, metrics, and performance reporting in a business context Excellent communication skills ‚Äì able to present insights clearly to non-technical stakeholders Strong problem-solving skills and attention to detail Proactive, self-motivated, and able to manage work independently Comfortable working in fast-paced, delivery-driven environments with tight deadlines Culturally aware and respectful ‚Äì capable of working effectively in an international, multicultural team English proficiency at B2 level or higher (spoken and written) Familiarity with Microsoft Dynamics CRM and Business Central Experience working with external data sources like Google Analytics Background in the automotive industry or dealership operations Previous experience in Agile, cross-functional, or distributed teams Strong collaboration skills ‚Äì open to feedback and able to contribute constructively in a team setting Adaptable mindset ‚Äì comfortable with change and shifting priorities Interest in the automotive and mobility domain Our Partner is a prestigious Saudi Arabian conglomerate that stands today as one of the Middle East‚Äôs most influential family businesses, blending commercial expansion, global partnerships, and impactful philanthropy. Over eight decades, it has diversified into seven major sectors, including automotive, energy and financial services, operating in over 30 countries. Together, we are looking for a skilled and proactive Power BI Data Engineers to join a cross-functional analytics team supporting a client from the automotive and mobility industry. This is a delivery-focused role requiring fast turnaround, strong attention to UX and quality, and close collaboration with both technical teams and non-technical stakeholders across various countries and cultures.","[{""min"": 90, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,450,Data Modeler / Data Engineer,emagine Polska,"Informacje o projekcie: Bran≈ºa: finanse/po≈ºyczki Lokalizacja: zdalnie Umowa: B2B Stawka: 200 pln/h netto + VAT D≈Çugo≈õƒá projektu: d≈Çugoterminowy Poszukujemy do≈õwiadczonego Data Engineera do zespo≈Çu Data Platform, kt√≥ry bƒôdzie modelowaƒá dane oraz implementowaƒá procesy ELT w chmurze. ObowiƒÖzki: Modelowanie struktur bazodanowych w podej≈õciu DDD oraz tworzenie logicznych i fizycznych modeli danych. Data Mapping. Przygotowywanie warstwy Data Contracts (wymaga≈Ñ HD do system√≥w ≈∫r√≥d≈Çowych pod merytorycznƒÖ p≈Çaszczyznƒô kontraktu na dane) na podstawie zamodelowanych uprzednio struktur dla poszczeg√≥lnych domen danych. Wsp√≥≈Çpraca w procesie ingerencji danych z system√≥w ≈∫r√≥d≈Çowych. Implementacja modeli danych dla poszczeg√≥lnych domen w Data Platform (warstwa Bronze, Silver i Gold) w podej≈õciu ELT w ≈õrodowisku Azure Databricks. Wymagania: Do≈õwiadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejƒôtno≈õƒá implementacji proces√≥w ELT. Umiejƒôtno≈õƒá tworzenia dokumentacji technicznej. Do≈õwiadczenie w mapowaniu danych ze ≈∫r√≥d≈Çowych do docelowych struktur w DWH. Umiejƒôtno≈õƒá interpretacji fizycznego/logicznego modelu danych (ERD, modele relacyjne) i synchronizacji struktur tych≈ºe modeli z wymaganiami ≈õwiata analityki. Do≈õwiadczenie w podej≈õciu do projektowania s≈Çownik√≥w i zarzƒÖdzania nimi (masterdata). Znajomo≈õƒá narzƒôdzia dbdiagram.io . Wiedza na temat zagadnie≈Ñ Data Quality, Data Lineage i zasad zarzƒÖdzania danymi.","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Mid,B2B,Remote,451,Programista Baz danych,Eyzee S.A.,"Poszukujemy Programist√≥w Baz danych, kt√≥rzy majƒÖ do≈õwiadczenie w projektowaniu i eksploatacji baz danych dla naszego klienta w bran≈ºy medycznej. Celem projektu jest opracowywanie i wdra≈ºanie innowacyjnych rozwiƒÖza≈Ñ, programowanie nowych funkcjonalno≈õci, rozwijanie, modyfikowanie i testowanie oprogramowania. Tworzymy przyjazne miejsce pracy i rozwoju dla specjalist√≥w w bran≈ºy IT, zapewniamy ciekawe wyzwania, dbajƒÖc o dobrƒÖ komunikacjƒô i atmosferƒô w zespole. Z nami przyspieszysz rozw√≥j swojej kariery! Praca zdalna, pe≈Çen etat. Zadania dla Ciebie: projektowanie, implementacja i utrzymanie struktur relacyjnych baz danych (np. PostgreSQL, MySQL, MSSQL, Oracle) dla systemu medycznego tworzenie i optymalizacja logiki biznesowej w bazie danych przy u≈ºyciu jƒôzyk√≥w proceduralnych (np. PL/SQL, PL/pgSQL), w tym procedur sk≈Çadowanych, funkcji i wyzwalaczy pisanie z≈Ço≈ºonych i wydajnych zapyta≈Ñ SQL oraz skrypt√≥w do zarzƒÖdzania danymi i ich przetwarzania zapewnienie integralno≈õci, bezpiecze≈Ñstwa i optymalnej wydajno≈õci bazy danych wsp√≥≈Çpraca z zespo≈Çem deweloperskim i analitykami w celu realizacji wymaga≈Ñ projektowych Wymagania: min. 5 lat do≈õwiadczenia w pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL, MSSQL, ORACLE) praktyczne do≈õwiadczenie w programowaniu w proceduralnych jƒôzykach bazodanowych (np. PL/SQL, PL/pgSQL) znajomo≈õƒá technik migracji danych pomiƒôdzy r√≥≈ºnymi platformami bazodanowymi i systemowymi bieg≈Çej znajomo≈õci SQL do≈õwiadczenie z bazƒÖ danych PostgreSQL. umiejƒôtno≈õƒá strojenia zapyta≈Ñ SQL i procedur PL/pgSQL. znajomo≈õƒá zasad zarzƒÖdzania, konfiguracji i optymalizacji PostgreSQL Mile widziane: do≈õwiadczenie w bran≈ºy medycznej Co oferujemy? stabilne zatrudnienie w oparciu o kontrakt B2B s≈Çu≈ºbowy laptop i monitor dofinansowanie prywatnej opieki medycznej sportowƒÖ kartƒô Multisport nauka jƒôzyka angielskiego omawianie postƒôp√≥w i rozwoju co p√≥≈Ç roku transparentna komunikacja z pracownikami mo≈ºliwo≈õƒá zaanga≈ºowania siƒô w rozw√≥j organizacji chƒôtnie dzielimy siƒô wiedzƒÖ - do≈ÇƒÖcz do Akademii Eyzee mocny kompetencyjnie zesp√≥≈Ç sk≈ÇadajƒÖcy siƒô w wiƒôkszo≈õci z senior√≥w praca z narzƒôdziami JIRA, Confluence, BitBucket dbamy o integracje i chƒôtnie wsp√≥lnie spƒôdzamy czas Kim jeste≈õmy? Jeste≈õmy polskƒÖ firmƒÖ specjalizujƒÖcƒÖ siƒô w realizacji z≈Ço≈ºonych projekt√≥w informatycznych oraz doradczych dla firm z sektora finansowego, telekomunikacyjnego i publicznego. Stanowimy zgrany zesp√≥≈Ç konsultant√≥w z wiedzƒÖ i wieloletnim do≈õwiadczeniem w tworzeniu i utrzymywaniu rozwiƒÖza≈Ñ. Wa≈ºne dla nas sƒÖ: doprecyzowanie wymaga≈Ñ przed napisaniem kodu, jako≈õƒá tworzonego kodu, testowanie oraz CI/CD. Nasze projekty to g≈Ç√≥wnie tworzenie nowych mikroserwis√≥w lub nowych funkcjonalno≈õci do istniejƒÖcych rozwiƒÖza≈Ñ. Dodatkowo rozwijamy w≈Çasne aplikacje i plugin‚Äôy, kt√≥re nie tylko usprawniajƒÖ pracƒô, ale te≈º pozwalajƒÖ rozwinƒÖƒá nasze do≈õwiadczenie. Jeden z nich mo≈ºesz pobraƒá tutaj (eZee Worklog). Jeste≈õmy partnerem Atlassian.","[{""min"": 15000, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Database Administration,Database Administration
Full-time,Mid,B2B,Hybrid,452,Data Visualization Specialist,ITDS,"Data Visualization Specialist (Qlik) Join us, and bring data to life through smart visuals Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As a Data Visualization Specialist , you will be working for our client, a globally recognized financial institution that is transforming how finance teams leverage data for strategic resource management. You will be contributing to the development of best-in-class visualization solutions by building advanced dashboards and reports in Qlik. Working closely with analysts and engineers, you will translate complex data into intuitive visual tools that support high-impact decision-making. This is a great opportunity to work in a dynamic, fast-paced environment where innovation, precision, and collaboration are key. Your main responsibilities: Developing advanced dashboards and reports using Qlik and QlikSense Managing data sources to ensure accuracy and integrity Collaborating with IT analysts and engineers to define visualization requirements Maintaining and overseeing project sites on Qlik Server Documenting processes, dashboards, and data sources clearly and thoroughly Promoting data sharing best practices and managing user access controls Ensuring optimal performance of Qlik workbooks with large datasets Supporting infrastructure and deployment requirements for Qlik tools Following internal control standards and compliance procedures Engaging with stakeholders to gather feedback and implement improvements You're ideal for this role if you have: 3+ years of experience in data analysis or data visualization Proficiency in Qlik and hands-on experience with QlikSense Ability to work with and visualize large, complex datasets efficiently Experience managing Qlik Server environments and deployments Strong analytical skills and a high level of mathematical competence Excellent collaboration and communication skills Ability to document technical and process information effectively Understanding of compliance and internal control frameworks Experience working in fast-paced environments with tight deadlines Proactive attitude and ability to solve problems independently It is a strong plus if you have: Working knowledge of Qlik architecture and administration Experience with data governance and regulatory compliance in finance Familiarity with offshoring workflows and documentation standards Background in financial services or resource management analytics Ability to influence stakeholders and drive data visualization best practices We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7099 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 20800, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,Permanent,Remote,453,"Senior Data Engineer (SQL, Python, Java) - Banking",Capco Poland,"CAPCO POLAND *We are looking for the candidates based in Poland. The job is remote, but may require occasional visits at the office in Warsaw. Capco Poland is a leading global technology and management consultancy, dedicated to driving digital transformation across the financial services industry. Our passion lies in helping our clients navigate the complexities of the financial world, and our expertise spans banking and payments, capital markets, wealth, and asset management. We pride ourselves on maintaining a nimble, agile, and entrepreneurial culture, and we are committed to growing our business by hiring top talent. We also are: Experts in banking and payments, capital markets, wealth and asset management Focused on maintaining our nimble, agile, and entrepreneurial culture Committed to growing our business and hiring the best talent to help us get there ABOUT THE ROLE As a Senior Data Engineer you will join an international team working for one of the biggest global banks. The Agile team focusing on capital markets is building an innovative platform leveraging data, analytics to drive automated decision-making, smart automation across the full lifecycle of the business. This complex, long-term project will give you an opportunity to work cross-teams and explore new technologies such as gen AI and clouds. This is a rare opportunity for a self-driven, strong and passionate developer to participate and contribute to a paradigm shift. KEY RESPONSIBILITIES: Collaborate with business stakeholders and PO to understand data requirements and translate them into technical solutions. Implement data models and schemas to support analytics, reporting, and machine learning initiatives. Optimize data processing and storage solutions for performance, scalability, and cost-effectiveness. Ensure data quality and integrity by implementing data validation, monitoring, and error handling mechanisms. Collaborate with data analysts and data scientists to provide them with clean, reliable, and accessible data for analysis and modeling. Stay current with emerging technologies and best practices in data engineering and recommend innovative solutions to enhance our data capabilities. Provide Production L3 support as required. KEY SKILLS AND QUALIFICATIONS Technical skills: 8+ years' experience as a data engineer or similar role. Proficiency in SQL, Python, Java and/or other programming languages for data processing and manipulation. Experience with relational and NoSQL databases, OLAP tools, vector stores (PGVector, FAISS, Chroma) and data modeling concepts. Strong understanding of distributed computing frameworks (e.g., Apache Spark, Apache Flink, Apache Storm). Familiarity with data visualization tools and strong experience with data pipeline tools and metadata stores (Informatica). Understanding of data and query optimization, query profiling, and query performance monitoring tools and techniques. Solid understanding of ETL/ELT processes, data validation, and data security best practices. Experience in version control systems (Git) and CI/CD pipelines. Familiarity with cloud-based data platforms will be a plus. Soft skills: Excellent problem-solving skills and attention to detail. Good communication skills in English. Strong communication and collaboration skills to work effectively with cross-functional teams. Experience in banking, financial services - is a huge plus WHY JOIN CAPCO? Employment contract and/or Business to Business - whichever you prefer Possibility to work remotely Speaking English on daily basis, mainly in contact with foreign stakeholders and peers Multiple employee benefits packages (MyBenefit Cafeteria, private medical care, life-insurance) Access to 3.000+ Business Courses Platform (Udemy) Access to required IT equipment Paid Referral Program Participation in charity events e.g. Szlachetna Paczka Ongoing learning opportunities to help you acquire new skills or deepen existing expertise Being part of the core squad focused on the growth of the Polish business unit A flat, non-hierarchical structure that will enable you to work with senior partners and directly with clients A work culture focused on innovation and creating lasting value for our clients and employees ONLINE RECRUITMENT PROCESS STEPS Screening call with the Recruiter Technical interview: first stage Client Interview Feedback/Offer Joining Capco means joining an organisation that is committed to an inclusive working environment where you‚Äôre encouraged to #BeYourselfAtWork. We celebrate individuality and recognize that diversity and inclusion, in all forms, is critical to success. It‚Äôs important to us that we recruit and develop as diverse a range of talent as we can and we believe that everyone brings something different to the table ‚Äì so we‚Äôd love to know what makes you different. Such differences may mean we need to make changes to our process to allow you the best possible platform to succeed, and we are happy to cater to any reasonable adjustments you may require. You will find the section to let us know of these at the bottom of your application form or you can mention it directly to your recruiter at any stage and they will be happy to help.",[],Database Administration,Database Administration
Full-time,Mid,B2B,Hybrid,454,FinOps Specialist (GCP),Ness Solution,"Poszukiwany FinOps Specialist (GCP)! üìÖ Start: wrzesie≈Ñ üåç Lokalizacja: Warszawa + praca zdalna (min. 1 dzie≈Ñ/tydz. w biurze) üìÉ Umowa B2B do 180 PLN/h üîç Projekt Do≈ÇƒÖcz do zespo≈Çu wspierajƒÖcego wdro≈ºenie ≈õrodowiska chmurowego w Google Cloud Platform ‚Äì obejmujƒÖcego obszary zarzƒÖdzania kosztami, struktury projektowej (Landing Zone) i bud≈ºetowania . TwojƒÖ rolƒÖ bƒôdzie zapewnienie wsparcia FinOps, optymalizacja koszt√≥w oraz wsp√≥≈Çpraca z zespo≈Çami technicznymi i finansowymi. üõ† Zakres zada≈Ñ Wsparcie wdro≈ºenia ≈õrodowiska GCP z perspektywy FinOps Tworzenie i zarzƒÖdzanie bud≈ºetami chmurowymi (alerty, limity, tagowanie) Monitorowanie i analiza koszt√≥w (Google Cloud Billing, Budgets & Alerts, BigQuery) Wsp√≥≈Çpraca z zespo≈Çami IT i finansowymi Identyfikowanie mo≈ºliwo≈õci optymalizacji koszt√≥w Wdra≈ºanie dobrych praktyk FinOps i prowadzenie szkole≈Ñ ‚úÖ Wymagania Do≈õwiadczenie w FinOps w ≈õrodowisku GCP Znajomo≈õƒá narzƒôdzi: Google Cloud Billing, Budgets & Alerts, BigQuery, Cloud Monitoring Umiejƒôtno≈õƒá pracy z bud≈ºetami chmurowymi i analiza koszt√≥w Znajomo≈õƒá GCP Landing Zone Accelerator Znajomo≈õƒá Terraform i/lub Ansible Umiejƒôtno≈õƒá wsp√≥≈Çpracy z dzia≈Çami technicznymi i finansowymi Certyfikat FinOps Certified Practitioner (mile widziany r√≥wnie≈º CCFM) üéØ Mile widziane Do≈õwiadczenie w szkoleniu zespo≈Ç√≥w z zakresu FinOps Praktyczna znajomo≈õƒá optymalizacji koszt√≥w w du≈ºych ≈õrodowiskach chmurowych Co oferujemy: Elastyczny model pracy : hybrydowy ‚Äì 1 dzie≈Ñ w biurze, pozosta≈Çe dni zdalnie Benefity : karta Multisport, prywatna opieka medyczna Jasny i sprawny proces rekrutacyjny : Rozmowa z rekruterem Spotkanie techniczne z klientem Je≈õli oferta jest dla Ciebie interesujƒÖca, prze≈õlij swoje CV!","[{""min"": 23520, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Hybrid,455,Data Warehouse Developer,ITDS,"Shape the Future of Banking Data ‚Äì Become a DWH Developer Warsaw/ Wroc≈Çaw based opportunity with hybrid work model (1 day in the office/week) As a Data Warehouse Developer , you will be working for our client, a leading financial institution dedicated to digital transformation and data excellence. You will join the Data Warehouse team supporting both strategic initiatives and day-to-day development efforts. The project involves maintaining and enhancing a complex enterprise data platform used for business analysis, regulatory compliance, and operational reporting. You will play a key role in building efficient ETL processes and supporting scalable data solutions, contributing to long-term data architecture goals. Your main responsibilities: Develop and implement ETL processes based on business and technical requirements Optimize existing data pipelines to improve performance and reliability Test and validate technical solutions against data quality standards Collaborate with analysts and developers to understand data flow and dependencies Maintain technical documentation related to ETL jobs and data architecture Support Business-as-Usual tasks and participate in ongoing project work Ensure compliance with internal coding and security standards Monitor and troubleshoot ETL jobs and provide issue resolution Contribute to the continuous improvement of the Data Warehouse platform Participate in team planning and development lifecycle activities You're ideal for this role if you have: Proven experience in developing ETL processes using any technology Strong knowledge of SQL in a Data Warehouse context Hands-on experience working in a Data Warehouse or enterprise data environment Understanding of data pipeline design and data integration concepts Familiarity with Microsoft Azure and Databricks Ability to analyze and troubleshoot performance issues in ETL jobs Good communication skills and the ability to work in a cross-functional team Experience working in an Agile or iterative development environment Strong attention to detail and quality mindset Ability to document technical solutions clearly and accurately We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here . Ref. number 7467","[{""min"": 16800, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Junior,Any,Hybrid,456,M≈Çodszy/a Programista/tka ERP,Unisoft,"Jeste≈õmy producentem oprogramowania wspomagajƒÖcego zarzƒÖdzaniem przedsiƒôbiorstwem. Od prawie 40 lat nie tylko tworzymy, ale tak≈ºe doradzamy, wdra≈ºamy i szkolimy w zakresie zarzƒÖdzania wiƒôkszo≈õciƒÖ proces√≥w zachodzƒÖcych w przedsiƒôbiorstwie. Poszukujemy trzech os√≥b, kt√≥re do≈ÇƒÖczƒÖ do zespo≈Çu i w ramach przydzielonego obszaru merytorycznego bƒôdƒÖ odpowiedzialne za rozwijanie naszego autorskiego systemu. programowanie wed≈Çug przygotowanych za≈Ço≈ºe≈Ñ projektowych, dbanie o rozw√≥j aplikacji zgodnie z wewnƒôtrznymi standardami programowania i projektowania, tworzenie dokumentacji technicznej umo≈ºliwiajƒÖcej dalszy rozw√≥j i eksploatacjƒô oprogramowania. uko≈Ñczone (bƒÖd≈∫ w trakcie ostatniego roku) studia na kierunku zwiƒÖzanym z informatykƒÖ, matematykƒÖ, elektronikƒÖ lub telekomunikacjƒÖ, znajomo≈õƒá dowolnej relacyjnej bazy danych, znajomo≈õƒá jƒôzyka SQL, znajomo≈õƒá programowania obiektowego, znajomo≈õƒá jƒôzyka angielskiego na poziomie czytania dokumentacji technicznej, umiejƒôtno≈õƒá pracy w zespole, chƒôƒá rozwoju zawodowego. Mile widziane: znajomo≈õƒá kt√≥rego≈õ z jƒôzyk√≥w programowania: Delphi, C#/C++ itp., umiejƒôtno≈õƒá pos≈Çugiwania siƒô systemami kontroli wersji (np. Git), do≈õwiadczenie w realizacji projekt√≥w informatycznych. pracƒô w firmie o prawie 40-letniej tradycji w bran≈ºy IT, zatrudnienie w oparciu o wybranƒÖ formƒô wsp√≥≈Çpracy (UoP, B2B, zlecenie), atrakcyjne wynagrodzenie dopasowane do kompetencji, stabilizacjƒô i work-life balance (pracujemy w sta≈Çych godzinach i nie zabieramy pracy do domu) mo≈ºliwo≈õƒá rozwoju zawodowego pod opiekƒÖ do≈õwiadczonych programist√≥w, wsparcie merytoryczne od lider√≥w technologicznych, zgrany zesp√≥≈Ç chƒôtny do pomocy i dzielenia siƒô wiedzƒÖ, pracƒô w centrum Gdyni: 10 min. do Dworca PKP/SKM Gdynia G≈Ç√≥wna, 2 minuty pieszo z przystanku ZKM ‚ÄûPlac Kaszubski-≈öwiƒôtoja≈Ñska 01‚Äù, 5 min pieszo do Skweru Ko≈õciuszki, 10 min. na pla≈ºƒô miejskƒÖ, co najmniej 40 restauracji i kawiarni w promieniu 800m, parking rowerowy pod biurem i prysznice w biurze (≈õwietna lokalizacja ma te≈º swoje minusy w postaci trudno dostƒôpnych miejsc parkingowych).","[{""min"": 6500, ""max"": 9000, ""type"": ""Gross per month - Any""}]",Unclassified,Unclassified
Full-time,Mid,Permanent,Office,457,"Cloud AI Engineer, Global Services Delivery",Google,"Bachelor's degree in Science, Technology, Engineering, Mathematics, or equivalent practical experience. Experience building Machine Learning or Data Science solutions, including software development in Python, Scala, or R, and experience in data structures, algorithms, and software design. Experience in solution engineering and experience in stakeholder management, professional services, or technical consulting. Experience writing code in one or more programming languages (e.g., Python, Java). Experience with Generative AI models and their applications. Master‚Äôs degree in Engineering, Computer Science, Business, or a related field. Experience in an analytical role such as business intelligence, data analytics, or statistics. Experience in software development, professional services, solution engineering and technical consulting, architecting and rolling out new technology and solution initiatives. Experience with recommendation engines, data pipelines or distributed ML, data analytics, data visualization techniques and software, and deep learning frameworks. Knowledge of data warehousing concepts, data warehouse technical architectures, infrastructure components, ETL/ ELT, reporting/analytic tools and environments (Apache Beam, Hadoop, Spark, Pig, Hive, MapReduce, Flume), and of MLOps methodologies for monitoring ML models. The Google Cloud Platform team helps customers transform and build what's next for their business ‚Äî all with technology built in the cloud. Our products are developed for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. Our teams are dedicated to helping our customers ‚Äî developers, small and large businesses, educational institutions and government agencies ‚Äî see the benefits of our technology come to life. As part of an entrepreneurial team in this rapidly growing business, you will play a key role in understanding the needs of our customers and help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. As a Cloud Engineer, you'll play a key role in ensuring that strategic customers have the best experience moving to the Google Cloud machine learning (ML) suite of products. You will design and implement machine learning solutions for customer use cases, leveraging core Google products. You'll work with customers to identify opportunities to transform their business with machine learning, and will travel to customer sites to deploy solutions and deliver workshops designed to educate and empower customers to realize the full potential of Google Cloud. You will have access to Google‚Äôs technology to monitor application performance, debug and troubleshoot product code, and address customer and partner needs. In this role, you will lead the timely execution of adopting the Google Cloud Platform solutions to the customer. Google Cloud accelerates every organization‚Äôs ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google‚Äôs cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems. Work with clients to understand their objectives and challenges, identify technical gaps, and surface opportunities for solution reuse or innovations. Design and implement solutions that meet client needs and are compliant with data and legal policies. Understand the nuances of clients within the industry and develop subject matter expertise in trending spaces. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",[],Data Science,Data Science
Full-time,Senior,B2B,Hybrid,458,Senior Data Engineer,emagine Polska,"Project information: Industry : Insurance and IT services Rate: up to 180 z≈Ç/h netto + VAT Location : Warsaw (first 2-3 months of office visits once a week, then occasionally) Project language : Polish, English Summary: As a Senior Data Engineer , you will play a pivotal role in developing scalable Data Hubs to support various applications, including analytical, reporting, operational, and Generative AI . This position emphasizes key skills such as Python and SQL programming, expertise in cloud environments like Azure , and proficiency in Databricks and Spark . Your responsibilities will include defining architecture standards, mentoring team members, and ensuring the performance and compliance of data solutions. Responsibilities: Data Hub Architecture: Design scalable Data Hub systems for integrating both structured and unstructured data. Technical Leadership: Define practices and patterns for data engineering implementations. Mentoring & Knowledge Sharing: Provide guidance and support to fellow engineers. Data Pipeline Optimization: Create and enhance batch and real-time data workflows. Quality & Monitoring: Implement measures for data validation and anomaly detection. Automation & CI/CD: Streamline deployment processes through automation and DevOps practices. Collaboration & Strategy: Work with diverse teams to align data solutions with business goals. Security & Compliance: Ensure data governance and adherence to regulations. Documentation & Knowledge Management: Produce quality technical documentation for reference. Key Requirements: Programming Skills: Proficient in Python and SQL . Cloud Experience: Familiarity with Azure Data Factory, ADLS, Azure SQL, Synapse . Databricks & Spark: Extensive knowledge of Databricks and Apache Spark . Data Architecture: Expertise in designing enterprise-scale platforms. Streaming Data Processing: Experience with real-time data analysis tools. Automation & DevOps: Skills in CI/CD, Terraform, Kubernetes, Docker . Data Governance: Knowledge of data compliance practices. Leadership: Capability to guide teams effectively. Communication: Proficient in technical documentation and fluent in English (minimum B2). Nice to Have: Stream Analytics Skills: Experience with Azure Stream Analytics. Event Streaming Tools: Familiarity with Azure Event Hubs. Agile Methodologies: Experience working in Agile/Scrum environments.","[{""min"": 160, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,459,Data Analyst,Wakacje.pl,O pracy w Wakacje.pl ‚Äì czym siƒô kierujemy?,[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Hybrid,460,"Analityk Hurtowni Danych - SQL, Azure",Upvanta sp. z o.o.,"Poszukujemy Analityka Hurtowni Danych (DWH), kt√≥ry bƒôdzie wspiera≈Ç rozw√≥j, utrzymanie i optymalizacjƒô hurtowni danych wykorzystywanej w analizach biznesowych na du≈ºƒÖ skalƒô. Je≈õli masz do≈õwiadczenie z SQL, Azure i Power BI, swobodnie poruszasz siƒô w ≈õwiecie modelowania danych, a jednocze≈õnie potrafisz ≈ÇƒÖczyƒá potrzeby techniczne z biznesowymi ‚Äì ta rola mo≈ºe byƒá w≈Ça≈õnie dla Ciebie. Miejsce pracy: Wroc≈Çaw (praca hybrydowa ‚Äì wymagane sta≈Çe miejsce zamieszkania we Wroc≈Çawiu lub w bliskiej okolicy) Zakres obowiƒÖzk√≥w: Analiza biznesowo-systemowa w kontek≈õcie hurtowni danych oraz projektowanie i implementacja struktur hurtowni. Analiza biznesowo-systemowa w kontek≈õcie hurtowni danych oraz projektowanie i implementacja struktur hurtowni. Modelowanie danych i tworzenie oraz interpretacja modeli ERD. Modelowanie danych i tworzenie oraz interpretacja modeli ERD. Optymalizacja zapyta≈Ñ SQL oraz analiza du≈ºych wolumen√≥w danych (setki milion√≥w transakcji). Optymalizacja zapyta≈Ñ SQL oraz analiza du≈ºych wolumen√≥w danych (setki milion√≥w transakcji). Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi i biznesowymi w celu dostosowania rozwiƒÖza≈Ñ do wymaga≈Ñ klienta. Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi i biznesowymi w celu dostosowania rozwiƒÖza≈Ñ do wymaga≈Ñ klienta. Tworzenie i wdra≈ºanie po≈ÇƒÖcze≈Ñ miƒôdzy hurtowniƒÖ danych a ≈∫r√≥d≈Çami danych, takimi jak sFTP, Azure DB, onPremDB, QuickBase oraz Power BI. Tworzenie i wdra≈ºanie po≈ÇƒÖcze≈Ñ miƒôdzy hurtowniƒÖ danych a ≈∫r√≥d≈Çami danych, takimi jak sFTP, Azure DB, onPremDB, QuickBase oraz Power BI. Utrzymanie hurtowni danych i procedur automatyzacji, diagnozowanie problem√≥w wydajno≈õciowych oraz naprawa b≈Çƒôd√≥w w ≈õrodowisku (np. Azure Data Factory, Azure SQL Server, maszyny wirtualne). Utrzymanie hurtowni danych i procedur automatyzacji, diagnozowanie problem√≥w wydajno≈õciowych oraz naprawa b≈Çƒôd√≥w w ≈õrodowisku (np. Azure Data Factory, Azure SQL Server, maszyny wirtualne). Wykorzystanie system√≥w kontroli wersji (Git) do zarzƒÖdzania kodem oraz wsp√≥≈Çpracy z zespo≈Çami. Wykorzystanie system√≥w kontroli wersji (Git) do zarzƒÖdzania kodem oraz wsp√≥≈Çpracy z zespo≈Çami. Tworzenie dokumentacji technicznej i analitycznej zwiƒÖzanej z implementacjƒÖ i utrzymaniem hurtowni danych. Tworzenie dokumentacji technicznej i analitycznej zwiƒÖzanej z implementacjƒÖ i utrzymaniem hurtowni danych. Dodawanie procedur sk≈Çadowanych, tabel, trigger√≥w (T-SQL, Git, Visual Studio) oraz wdra≈ºanie zmian w ≈õrodowiskach produkcyjnych. Dodawanie procedur sk≈Çadowanych, tabel, trigger√≥w (T-SQL, Git, Visual Studio) oraz wdra≈ºanie zmian w ≈õrodowiskach produkcyjnych. Wymagania: Wykszta≈Çcenie wy≈ºsze z zakresu informatyki lub pokrewne. Wykszta≈Çcenie wy≈ºsze z zakresu informatyki lub pokrewne. Minimum 2 lata do≈õwiadczenia w implementacji i zarzƒÖdzaniu hurtowniami danych. Minimum 2 lata do≈õwiadczenia w implementacji i zarzƒÖdzaniu hurtowniami danych. Bardzo dobra znajomo≈õƒá jƒôzyka SQL (T-SQL, PostgreSQL) na poziomie ≈õredniozaawansowanym. Bardzo dobra znajomo≈õƒá jƒôzyka SQL (T-SQL, PostgreSQL) na poziomie ≈õredniozaawansowanym. Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych (setki milion√≥w transakcji). Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych (setki milion√≥w transakcji). Umiejƒôtno≈õƒá modelowania danych oraz tworzenia modeli ERD. Umiejƒôtno≈õƒá modelowania danych oraz tworzenia modeli ERD. Zdolno≈õci analityczne oraz umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w w kontek≈õcie hurtowni danych. Zdolno≈õci analityczne oraz umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w w kontek≈õcie hurtowni danych. Znajomo≈õƒá systemu kontroli wersji (Git) i narzƒôdzi do zarzƒÖdzania kodem. Znajomo≈õƒá systemu kontroli wersji (Git) i narzƒôdzi do zarzƒÖdzania kodem. Praktyczna znajomo≈õƒá Microsoft Azure oraz Power BI. Praktyczna znajomo≈õƒá Microsoft Azure oraz Power BI. Mile widziane: Znajomo≈õƒá Oracle. Znajomo≈õƒá Oracle. Do≈õwiadczenie w modelowaniu i implementacji hurtowni danych w metodologii Data Vault. Do≈õwiadczenie w modelowaniu i implementacji hurtowni danych w metodologii Data Vault. Znajomo≈õƒá narzƒôdzi do automatyzacji implementacji hurtowni danych (np. Wherescape). Znajomo≈õƒá narzƒôdzi do automatyzacji implementacji hurtowni danych (np. Wherescape). Znajomo≈õƒá technologii SSAS Tabular. Znajomo≈õƒá technologii SSAS Tabular. Znajomo≈õƒá Python. Znajomo≈õƒá Python. Brzmi interesujƒÖco?Je≈õli chcesz pracowaƒá z du≈ºymi zbiorami danych, mieƒá realny wp≈Çyw na architekturƒô hurtowni i rozwijaƒá siƒô w ≈õrodowisku opartym na nowoczesnych technologiach, czekamy na TwojƒÖ aplikacjƒô.","[{""min"": 600, ""max"": 800, ""type"": ""Net per day - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,461,Data Engineer,Haddad Brands Europe,"Building a scalable Data Warehouse that consolidates transactional data from our AS400 (IBM i) ERP and DB2 databases Integrating data from MS SQL-based applications to support analytics and reporting Automating ETL pipelines for nightly and real-time data ingestion Implementing data quality, lineage, and governance processes Design, develop, and maintain ETL processes to extract data from AS400 DB2 and MS SQL systems Model and optimize Data Warehouse schemas (star, snowflake) for performance and scalability Collaborate with Business Analysts and stakeholders to translate requirements into data solutions Implement and monitor data quality checks, alerts, and remediation workflows Automate deployments and version control of data pipelines using tools like Git Troubleshoot data issues and tune database performance for large datasets Proven experience as a Data Engineer or similar role in a corporate environment Strong SQL skills and hands-on experience with DB2 on AS400 (IBM i) and MS SQL Server Proficiency in ETL frameworks or orchestration tools Programming experience in Python, Shell scripting, or similar languages Solid understanding of data modeling, warehousing concepts, and performance tuning Familiarity with data governance, lineage, and quality best practices Excellent communication skills and ability to work with cross-functional teams English proficiency at C1 level or higher Initial meeting in Polish to get acquainted and discuss your background Technical interview within two weeks, focusing on your data architecture and ETL skills Brief English conversation to assess collaboration and communication abilities Private medical insurance 21 days of paid vacation per year (B2B contract) Company-provided hardware and software Training programs and clear career-development path Freedom to choose tools and technologies Supportive, trust-based work environment Get to know us video: -< http: //gofile.me/20s0Q/YMS1drNMp Password: HaddadFamilyB2BA@Retail","[{""min"": 17000, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,462,Senior Data Scientist,Transition Technologies MS,"End-to-End Model Ownership: Drive the entire machine learning lifecycle, from exploratory data analysis (EDA) and advanced feature engineering to model training, validation, deployment, and post-launch monitoring for performance and concept drift. Problem Formulation: Translate ambiguous business requirements and domain challenges into well-defined technical problems, testable hypotheses, and robust machine learning solutions. Rigorous Experimentation: Design, test, and validate multiple modeling approaches to find the optimal solution, establishing clear and relevant evaluation metrics that directly align with business goals. Technical Implementation & Deployment: Utilize our Triple AI SageMaker environment to efficiently train, deploy, and manage scalable models in a production setting. Data Storytelling & Visualization: Communicate complex model outputs and data-driven insights through compelling storytelling and clear visualizations, empowering business stakeholders to make informed, data-backed decisions. Product-Oriented Mindset: Develop a deep understanding of the business domain and product vision, ensuring that your work is not just technically sound but also delivers tangible and measurable value to the end-user. Master's degree in Computer Science, Statistics, Mathematics, Engineering, or a related quantitative field. 3+ years of hands-on professional experience in a data science role focused on building and deploying machine learning models. Strong proficiency in Python and its core data science libraries (e.g., pandas, NumPy, scikit-learn, Matplotlib/Seaborn). Solid proficiency in SQL for complex data querying, transformation, and analysis. Experience building models for business applications such as forecasting, classification, clustering, or regression. Familiarity with at least one major cloud platform (AWS, GCP, Azure). Proven experience implementing and managing model monitoring systems to detect data and model drift in a production environment is highly desired. A forward-looking interest in the application of Generative AI, with an enthusiasm to learn how to combine LLMs and other generative techniques with traditional machine learning as part of your professional development in the role. Hands-on experience with MLOps principles and tools (e.g., MLflow, Kubeflow, feature stores). Participation in interesting and challenging projects Flexible working hours A great, non-corporate atmosphere Stable employment conditions (contract of employment or B2B contract) Opportunities for development and promotion Attractive package of benefits Remote work We reserve the right to contact the selected candidates.",[],Data Science,Data Science
Full-time,Senior,Permanent,Hybrid,463,Data Scientist,ERGO Technology & Services,"ERGO Technology & Services S.A. (ET&S S.A.) was established in January 2021 following the integration of ERGO Digital IT and Atena into one entity, leveraging both companies‚Äô strengths and best practices. As a part of ERGO Technology & Services Management AG, the technology holding of ERGO Group AG, we support millions of internal and external customers with state-of-the-art IT solutions to everyday problems. In October 2022, ET&S S.A. expanded its scope of operations by creating a Business Services unit to contribute in a new way to the growth of ERGO‚Äôs business. Acting as a co-partner and internal consultant, it adds non-IT value and supports the development of the entire ERGO Group, currently offering skills in reporting, analysis, actuarial, and input management. We are committed to fostering innovation and meeting the evolving needs of our clients worldwide. Discover how we implement AI, IoT, Voice Recognition, Big Data science, advanced mobile solutions, and business-related services to anticipate and address our customers‚Äô future needs. As a Data Scientist, you‚Äôll work with stakeholders across the business to design and deliver AI-powered solutions that solve meaningful problems. From shaping prototypes to deploying real-world applications, you'll bridge the gap between technical possibility and business impact. Whether it‚Äôs machine learning, NLP, or unlocking the value of unstructured data ‚Äî this role is about making AI practical, scalable, and useful. You'll be joining a team, where data and analytical excellence have been integral to the company‚Äôs success from the very beginning. With a business model that is inherently international and data-driven, you‚Äôll contribute to advancing how we apply AI across diverse business domains ‚Äî at real scale. collaborating with business and tech teams to translate ideas into data- and AI-driven solutions developing and deploying machine learning and NLP models using modern tools and methods preparing structured and unstructured data for analysis and modeling creating visual and analytical outputs that support decision-making and tell compelling data stories contributing to internal initiatives to improve our data infrastructure, tools, and methods working in cross-functional, agile teams ‚Äî and keeping stakeholders informed along the way fluent English (C1+) a degree (Master‚Äôs or higher) in a quantitative or technical field (e.g., Mathematics, Informatics, Natural Sciences, or Engineering) strong hands-on experience in programming; knowledge of Python and Kubernetes (K8s) is a plus experience working with NLP tasks and unstructured data familiarity with cloud platforms and tools that support scalable model deployment (e.g., MLOps, Docker, Git) sharp analytical and conceptual skills with a structured approach to problem-solving experience working in high-performance, cross-functional teams and fast-paced project environments proven ability to distill complexity and communicate insights clearly to both technical and non-technical audiences comfortable working at the interface of business, data, and technology a proactive mindset and willingness to take ownership from idea to implementation knowledge of German (B1+) experience working with Large Language Models and Generative AI in a business setting exposure to agile working methods and cross-functional product teams understanding of SQL, data pipelines, and basic software engineering principles certifications or formal training in AI/ML or cloud environments (e.g., Azure, AWS, Databricks) Medical package, sports card, and numerous sports sections ‚Äì these are some of the beneÔ¨Åts that help our employees stay in good shape. Work-life balance is a key aspect of a healthy workplace. We offer our employees flexible working hours, a confidential employee assistant program, as well as the possibility of remote working. However, staying at home with our in-office gaming room and dog-friendly office in Warsaw won‚Äôt be easy. We organize numerous workshops and training courses. Thanks to hackathons and meetups, our specialists share their expertise with others. Additionally, we have a wide range of digital learning platforms and language courses. Each year, we participate in several CSR activities, during which, together with our colleagues, we do our best to create a better future. Company-wide bike races and soccer matches, Ô¨Ålm marathons in our cinema room or other engaging team-building activities ‚Äì we got it covered! Every team member is valued, regardless of gender, nationality, religious beliefs, disability, age, and sexual orientation or identity. Your qualiÔ¨Åcations, experience, and mindset are our greatest beneÔ¨Åt!",[],Data Science,Data Science
Full-time,Senior,Permanent or B2B,Remote,464,Senior Data Scientist,Transition Technologies MS,"Your responsibilities: You'll lead the implementation of advanced user behavior analytics that directly impact our product adoption. You will be a critical bridge between our Informatics Experience project and the organisation's AI initiatives with emerging AI agent capabilities. You'll facilitate team collaboration and translate data requirements to ensure the new autonomous systems can become integrated into our workflows. You'll be part of the architecture, development, and UX streams that design the comprehensive observability pipeline and build expertise across the whole data life cycle‚Äîfrom collecting and processing to visualization and storytelling. We will rely heavily on behavioral modelling techniques to transform raw user and system data into actionable intelligence. You will have significant technical autonomy to shape our observability architecture from the ground up, integrating Grafana, Enterprise Search, and OpenTelemetry standards. We are looking for you, if you have: Creating and executing an observability strategy for user profiling with tailored metric Ability to select the proper data sources for tracking user behavior Experience implementing Grafana and Enterprise Search (Sinequa), including architectural review of requirements, POC for production deployment Experience with working with AI tools and their practical application in user segmentation Strong understanding of AI agent architectures and their implementation in enterprise environments Knowledge of analysis and behavioural modelling techniques for user profiling Ability to optimise and evaluate behavioural model quality using FAIR standards Ability to create data governance principles and lifecycle Ability to visualise data and insights with data-driven storytelling. Experience serving as a technical liaison between traditional data science teams and emerging AI agent initiatives. Experience collaborating with multidisciplinary teams (Reliability & Observability Product, Enterprise Search/Sinequa, Galileo, UX, Product Design, Development and RDI) We offer: Interesting and challenging projects Flexible working hours Friendly, non-corporate atmosphere Stable working conditions (CoE or B2B) Possibility for self-development and promotion in the company Rich benefits package Possibility to work remotely We reserve the right to contact the selected candidates.",[],Data Science,Data Science
Full-time,Senior,Permanent or B2B,Remote,465,Senior Big Data Developer,EPAM Systems,"We are looking for a Senior Big Data Developer to join our growing Data Practice and make our team even stronger. Our projects and technologies are very diverse and cover all technologies currently on the market and represented by open-source communities. We are providing our service to Clients in different domains: Financial, Health Care, Insurance and many others, so you will have a chance to develop yourself in any direction you want. RESPONSIBILITIES Implement data ingestion from diverse sources utilizing technologies including RDBMS, REST HTTP API, flat files, Streams, Time series data, SAP Research and utilize Big Data technologies for effective data ingestion Process and transform data using tools such as Spark and various Cloud Services Understand and execute project-specific business logic through programming languages supported by the primary data platform Optimize data retrieval, develop dashboards, and perform data validation Collaborate with diverse teams including data scientists, analysts, and IT professionals Provide thought leadership on Big Data best practices and technology trends Maintain and ensure integrity and security of big data sources and platforms Perform troubleshooting, debugging, and upgrading of big data systems and applications REQUIREMENTS Bachelor‚Äôs degree in Computer Science, Mathematics or related technical field 5+ years experience in software development with Big Data technologies Advanced knowledge of Python, Scala, or Java Proficient with Spark and Databricks Experience with cloud platforms such as Azure, Google Cloud Platform, Amazon Web Services Demonstrated experience with big data models, databases, and tools Strong understanding of different data file formats like JSON, Parquet, Avro Hands-on experience with SQL and NoSQL databases, including Cassandra and MongoDB Proficiency in data warehousing solutions, ETL processes Proven ability to work within a fast-paced, team-oriented environment Detail-oriented with excellent analytical and problem-solving skills Knowledge of software development methodologies and best practices Exceptional oral and written communication skills in English (B2+) NICE TO HAVE Experience with Hadoop, Spark, Kafka, Oozie, Airflow, HDFS, YARN Experience with message queues, especially Kafka Production experience in search technologies such as Solr, OpenSearch, or Lucene WE OFFER We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Relocation within our 50+ offices We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, O‚ÄôReilly, Cloud Guru Language classes in English and Polish for foreigners We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Unclassified,Unclassified
Full-time,Mid,B2B,Remote,466,Data Engineer (MS Azure) - Future Opportunities,Avanade Poland,"This is ""future opportunities"" ad - we will begin recruitment process after summer (September). You can expect to hear from us at that time. The role 100% remote so it is available for Candidates from all over the Poland. Do you love making sure that information is available and consumable? So do we. Are you passionate about transforming raw data into actionable insights? We're seeking a Data Engineer with expertise in the Microsoft Azure Data realm, including Delta Lake, Databricks, and ETL processes. Key Responsibilities: Design, build, and maintain scalable, efficient, and reliable data pipelines using Databricks and Azure services (e.g., Azure Data Lake, Azure SQL Database, Azure Databricks, Azure Data Factory). Lead the end-to-end development of data architectures and ETL processes to support data-driven decision-making across the organization. Collaborate with data scientists, analysts, and other engineering teams to ensure data integration and optimization. Implement data governance, data quality, and data security best practices across data systems. Mentor and provide guidance to junior data engineers, fostering a culture of continuous learning and improvement. Optimize performance of data workflows, troubleshoot issues, and ensure the reliability of data solutions. Stay up-to-date with the latest developments in cloud technologies, data engineering best practices, and emerging tools and frameworks Required Skills and Qualifications: Extensive experience with Databricks and Azure services (e.g., Azure Data Lake, Azure SQL Database, Azure Databricks, Azure Data Factory). Strong expertise in SQL, Python and other data engineering programming languages. Proven experience in building and managing complex data pipelines and distributed data architectures. Solid understanding of data engineering concepts, such as ETL, data modeling, data warehousing, and data governance. Ability to optimize large-scale data workflows and troubleshoot complex data issues. Strong leadership and communication skills, with the ability to guide teams, collaborate with cross-functional stakeholders, and communicate technical concepts to non-technical audiences. Proven track record of successfully delivering large-scale data projects and systems. Preferred Skills: Experience with other Azure tools such as Azure Microsoft Fabric, Azure Synapse Analytics, Azure IoT Hub, Azure HDInsight, Kafka and Azure Stream Analytics. Experience with containerization (Docker/Kubernetes) and orchestration tools (e.g., Apache Airflow). Familiarity with DevOps principles and CI/CD pipelines for data engineering workflows. Experience with data warehouses and real-time data processing frameworks. Experience with LLMs and orchestration frameworks (e.g., LangChain, Semantic Kernel) is a plus. Exposure to machine learning concepts and frameworks (e.g., MLflow, TensorFlow) is a plus. We offer: Remote/hybrid work ‚Äì you choose! Access to Microsoft certifications, trainings and e-learning platforms (Pluralsight, Rosetta Stone) Career/development path ‚Äì every employee has his/her Mentor Chill rooms at the offices: table football, game console, board games Free parking spot at the offices We participate in charity programs: Szlachetna Paczka, Business Run Sports section on the Strava: Poland Club | Avanade Poland Running on Strava Company events (Team outings, Meetups) - check our Instagram! Avanade Poland These are just a few of the Avanade benefits, let's talk about the others!","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,467,Data Scientist,ING Bank ≈ölƒÖski,"Forma zatrudnienia : umowa o pracƒô Lokalizacja: Katowice lub Warszawa (praca hybrydowa - 2 dni z biura) O zespole: Do≈ÇƒÖcz do zespo≈Çu majƒÖcego za zadanie wdro≈ºenie zaawansowanych rozwiƒÖza≈Ñ wspierajƒÖcych analizƒô danych finansowych i ESG. Buduj z nami najlepsze ≈õrodowisko analityczne rozwijajƒÖce raportowanie w Pionie Finans√≥w. Jako Data Scientist w naszym zespole bƒôdziesz poszukiwa≈Ç nowego podej≈õcia do wyciƒÖgania wniosk√≥w, projektowa≈Ç rozwiƒÖzania E2E na potrzeby zaawansowanej analizy i monitoringu danych, projektowa≈Ç rozwiƒÖzania wykorzystujƒÖce zaawansowane mo≈ºliwo≈õci GCP. Twoje zadania projektujesz i wdra≈ºasz produkcyjnie zaawansowane modele statystyczne oraz algorytmy uczenia maszynowego, aby analizowaƒá i wyciƒÖgaƒá wnioski ze z≈Ço≈ºonych zbior√≥w danych ‚Äì w szczeg√≥lno≈õci szereg√≥w czasowych i danych tekstowych z uwzglƒôdnieniem optymalizacji zasob√≥w, czasu przetwarzania i uniwersalno≈õci podej≈õcia rekomendujesz sposoby weryfikacji jako≈õci danych zgodnie z zasadami Data Governance - m.in. poprzez identyfikacjƒô trend√≥w, definiowanie prog√≥w (threshold√≥w) i detekcjƒô anomalii monitorujesz metryki jako≈õci modeli dzia≈ÇajƒÖcych w ≈õrodowisku produkcyjnym i reagujesz na zmiany ich warto≈õci dbasz o bie≈ºƒÖce aktualizacje kodu w repozytorium GIT oraz dokumentacji technicznej dla proces√≥w produkcyjnych zapewniasz testy jednostkowe, by zwiƒôkszyƒá kontrolƒô nad poprawno≈õciƒÖ dzia≈Çania kodu po wprowadzeniu zmian wsp√≥≈Çpracujesz z zespo≈Çami IT i biznesowymi przy wdra≈ºaniu rozwiƒÖza≈Ñ analitycznych migrujesz i rozwijasz procesy w chmurze (GCP, Azure) oraz rekomendujesz zmiany w istniejƒÖcych rozwiƒÖzaniach, by lepiej wykorzystaƒá mo≈ºliwo≈õci chmurowe przedstawiasz wnioski i rekomendacje interesariuszom w spos√≥b jasny i zwiƒôz≈Çy, dbajƒÖc o efektywnƒÖ komunikacjƒô z biznesem ustalasz priorytety i zarzƒÖdzasz zadaniami zgodnie z metodykƒÖ Agile/Scrum, zapewniajƒÖc ich terminowƒÖ realizacjƒô Nasze oczekiwania: masz do≈õwiadczenie jako developer Python (5 lat) - poziom zaawansowany sprawnie budujesz testy jednostkowe bardzo dobrze znasz SQL i PostgreSQL masz do≈õwiadczenie w zakresie in≈ºynierii danych i w budowaniu produkcyjnych rozwiƒÖza≈Ñ BIG DATA masz do≈õwiadczenie w wdra≈ºaniu produkcyjnie rozwiƒÖzania z wykorzystaniem technologii Machine Learning masz do≈õwiadczenie w pracy w ≈õrodowisku Hadoop Data Lake znasz technologie dostƒôpne w ramach GCP (mile widziane do≈õwiadczenie we wdra≈ºaniu produkcyjnych rozwiƒÖza≈Ñ w Vertex AI, Dialog Flow) znasz techniki monitorowania modeli dzia≈ÇajƒÖcych produkcyjnie w swojej pracy wykorzystujesz: Control-M, Azure, GIT, SQL Server Management Studio, SQL Developer, PGAdmin, AzureDataStudio To oferujemy: Inwestujemy w rozw√≥j - proponujemy dofinansowanie szkole≈Ñ i kurs√≥w oraz dostƒôp do platform edukacyjnych: Udemy Business, eTutor, wewnƒôtrznej platformy elearningowej. Oferujemy tak≈ºe sta≈ºe rozwojowe czy konsultacje z doradcami kariery. Dbamy o zdrowie i bezpiecze≈Ñstwo - oferujemy pakiet medyczny i ubezpieczenie na ≈ºycie dla Ciebie i Twoich bliskich, dostƒôp do platformy Mindgram z indywidualnymi konsultacjami ze specjalistami i webinariami. PracujƒÖc w ING masz mo≈ºliwo≈õƒá do≈ÇƒÖczenia do Pracowniczego Programu Emerytalnego i Programu Profilaktyki Onkologicznej. Work life balance w praktyce to dodatkowe dni wolne: ‚Äûdzie≈Ñ dla rodziny‚Äù (od Ciebie zale≈ºy jak go wykorzystasz), wolne dni w formie nagrody i wolne godziny na wolontariat czy profilaktykƒô zdrowotnƒÖ. Dla rodzic√≥w mamy dodatkowe dni urlopu macierzy≈Ñskiego i ojcowskiego oraz dni wolne na opiekƒô nad dzieckiem, niezale≈ºenie od wykorzystanego limitu przez drugiego rodzica. Do≈ÇƒÖczajƒÖc do nas decydujesz siƒô na pracƒô w stabilnej i zorientowanej na komfortowe warunki pracy organizacji z certyfikatem Top Employer. Przysz≈Ço≈õƒá to nasza wsp√≥lna inwestycja - razem z ING mo≈ºesz zaanga≈ºowaƒá siƒô w ka≈ºdƒÖ inicjatywƒô wspierajƒÖcƒÖ ESG i odpowiedzialno≈õƒá spo≈ÇecznƒÖ. Decydujesz na jakim sprzƒôcie pracujesz. Ka≈ºdemu pracownikowi zapewniamy niezbƒôdny sprzƒôt do pracy, ale je≈õli chcesz pracowaƒá na wybranej przez siebie marce laptopa czy smartfona, kt√≥ry jednocze≈õnie wykorzystasz do swoich cel√≥w prywatnych, to dajemy takƒÖ mo≈ºliwo≈õƒá. Oferujemy program refundacji i zni≈ºek na wybrane zakupy. ≈öwiƒôtujemy wsp√≥lnie i poznajemy siƒô na zespo≈Çowych wyjazdach, imprezach rodzinnych i sportowych, jak Dzie≈Ñ Dziecka, Biegnij Warszawo czy Turnieje sportowe. Pe≈Çne zieleni przestrzenie biurowe przeznaczone do pracy i spotka≈Ñ oraz miejsca relaksu - wszystko do Twojej dyspozycji.",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,468,Programista Baz Danych,TSS,"W TSS tworzymy najwy≈ºszej jako≈õci rozwiƒÖzania z zakresu Software Development, FinTech, AI Solution. Tworzymy systemy p≈Çatnicze, bramki p≈Çatnicze online oraz rozwiƒÖzania umo≈ºliwiajƒÖce innowacyjne procesowanie p≈Çatno≈õci. Nasze zespo≈Çy uczestniczƒÖ r√≥wnie≈º w projektach wykonywanych dla klient√≥w z wielu r√≥≈ºnych bran≈º i specjalizacji. Je≈õli chcesz do≈ÇƒÖczyƒá do zespo≈Çu entuzjast√≥w, dla kt√≥rych praca jest jednocze≈õnie pasjƒÖ, przygodƒÖ i mo≈ºliwo≈õciƒÖ rozwoju zawodowego do≈ÇƒÖcz do team‚Äôu TSS ju≈º teraz! Do≈õwiadczenie: 5 lat w pracy z relacyjnymi bazami danych np. PostgreSQL, MySQL, MSSQL, ORACLE 4 lata do≈õwiadczenia w zakresie wykorzystania jednego z proceduralnych jƒôzyk√≥w programowania np. PL/SQL, PL/PqSQL Bardzo dobra znajomo≈õƒá SQL oraz PostgreSQL Znajomo≈õƒá zasad zarzƒÖdzania, konfiguracji i optymalizacji bazy danych PostgresSQL Do≈õwiadczenie w migracji danych z system√≥w klasy enterprise Do≈õwiadczenia w analizie i transformacji danych Projektowanie i eksploatacja baz danych systemu Optymalizacja obecnie eksploatowanych baz danych systemu Bie≈ºƒÖca wsp√≥≈Çpraca z zespo≈Çem wytw√≥rczym Co oferujemy? Mo≈ºliwo≈õƒá pracy w pe≈Çni zdalnej lub w biurze w Warszawie; StabilnƒÖ wsp√≥≈Çpracƒô na podstawie B2B; Dofinansowanie do prywatnej opieki medycznej w PZU; Wsparcie w rozwoju zawodowym - wewnƒôtrzne szkolenia z zakresu cyberbezpiecze≈Ñstwa;","[{""min"": 14000, ""max"": 18000, ""type"": ""Net per month - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,469,Data Engineer with Databricks,emagine Polska,"üí∞ Rate: B2B ‚Äì 37-43 EUR/h. üåç Work model: fully remote. ‚è≥ Project length: 12 months + extensions.‚è∞ Start date for assignment: ASAP/ 1 month.üìï Project language: English, Polish. üíº Industry: pharmaceutical. ‚öôÔ∏è Recruitment process : 2 interviews with the client.üíª Workload: Full time. Summary: The Data Engineer role focuses on developing and integrating data solutions to enhance production processes and support business needs within the Manufacturing Intelligence (MI) team. Main Responsibilities: Software development activities, including writing, reviewing, refactoring, testing, and documenting code. Build stable and scalable data pipelines for production environments. Provide technical expertise and act as a trusted advisor on projects. Collaborate with Agile teams to understand and manage technical work. Participate in Agile events such as Program Increment planning and system demos. Key Requirements: Experience with data engineering principles (data warehousing, batch processing, data streaming, etc.). Experience with Databricks Experience building CI/CD pipelines . Coding/scripting skills in Bash, Python , or similar languages. Hands-on experience with AWS services (Lambda, Glue, IAM, etc.) or Azure Experience with Infrastructure as Code (IaC) for managing cloud resources. Strong mathematical, statistical, and problem-solving skills. Experience in pharma, manufacturing, medical or life since industry Nice to Have: Strategic and innovative mindset. Strong communication skills. Systematic approach to problem-solving.","[{""min"": 169, ""max"": 182, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,470,Data Engineering Architect (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Data Engineering Architect , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company. This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. This role represents a gradual shift away from hands-on coding towards a more strategic focus on system design, business consultation, and creative problem-solving. It offers an opportunity to engage more deeply with architecture-level decisions, collaborate closely with clients, and contribute to building innovative data-driven solutions from a broader perspective. üöÄ Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Airflow, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. üéØ What you'll need to succeed in this role: 5+ years of proven commercial experience in implementing, developing, or maintaining Big Data systems. Strong programming skills in Python or Java/Scala : writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Hands-on experience with Big Data technologies like Spark , Airflow , Iceberg , CI/CD, Kafka. Proven expertise in implementing and deploying solutions in cloud environments (with a preference for AWS ). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master‚Äôs or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. Fluent English (C1 level) is a must. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn , Instagram ).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,471,Senior Data Engineer with Snowflake,N-iX,"#3668 Join an exciting journey to create a greenfield, cutting-edge Consumer Data Lake for a leading global organization based in Europe. This platform will unify, process, and leverage consumer data from various systems, unlocking advanced analytics, insights, and personalization opportunities. As a Senior Data Engineer, you will play a pivotal role in shaping and implementing the platform's architecture, focusing on hands-on technical execution and collaboration with cross-functional teams. Your work will transform consumer data into actionable insights and personalization on a global scale. Using advanced tools to tackle complex challenges, you‚Äôll innovate within a collaborative environment alongside skilled architects, engineers, and leaders. Key Responsibilities: Hands-On Development: Build, maintain, and optimize data pipelines for ingestion, transformation, and activation. Create and implement scalable solutions to handle diverse data sources and high volumes of information. Data Modeling & Warehousing: Design and maintain efficient data models and schemas for a cloud-based data platform. Develop pipelines to ensure data accuracy, integrity, and accessibility for downstream analytics. Collaboration: Partner with Solution Architects to translate high-level designs into detailed implementation plans. Work closely with Technical Product Owners to align data solutions with business needs. Collaborate with global teams to integrate data from diverse platforms, ensuring scalability, security, and accuracy. Platform Development: Enable data readiness for advanced analytics, reporting, and segmentation. Implement robust frameworks to monitor data quality, accuracy, and performance. Testing & Quality Assurance: Implement robust security measures to protect sensitive consumer data at every stage of the pipeline Ensure compliance with data privacy regulations (e.g., GDPR, CCPA ..) and internal policies. Monitor and address potential vulnerabilities, ensuring the platform adheres to security best practices. Requirements: Over 4+ years of experience showcasing technical expertise and critical thinking in data engineering. Hands-on experience with DBT and strong Python programming skills. Proficiency in Snowflake and expertise in data modeling are essential. Demonstrated experience in building consumer data lakes and developing consumer analytics capabilities is required. In-depth understanding of privacy and security engineering within Snowflake , including concepts like RBAC, dynamic/tag-based data masking, row-level security/access policies, and secure views. Ability to design, implement, and promote advanced solution patterns and standards for solving complex challenges. Familiarity with multiple cloud platforms ( Azure or GCP preferred, with a focus on Azure). Practical experience with Big Data batch and streaming tools. Competence in SQL, NoSQL, relational database design (SAP HANA experience is a bonus), and efficient methods for data retrieval and preparation at scale. Proven ability to collect and process raw data at scale, including scripting, web scraping, API integration, and SQL querying. Experience working in global environments and collaborating with virtual teams. A Bachelor‚Äôs or Master‚Äôs degree in Data Science, Computer Science, Economics, or a related discipline. We offer*: Flexible working format - remote, office-based, or flexible. A competitive salary and a good compensation package. Personalized career growth. Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more). Active tech communities with regular knowledge sharing. Education reimbursement. Memorable anniversary presents. Corporate events and team building. Other location-specific benefits. *not applicable for freelancers","[{""min"": 21872, ""max"": 30584, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,472,Senior Azure Data Engineer,Link Group,"Data Engineer ‚Äì Azure üìç 100% remote | üïí Full-time | üåç International environment We are looking for an experienced Data Engineer with strong expertise in the Azure ecosystem to join a dynamic data team delivering scalable and high-performance data solutions. You‚Äôll play a key role in designing, building, and optimizing modern data pipelines and data lake architectures using cutting-edge cloud technologies. üîß Key Responsibilities: Design and develop robust and efficient data pipelines using Azure Databricks, Spark, and PySpark Work with Delta Lake architecture to manage structured and semi-structured data Perform data modeling, transformation, and performance tuning for large datasets Build and manage Azure Data Factory pipelines and Azure Functions for orchestrating workflows Integrate various data formats such as Parquet, Avro, and JSON Collaborate with cross-functional teams to understand data requirements and deliver optimal solutions Use Git for version control and manage code in a collaborative environment Write efficient Python and SQL code for data processing and querying Ensure data quality, consistency, and reliability across the platform ‚úÖ Core Requirements: Solid hands-on experience in Azure Databricks, Spark, and PySpark Deep knowledge of Delta Lake and modern data lakehouse architectures Proficiency in data modeling and performance optimization techniques Experience with ADF (Azure Data Factory) and Azure Functions Strong skills in Python, SQL, and data serialization formats (Parquet, Avro, JSON) Familiarity with version control systems, especially Git Ability to work independently in a fully remote, distributed team Good communication skills in English","[{""min"": 18000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,473,Specjalistka/Specjalista ds. Data Science (LLM),UNIQA,"Grupƒô UNIQA w Polsce reprezentujƒÖ sp√≥≈Çki UNIQA TU S.A. oraz UNIQA TU na ≈ªycie S.A., oferujƒÖce ubezpieczenia majƒÖtkowe, komunikacyjne i ≈ºyciowe, zar√≥wno dla klient√≥w indywidualnych, jak i instytucjonalnych. UNIQA jest blisko klient√≥w, zapewniajƒÖc im profesjonalne doradztwo i pomoc w wyborze ubezpieczenia. Doceniamy i promujemy zaanga≈ºowanie, niekonwencjonalne pomys≈Çy i dzia≈Çanie. Je≈õli cenisz miejsce pracy, w kt√≥rym wa≈ºna jest atmosfera wsp√≥≈Çpracy i zaufania, zapraszamy do nas! Specjalistka/Specjalista ds. Data Science (LLM) Opis stanowiska ≈öcis≈ÇƒÖ wsp√≥≈Çpraca z wieloma zespo≈Çami z r√≥≈ºnych obszar√≥w organizacji w celu wspierania strategii biznesowej za pomocƒÖ zaawansowanych technik pozwalajƒÖcych uwolniƒá potencja≈Ç danych. Projektowanie, wdra≈ºanie i optymalizacja modeli uczenia maszynowego oraz integracja ich z istniejƒÖcymi systemami. Tworzenie rozwiƒÖza≈Ñ opartych o LLM: analiza du≈ºych zbior√≥w danych tekstowych i przygotowanie ich do wykorzystania w du≈ºych modelach jƒôzykowych; fine-tuning istniejƒÖcych modeli w celu dostosowania ich do specyficznych potrzeb. Rozw√≥j wewnƒôtrznego feature store oraz bazy wektorowej. ZarzƒÖdzanie cyklem ≈ºycia modeli ML na wszystkich etapach ich rozwoju i rozw√≥j wewnƒôtrznego ekosystemu MLOps/LLMOps. Monitorowanie wydajno≈õci algorytm√≥w zar√≥wno pod kƒÖtem metryk jako≈õci jak i generowanej warto≈õci biznesowej. Wymagania Wykszta≈Çcenie kierunkowe, obejmujƒÖce wiedzƒô z obszaru AI/ML (Matematyka, Statystyka, Informatyka i inne pokrewne). Do≈õwiadczenie w obszarze in≈ºynierii AI/ML w ≈õrodowisku zarzƒÖdzanym w zwinnych metodologiach. Praktyczne do≈õwiadczenie w projektowaniu, wdra≈ºaniu i utrzymywaniu rozwiƒÖza≈Ñ AI/ML we wszystkich fazach ich rozwoju. Praktyczna znajomo≈õƒá algorytm√≥w uczenia maszynowego (uczenie nadzorowane/nienadzorowane, g≈Çƒôbokie sieci neuronowe, regresja, klasyfikacja, segmentacja, NLP itp.) Do≈õwiadczenie w pracy z bazami wektorowymi, modelami LLM, ich fine-tuningiem, optymalizacjƒÖ i kontrolƒÖ wersji. Praktyczna znajomo≈õƒá jƒôzyka Python oraz bibliotek wykorzystywanych w AI/ML (pandas, numpy, scikit-learn, tensorflow/pytorch, xgboost, nltk, pyspark, hugging face, langchain itp.) Bardzo dobra znajomo≈õƒá jƒôzyka SQL oraz systemu kontroli wersji git. Do≈õwiadczenie w pracy ze ≈õrodowiskami chmurowymi oraz w przetwarzaniu du≈ºych zbior√≥w danych (big data). Doskona≈Ça komunikacja z umiejƒôtno≈õciƒÖ przekazywania skomplikowanych rozwiƒÖza≈Ñ r√≥≈ºnym grupom interesariuszy. Jƒôzyk angielski min. B2. Ôªø Oferujemy Kulturƒô organizacyjnƒÖ opartƒÖ na 5 warto≈õciach: wsp√≥lnota, prostota, klient przede wszystkim, odpowiedzialno≈õƒá i wiarygodno≈õƒá. Pracƒô w modelu hybrydowym. Wsp√≥≈Çpraca na podstawie umowy o pracƒô na pe≈Çen etat. Kafeteryjny system benefit√≥w ‚Äì to Ty decydujesz z czego korzystasz.",[],Data Science,Data Science
Full-time,Senior,B2B,Hybrid,474,Data Engineer Hadoop,Antal Sp. z o.o.,"Hadoop Data Engineer (GCP, Spark, Scala) ‚Äì Krak√≥w / Hybrid We are looking for an experienced Hadoop Data Engineer to join a global data platform project built in the Google Cloud Platform (GCP) environment. This is a great opportunity to work with distributed systems, cloud-native data solutions, and a modern tech stack. The position is based in Krak√≥w (hybrid model ‚Äì 2 days per week in the office). Your responsibilities: Design and build large-scale, distributed data processing pipelines using Hadoop, Spark, and GCP Develop and maintain ETL/ELT workflows using Apache Hive, Apache Airflow (Cloud Composer), Dataflow, DataProc Work with structured and semi-structured data using BigQuery, PostgreSQL, Cloud Storage Manage and optimize HDFS-based environments and integrate with GCP components Participate in cloud data migrations and real-time data processing projects Automate deployment, testing, and monitoring pipelines (CI/CD using Jenkins, GitHub, Ansible ) Collaborate with architects, analysts, and product teams in Agile/Scrum setup Troubleshoot and debug complex data logic at the code and architecture level Contribute to cloud architecture patterns and data modeling decisions Must-have qualifications: Minimum 5 years of experience as a Data Engineer / Big Data Engineer Hands-on expertise in Hadoop, Hive, HDFS, Apache Spark, Scala, SQL Solid experience with GCP and services like BigQuery, Dataflow, DataProc, Pub/Sub, Composer (Airflow) Experience with CI/CD processes and DevOps tools: Jenkins, GitHub, Ansible Strong data architecture and data engineering skills in large-scale environments Experience working in enterprise environments and with external stakeholders Familiarity with Agile methodologies such as Scrum or Kanban Ability to debug and analyze application-level logic and performance Nice to have: Google Cloud certification (e.g., Professional Data Engineer) Experience with Tableau, Cloud DataPrep, or Ansible Knowledge of cloud design patterns and modern data architectures Work model: Hybrid ‚Äì 2 days per week from the Krak√≥w office (rest remotely) Opportunity to join an international team and contribute to global-scale projects To learn more about Antal, please visit www.antal.pl","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,475,Data Scientist,Provident Polska,"Na tym stanowisku bƒôdziesz odpowiedzialna/odpowiedzialny za ca≈Çy proces budowania modeli predykcyjnych od zebrania wymaga≈Ñ biznesowych, poprzez budowƒô modelu, a≈º po jego wdro≈ºenie. Nasz zesp√≥≈Ç Data Science sk≈Çada siƒô zar√≥wno z m≈Çodszych jak i starszych analityk√≥w, wiƒôc mo≈ºesz aplikowaƒá niezale≈ºnie od swojego do≈õwiadczenia. Tw√≥j zakres obowiƒÖzk√≥w: Budowa i implementacja modeli klasyfikacyjnych oraz regresyjnych w obszarach takich jak zarzƒÖdzanie ryzykiem kredytowym, strategie marketingowe czy zarzƒÖdzanie administracyjne. Opracowywanie oraz testowanie struktur danych niezbƒôdnych do tworzenia modeli prognostycznych. Okresowe przeprowadzanie walidacji modeli prognozujƒÖcych i analiza ich jako≈õci. Projektowanie i implementacja rozwiƒÖza≈Ñ chmurowych wspierajƒÖcych procesy analityczne. Prezentowanie wynik√≥w modelowania r√≥≈ºnym grupom interesariuszy. Nasze wymagania: Wykszta≈Çcenie wy≈ºsze magisterskie w zakresie informatyki, statystyki stosowanej, ekonometrii, matematyki, bada≈Ñ operacyjnych lub innym obszarze nauk ilo≈õciowych lub w trakcie ostatniego semestru studi√≥w. Do≈õwiadczenie w pracy z du≈ºymi i zr√≥≈ºnicowanymi zbiorami danych, pozyskiwanymi z wielu ≈∫r√≥de≈Ç (w tym z nowych ≈∫r√≥de≈Ç cyfrowych). Do≈õwiadczenie w modelowaniu statystycznym i/lub uczeniu maszynowym. Do≈õwiadczenie w stosowaniu metod data science do rozwiƒÖzywania problem√≥w biznesowych oraz chƒôƒá zrozumienia jak praktycznie wykorzystaƒá te informacje. Znajomo≈õƒá jƒôzyka programowania R or Python (wymagane). Znajomo≈õƒá architektury baz danych (SQL). Dobra komunikacja i umiejƒôtno≈õƒá prezentacji, oraz ≈Çatwo≈õƒá w wyja≈õnianiu z≈Ço≈ºonych pojƒôƒá analitycznych osobom nie bƒôdƒÖcym specjalistami w tej dziedzinie. Bardzo dobra znajomo≈õƒá j. angielskiego w mowie i pi≈õmie. Co mo≈ºesz zyskaƒá, pracujƒÖc z nami: Stabilne zatrudnienie ‚Äì 93% os√≥b jest zatrudnionych na umowƒô o pracƒô, na czas nieokre≈õlony. Bezpiecze≈Ñstwo ‚Äì jeste≈õmy na polskim rynku ju≈º 28 lat. Przyjazne ≈õrodowisko pracy ‚Äì 12 razy z rzƒôdu otrzymali≈õmy nagrodƒô Top Employer. Pracƒô hybrydowƒÖ ‚Äì zazwyczaj 2 razy w tygodniu widzimy siƒô w biurze (metro Dworzec Gda≈Ñski). Ekstra 3 dni p≈Çatnego urlopu ‚Äì je≈õli wykorzystasz ca≈Çy urlop w danym roku kalendarzowym. PrywatnƒÖ opiekƒô medycznƒÖ z us≈ÇugƒÖ gwarancji terminu (Medicover). Pe≈Çne wdro≈ºenie pod okiem mentora, w tym pakiet profesjonalnych szkole≈Ñ wdro≈ºeniowych. Dostƒôp do platformy rozwojowej, obejmujƒÖcej szkolenia e-learningi, podcasty i webinary. Aktywno≈õci wspierajƒÖce rozw√≥j w organizacji np. cykl szkole≈Ñ ‚ÄûSkuteczny Manager‚Äù dla os√≥b obejmujƒÖcych stanowiska kierownicze. Telefon s≈Çu≈ºbowy (r√≥wnie≈º do u≈ºytku prywatnego). Dostƒôp do platformy kafeteryjnej ProviBenefity, zasilanej co miesiƒÖc kwotƒÖ do wykorzystania lub dofinansowanie do Twojej karty Multisport ‚Äì wybierasz z po≈õr√≥d 5 rodzaj√≥w kart. Ubezpieczenie na ≈ºycie (UNUM ≈ªycie TUiR S.A.) na preferencyjnych warunkach. ≈öwiadczenia ≈õwiƒÖteczne oraz dofinansowanie do wypoczynku ‚ÄûWczasy pod gruszƒÖ‚Äù dla Ciebie oraz dla Twoich dzieci. Wsparcie psychologiczne pracownik√≥w, obejmujƒÖce m.in . opiekƒô psychologa (r√≥wnie≈º dzieciƒôcego), psychoterapeuty, dietetyka, coaching.",[],Data Science,Data Science
Full-time,Mid,B2B,Remote,476,Data Engineer,emagine Polska,"PROJECT INFORMATION: Industry: Banking Location: Remote Type of assignment: B2B RESPONSIBILITIES: As a Data Engineer, your core responsibility will be to manage data artifacts for integration. Develop Component Data Artifacts (CDAs). Break down Master Data Artifacts (MDAs) and Global Data Artifacts (GDAs) into CDAs for integration. Focus on data modeling, data reuse, and understanding data domains. Implement data normalization concepts. Efficiently query data and prepare it for integration into the data lake using Juniper pipelines. REQUIREMENTS: Strong technical understanding of Google Cloud Platform (GCP). Experience with Tableau or Looker. Experience with Apache Airflow or Hadoop. Proficiency in data modeling concepts. Experience with data normalization techniques. Knowledge of data integration methodologies. NICE TO HAVE: Understanding of Juniper pipelines. Knowledge of refinery data processes. OTHER DETAILS: The Data Engineer will collaborate closely with GCP engineers and platform teams. This role focuses on ensuring effective management of data modeling and reuse across various platforms.","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Office,477,"Data Scientist, Chrome",Google,"Master's degree in Statistics, Data Science, Mathematics, Physics, Economics, Operations Research, Engineering, or a related quantitative field. 3 years of work experience using analytics to solve product or business problems, coding (e.g., Python, R, SQL), querying databases or statistical analysis, or a PhD degree. 5 years of work experience using analytics to solve product or business problems, coding (e.g., Python, R, SQL), querying databases or statistical analysis, or a PhD degree. 2 years of work experience in data science or quantitative analytics with focus on statistical modeling, machine learning, and visualization. Experience, education/training, or demonstrated interest in machine learning. Excellent problem-solving skills and business judgment, articulating product questions, pulling data from datasets (SQL, BigQuery or equivalent technologies). Excellent written and verbal communication skills and ability of translating analysis results into business recommendations. Chrome is a consumer, developer and enterprise facing product and platform with billions of users. Data insights and metrics are a part of making decisions about Chrome. You will have a tremendous impact throughout Chrome, as you will have the opportunity to work with many teams that use our systems to gate features and understand users. As a Data Scientist, you will have a unique opportunity to work across various teams to enhance Chrome's features and our understanding of user needs. You will contribute to evolving Chrome into a more personalized and helpful user agent. You will play a role in integrating Google's technology and adapting Chrome to meet the needs of users in their homes, workplaces, and beyond. In addition to our consumer and enterprise users, we are dedicated to improving and optimizing both the standalone app and web experiences through capabilities and Generative AI. You will work on an adopted product, influencing product and engineering directions. You will directly impact users by informing feature development and user understanding. Chrome is dedicated to building a better, more open web. We‚Äôre focused on making a better browser (on both desktop and mobile) to help users take advantage of all the web has to offer in a safe and secure way.Chrome is available across all major platforms ‚Äî iOS, Android, Windows, Mac, Linux and Chrome OS. We also built Chrome as an open source project so the entire web ecosystem could benefit from the latest innovations in speed, simplicity and security. Leverage advanced statistical methods on massive, datasets to extract insights from billions of events and thousands of features across organizational sources. Use custom data infrastructure or existing data models as appropriate, using specialized ideas. Design and evaluate models to mathematically express and solve defined problems with limited precedent. Develop and deploy automated solutions, ranging from SQL query automation to real-time Python classification and ML modeling, to address key issues. Analyze intricate product and platform usage patterns, translating data-driven insights into actionable product strategy and engineering decisions. Collaborate with stakeholders in cross-projects and team settings to identify and clarify business or product questions to answer. Provide feedback to translate and refine business questions into tractable analysis, evaluation metrics, or mathematical models. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",[],Data Science,Data Science
Full-time,Senior,Permanent,Remote,478,Service Delivery Manager,INFOPLUS TECHNOLOGIES,"Summary ‚Äì Service Delivery Manager with a technical background that is comfortable managing projects, and interacting with the business owners. Technical skills that are required is a rounded understanding of web frontend development, Enterprise reporting tools, data warehousing, and databases. Overall a firm understanding of how to build a solution well, and set it up to be a sustainable Enterprise service. Looking for about 10 years of experience. Pharma experience is an asset. Job Description: DSM - Global Portfolio Planning and Business Excellence Service Delivery Manager with a technical background that is comfortable managing projects, and interacting with the business owners. Technical skills that are required is a rounded understanding of web frontend development, Enterprise reporting tools, data warehousing, and databases. Overall a firm understanding of how to build a solution well, and set it up to be a sustainable Enterprise service. Looking for about 10 years of experience. Pharma experience is an asset. Knowledge/Competencies: Ability to influence and ensure efficient and effective delivery of operational plans Knowledge on customer service standards and ITSM processes Knowledge and experience defining and controlling SLAs, KPIs and contract management Significant experience working with virtual teams in a cross-functional organization. Demonstrates customer, quality, cost, and delivery focus Advanced analytical, problem-solving and decision making skills Interpersonal skills to interact and communicate effectively including Conflict Management Technical Skills: Experience in an audited, or validated sytems environment. Core ServiceNow knowledge (Incident, Service Requests, Problem and Change) Strong understanding of frontend and backend development. Data Warehousing, ETL, and Reporting background (Snowflake, TalenD, and Tableau preferred)","[{""min"": 240000, ""max"": 265000, ""type"": ""Gross per year - Permanent""}]",Unclassified,Unclassified
Full-time,Senior,Permanent or B2B,Remote,479,Senior Data Science/AI Engineer,N-iX,"(3767) About client: Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management. Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact. Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company‚Äôs R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 21872, ""max"": 25517, ""type"": ""Net per month - B2B""}, {""min"": 18226, ""max"": 20778, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,480,Data Engineer - SAMs Programme,emagine Polska,"Work model: remote Assignment type: B2B Start: ASAP Project length: 6 months + possible extension Project language: English Industry: Banking The SAMS Programme is focused on decommissioning a legacy CRM system and successfully migrating to a new platform by the end of the year. The main function of the Data Engineer is to support the development of robust data pipelines and ensure smooth data migration and integration. Main Responsibilities: Designing and implementing ETL pipelines to move data into and out of data repositories. Supporting cloud-based engineering tasks , with a strong preference for GCP experience. Contributing to CI/CD pipeline development , ensuring reliable and efficient deployment processes. Writing and maintaining automated unit tests to support integrated testing and ensure data quality. Key Requirements: Experience with ETL pipeline design and implementation. Proficiency in cloud-based engineering, especially with Google Cloud Platform (GCP). Experience with CI/CD pipeline development. Ability to write and maintain automated unit tests for software verification. Nice to Have: Familiarity with legacy CRM systems. Experience in data quality assurance practices. Knowledge of additional cloud platforms (e.g., AWS or Azure).",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,481,BI Developer (WCL Qlik),ITDS,"BI Developer (WCL Qlik) Join us, and turn data into powerful business decisions! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As a BI Developer (WCL Qlik), you will be working for our client, a leading global financial institution undergoing a major digital transformation to enhance its data analytics capabilities. You will be part of a dynamic team focused on developing and maintaining scalable Qlik dashboards and reports, aimed at improving decision-making across various business units. The project involves managing complex data sets, implementing infrastructure best practices, and ensuring compliance within a fast-paced, highly regulated environment. Your role plays a key part in optimizing how data is shared, visualized, and utilized to deliver impactful business insights. Your main responsibilities: Developing advanced dashboards and reports using QlikSense Ensuring data integrity and managing updates to data sources Managing project sites and content on Qlik Server Documenting data sources, processes, and dashboards clearly Analyzing data sharing policies and promoting compliance best practices Collaborating with cross-functional teams and stakeholders at all levels Supporting Qlik deployment by following infrastructure best practices Enhancing dashboard performance for large data sets Influencing stakeholders through thoughtful data presentations Following established internal control standards and audit requirements You're ideal for this role if you have: Proficiency in Qlik with strong dashboard development experience 3+ years of experience in data analysis roles Practical knowledge of QlikSense and Qlik architecture Ability to work with large data sets while maintaining performance High level of mathematical and analytical skills Proven experience in stakeholder management and communication Strong collaboration skills within cross-functional teams Experience documenting technical processes and data sources Ability to adapt quickly in fast-changing environments Understanding of regulatory requirements for data sharing It is a strong plus if you have: Experience with Qlik administration Familiarity with financial services or highly regulated industries Knowledge of best practices for offshore project coordination Prior involvement in change or transformation projects Awareness of risk management and internal control standards We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #...7316 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 21000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,482,Power BI Developer,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Dla naszego Klienta, miƒôdzynarodowej organizacji, poszukujemy os√≥b na stanowisko Power BI Developer. Zakres obowiƒÖzk√≥w: Projektowanie, rozw√≥j i utrzymanie raport√≥w oraz dashboard√≥w w Power BI. Wsp√≥≈Çpraca z zespo≈Çami analitycznymi i biznesowymi w celu identyfikacji wymaga≈Ñ oraz przekszta≈Çcania danych w warto≈õciowe informacje. Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç (SQL, Excel, API, itd.) i ich przetwarzanie. Praca z zapytaniami SQL w celu ekstrakcji, transformacji i ≈Çadowania danych (ETL). Optymalizacja raport√≥w i zapyta≈Ñ w Power BI pod kƒÖtem wydajno≈õci. Tworzenie i wdra≈ºanie modeli danych w Power BI i SQL. Wspieranie proces√≥w analizy danych i podejmowania decyzji biznesowych dziƒôki interaktywnym dashboardom i raportom. Wymagania: Minimum 3-letnie do≈õwiadczenie w pracy z Power BI, w tym tworzeniu raport√≥w, dashboard√≥w i modeli danych. Umiejƒôtno≈õƒá pracy z SQL (zapytania, agregacje, joiny, optymalizacja zapyta≈Ñ). Do≈õwiadczenie w pracy z danymi: przetwarzanie, analiza i modelowanie danych. Znajomo≈õƒá narzƒôdzi ETL i integracji r√≥≈ºnych ≈∫r√≥de≈Ç danych. Umiejƒôtno≈õƒá tworzenia rozwiƒÖza≈Ñ frontendowych w Power BI (dynamiczne raporty, wykresy, tabele). Znajomo≈õƒá podstawowych zasad analizy danych oraz tworzenia wizualizacji, kt√≥re wspierajƒÖ procesy podejmowania decyzji. Dobra znajomo≈õƒá jƒôzyka angielskiego w mowie i pi≈õmie. Dodatkowe atuty: Do≈õwiadczenie z Power Query i DAX. Znajomo≈õƒá narzƒôdzi do automatyzacji proces√≥w analitycznych. Zrozumienie proces√≥w biznesowych i zdolno≈õƒá do przekszta≈Çcania danych w u≈ºyteczne informacje. Oferujemy: Udzia≈Ç w dynamicznych, miƒôdzynarodowych projektach, Mo≈ºliwo≈õƒá pracy w pe≈Çni zdalnej (sporadyczne wizyty w zale≈ºno≈õci od potrzeb biznesowych), Elastyczne godziny pracy oraz przyjaznƒÖ atmosferƒô w zespole, Mo≈ºliwo≈õƒá rozwoju w nowoczesnych technologiach i realny wp≈Çyw na innowacyjne projekty. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 14000, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,483,Data Engineer,best HR and PM solutions,"For our client we are looking for Data Engineer. Client is the emerging leader in the $100B+ cloud communications platform market. Customers like Airbnb, Viber, Whatsapp, Snapchat, and many others depend on client's APIs and SDKs to connect with their customers all over the world. As businesses continue to shift to a real-time, customer-centric communications model, we are experiencing a time of impressive growth. They're looking for a Data Analyst or Data Engineer to join the Engineering Productivity team and help them make smarter, data-driven decisions that improve developer productivity and engineering outcomes. You'll work closely with development, platform, and product teams to analyze, structure, and optimize data flows related to engineering metrics, DevOps performance, and platform usage. This role is ideal for someone who thrives in transforming ambiguous data into actionable insight and is excited about the potential of AI tools in developer platforms. Key Responsibilities: Analyze engineering, operational, and productivity data to uncover trends, risks, and opportunities Design and implement data models that improve accessibility, structure, and long-term maintainability of engineering metrics. Build or enhance ETL pipelines to collect, transform, and export data from various systems (e.g., GitHub, Jira, Security Scans, Costs tools). Partner with stakeholders to define meaningful KPIs across engineering domains (e.g., reliability, security, velocity). Explore and implement GenAI tooling to support automation, summarization, and pattern detection in engineering workflows. Maintain data hygiene and enforce best practices in data governance and lineage within the API Engineering environment. What You‚Äôll Gain: A unique opportunity to shape how engineering data is used across a large and evolving platform organization. The chance to make a difference using GenAI tools in a real-world engineering context. Collaboration with a team driving developer experience, reliability, and engineering consistency at scale. Required Skills and Experience: Proven experience as a Data Analyst or Data Engineer, preferably in a software engineering or DevOps context. Strong SQL skills and experience with Python or another scripting language for data transformation and analysis. Hands-on experience working with APIs and integrating data across SaaS tools (e.g., Jira, GitHub, Datadog). Familiarity with dashboarding/visualization platforms like Looker, Grafana or Tableau. Demonstrated experience structuring unorganized or siloed data into actionable reporting models. Desirable: Experience designing and building ETL pipelines and data lakes or warehouses (e.g. Snowflake). Exposure to GenAI tooling and experience applying AI to engineering or operational workflows. Knowledge of modern data orchestration tools (e.g., Airflow, dbt). Understanding of software development lifecycle and metrics used in engineering productivity and platform health. What we offer: Contract: B2B directly with US company Salary: up to 160 pln/h 100% remote Polish time zone Polish public holidays Long term cooperation Recruitment process: 1-2 technical calls","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,484,Senior Data & Analytics Engineer,N-iX,"About the project: Our customer is the European online car market with over 30 million monthly users, with a market presence in 18 countries. The company is now merging with a similar company in Canada and needs support in this way. As a Data& Analytics Engineer, you will play a pivotal role in shaping the future of online car markets and enhancing the user experience for millions of car buyers and sellers. Requirements: 5+ years of experience in Data Engineering or Analytics Engineering roles Strong experience building and maintaining pipelines in BigQuery, Glue, Athena, and Airflow Solid Python skills, especially for data processing and workflow orchestration Advanced SQL skills and experience designing dimensional models (star/snowflake) Familiarity with data quality tools like Great Expectations Understanding of data governance, privacy, and security principles Experience working with large datasets and optimizing performance Proactive problem solver who enjoys building scalable, reliable solutions English - Upper-Intermediate+ Responsibilities: Build and maintain robust data pipelines that deliver clean and timely data Organize and transform raw data into well-structured, scalable models Ensure data quality and consistency through validation frameworks like Great Expectations Work with cloud-based tools like Athena and Glue to manage datasets across different domains Collaborate with analysts, engineers, and stakeholders to understand data needs and deliver solutions Help set and enforce data governance, security, and privacy standards Continuously improve the performance and reliability of data workflows Support the integration of modern cloud tools into the broader data platform","[{""min"": 18226, ""max"": 21507, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,485,Data Engineer/Data Modeler,Altimetrik Poland,"5 days per week you need to be available until 10: 00pm due to meetings with the US team Altimetrik Poland is a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are looking for Data Engineer/ Data Modeler for our client which is an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. As a Data Engineer/Data Modeler you will be responsible for designing and implementing various scalable data solutions for our customers providing hospitality and travel services. This role requires a strong analytical mindset, technical expertise, and the ability to collaborate across teams to ensure data solutions align with business needs. Responsibilities: Build and maintain scalable data models and metrics to support a variety of use cases such as: fraud detection, safety incident tracking, and ID verification processes. Scope and implement metrics for experimentation, KPI tracking, and operational insights. Effectively collaborate with stakeholders to establish overarching data architecture and data flow requirements. And if you possess... Strong knowledge of relational databases and query authoring (SQL). Proficiency in Python for data analysis, scripting, and automation. Excellent communication skills, both written and verbal. Exceptional problem-solving skills and a data-driven decision-making approach. Proven ability to work cross-functionally with Product Managers, Engineers, and Analysts. Experience with Airflow for orchestration and Hive/Spark/Presto for large-scale data processing. Experience in fraud detection, safety incident monitoring, or related domains. We work 100% remotely or from our hub in Krak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 21000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,486,Data Architect,TechTorch,"TechTorch is a pioneer among service companies in the way it conducts projects in the area of digital transformation. Through an implementation process supported by an AI-powered platform and a global team of world-class managers and technology experts, TechTorch has enabled dozens of private equity sector companies to accelerate the realization of business benefits. Position Overview: The Data Architect is responsible for designing, creating, deploying, and managing an organization‚Äôs data architecture. This role involves defining how the data will be stored, consumed, integrated, and managed by different data entities and IT systems, as well as any applications using or processing that data. The Data Architect works closely with business and IT stakeholders to ensure that the data architecture aligns with business objectives and supports the needs of the organization. Key Responsibilities: Data Architecture Design: Develop and maintain the overall data architecture, including data models, data flow diagrams, and data integration plans. Define and document the data architecture framework, standards, and principles. Design scalable and flexible data solutions that meet the business requirements and integrate with existing systems. Data Governance and Management: Establish and enforce data management and governance policies, procedures, and standards. Ensure data integrity, quality, and security across the organization. Define and manage data architecture principles and guidelines for data modeling, design, and implementation. Data Integration: Design and oversee data integration processes, ensuring seamless data flow across various systems and platforms. Implement data integration solutions using ETL (Extract, Transform, Load) tools and other data integration technologies. Collaborate with application architects and developers to design and implement data interfaces and APIs. Qualifications: Experience: Minimum of 7 years of experience in data architecture, data management, or related roles. Proven experience in designing and implementing data solutions in complex environments. Strong background in data modeling, database design, and data warehousing. Technical Skills: Proficiency in data modeling tools (e.g., ER/Studio, Erwin). Expertise in database technologies (e.g., SQL Server, Oracle, MySQL, NoSQL databases). Experience with data integration tools (e.g., Informatica, Snowflake, Talend, Apache Nifi). Knowledge of big data technologies (e.g., Hadoop, Spark, Kafka) and cloud data platforms (e.g., AWS, Azure, Google Cloud). Understanding of BI systems such as PowerBI and Tableaux Key Competencies: Strong analytical and problem-solving skills. Excellent communication and interpersonal skills. Ability to lead and influence cross-functional teams. Strategic thinking with a focus on delivering business value. Adaptability and continuous learning mindset What We Offer: Work in an international team Work with the largest capital groups in the world Development and rapid promotion opportunities Autonomy in action Participation in building a new global company Team-building activities","[{""min"": 30000, ""max"": 35000, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,Permanent,Remote,487,BI Manager / Tech Lead - Zesp√≥≈Ç Data Intelligence Solution,KPMG,"Zesp√≥≈Ç Data Intelligence Solutions zajmuje siƒô dostarczaniem naszym Klientom us≈Çug z zakresu szeroko pojƒôtej analityki danych, modelowania platform danych i Business Intelligence. Wykorzystujemy nowoczesne technologie Microsoft, jednocze≈õnie stale rozwijamy siƒô i poszukujemy nowych sposob√≥w, by zaspokoiƒá wymagania rynku. Aktualnie poszukujemy do≈õwiadczonego BI Managera / Tech Leada, kt√≥ry bƒôdzie kluczowym cz≈Çonkiem naszego zespo≈Çu wsp√≥≈Çodpowiedzialnym za kszta≈Çtowanie strategii i kierunk√≥w rozwoju oraz pozyskiwanie i prowadzenie projekt√≥w wdro≈ºeniowych. Je≈õli jeste≈õ pasjonatem danych, masz do≈õwiadczenie w kierowaniu zespo≈Çem i chcesz wp≈Çywaƒá na rozw√≥j naszej firmy, to ta rola jest dla Ciebie! BI Manager / Tech Lead - Zesp√≥≈Ç Data Intelligence Solution Odpowiedzialno≈õƒá za prowadzenie projekt√≥w wdro≈ºenia chmurowych rozwiƒÖza≈Ñ analitycznych (platforma danych, BI); Wsp√≥≈Çodpowiedzialno≈õƒá za zesp√≥≈Ç specjalist√≥w BI (analitycy biznesowi, deweloperzy BI, in≈ºynierowie danych, specjali≈õci DevOps); Weryfikacja potencja≈Çu wykorzystania najnowszych technologii w celu zaadresowania potrzeb biznesowych klient√≥w; Wsparcie w procesach sprzeda≈ºowych (mile widziane); Doradztwo w obszarze wyboru optymalnych rozwiƒÖza≈Ñ, dostosowanych do potrzeb klienta; Wp≈Çyw na rozw√≥j portfela us≈Çug i budowanie d≈Çugoterminowej relacji z klientami; Wspieranie rozwoju umiejƒôtno≈õci w zespole poprzez mentoring i dzielenie siƒô najlepszymi praktykami; Wspieranie zdrowej kultury wsp√≥≈Çpracy w zespole poprzez promowanie otwartej komunikacji oraz ciƒÖg≈Çego doskonalenia. Minimum 4 lata do≈õwiadczenia zawodowego w obszarze Business Intelligence, w tym przynajmniej 2 lata do≈õwiadczenia w prowadzeniu merytorycznym projekt√≥w wdro≈ºenia rozwiƒÖza≈Ñ analitycznych ; Minimum 3 lata do≈õwiadczenia we wdra≈ºaniu rozwiƒÖza≈Ñ analitycznych w ≈õrodowisku Microsoft ; Do≈õwiadczenie w tworzeniu architektury technicznej rozwiƒÖza≈Ñ w ekosystemie Microsoft dostosowanej do potrzeb klienta (po≈ÇƒÖczenie wymiaru funkcjonalnego i kosztowego); Praktyczne kompetencje w zakresie projektowania modeli danych na poziomie platform danych i modeli analitycznych; Wysokie kompetencje techniczne w zakresie technologii analitycznych Microsoft: Power BI, Azure (MS Fabric, SQL, Synapse, Databricks, Data Lake, Data Factory, Logic Apps); Do≈õwiadczenie w automatyzacji wdra≈ºania rozwiƒÖza≈Ñ chmurowych oraz tworzenia infrastruktury w postaci kodu (DevOps, CI/CD, Infrastructure as a Code) oraz wykorzystaniu narzƒôdzia Azure DevOps; Do≈õwiadczenie w zarzƒÖdzaniu lud≈∫mi, w tym w zakresie oceny wynik√≥w, coachingu i rozwoju kariery; Doskona≈Çe umiejƒôtno≈õci komunikacyjne i zdolno≈õƒá do wsp√≥≈Çpracy zar√≥wno z osobami technicznymi, jak i interesariuszami biznesowymi; Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego w mowie i pi≈õmie. Mo≈ºliwo≈õƒá wsp√≥≈Çpracy z rynkowymi liderami zar√≥wno w Polsce jak i za granicƒÖ; Atrakcyjny system premiowy; Udzia≈Ç w projektach wdro≈ºeniowych w najnowszych technologiach i frameworkach (data lakehouse, Azure, zaawansowana analityka oparta o Power Platform, Data Science); 95% czasu pracy w ramach projekt√≥w rozwojowych; Osobi≈õcie satysfakcjonujƒÖcƒÖ karierƒô i pe≈ÇnƒÖ wyzwa≈Ñ pracƒô w wiodƒÖcej na rynku polskim oraz miƒôdzynarodowym firmie wdra≈ºajƒÖcej rozwiƒÖzania cyfrowej transformacji i us≈Çug chmurowych; IndywidualnƒÖ i szybkƒÖ ≈õcie≈ºkƒô kariery; Bezpo≈õredni wp≈Çyw na zakres ≈õwiadczonych us≈Çug i wykorzystywane technologie; Mo≈ºliwo≈õƒá rozwoju osobistego poprzez udzia≈Ç w dedykowanych szkoleniach, w tym z technologii Microsoft i Databricks; Szeroki dostƒôp do platform e-learningowych, takich jak: Degreed, LinkedIn Learning, Pluralsite i Databricks Academy. Zaplanowany czas na przygotowanie do egzamin√≥w oraz bud≈ºet na zdobycie certyfikat√≥w; Program mentoringowy ‚Äì wsparcie w zaplanowaniu ≈õcie≈ºki kariery; Program polece≈Ñ ‚Äì szansƒô na dodatkowy bonus finansowy za skutecznƒÖ rekomendacjƒô znajomego do pracy; - to jest ju≈º w pierwszym bulecieAtrakcyjny pakiet socjalny, kartƒô MultiSport, bilety do kina, teatru, vouchery i zni≈ºki, bony okoliczno≈õciowe; PrywatnƒÖ opiekƒô medycznƒÖ, dodatkowe ubezpieczenie i program wellbeing; PrzyjaznƒÖ atmosferƒô pracy, w tym m.in . wyjazdy integracyjne i zespo≈Çowe spotkania okoliczno≈õciowe; Dni wolne na wolontariat - mo≈ºliwo≈õƒá zaanga≈ºowania siƒô w r√≥≈ºnorodne formy wolontariatu pracowniczego oraz program grantowy wspierajƒÖcy zg≈Çaszane przez pracownik√≥w inicjatywy; Mo≈ºliwo≈õƒá: pracy zdalnej / hybrydowej / w biurze.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Remote,488,Senior Data Engineer for Data Streaming Platform,GR8 Tech,"GR8 Tech is a leading B2B provider of iGaming solutions that empowers operators to grow, lead, and win. We deliver high-impact, full-cycle tech solutions designed to scale. From seamless integration and expert consulting to long-term operational support, our platform powers millions of active players and drives real business growth. It‚Äôs more than just a product ‚Äî it‚Äôs the iGaming Platform for Champions, built for those who play to lead. We know the game and how to take it to the next level. With 1000+ talented professionals on board, we don't just build tech ‚Äî we build success stories for iGaming operators all over the world. Our ambition drives us, our people make it real. Join us and be part of building champion-level success! As Senior Developer in the BI&AP Streaming Team, you will play a critical role in designing, developing, and maintaining our real-time data streaming platform. You will be responsible for the implementation of mission-critical use cases from real-time analytics to personalized user experiences, by building scalable, low-latency data pipelines using Kafka Streams, Flink, and the JVM ecosystem (Java/Scala). Designing and implementing real-time streaming pipelines using Kafka, Flink, and Kafka Streams; Leading architecture and design decisions for the event-driven data platform; Mentoring junior engineers and fostering best practices in stream processing; Collaborating with product & project managers to translate business requirements into robust data systems; Ensuring data quality, observability, and reliability in streaming data; Championing performance, fault tolerance, and exactly-once semantics in streaming jobs; Evaluating and adopting new technologies to drive innovation; Enforcing quality assurance standards and ensuring thorough testing before release; Maintaining comprehensive documentation for system architecture and technical designs. At least 5 years of experience in a similar role with a strong track record of leadership and successful project delivery; Proven expertise with Apache Kafka and hands-on experience with Flink, Kafka Streams, or similar real-time systems; Strong programming skills in Java, Scala, Python; In-depth understanding of distributed systems, data consistency models, and streaming semantics; Experience with CI/CD, containerization, and infrastructure-as-code tools is a plus; Communicate effectively with stakeholders, including presenting technical concepts and project updates. Benefits Cafeteria An annual fixed budget that you can use based on your needs and lifestyle. You decide how to allocate it: Sports ‚Äì gym, yoga, or any activity to keep you active; Medical ‚Äì insurance and wellness services; Mental health‚Äì therapy or coaching support; Home office ‚Äì ergonomic furniture, gadgets, and tools; Languages ‚Äì courses to improve or learn new skills. Work-life Parental support with paid maternity/paternity leave and monthly childcare allowance; 20+ vacation days, unlimited sick leave, and emergency time off; Remote-first setup with full tech support and coworking compensation; Regular team events ‚Äì online, offline, and offsite; Learning culture with internal courses, career development programs, and real growth opportunities. Our Culture & Core Values GR8 Tech culture is how we win. Behind every bold idea and breakthrough is a foundation of trust, ownership, and a growth mindset. We move fast, stay curious, and always keep it real, with open feedback, room to experiment, and a team that‚Äôs got your back. FUELLED BY TRUST: we‚Äôre open, honest, and have each other‚Äôs backs; OWN YOUR GAME: we take initiative and own what we do; ACCELER8: we move fast, focus smart, and keep it simple; CHALLENGE ACCEPTED: we grow through challenges and stay curious; BULLETPROOF: we‚Äôre resilient, ready, and always have a plan. To keep things efficient, please apply only for roles that closely match your experience.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,489,Data Analyst - Product Data,Bayer Sp. z o.o.,"Data Analyst ‚Äì Product Data Are you ready to make a significant impact in the world of data analytics and data management? We are seeking a talented Data Analyst to become a vital part of our dynamic Data Assets, Analytics, and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in building and maintaining our core data assets across various domains, ensuring their completeness, semantics and quality. You will work closely with data owners, product managers, data engineers, data architects, data stewards, data governors and data scientist to enable our data analytics solutions, enhance the strategic value of our data assets and enable cutting-edge AI solutions and support data-driven decision-making. If you‚Äôre passionate about transforming data into actionable insights and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Collaborate with data owners, architects, engineers, stewards, governors, scientists and product managers to understand objectives and requirements for data assets. Serve as a liaison between technical teams and business stakeholders to ensure data assets meet both business requirements and technical standards. Proactively identify new datasets from across the organization and collaborate with data architects and data engineers to integrate them into the core data assets Define transformation logic and enrich data to create valuable KPIs and features in the consumption layer. Develop and maintain clear semantics and metadata for data assets, ensuring that all data is well-documented and easily understandable. Ensure data quality, availability, and completeness through implementation of quality checks, validation processes, and continuous monitoring in collaboration with data architects and data engineers. Serve as the primary point of contact for users of data assets, providing guidance and support to help them understand and utilize the data effectively. Analyze complex datasets to extract actionable insights that streamline the development of analytics and AI solutions. Become a go-to expert for product data. Qualifications & Competencies: Master's degree in Statistics, Computer Science, Data Management, Data Science or a related field. 3+ years of experience as a Data Analyst or Data Steward, preferably within the consumer-packaged goods, FMCG, pharmaceutical or healthcare industry. Strong knowledge of data management principles, data quality frameworks, and metadata management practices and tools. Understanding of data lineage and data cataloging concepts. Business acumen in the area of product supply analysis. Familiarity with SAP product-related modules (e.g. Product Lifecycle Management, Materials Management, Quality Management, Advanced Planner and Optimizer, S/4HANA Supply Chain) and Supply Chain Planning Solutions (e.g. OMP). Experience with data manipulation and analysis using Azure Databricks, SQL and Python. Familiarity with relational databases (PostgreSQL, MSSQL) and data modelling. Excellent analytical and problem-solving skills with a keen attention to detail. Understanding about data compliance & security standards such as data privacy regulations (GPDR, HIPAA), EU AI Act, management of confidential data, and experience with measures to mitigate data risks. Strong communication skills, with the ability to present complex data in a clear and understandable manner. Interest and experience with AI tools supporting data analysis and stewardship is a plus. Experience with preparing data for AI solutions (e.g. traditional machine learning models, AI Agents) is a plus. Ability to work collaboratively in a team-oriented environment. Fluent in English, both written and spoken. What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 14000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,490,Senior Data Engineer (Healthcare domain),Sigma Software,"Unleash your ambition and lead a team as a Senior Data Engineer! We are looking for a professional who not only has impressive data engineering experience but is also ready to take on a leadership role. Join us, and you will have the opportunity to innovate, lead the team, improve our solutions, and help us reach new heights! If you are interested in this position, submit your CV now. Our client is a leading medical technology company. Its portfolio of products, services, and solutions is focused on clinical decision-making and treatment pathways. Patient-centered innovation has always been, and will always be, at the core of the company. The client is committed to improving patient outcomes and experiences, regardless of where people live or what problems they face. The client innovates sustainably to provide healthcare for everyone, everywhere. The project‚Äôs mission is to enable healthcare providers to increase their value by providing them with innovative technology and services in diagnostic and therapeutic imaging, laboratory diagnostics, molecular medicine, digital health, and enterprise services. Requirements Experience with data engineering and cloud computing services and solutions in the field of data and analytics. Azure is preferable Conceptual knowledge of data analytics fundamentals, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, and structured and unstructured data Knowledge of SQL and experience with at least one programming language (Python or Scala) Understanding of big data databases such as Snowflake, BigQuery, etc. Snowflake is preferable Experience with database development and data modeling, ideally with Databricks or Spark Fluency in business English Personal Profile Excellent communication skills Responsibilities Work closely with the client (PO) and other team leads to clarify technical requirements and expectations Coordinate and supervise your team (up to 4 team members), track performance, and provide support where TL‚Äôs support is required Implement architectures based on Azure cloud platforms (Data Factory, Databricks, and Event Hub) Design, develop, optimize, and maintain squad-specific data architectures and pipelines that adhere to defined ETL and Data Lake principles Discover, analyze, and organize disparate data sources and structure them into clean data models with clear, understandable schemas Contribute to evaluating new tools for analytical data engineering or data science Suggest and contribute to training and improvement plans related to analytical data engineering skills, standards, and processes",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,491,Programista / Programistka Hurtowni Danych,Vienna Life Towarzystwo Ubezpiecze≈Ñ na ≈ªycie S.A.,"Vienna Life to instytucja finansowa, kt√≥ra zapewnia dostƒôp do sprawdzonych produkt√≥w ubezpieczeniowych, oszczƒôdno≈õciowych i inwestycyjnych. Nale≈ºymy do Vienna Insurance Group, wiodƒÖcej grupy ubezpieczeniowej w Europie ≈örodkowo-Wschodniej, dzia≈ÇajƒÖcej w 30 krajach, kt√≥rej korzenie siƒôgajƒÖ roku 1824. Nieustannie szukamy nowych ≈õcie≈ºek rozwoju i rozwijamy innowacyjne projekty, by dynamicznie odpowiadaƒá na potrzeby Klient√≥w. rozw√≥j i utrzymanie hurtowni danych zbudowanej na platformie MS SQL Server / Microstrategy nadz√≥r nad procesami zasilajƒÖcymi Hurtowniƒô Danych (ETL) projektowanie i tworzenie analitycznych modeli danych SSAS tworzenie raport√≥w i analiz uczestnictwo w ca≈Çym procesie przygotowania rozwiƒÖza≈Ñ: analiza, projektowanie, testy i development wykszta≈Çcenie wy≈ºsze informatyczne lub pokrewne minimum 2 lat do≈õwiadczenia zawodowego do≈õwiadczenie w obszarze Hurtowni Danych opartej o platformƒô Microsoft bardzo dobra znajomo≈õƒá SQL, T-SQL znajomo≈õƒá SSRS (SQL Server Reporting Services) oraz SSAS ( SQL Server Analysis Services) znajomo≈õƒá platformy Microstrategy lub innego systemu raportowania bƒôdzie dodatkowym atutem zdolno≈õƒá szybkiego diagnozowania i rozwiƒÖzywania problem√≥w komunikatywno≈õƒá i umiejƒôtno≈õƒá pracy w zespole kreatywno≈õƒá i chƒôƒá rozwoju znajomo≈õƒá jƒôzyka angielskiego na poziomie umo≈ºliwiajƒÖcym komunikacjƒô Zatrudnienie w oparciu o dowolny model wsp√≥≈Çpracy (umowa o pracƒô, B2B) Praca w modelu hybrydowym Prywatna opieka medyczna w grupie Luxmed Ubezpieczenie na ≈ºycie Karta przedp≈Çacona Dofinansowanie do karty Multisport Dofinansowania do szkole≈Ñ oraz kurs√≥w jƒôzykowych Zni≈ºki pracownicze na produkty ubezpieczeniowe Refundacja zakupu okular√≥w/szkie≈Ç kontaktowych Dzie≈Ñ wolny w dniu urodzin Nieoprocentowane po≈ºyczki Akcje komunikacyjne dotyczƒÖce profilaktyki zdrowia i wellbeing‚Äôu (spotkania z ekspertami i webinary)",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,492,Senior Python Developer,Capco Poland,"At Capco Poland, we‚Äôre not just another consultancy ‚Äî we‚Äôre the spark behind digital transformation in the financial world. As a global leader in technology and management consulting, we thrive on helping clients tackle the toughest challenges across banking, payments, capital markets, wealth, and asset management. Our secret? A culture that‚Äôs fast, flexible, and fiercely entrepreneurial. We move quickly, think creatively, and always put our people first. We‚Äôre passionate about growth ‚Äî both for our clients and ourselves ‚Äî and that means attracting the very best talent to join us on this exciting journey. We‚Äôre proud to be: ‚Ä¢ Trailblazers in banking, payments, capital markets, wealth, and asset management ‚Ä¢ Champions of an agile, nimble, and innovative work environment ‚Ä¢ Dedicated to building a team of top-notch professionals who share our drive and vision THE BIG PICTURE Hey there! üëã We‚Äôre looking for a passionate developer to join full-time, long-term project related to Market Risk Platform (MRP) team within one of the biggest players in financial domain. MRP is a go-to system for pulling together and reporting market risk data‚Äîand it plays a big role in keeping Financial Markets business running smoothly. Our team is full of smart, friendly tech experts who love solving tough problems and building cool, useful stuff. If you're someone who enjoys a challenge, likes learning new things, and wants to work with great people, you‚Äôll fit right in. HOW YOU WILL MAKE MAGIC HAPPEN You will be: Building important parts of the MRP system that is strong, fast, and ready for the future . Working on exciting, high-impact projects. Creating smart solutions that we can test quickly and improve as we go. Collaborating with awesome teammates from around the world‚Äîtechies, analysts, project managers, and more. Build and maintain a strong, efficient framework for data transformation and reporting that supports business needs. Lead and mentor a team of local data engineers‚Äîset the tone, share your knowledge, and help the team grow. Design smart and scalable application architecture that follows best practices and meets both technical and business goals. Collaborate closely with business users to understand their needs and deliver practical, high-quality solutions. Be a key part of an Agile Scrum team, working together to deliver great results, sprint by sprint. Make sure your team writes clean, maintainable, and well-designed code that‚Äôs easy to understand and build on. Support production systems by identifying and fixing issues related to data, performance, or setup quickly and efficiently. Help test and maintain core software and databases to ensure everything runs smoothly and performs well. Contribute to every stage of the development process, from idea to release and beyond WHAT MAKES YOU AWESOME 6-12 years of experience in software development Python Expert ‚Äì Deep hands-on experience with Python and its core libraries like pandas, numpy, json, requests, pyarrow, openpyxl, xlsxwriter, and testing tools like pytest and unittest. I follow solid principles of clean architecture, modular design, and design patterns . Data-Driven Development ‚Äì Skilled in building and managing data-centric applications , from backend processing to reporting and integration with business logic. ETL & Data Integration ‚Äì Comfortable working with diverse data formats (CSV, JSON, XML, Parquet, Avro, OLAP Cube, etc.) and designing ETL pipelines that clean, enrich, and transform data for downstream use. Modern Data Architecture ‚Äì Experience with Lambda architecture , building unified pipelines that handle both batch and real-time data processing. API Development ‚Äì Proficient in designing and integrating RESTful APIs to connect services and share data effectively. DevOps & Automation ‚Äì Familiar with CI/CD practices using tools like Jenkins , streamlining deployment and testing processes. Agile Development Database Knowledge ‚Äì Strong understanding of relational databases like PostgreSQL and Oracle , with additional exposure to OLAP and MDX queries‚Äîgreat for analytical reporting. Problem-Solver ‚Äì You enjoy building smart algorithms that turn raw data into meaningful insights, helping drive decision-making and business value. NICE TO HAVE Java, AWS/Azure, Kafka/RabbitMQ WHY WORTH JOINING US Employment contract and/or Business to Business as you prefer Hybrid work Speaking English on daily basis, mainly in contact with foreign stakeholders and peers Multiple employee benefits packages (MyBenefit Cafeteria, private medical care, insurance) Access to 3.000+ Business Courses Platform (Udemy) Access to required IT equipment Ongoing learning opportunities to help you acquire new skills or deepen existing expertise Being part of the core squad focused on the growth of the Polish business unit A flat, non-hierarchical structure that will enable you to work with senior partners and directly with clients A work culture focused on innovation and creating lasting value for our clients and employees ONLINE RECRUITMENT PROCESS STEPS Screening call with the Recruiter Take home assignment Client interview Feedback/Offer",[],Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Remote,493,Machine Learning Engineer (LLM),UNIVIO,"Jeste≈õmy polskƒÖ firmƒÖ technologicznƒÖ z ponad 25-letnim do≈õwiadczeniem jako partner cyfrowej transformacji handlu. Realizujemy miƒôdzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocze≈õnie lu≈∫nƒÖ, niezobowiƒÖzujƒÖcƒÖ atmosferƒô. Nasza organizacja opiera siƒô na kulturze otwarto≈õci i dzielenia siƒô wiedzƒÖ. Dowiedz siƒô kogo szukamy i zaaplikuj, je≈õli spe≈Çniamy Twoje oczekiwania üòâ","[{""min"": 18480, ""max"": 22680, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 16500, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Part-time,Senior,B2B,Remote,494,Data Visualization Expert (Senior Power BI Developer),Upvanta sp. z o.o.,"Projektowanie i wdra≈ºanie organizacyjnych standard√≥w wizualizacji danych w Power BI Tworzenie i rozw√≥j oficjalnych raport√≥w i dashboard√≥w zgodnych z najlepszymi praktykami UX/UI oraz Data Storytellingu Projektowanie i optymalizacja modeli danych z naciskiem na wydajno≈õƒá i reu≈ºywalno≈õƒá Mentoring oraz doradztwo dla innych deweloper√≥w Power BI Udzia≈Ç w projektowaniu architektury ≈õrodowiska raportowego Zapewnienie zgodno≈õci z politykami bezpiecze≈Ñstwa, w tym wdra≈ºanie RLS (Row-Level Security) Minimum 3‚Äì4 lata do≈õwiadczenia w pracy z Power BI w zakresie raportowania i modelowania danych Ekspercka znajomo≈õƒá Power BI, DAX oraz najlepszych praktyk w zakresie wizualizacji danych (np. IBCS, Storytelling, UX/UI) Bardzo dobra znajomo≈õƒá SQL i relacyjnych baz danych (Oracle, Snowflake) Do≈õwiadczenie w projektowaniu warstw semantycznych oraz architektur raportowych Umiejƒôtno≈õƒá przek≈Çadania z≈Ço≈ºonych zagadnie≈Ñ biznesowych na przejrzyste, intuicyjne dashboardy Wysoko rozwiniƒôte umiejƒôtno≈õci komunikacyjne i do≈õwiadczenie we wsp√≥≈Çpracy z interesariuszami na r√≥≈ºnych poziomach organizacji",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,495,Senior Data Engineer,Jit Team,"Salary: 1100‚Äì1440 PLN net/day ( B2B ) Work mode: Hybrid ‚Äì 1 day/week in Warsaw or Gdynia office Why choose this offer? You‚Äôll join a long-term , high-impact project focused on modern data processing and software asset management You‚Äôll work with an international team in a cloud-native environment ( GCP , Cloud Run , Pub/Sub , FastAPI ) Opportunity to develop scalable backend systems using Apache Beam and modern serverless patterns Full autonomy in technical decision-making and architecture evolution Stable , long-term collaboration with a global HR industry leader Project You‚Äôll join the Global SAM (Software Asset Management) team responsible for developing a cloud-native , data-driven platform used to analyze and manage enterprise software usage. The system is built on Google Cloud Platform , processes data in near real-time , and integrates with multiple systems using an event-driven architecture . You‚Äôll be responsible for designing and implementing data pipelines , backend APIs, and scalable serverless functions. This is a hands-on engineering role with lots of space for ownership and technical influence . Expected competences and knowledge Solid Python backend development experience, ideally with FastAPI Experience building data pipelines using Apache Beam or equivalent frameworks Familiarity with GCP services, especially Dataflow, Pub/Sub, Cloud Run Understanding of event-driven architecture and observability best practices Experience with CI/CD tools, preferably GitLab Ability to work independently in a distributed team Optional but welcome: knowledge of Google Cloud Firestore Technologies you'll work with Python backend development (FastAPI, data pipelines) Experience with Apache Beam or streaming frameworks Strong knowledge of GCP services (Dataflow, Pub/Sub, Cloud Run) Understanding of event-driven architecture and observability Familiarity with CI/CD workflows (GitLab preferred) Ability to work in a distributed team environment Optional: experience with Firestore or other GCP data stores Client ‚Äì why choose this particular client from the Jit portfolio? We have partnered with our Client, a leading HR company with over 60 years of experience and a presence in multiple countries around the world. Our Client provides a wide range of recruitment and job placement services and is dedicated to helping individuals achieve their career goals. In Poland, our Client has been operating for over 20 years and is at the forefront of digitization in the HR services industry. Our team has been building the Work Time system for our Client since 2019 , a complex tool for registering and settling the working time of full-time and temporary employees working under different legal and organizational conditions, supporting on-boarding processes, and integrating with the employee portal and mobile app. As a member of Jit Team, you will have the opportunity to work on a modern, complex enterprise-class system built in modern technologies such as Java and the AWS cloud . With a business-critical operation, the Work Time system settles working time for tens of thousands of employees every month and contributes to our Client's revenue. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 22000, ""max"": 28800, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,496,ML Platform / Data Engineer,Beesafe,"About Us: Join our trailblazing team as we expand our digital horizons. From our beginnings as visionary InsurTech to becoming a key player in the Polish digital insurance market, we are part of the esteemed Vienna Insurance Group. We're redefining the rules in the insurance industry with our innovative approach. Our hybrid working model supports both the collaborative energy of office work and the flexibility of remote work. About the Role: We're seeking passionate Data Engineer, both at mid and senior levels, to join our team. You'll be at the heart of developing and implementing high-quality application software, using state-of-the-art tools and technologies. This is your chance to make a significant impact on one of the most exciting and unique products in the Polish digital insurance market. You‚Äôll be leading cutting-edge tech initiatives in the Azure Cloud, using state-of-the-art tools and methodologies. You‚Äôll own your work , influencing the project‚Äôs architecture, strategy, and direction . You‚Äôll play a key role in shaping MLOps/LLMOps pipelines , improving data availability, and ensuring seamless collaboration between analytics and IT teams. You‚Äôll be joining a team of customer-focused Data Analysts, Data Engineers, and Machine Learning Engineers to drive one of the most exciting products on the Polish insurance market. Proven experience as a Data Engineer or a strong aspiration to take on a leadership role in cloud-based data and AI projects. Commercial experience in building Data, Analytics, or MLOps/LLMOps Platforms in the Cloud (Azure preferred). Hands-on expertise in machine learning model lifecycle management , deployment, and monitoring in a cloud environment. Practical knowledge of open-source Big Data tools (e.g., Kafka, Airflow, Presto, Spark). +3 years of programming experience (Python preferred, but Java/Scala experience with a willingness to learn Python is also welcome). Experience in database development, data model design, and distributed systems . Strong business acumen and collaboration skills , with a proactive approach to addressing service needs and operational demands. Understanding of Agile and Scrum principles (we work in Scrum, by the way). Hands-on experience in MLOps/LLMOps , including model versioning, CI/CD pipelines for ML, and monitoring of deployed AI models. Familiarity with Databricks, BI Tools, and Azure Machine Learning Services . Experience with DevOps, Kubernetes, and microservices architectures . Strong mentoring skills , supporting other engineers in their professional growth. Passion for informal discussions over coffee or tea ‚Äîwe believe great ideas start in a relaxed atmosphere! Why Join Us? Be part of a dynamic team driving digital innovation in the insurance and eCommerce sectors Opportunity to work in a collaborative and forward-thinking environment Contract options: B2B cooperation Engage in meaningful work that directly impacts business success Join a company that values work-life balance and fosters a positive team culture Comprehensive onboarding, including a dedicated Buddy program Remote work flexibility with hybrid office visits Flexible working hours Access to the latest tools and cloud-native solutions A comprehensive benefits package, including health insurance and MultiSport card Employee discounts on insurance products Referral program and sports club memberships Join us and help shape the future of AI-driven insurance solutions ! üöÄ","[{""min"": 18000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,Any,Hybrid,497,Technology Specialist - BI Consultant,Heineken,"Digital & Technology Team (D&T) is an integral division of HEINEKEN Global Shared Services Center . We are committed to making Heineken the most connected brewery. That includes digitalizing and integrating our processes, ensuring best-in-class technology, and embedding a data-driven culture. By joining us you will work in one of the most dynamic and innovative teams and have a direct impact on building the future of Heineken ! Would you like to meet the Team, see our office and much more? Visit our website: Heineken ( heineken-dt.pl ) Your responsibilities would include: ¬∑ collaborating with the BI Architect on the solution design to ensure it conforms to overall architectural vision and technical roadmap,¬∑ working with the Product Owner and BI Architect to understand, elicit, analyze, document and communicate BI solution requirements,¬∑ participating in the selection of new technologies to accommodate BI solution needs,¬∑ managing the quality of the solutions and technical debt,¬∑ designing and develops BI solutions or keeps oversight on developers (internal and external) to assure technical quality and overall HEINEKEN standards are met,¬∑ identifying potential bottlenecks and their underlying causes that affect the product and proposes how to resolve these,¬∑ keeping up with relevant developments in own functional area and ensure development, transfer and retention of knowledge. Being together with team members end-to-end responsible to deliver the value of Products through the entire Product life cycle,¬∑ delivering working product enhancements with the allocated timescale and quality standards that minimize rework and aspire a zero-defect culture,¬∑ understanding business requirements and acceptance criteria in the form of user stories, assess the complexity of these and translate into implementable solutions and configurations,¬∑ ensuring the product design and implementation adheres to functional requirements and architecture standards,¬∑ demonstrating product features during sprint reviews to stakeholders,¬∑ providing expertise for deployment of new solutions and technologies, which are centrally used (in central HEINEKEN functions as well as regional headquarters) by most senior stakeholders (including Executive Team members ‚Äì CFO and CIO),¬∑ managing the build-out of technologies required for new implementations as well as 3rd party suppliers supporting the process,¬∑ being a self-organized expert, who can work under minimal to no supervision to deliver results along with her/his business counterparts and stakeholders. You are a good candidate if you: have Bachelors or Master‚Äôs degree in Computer Science or similar IT field, have at least 7 years‚Äô experience in data analytics / Business Intelligence role with a similar scope of responsibilities, have excellent communication skills (verbal and in writing); ability to communicate well with technical and non-technical people and identify and understand multiple stakeholders' perspectives and ‚Äútruths‚Äù, have hands-on experience (developer level) of SAP BW4HANA with S/4 (SAC knowledge in addition will be welcomed), have greenfield implementation experience of SAP BW4HANA/ SAP BW on HANA, have strong knowledge of applications across S/4 HANA will be preferred, have knowledge of Enterprise level systems, databases and BW solutions, have an open mind to understand and build on the views of others, have ability to work in teams and work dedicated for a team, have ability to understand Business requirements and translate these into technology solutions, have experience in delivering large-scale BI solutions and reporting tools, have experience in migrations between BI technologies. At HEINEKEN Krak√≥w, we take integrity and ethical conduct seriously. If someone has concerns about a possible violation of legal regulations indicated in Polish Whistleblowing Act or our Code of Business Conduct, we encourage them to speak up . Cases can be reported to global team or locally (in line with the local HGSS Whistleblowing procedure) by selecting proper option in this tool or by communicating it on hotline. We Offer: üè† Flexible Work from Home scheme üí∏ Attractive Performance Bonus üöó Parking Space for Employees ‚è∞ Flexible working hours üí≥ Sodexo Card ‚òÇ Life Insurance ‚ûï Employee Referral Programme üåê Job Opportunities within HEINEKEN ü©∫ Private Medical Healthcare ‚≠ê Social Event",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,498,Data Scienist,Madiff Sp. z o.o.,"At Madiff, we connect top IT talent with cutting-edge companies through remote Agile teams and nearshore outsourcing. With operations in Poland, Portugal, France, and the UK, we specialize in AI, DevOps, and Cybersecurity ‚Äì supporting industries like Telecom, Banking, and Hi-Tech. Are you excited about the future of Generative AI and want to shape how enterprises apply LLMs and ML at scale? We‚Äôre looking for a Data Scientist with Generative AI expertise to support the AI transformation program of a leading global consulting firm. Design, fine-tune and optimize LLMs and GenAI pipelines using Python , PyTorch , TensorFlow , MLflow , and LangChain Build and scale RAG systems and advanced prompting (chain-of-thought, function-calling) Develop and deploy predictive models (classification, time-series forecasting) in CI/CD environments with Docker and MLflow Implement robust AI governance frameworks ‚Äì including compliance , monitoring , and ethical guidelines Lead internal adoption workshops , mentor junior team members, and drive business alignment Collaborate on data ingestion , feature engineering , model evaluation , and feedback loops Min. 3 years as a Data Scientist , with practical experience in GenAI and ML workflows Excellent Python skills with PyTorch , TensorFlow , Scikit-learn , Pandas , and NumPy Experience with cloud platforms (Azure, Databricks) and MLOps tools like MLflow , CI/CD Proven success in LLM fine-tuning , prompt engineering , and RAG architecture Solid knowledge of forecasting and predictive modeling Strong communication, stakeholder engagement, and mentoring abilities Fluent English - spoken and written Familiarity with Dataiku , Apache Spark , GraphQL , or React Native Domain experience in healthcare or fintech Advanced degree (MSc/PhD) in Data Science, AI, or related field Fully remote work set-up Comprehensive medical care Long-term opportunity to shape GenAI adoption in highly regulated, high-impact industries","[{""min"": 19000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,499,Senior Data Engineer ‚Äì Grooming Digital Assets,Procter & Gamble,"WARSAW DOWNTOWN OFFICE Join our dynamic Grooming Data Hub team in Warsaw as a Senior Data Engineer and be a pivotal force in shaping the future of our iconic Braun and Gillette brands. We are on the lookout for self-motivated professionals who bring a wealth of experience in data ingestion, cleansing, harmonization, and the development of robust data pipelines and transformations across Azure and IT/OT on-premise platforms. In this influential role, you'll empower data-driven decision-making and automation supported by cutting-edge AI technology. This is an extraordinary opportunity to engage in exciting global projects and collaborate with diverse, multi-functional teams both within and beyond P&G, making a meaningful impact on our business. Key Responsibilities: Data Maintenance and Management: Take charge of maintaining and managing extensive datasets from digital twin simulations and curate libraries of sampled data from production systems. Oversee libraries of model versions and validation datasets, ensuring they are up-to-date and accessible. Develop and maintain annotation schemas and labeling ontologies in partnership with P&G‚Äôs AI Factory. Guarantee data integrity, consistency, and availability across IT/OT systems Set up and document a Data Version Control (DVC) system to meticulously track datasets, model weights, and configurations; in coordination with GitHub version controls. Data Modeling and Architecture: Enforce data governance policies to uphold data security, privacy, and compliance. Collaborate with business stakeholders to deeply understand data requirements, ensuring our data models are aligned with their needs. Cloud / Hybrid Infrastructure Management: Efficiently distribute and store data within P&G's Microsoft Azure cloud infrastructure. Collaborate with IT/OT platform teams to create configurations for Kubernetes, GitHub runner VMs, and model registries. Implement best practices for cloud data storage, security, and distribution, ensuring a balanced approach to cost and value. Design and build scalable data pipelines using cloud technologies, including advanced ETL (extract, transform, load) processes. Collaborate closely with IT and digital infrastructure teams to streamline data access and usage. Continuous Improvement: Stay ahead of the curve by keeping abreast of the latest industry trends and technologies in data engineering and cloud computing. Propose and implement innovative solutions that enhance data management, processing, and distribution practices. Proven experience as a Data Engineer with large datasets (5+ years). Proficient in Microsoft Azure or other cloud platforms. Strong programming skills in languages such as Python, SQL, and SPARK. Solid understanding of cloud data storage and distribution best practices. Exceptional problem-solving skills and meticulous attention to detail. Ability to thrive in a collaborative team environment and communicate effectively across cross-functional teams. Excellent organizational skills with the capability to manage multiple projects simultaneously. Preferred Qualifications: Knowledge of SAP data tables. Experience with Nvidia Omniverse or equivalent digital twin platforms. Familiarity with 3D Digital Twin Assets and simulation data management. Experience with annotation ontologies for labeling data using platforms like Labelbox/Azure ML. Understanding of data governance and compliance standards. Experience with big data technologies such as Kubernetes, Spark, and other relevant technologies. Expertise in Databricks. Experience with real-time data processing and streaming technologies. We offer P&G-sized projects and access to world leading IT partners and technologies from Day 1. Wide range of self-development possibilities (training and certifications paths). Competitive starting salary and benefits program (private health care, P&G stock, saving plans, sport cards). Regular salary increases and possible promotions - in line with your results and performance. Opportunity to change role every few years to be in the best place for you and best for P&G. At Procter & Gamble we embrace a hybrid work model that combines the flexibility of remote work with the collaborative benefits of in-office engagement. Employees can enjoy the option to work from home two days a week while also spending time in the office to foster teamwork and enhance communication. Watch this video to learn more about our full recruiting process: https: //www.youtube.com/watch?v=0bicvbpy0gI Kindly be advised that at P&G, employment is exclusively extended on the basis of an ""Umowa o Pracƒô"" (Full-time Employment Contract). Apply only if you agree to these conditions. About us We produce globally recognized brands and we grow the best business leaders in the industry. With a portfolio of trusted brands as diverse as ours, it is paramount our leaders can lead with courage the vast array of brands, categories and functions. We serve consumers around the world with one of the strongest portfolios of trusted, quality, leadership brands, including Always¬Æ, Ariel¬Æ, Gillette¬Æ, Head & Shoulders¬Æ, Herbal Essences¬Æ, Oral-B¬Æ, Pampers¬Æ, Pantene¬Æ, Tampax¬Æ and more. Our community includes operations in approximately 70 countries worldwide.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,500,Data Engineer,Antal Sp. z o.o.,"üìå Data Engineer üìç Krak√≥w (Hybrid: 2 days/week in office) | Full-time | B2B Contract We are looking for an experienced Data Engineer to join a global data & analytics team focused on building a modern data ecosystem. The role involves designing and implementing scalable data solutions that power business insights and decision-making. You‚Äôll work in a collaborative Agile/DevOps environment alongside data scientists, analysts, and engineers. What You‚Äôll Do: Develop and maintain data pipelines using tools like Python, Java, Scala, PySpark, Hive Migrate existing data solutions from Hadoop to Google Cloud Platform (GCP) , including BigQuery, DataProc Build and maintain reliable, scalable, and secure data products and microservices Collaborate with business analysts to understand requirements and translate them into technical solutions Support production systems, troubleshoot issues, and ensure high availability Participate in Agile ceremonies, design discussions, and peer code reviews What You Bring: Solid experience in software/data engineering within large-scale environments Strong programming skills in Java, Scala, Python , and working knowledge of SQL Familiarity with big data ecosystems (Hadoop, Spark, Hive) Experience with cloud platforms , especially GCP (BigQuery, DataProc preferred) Understanding of data integration , data modeling , and metadata management Comfortable working in Agile and DevOps cultures with CI/CD pipelines Effective communicator with the ability to work in distributed, cross-functional teams Nice to Have: Experience with Airflow , data warehousing , and scheduling tools Exposure to Agile , Scrum , and DevOps methodologies Enthusiasm for innovation, clean code, and automation Ability to mentor junior engineers and contribute to best practices",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,501,Big Data Engineer,Link Group,"üöÄ Big Data Engineer Full-time | AdTech Platform | Python/Java/Scala We‚Äôre looking for an experienced Big Data Engineer to join our high-impact team building the backbone of a global advertising platform delivering personalized content to millions of media-enabled devices. Your work will directly influence data-driven decision making, real-time targeting, and analytics pipelines for one of the most advanced AdTech ecosystems in the industry. üîç What You‚Äôll Be Doing Build and maintain robust, scalable data pipelines to support attribution, targeting, and analytics Collaborate closely with Data Scientists, Engineers, and Product Managers Design and implement efficient storage and retrieval layers for massive datasets Optimize data infrastructure and streaming processing systems (e.g. Flink, Apache Ignite) Drive quality through unit tests, integration tests, and code reviews Develop and maintain Airflow DAGs and other orchestration pipelines Translate business needs into robust, technical data solutions Lead or support A/B testing and data-driven model validation Contribute to R&D initiatives around cloud services and architecture ‚úÖ What You Bring 5+ years of experience in backend/data engineering using Python, Java, or Scala Strong experience with Big Data frameworks (Hadoop, Spark, MapReduce) Solid knowledge of SQL/NoSQL technologies (e.g. Snowflake, PostgreSQL, DynamoDB) Hands-on with Kubernetes, Airflow, and AWS (or similar cloud platforms) Stream processing experience: Flink, Ignite Experience with large-scale dataset processing and performance optimization Familiarity with modern software practices: Git, CI/CD, clean code, Design Patterns Fluent in English (B2+) Degree in Computer Science, Telecommunications, or related technical field ‚≠ê Bonus Points Experience with GoLang or GraphQL Hands-on with microservices or serverless solutions Experience in container technologies (Docker, Kubernetes) Previous work in AdTech, streaming media, or real-time data systems","[{""min"": 100, ""max"": 130, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,502,Senior MLOps Engineer (Azure/ AWS),Scalo,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in .: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! budowa nowoczesnej Platformy Analitycznej opartej o Azure Databricks w domenie finansowej, implementacja narzƒôdzi monitorujƒÖcych dzia≈Çanie modeli ML w ≈õrodowisku produkcyjnym, bliska wsp√≥≈Çpraca z zespo≈Çami Data Science w zakresie MLOps i platformy zapewnienie wysokiej dostƒôpno≈õci, bezpiecze≈Ñstwa i skalowalno≈õci platformy ML, implementacja proces√≥w trenowania, wdra≈ºania, wersjonowania i monitorowania modeli w ≈õrodowisku chmurowym, integracja rozwiƒÖza≈Ñ ML z innymi systemami biznesowymi w organizacji, udzia≈Ç w d≈Çugoterminowej rozbudowie i doskonaleniu Platformy Analitycznej, praca 100% zdalna, a dla chƒôtnych mo≈ºliwo≈õƒá pracy z biura we Wroc≈Çawiu, stawka do 200 z≈Ç/h przy B2B, w zale≈ºno≈õci od do≈õwiadczenia. masz minimum 3-4 lat do≈õwiadczenia w MLOps oraz budowie platform ML w chmurze (Azure preferowane, AWS tak≈ºe akceptowalne), znasz bardzo dobrze Azure Databricks (konfiguracja, utrzymanie, optymalizacja koszt√≥w), posiadasz umiejƒôtno≈õci programistyczne w Python i SQL (R bƒôdzie dodatkowym atutem), pracowa≈Çe≈õ z MLFlow, Spark, Rest API oraz narzƒôdziami CI/CD i automatyzacjƒÖ pipeline‚Äô√≥w, masz do≈õwiadczenie z Docker, Kubernetes; Terraform bƒôdzie plusem, znasz cykl ≈ºycia modeli ML od trenowania po wdro≈ºenie i monitoring, pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie B1/B2 (dokumentacja, okazjonalne spotkania). d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 26880, ""max"": 33600, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,B2B,Remote,503,GCP Cloud Data Engineer,in4ge sp. z o.o.,"We are actively seeking a highly skilled Senior Cloud Data Engineer with proven expertise in Google Cloud Platform (GCP) and mandatory hands-on experience with Cloudera, Hadoop, and Apache Airflow. Fluency in Italian and good command of English are essential for this role. Design, implement, and optimize scalable data pipelines and architectures on GCP. Develop and tune complex SQL queries and data models in BigQuery, applying advanced techniques such as partitioning and clustering to optimize performance. Lead data workflow orchestration and automation using Apache Airflow (Composer). Work extensively with Cloudera and Hadoop (Spark/Dataproc) ecosystems to process and analyze large datasets. Apply best practices in data warehousing, data lakes, and cloud data governance. Collaborate with cross-functional teams to deliver high-quality, scalable data solutions aligned with business needs. Maintain code quality and version control standards using Git. Minimum 4 years of hands-on experience as a Data Engineer, with at least 2 years dedicated to Google Cloud Platform data services. Proven, hands-on experience with Cloudera, Hadoop and Apache Airflow ‚Äî these are mandatory. Advanced proficiency in SQL, including schema design and query optimization for large datasets. Deep expertise in BigQuery, including performance tuning and advanced SQL. Experience with at least one GCP data processing service: Dataflow (Apache Beam), Dataproc (Apache Spark/Hadoop), or Composer (Apache Airflow). Proficiency in at least one programming language such as Python, Java, or Scala for data pipeline development. Strong understanding of data warehousing and data lake concepts and best practices. Experience with version control tools, preferably Git. Certifications: Professional Cloud Architect or Professional Data Engineer. Language skills: Fluent in Italian, Good command of English (minimum B2 level). Experience with CI/CD pipelines and automated deployment for data engineering workflows. Familiarity with cloud security, data privacy, and compliance standards. Fully remote work with flexible working hours. Long-term collaboration on B2B contract. Opportunity to work on complex cloud projects for international clients. Professional growth in a highly skilled and supportive team. Collaborative and open working culture. üí° Don‚Äôt miss out on tailored opportunities! We have many ongoing recruitments, and new projects are constantly coming in. By giving your consent to process your data for future recruitment processes , we‚Äôll be able to invite you to roles that match your experience and expectations! PS: We‚Äôll only reach out to you when we have projects that might genuinely interest you ‚Äî without your consent, we won‚Äôt be able to do that. Our recruitment process is transparent and focused on finding the right candidate for our clients. When you apply, you can count on our objectivity, respect, and full professionalism. We look forward to receiving your CV. We connect you with the right people.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,504,Snowflake Data Engineer,Onwelo Sp. z o.o.,"Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Do≈ÇƒÖczysz do zespo≈Çu realizujƒÖcego projekt dla globalnej organizacji z sektora life science i healthcare , specjalizujƒÖcej siƒô w rozwiƒÖzaniach laboratoryjnych i biotechnologicznych. Klient prowadzi dzia≈Çalno≈õƒá na wielu rynkach i obs≈Çuguje tysiƒÖce jednostek operacyjnych na ca≈Çym ≈õwiecie.Celem projektu jest budowa i rozw√≥j skalowalnej platformy danych w ≈õrodowisku chmurowym, kt√≥ra wspiera analitykƒô biznesowƒÖ, planowanie operacyjne oraz zaawansowane modele predykcyjne.Zesp√≥≈Ç Onwelo wspiera klienta m.in . w rozwoju hurtowni danych, projektowaniu potok√≥w ETL, modelowaniu danych i zapewnieniu jako≈õci danych w ≈õrodowisku enterprise.Szukamy osoby, kt√≥ra wniesie swoje do≈õwiadczenie i wesprze zesp√≥≈Ç w rozwoju architektury danych, zapewniajƒÖc wydajno≈õƒá, jako≈õƒá i bezpiecze≈Ñstwo danych. Projektowaƒá, budowaƒá i rozwijaƒá hurtownie danych oparte na platformie Snowflake Tworzyƒá oraz optymalizowaƒá potoki danych (ETL/ELT) w ≈õrodowiskach chmurowych Wdra≈ºaƒá i zarzƒÖdzaƒá komponentami Snowflake: Snowpipe, Streams, Tasks, Secure Views Projektowaƒá i rozwijaƒá modele danych wspierajƒÖce analitykƒô biznesowƒÖ Wsp√≥≈Çpracowaƒá z zespo≈Çami Data Science i BI w zakresie zasilania modeli i dashboard√≥w Wspieraƒá automatyzacjƒô proces√≥w danych poprzez integracjƒô z narzƒôdziami CI/CD (GitLab, Jenkins) Posiadasz minimum 3-letnie do≈õwiadczenie jako Data Engineer ‚Äì z naciskiem na Snowflake Znasz platformƒô Snowflake : strukturƒô danych, architekturƒô, optymalizacjƒô zapyta≈Ñ, zarzƒÖdzanie schematami i dostƒôpem Biegle pos≈Çugujesz siƒô SQL (w tym: CTE, window functions, UDF, optymalizacja zapyta≈Ñ) Masz do≈õwiadczenie z procesami ETL/ELT , r√≥wnie≈º z wykorzystaniem danych p√≥≈Çstrukturalnych (JSON, XML, Parquet) Znasz zasady projektowania nowoczesnych modeli danych (np. Kimball, Data Vault) Masz wy≈ºsze wykszta≈Çcenie techniczne (np. informatyka, matematyka, in≈ºynieria danych) Komunikujesz siƒô po angielsku na poziomie min. B2 Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 16800, ""max"": 23100, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 17000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,505,Staff Engineer,dotLinkers,"The role: Staff Engineer Salary: up to 37 500 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Staff Software Engineer on the Compute team, you will lead the design and development of a next-generation compute platform that powers scalable workloads. Your focus will be on creating robust, event-driven, and batch-capable compute infrastructure using Kubernetes, KEDA, Temporal, Apache Spark, and advanced stream processing technologies. You will be part of an Infrastructure Services organization, structured into specialized teams focused on core platform areas such as compute, networking, and storage. The Compute team enables developers to deploy scalable workloads‚Äîfrom microservices and scheduled jobs to real-time data pipelines‚Äîleveraging cloud-native patterns that prioritize performance, elasticity, and developer autonomy. Key Responsibilities: Architect and implement cloud-native compute solutions to orchestrate background jobs, long-running workflows, and streaming data pipelines. Develop and maintain compute abstractions integrating Kubernetes, KEDA, and Temporal to support scalable job and service orchestration. Lead development of elastic data processing approaches using Apache Spark for batch workloads and streaming frameworks for real-time analytics. Define and promote best practices for running event-driven and parallel workloads in production environments. Collaborate with engineering teams to support various compute use cases, including microservices, cron jobs, ETL, workflow engines, and machine learning workloads. Ensure reliable autoscaling, failure handling, and resource optimization across compute workloads. Partner with platform security and observability teams to ensure compliance, transparency, and monitoring of workload execution. Mentor engineers on distributed system design and modern compute orchestration techniques. Minimum Qualifications: 8+ years of experience in backend, infrastructure, or data platform engineering roles. Extensive production experience with Kubernetes (preferably AKS). Hands-on experience with orchestration/eventing frameworks like KEDA, Temporal, or similar. Skilled in developing batch (e.g., Spark) and streaming (e.g., Kafka, Flink, Azure Event Hubs) processing systems. Strong programming skills in Go, Python, or C#, focusing on distributed or event-driven systems. Familiarity with secure compute design, autoscaling, and high-availability architectures. Preferred Qualifications: Experience implementing KEDA, Temporal, or Argo Workflows in production environments. Knowledge of Azure Synapse, Azure Data Explorer, Azure Data Factory, or Databricks. Experience building compute platforms for streaming analytics, ETL pipelines, or AI/ML workloads. Understanding of event-driven and pub-sub architectures like Kafka or Azure Event Grid. Contributions to cloud-native or CNCF open-source projects. Leadership Expectations: Define the strategy and technical roadmap for a unified compute platform. Promote cloud-native, event-driven, and scalable architecture practices across teams. Lead architecture reviews and encourage adoption of modern orchestration technologies. Mentor engineers and lead teams toward resilient, observable, and developer-friendly compute solutions. Drive long-term initiatives to improve workload performance, efficiency, and maintainability. Core Competencies: Cloud-Native Orchestration: Expert knowledge of Kubernetes-based job orchestration and scalable compute architectures. Data Processing: Ability to design for hybrid streaming and batch workloads in modern applications. Technical Leadership: Influences platform strategy and architectural direction organization-wide. Scalable System Design: Proven track record of scaling compute across diverse workloads and tenants. Developer Experience: Passionate about empowering engineering teams with powerful yet easy-to-use platforms. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 32500, ""max"": 37500, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Office,506,Specjalist/ka ds. business intelligence,Action S.A.,"üß© Twoje zadania: Tworzenie i rozw√≥j aplikacji biznesowych w ramach projektu wdro≈ºenia ERP. Pisanie i optymalizacja kartoteki produktowej (PIM) w technologii .NET (C#). Integracja system√≥w on-premises z chmurƒÖ (Microsoft Azure). Udzia≈Ç w projektach wymagajƒÖcych wszechstronnych kompetencji programistycznych. Bliska wsp√≥≈Çpraca z zespo≈Çem programist√≥w i architekt√≥w. Udzia≈Ç w procesach DevOps ‚Äì Azure DevOps, Jira, kontrola jako≈õci kodu. Praca z technologiami chmurowymi oraz integracja przez ASB Microsoft. üîß Wymagania: Do≈õwiadczenie: Min. 2‚Äì3 lata na stanowisku .NET Developera. Praktyczna znajomo≈õƒá jƒôzyka C# oraz ≈õrodowiska .NET. Do≈õwiadczenie w integracji system√≥w oraz pracy z aplikacjami biznesowymi. Kompetencje techniczne: Znajomo≈õƒá wzorc√≥w projektowych i dobrych praktyk programistycznych. Umiejƒôtno≈õƒá pracy z Microsoft Azure i narzƒôdziami DevOps. Znajomo≈õƒá API Rest Mile widziana znajomo≈õƒá Blazor, React. Dodatkowo: Znajomo≈õƒá j. angielskiego na poziomie technicznym. Gotowo≈õƒá do pracy hybrydowej (preferowana obecno≈õƒá stacjonarna na poczƒÖtkowym etapie wsp√≥≈Çpracy). üéÅ Co oferujemy? Dowolna forma wsp√≥≈Çpracy ‚Äì umowa o pracƒô / B2B. Udzia≈Ç w nowoczesnych projektach w ≈õrodowisku technologicznym. Workation ‚Äì pracuj z dowolnego miejsca w Polsce. Prywatna opieka medyczna, ubezpieczenie na ≈ºycie. Karta MultiSport. Bezp≈Çatna si≈Çownia i korty do squasha w siedzibie firmy. Przestronny parking i zaplecze sanitarne dla aut i rower√≥w. Dofinansowanie do okular√≥w korekcyjnych. Imprezy integracyjne i system polece≈Ñ pracowniczych z atrakcyjnymi nagrodami. Do≈ÇƒÖcz do ACTION S.A. i rozwijaj siƒô w ≈õwiecie nowoczesnych technologii i e-commerce! üì© Aplikuj ju≈º dzi≈õ i buduj z nami przysz≈Ço≈õƒá!","[{""min"": 11000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 12000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,507,Big Data Engineer,EndySoft,"Position Overview: We are seeking a skilled Big Data Engineer to join our data engineering team. The ideal candidate will have extensive experience in building and managing large-scale data processing systems. This role involves designing, implementing, and optimizing data pipelines and infrastructure to support analytics, machine learning, and business intelligence efforts. MD rate: 16600 ‚Äì 20000 PLN Roles and Responsibilities: Design, develop, and maintain big data pipelines to process and analyze large datasets. Implement data ingestion , processing , and storage solutions using big data frameworks such as Apache Spark , Hadoop , and Kafka . Optimize data pipelines for performance , scalability , and fault tolerance . Collaborate with data scientists, analysts, and other stakeholders to ensure data availability and usability. Develop and maintain data storage solutions such as HDFS , Amazon S3 , Google Cloud Storage , or Azure Data Lake . Ensure data quality and integrity through automated testing and validation processes. Monitor and troubleshoot big data infrastructure to ensure optimal performance and reliability. Document technical solutions, workflows, and best practices. Required Skills and Experience: Proficiency in big data technologies such as Apache Spark , Hadoop , Kafka , or Flink . Strong programming skills in languages like Python , Scala , or Java . Experience with SQL and NoSQL databases such as PostgreSQL , MongoDB , or Cassandra . Familiarity with cloud platforms such as AWS , Azure , or Google Cloud , including their big data services (e.g., EMR , BigQuery , Databricks ). Knowledge of data modeling , ETL processes , and data pipeline orchestration tools like Apache Airflow , Luigi , or Dagster . Strong understanding of distributed computing principles and parallel processing . Experience with containerization tools such as Docker and orchestration tools like Kubernetes . Strong problem-solving skills and ability to troubleshoot large-scale data systems. Nice to Have: Experience with real-time data processing and streaming platforms such as Apache Kafka Streams , Kinesis , or Pulsar . Familiarity with machine learning pipelines and integration with big data systems. Knowledge of data governance , security , and compliance in big data environments. Experience with CI/CD tools for automating data pipeline deployment and management. Exposure to Agile/Scrum methodologies. Understanding of data visualization tools such as Power BI , Tableau , or Looker . Additional Information: This role offers an opportunity to work on complex, large-scale data projects and help shape the future of data-driven decision-making. If you are passionate about big data technologies and thrive in a fast-paced, innovative environment, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,508,Data Engineer,SIX,"Data Engineer Warsaw | working from home up to 40% | Reference 7217 Are you curious about the stock market and passionate about data engineering? Would you like to work on cutting-edge technology at one of the most innovative and stable stock exchanges in the world? We are looking for a motivated Data Engineer to join our Data Management team. In this role, you will support the delivery of high-quality data products and contribute to the design and implementation of modern, cloud-based data pipelines that drive business value and improve user experience. support the migration of legacy data systems and applications to a new, cloud-native data platform (Azure) assist in the design and implementation of transformation projects in the data warehouse and big data environments contribute to the development and documentation of data-centric solutions, including involvement in solution design collaborate within a cross-functional agile team (SCRUM, SAFe) including developers, testers, product owners, and business stakeholders help ensure delivery of scalable, high-quality data solutions that meet business needs 3-4 years of hands-on experience building data pipelines using cloud platforms (e.g. Microsoft Azure) and/or Hadoop ecosystems (e.g. Hortonworks, Cloudera) working knowledge of relational and NoSQL databases, with basic experience in data modeling and processing using tools like Spark or Databricks familiarity with agile methodologies (SCRUM, Kanban) and collaboration tools such as Jira and Confluence a proactive mindset with a passion for learning, strong communication skills, and a collaborative attitude strong customer focus and the ability to work independently in a multicultural and distributed team environment; fluency in English (German is a plus) sharing the costs of sports activities private medical care & life insurance sharing the costs of foreign language classes sharing the costs of professional training & courses remote work opportunities &flexible working time integration events &charity initiatives fruits and popcorn in the office video games at work, no dress code & leisure zone extra social benefits & holiday funds (Christmas/Easter gifts) meal and transportation allowance employee referral program Employee Assistance Program Day for U (Day for Medical Checkup) My Benefit Cafeteria Udemy for Business days for remote work from abroad If you have any questions, please call Gabriela Swiatek at +48 22 104 67 70. For this vacancy we only accept direct applications in English . Diversity is important to us. Therefore, we are looking to receiving applications regardless of any personal background.","[{""min"": 14000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,509,Senior Business Intelligence Operations Administrator,Coloplast Business Centre,"Coloplast develops products and services that make life easier for people with very personal and private medical conditions. In Coloplast Business Centre we are responsible for global financial operations, orders administration, P&C, IT, Legal support, marketing & data analysis activities, and many others. If you have the right profile and the right amount of curiosity and ambition, you can be a part of this exciting challenge. Curiosity works here! About the department In our department, we value innovation, collaboration, and continuous improvement. We offer a dynamic work environment with opportunities for professional growth and development. Join us and be a part of a team that is dedicated to providing outstanding user support, ensuring stable and efficient operations and that continously evolves our processes and technical solutions. We are part of Digital, Data &IT and we work with the entire company ‚Äì we are a truely global partner for our stakeholders and data customers. About the job We are seeking a highly skilled and experienced Senior Business Intelligence (BI) Operations Administrator to join our team. In this role, you will be responsible for ensuring high quality operational monitoring & reporting of our Microsoft Azure data platform and our Databricks data platform. You will collaborate closely with our BI team in Denmark to assist with daily tasks, advanced troubleshooting and setting the direction for future operations. Additionally, you will support and guide global users to restore normal BI services using existing documentation or workarounds/solutions that must be documented, adhering to ITIL standards and Agile principles in the BI domain. Your responsibilities Deployment of BI solutions. System configuration, setup, documentation, and enhancements. 2nd and 3rd level end-user support. Monitoring, maintaining, and improving our current BI systems and infrastructure. Collaborate with the BI team in Denmark on daily tasks. Work with database management software to store, analyze, utilize, and present data. Manage user access and security. Requirements 5-6 years of experience working with a large cloud-based BI setup Solid knowledge of Business Intelligence platforms Minimum 3-year experience working in an international work environment Advanced SQL skills and strong experience with SQL databases Proven experience with managing and administering Azure Data Factory (ADF), Azure Analysis Services, or equivalent tools Hands-on experience with Power BI Master‚Äôs or bachelor‚Äôs degree in IT or a related field Experience working with Databricks platform will be considered a plus Knowledge of ITIL Fundamentals framework will be considered a plus Experience working with DevOps as a deployment tool, including documentation of processes Travel Requirements: 5-10 days away from the office annually Competences Good business understanding. Good team-player. Fluent in spoken and written English. Broad BI understanding. We offer ‚Ä¢ Hybrid and onsite work from our office in Szczecin/ Remote work possibilities (Poland) ‚Ä¢ Flexible working hours, Mon-Fri ‚Ä¢ International work environment with Scandinavian culture ‚Ä¢ Opportunity to use and develop foreign languages in daily work ‚Ä¢ Fantastic work atmosphere full of respect and partnership ‚Ä¢ Internal trainings available in CPBC Academy That's not all! We have even more for you, if working onsite: ‚Ä¢ Modern workplace ‚Ä¢ No dress code zone ‚Ä¢ Great company events ‚Ä¢ Sports card ‚Ä¢ Private medical care ‚Ä¢ Transportation co-funding ‚Ä¢ Restaurant card ‚Ä¢ Holiday bonus and occasional cards ‚Ä¢ Delicious coffee and fresh fruits Recruitment details Please apply online with your CV in English. We will review all applications continuously and invite selected candidates further to the recruitment process. We will close the offer down once we find the best match to the role. Thank you!",[],Database Administration,Database Administration
Full-time,Mid,B2B,Remote,510,Insurance Data Analyst,Link Group,"Opis stanowiska: Poszukujemy do≈õwiadczonego Insurance Data Analyst do pracy w miƒôdzynarodowych projektach technologicznych. Osoba na tym stanowisku bƒôdzie odpowiedzialna za analizƒô danych, przygotowanie wymaga≈Ñ biznesowych oraz wsp√≥≈Çpracƒô z klientami z sektora ubezpiecze≈Ñ. Szukamy specjalisty o podej≈õciu technologicznym, kt√≥ry wesprze nasze zespo≈Çy w realizacji ambitnych projekt√≥w. Zakres obowiƒÖzk√≥w: Analiza biznesowa oraz zbieranie wymaga≈Ñ funkcjonalnych i niefunkcjonalnych Budowanie i utrzymywanie relacji z klientami oraz onsite koordynatorami Wsparcie w planowaniu i realizacji projekt√≥w Analiza, przygotowanie i eksploracja danych Tworzenie raport√≥w oraz wizualizacji danych przy u≈ºyciu narzƒôdzi no-code/low-code Przestrzeganie standard√≥w jako≈õci oraz wspieranie inicjatyw organizacyjnych Wsp√≥≈Çpraca w ≈õrodowisku Agile (Scrum, Kanban) Wymagania: Minimum 3 lata do≈õwiadczenia w pracy na podobnym stanowisku w ≈õrodowisku korporacyjnym Do≈õwiadczenie w bran≈ºy ubezpieczeniowej (min. 2 lata, aktualne) Bieg≈Ça znajomo≈õƒá SQL oraz technik projektowania relacyjnych baz danych, data mart√≥w, hurtowni danych i data lakes Umiejƒôtno≈õci w zakresie czyszczenia, eksploracji i analizy danych Znajomo≈õƒá narzƒôdzi do wizualizacji danych Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (w mowie i pi≈õmie) Wykszta≈Çcenie wy≈ºsze techniczne (In≈ºynieria, IT, Nauki ≈öcis≈Çe) Mile widziane: Znajomo≈õƒá OpenL Tablets Certyfikaty potwierdzajƒÖce umiejƒôtno≈õci (Java, Spring, SQL, AWS lub Azure Cloud, Angular, React) Do≈õwiadczenie w bezpo≈õredniej pracy z klientem","[{""min"": 95, ""max"": 125, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,511,Data & BI Software Engineer,Spyrosoft,"Requirements: Experience in any SQL (or similar) Knowledge of BI tools (particularly Power BI) Ability to design data models Optimizing interactive dashboards and reports Familiarity with ETL/ELT processes Experience with GCP, especially Looker and LookML Data governance principles and experience in monitoring data quality Fluency in English We are seeking a Data & BI Specialist responsible for designing, optimizing, and developing BI solutions. Depending on your experience level (Regular/Senior), your tasks will include advanced data modeling, performance optimization, and data visualization. Additionally, you may work on developing infrastructures and processing large datasets, ensuring their high quality and availability across the organization. Data Visualization : Creating and optimizing interactive dashboards and data visualizations. Performance optimization of reports. UX/UI development. Data Modeling & SQL : Designing scalable data models. Advanced SQL queries. Supporting Master Data Management (MDM). Infrastructure & Data Processing : Building and optimizing ETL/ELT processes. Incident management and monitoring cloud infrastructure. Data Governance & Quality : Implementing data governance principles. Monitoring data quality (KPIs). Managing data catalogs (e.g., Purview). Business Support : Supporting stakeholders through data analysis and delivering key insights. Gathering business requirements. Proficiency in SQL (or Python, Scala, etc.) and ability to design data models (Lookml, AAS) Hands-on experience with BI tools (Looker, Power BI) and optimizing interactive dashboards and reports. Knowledge of ETL/EL processes Strong expertise in Google Cloud Platform, particularly with Looker and LookML. Understanding of data governance principles and experience in monitoring data quality. Fluent communication in English, especially in the context of technical documentation and collaboration with international teams. Ability to create scripts for process automation (Python, Scala). Experience in Fabric, Databricks, Snowflake, etc. Basic knowledge of frameworks for processing large datasets, such as Hadoop or Spark. Experience with data management tools (e.g., Purview, Collibra).","[{""min"": 70, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,512,Experienced Data Scientist,Future Mind,"Future Mind is a brilliant, inspiring team, one of the most awarded tech consulting companies in the region with a broad portfolio of clients, including ≈ªabka, Jeronimo Martins (Hebe, Biedronka), LPP (Reserved, Sinsay, Mohito), eObuwie, Modivo, and other well-recognized brands. We have received several industry awards for delivering some of the best eCommerce applications in Poland, listed among the most popular across app stores. Our expert engineers, designers, project managers, and analysts work on projects ranging from top mobile commerce apps used daily by millions of customers to IoT, and telematics platforms that produce vast amounts of data. In 2023, we joined forces with Solita, a Finnish tech powerhouse with a vibrant community of over 2000 specialists across Europe, that combines data, business, and technology skills to build and improve digital services for leading organizations in manufacturing, medical, shipping, and other major industries, including such clients as NATO, Nokian Tires, Pfizer and many others. Together we are dedicated to delivering cutting-edge, data-driven solutions. We value proactive professionals who take ownership, enjoy solving problems, sharing knowledge, and collaboration. Together, we create high-quality software solutions that fulfil our clients' business needs and impact their customers' lives every day. As part of the Solita Group, Future Mind provides digital advisory & delivery services with the support of our international partners and upholds equally high cultural standards. Role description: As a Data Scientist, your role involves taking part in data insights initiatives, designing and implementing machine learning solutions, while collaborating closely with the business stakeholders. You have a strong statistics background and have worked with cloud technologies for several years. This job is all about: Designing data science solutions based on large, complex data sets that meet both functional and non-functional business requirements; Building, testing, and deploying machine learning and statistics models on cloud environments; Working with cross-functional teams on delivering business value; Collaborating within our project teams to meet client needs and deliver high-quality solutions. Here‚Äôs what we‚Äôre looking for: Ability to analyze large datasets, understand data patterns, and implement AI solutions that drive business impact. Strong background in statistics, technical proficiency in Python, SQL, and leading modern ML frameworks. Familiarity with cloud platforms (AWS, Google Cloud, Azure) and deploying machine learning models in production environments. Previous experience in Ad Tech or analysing large behavioral datasets would be an advantage. Effective communication skills, proficient in Polish and English. You also must have the legal right to work in the EU to apply for this position. ‚Ä¶and here‚Äôs what we offer: A dynamic work environment where innovation and collaboration are valued. Access to cutting-edge projects and technologies in a variety of industries. A supportive community of experts to foster your professional growth and development. Competitive compensation, comprehensive benefits, and a focus on work-life balance. Opportunities for continuous learning and career advancement, including specialized training in big data technologies and Snowflake certifications. The ability to work fully remotely or check into one of our offices whenever you like, Fully paid private health insurance, subsidized sports membership, mental health support, and language courses.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,Permanent or B2B,Remote,513,Data Engineer with Python,Sii,"We are looking for a Senior Data Engineer who is not merely familiar with Google Cloud Platform and data engineering concepts through adjacent development or integration tasks, but someone who truly lives and breathes data. You should feel fully at ease working in cloud environments and have a deep commitment to building reliable, scalable, and high-quality data infrastructure. This role is ideal for someone passionate about transforming raw data into meaningful insight, who understands the strategic value of data governance, and who thrives in dynamic, cross-functional teams. Designing, developing, and maintaining scalable data pipelines and ETL processes in GCP Optimizing data processing workflows for performance, reliability, and cost-efficiency Collaborating with data scientists, analysts, and business stakeholders to translate requirements into technical solutions Ensuring compliance with data quality standards and implementing governance best practices Driving and supporting the migration of on-premise data products to GCP Managing and evolving cloud-based infrastructure and deployment practices A minimum of 5 years of experience in data engineering or a similar role Proficiency in Python and SQL, with hands-on experience working in cloud environments (GCP preferred) Solid experience with Google Cloud services: BigQuery, Cloud Storage, Pub/Sub, Cloud Build, Cloud Composer, DataFlow Familiarity with infrastructure and deployment tools such as Terraform, OpenShift, Docker, Kubernetes, and CI/CD pipelines Working knowledge of Oracle, PL/SQL, Airflow, Git, Java, and Kafka Strong problem-solving skills, autonomy, and the ability to collaborate effectively in cross-functional teams Fluent Polish required Residing in Poland required Continuous Deployment Continuous Integration Great Place to Work since 2015 - it‚Äôs thanks to feedback from our workers that we get this special title and constantly implement new ideas Employment stability - revenue of PLN 2.1BN, no debts, since 2006 on the market We share the profit with Workers - over PLN 60M has already been allocated for this aim since 2022 Attractive benefits package - private healthcare, benefits cafeteria platform, car discounts and more Comfortable workplace ‚Äì class A offices or remote work Dozens of fascinating projects for prestigious brands from all over the world ‚Äì you can change them thanks to Job Changer application PLN 1 000 000 per year for your ideas - with this amount, we support the passions and voluntary actions of our workers Investment in your growth ‚Äì meetups, webinars, training platform and technology blog ‚Äì you choose Fantastic atmosphere created by all Sii Power People 1 Send your CV 2 Talk to us about your expectations 3 Learn more about our projects and choose the best 4 Start your adventure with Sii! Sii is the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We already employ more than 7 500 professionals and implement projects in a variety of industries for clients from many countries around the world. The Great Place to Work title, won 10 times in a row, proves that at Sii we create a friendly work environment. In a survey, as many as 90% of our employees responded that Sii is a great place to work, and 95% of them think we have a great atmosphere here.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,514,Senior Data Analyst,ICAP People Solutions,"ICAP Executive Search, on behalf of our client, a leading Global Fintech Group , we are seeking an experienced and motivated Senior Data Analyst to join their expanding team. Senior Data Analyst - (Ref. code: 5957/SDA/ESS/POL) Location: Warsaw, Poland (Hybrid) About the company: Our client is a globally recognized, multi-regulated Fintech Group. Since its inception in 2014, the company has experienced rapid growth, expanding its presence with offices in London, Cyprus, Estonia, Dubai, South Africa, and employing over 300 professionals worldwide. Built on a foundation of trust, transparency, and diversity, the organization brings together a highly skilled, multilingual team committed to innovation and excellence. The Warsaw IT Hub is poised to become a key driver in advancing the company‚Äôs fintech initiatives. Key Responsibilities: Collaborate with stakeholders, Product Owners, and Analysts to understand business needs and translate them into actionable insights. Analyze large, complex datasets to identify trends, patterns, and opportunities, working closely with the BI team. Design, develop, and maintain dashboards, reports, data models, and visualizations that clearly communicate findings to both technical and non-technical stakeholders. Drive data-led initiatives to improve strategic planning and business operations, focusing on key areas like product profitability and client lifecycle insights. Lead analytics projects including Client Lifetime Value (CLV) calculations, acquisition funnel optimization, and client journey mapping. Monitor and optimize key conversion points and drop-offs in the client acquisition funnel. Research and integrate external data sources to enrich internal analysis and generate new insights. Mentor junior analysts and internal data champions on analytical best practices and data governance principles. Keep up to date with the latest industry trends, tools, and methodologies in data analytics and business intelligence. Candidate Profile Bachelor‚Äôs degree in a quantitative field such as Mathematics, Statistics, Computer Science, Information Systems, or similar. At least 3 years in a Senior Data Analyst or similar role, preferably within the Finance or FX industry. Advanced SQL skills with a solid understanding of data modeling (Star Schema, Semantic Models). Proficiency in Power BI (or similar tools) with a keen eye for selecting the most effective visualizations. Strong knowledge of DAX for advanced data calculations. Experience developing predictive models for forecasting and client behavior analysis. Strong analytical thinking with attention to detail and a curiosity-driven mindset. Excellent communication skills; able to translate complex findings into clear, actionable recommendations. Team-oriented with a proactive attitude and solid project management skills.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Hybrid,515,Software Consultant (Advanced SQL),Dassault Systemes,"We are looking for a Software Expert for our DELMIA Apriso Services Team. You will be primarily responsible for the development, configuration and delivery of solutions for international manufacturing companies. You will work closely with clients and address customer‚Äôs business needs. Role Description & Responsibilities Configure software solutions; including process authoring, development and testing of prepared extensions Configure interfaces to integrate external systems Assist and provide support during testing, go-live and post go-live phases of the project Collaborate with Solution Architects while creating functional and technical designs and solution documentation Based on your experience you may also lead the technical delivery of projects, manage a group of consultants and participate in the entire project lifecycle including activities at presales and design stages Analyze the performance of implemented solutions and conduct troubleshooting and optimization Be proficient in diagnosing and resolving database performance issue Configure software solutions; including process authoring, development and testing of prepared extensions Configure interfaces to integrate external systems Assist and provide support during testing, go-live and post go-live phases of the project Collaborate with Solution Architects while creating functional and technical designs and solution documentation Based on your experience you may also lead the technical delivery of projects, manage a group of consultants and participate in the entire project lifecycle including activities at presales and design stages Analyze the performance of implemented solutions and conduct troubleshooting and optimization Be proficient in diagnosing and resolving database performance issue Qualifications Minimum 2 years of experience in MS SQL database querying Familiarity with best practices for database optimization in the MS SQL environment Practical knowledge of execution plans, indexes, statistics analysis Basics in: C#, HTML, CSS, JavaScript, XML, JSON Bachelor or Master Degree in computer science, information technologies or a related field Ability to travel up to maximum 30% of the time Fluent English Good knowledge of German would be an asset Understanding manufacturing and logistic processes would be an asset What's in it for you Opportunity, to work for one of the biggest software company in Europe Attractive salary Yearly bonus Multinational work environment Internal and external trainings portfolio and participation in industry conferences Private health care with dental, life Insurance and additional benefits (Multisport, a parking spot, integration events, flexible working hours, bridge days, PPE ‚Äì Pracowniczy Program Emerytalny, program KUP)",[],Database Administration,Database Administration
Full-time,Mid,B2B,Hybrid,516,Data Modeler üß©,ITDS,"Join us, and craft the foundation for intelligent analytics! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As a Data Modeller, you will be working for our client, a leading global financial institution driving large-scale digital transformation and data strategy initiatives. You will be supporting a major data architecture project aimed at improving data quality, accessibility, and consistency across the organization. You will collaborate with cross-functional teams and lead the development of enterprise-level data models that support both operational efficiency and analytical insights. This role is central to implementing scalable, secure, and compliant data solutions that empower better decision-making and foster innovation across the business. Your main responsibilities: Leading the design and implementation of conceptual, logical, and physical data models Collaborating with stakeholders to gather and translate data requirements into scalable models Establishing and maintaining best practices and governance standards for data modelling Mentoring and training junior data modellers on tools, methodologies, and techniques Coordinating with data architects, engineers, and analysts to align data strategies Participating in architecture and design discussions to ensure consistency across platforms Developing and enforcing data quality and integrity standards across modelling initiatives Supporting the integration of data modelling solutions into enterprise systems Researching and recommending new modelling tools and technologies Ensuring compliance with data governance and protection regulations You're ideal for this role if you have: Demonstrated leadership experience in managing data modelling teams Deep expertise in data modelling for both operational and analytical systems Proven ability to translate complex business requirements into data models Excellent verbal and written communication skills for diverse audiences Strong consulting skills with the ability to advise cross-functional teams Experience in stakeholder management and navigating complex organizations Advanced problem-solving capabilities for data-related challenges Solid understanding of metadata and data lineage concepts Knowledge of data governance frameworks and data quality standards Proficiency with data modelling tools such as ER/Studio, ERwin, or similar It is a strong plus if you have: Hands-on experience with cloud data platforms such as AWS, Azure, or Google Cloud Familiarity with big data technologies like Hadoop or Spark Experience working with industry-standard models like BIAN, FSDM, or BDW Understanding of data architecture patterns in financial services Exposure to master data management (MDM) concepts and platforms We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #6913 üìå You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here.","[{""min"": 23100, ""max"": 27720, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,517,Senior Data Engineer,INFOPLUS TECHNOLOGIES,"Job Title: Senior Data Engineer üìç Location: Poland (Remote) üí∞ Rate: 1000 PLN/day üïí Seniority Level: Senior Required Skills & Experience: 5+ years of experience in data engineering roles Strong proficiency in Python for data engineering tasks Hands-on experience with AWS services such as: S3, Glue, Lambda, Step Functions, Redshift, Athena, CloudFormation Experience with CI/CD tools (e.g., GitLab CI, Jenkins, GitHub Actions) Proficiency with AWS CDK or other Infrastructure as Code frameworks Solid understanding of data warehousing , data lakes , and ETL best practices Familiar with version control (Git), unit testing, and agile delivery practices Excellent problem-solving skills and ability to work independently Strong communication skills with the ability to explain technical topics to stakeholders 5+ years of experience in data engineering roles Strong proficiency in Python for data engineering tasks Hands-on experience with AWS services such as: S3, Glue, Lambda, Step Functions, Redshift, Athena, CloudFormation Experience with CI/CD tools (e.g., GitLab CI, Jenkins, GitHub Actions) Proficiency with AWS CDK or other Infrastructure as Code frameworks Solid understanding of data warehousing , data lakes , and ETL best practices Familiar with version control (Git), unit testing, and agile delivery practices Excellent problem-solving skills and ability to work independently Strong communication skills with the ability to explain technical topics to stakeholders","[{""min"": 800, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,518,Data Architect (public/health),Britenet,"Projekt dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania: Minimum 5 lat do≈õwiadczenia na stanowisku Architekta IT, poparte projektowaniem architektur system√≥w zorientowanych na us≈Çugi (SOA, mikroserwisy), wielowarstwowych oraz o wysokiej wydajno≈õci i niezawodno≈õci. Praktyczne do≈õwiadczenie w roli Architekta w projektach o bud≈ºecie min. 1 mln z≈Ç brutto oraz w szacowaniu pracoch≈Çonno≈õci (min. 1000 roboczogodzin) i z≈Ço≈ºono≈õci rozwiƒÖza≈Ñ (ilo≈õƒá/wielko≈õƒá komponent√≥w). Umiejƒôtno≈õƒá skalowania aplikacji (horyzontalne i wertykalne) oraz badania i oceny bezpiecze≈Ñstwa system√≥w teleinformatycznych. Do≈õwiadczenie w architekturze rozwiƒÖza≈Ñ data warehouse i narzƒôdzi analitycznych, w tym znajomo≈õƒá projektowania hurtowni danych i data mart√≥w. G≈Çƒôboka znajomo≈õƒá architektury, projektowania i integracji system√≥w IT, w tym wzorc√≥w projektowych i architektonicznych, serwer√≥w aplikacyjnych oraz zagadnie≈Ñ DevOps i konteneryzacji. Bieg≈Ço≈õƒá w programowaniu SQL i Python, a tak≈ºe w projektowaniu i programowaniu proces√≥w ETL. Praktyczna znajomo≈õƒá baz danych: PostgreSQL/EDB, MySQL, MongoDB, Oracle. Swobodna obs≈Çuga narzƒôdzi: Enterprise Architect, MS Project, Jira. Doskona≈Ça organizacja pracy, zorientowanie na cel, umiejƒôtno≈õci interpersonalne (planowanie, definiowanie, monitorowanie cel√≥w) oraz efektywna komunikacja. Kreatywno≈õƒá, samodzielno≈õƒá, wysoka kultura osobista, odporno≈õƒá na stres, proaktywno≈õƒá oraz otwarto≈õƒá na rozw√≥j i nowe technologie. Mile widziane: Do≈õwiadczenie projektowe z obszaru Hurtowni Danych Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Certyfikaty z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny); potwierdzajƒÖcy umiejƒôtno≈õci z obszaru projektowania architektur rozwiƒÖza≈Ñ IT (np.. TOGAF¬Æ EA Foundation lub r√≥wnowa≈ºny); potwierdzajƒÖcy wiedzƒô z zakresu administrowania EDB (np. EDB Certification - PostgreSQL Essentials 15 lub r√≥wnowa≈ºny); z obszaru administrowania ≈õrodowiskiem Hadoop (np. ClouderaCertified Administrator for Hadoop (CCAH), Hortonworks Certified Apache Hadoop Administrator (HCAHA) lub r√≥wnowa≈ºny) Kluczowe zadania: Projektowanie architektur system√≥w zorientowanych na us≈Çugi (SOA) i mikroserwis√≥w Tworzenie wielowarstwowych system√≥w o wysokiej wydajno≈õci i niezawodno≈õci Skalowanie aplikacji zar√≥wno horyzontalnie, jak i wertykalnie Zapewnienie sp√≥jno≈õci i integralno≈õci architektury system√≥w Aktywny udzia≈Ç w projektach IT, w tym w projektach o znacznym bud≈ºecie (minimum 1 mln z≈Ç brutto) Szacowanie pracoch≈Çonno≈õci zada≈Ñ programistycznych i architektonicznych (na poziomie minimum 1000 roboczogodzin) Ocena z≈Ço≈ºono≈õci aplikacji i rozwiƒÖza≈Ñ (liczba i wielko≈õƒá komponent√≥w) Badanie i ocena bezpiecze≈Ñstwa informacji w systemach teleinformatycznych Zapewnienie zgodno≈õci system√≥w z zasadami bezpiecze≈Ñstwa IT Zastosowanie wzorc√≥w projektowych i architektonicznych w celu zapewnienia jako≈õci i niezawodno≈õci Realizacja architektury rozwiƒÖza≈Ñ zawierajƒÖcych hurtownie danych i narzƒôdzia analityczne Projektowanie i programowanie proces√≥w ETL Znajomo≈õƒá i wykorzystanie relacyjnych baz danych (PostgreSQL/EDB, MySQL, Oracle) oraz nierelacyjnych (MongoDB) Projektowanie i implementacja integracji system√≥w IT","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,519,IT Business Intelligence Analyst,Primaris,"Forma wsp√≥≈Çpracy : kontrakt B2B Tryb: 100% zdalnie Aktualnie do jednego z projekt√≥w poszukujemy osoby na stanowisko IT Business Intelligence Analyst , kt√≥ra posiada min. 5 lat komercyjnego do≈õwiadczenia. Dlatego je≈õli szukasz pracy w obszarze hurtowni danych to chƒôtnie z TobƒÖ porozmawiamy! WiodƒÖcy stack technologiczny: SQL, procesy ETL Projekt kt√≥ry bƒôdziemy gotowi Ci zaproponowaƒá dotyczy : budowy hurtowni danych w obszarze raportowania regulacyjnego w kontek≈õcie Systemu Informacji Finansowej. Wymagania: Znajomo≈õƒá proces√≥w AML i KYC oraz wynikajƒÖcych z nich wymaga≈Ñ raportowych Do≈õwiadczenie w raportowaniu regulacyjnym, szczeg√≥lnie w sektorze finansowym lub bankowym Bieg≈Ça znajomo≈õƒá SQL oraz umiejƒôtno≈õƒá pisania zaawansowanych zapyta≈Ñ i skrypt√≥w Do≈õwiadczenie w modelowaniu danych oraz pracy z hurtowniami danych korporacyjnych Znajomo≈õƒá i praktyczne do≈õwiadczenie z procesami ETL lub ELT Znajomo≈õƒá jƒôzyka angielskiego na poziomie B2/C1 Umiejƒôtno≈õƒá analizy danych i proponowania usprawnie≈Ñ modelu danych Znajomo≈õƒá Systemu Informacji Finansowej (SInF) Umiejƒôtno≈õƒá pracy w zespole i komunikacji miƒôdzydzia≈Çowej, zw≈Çaszcza z zespo≈Çami AML i Compliance Znajomo≈õƒá regulacji dotyczƒÖcych przeciwdzia≈Çania praniu pieniƒôdzy i finansowaniu terroryzmu (AML/CFT) oraz system√≥w wspierajƒÖcych te procesy (takich jak SCU AML) Zakres obowiƒÖzk√≥w: Tworzenie i nadzorowanie proces√≥w raportowania regulacyjnego w ramach Systemu Informacji Finansowej (SInF) Przygotowywanie raport√≥w regulacyjnych na potrzeby Sektorowego Centrum Us≈Çug AML, zgodnie z wymogami prawnymi i regulacyjnymi Wsp√≥≈Çpraca z zespo≈Çami AML i KYC w celu zapewnienia poprawno≈õci i aktualno≈õci danych s≈Çu≈ºƒÖcych do raportowania Modelowanie danych pochodzƒÖcych z korporacyjnej hurtowni danych, z uwzglƒôdnieniem specyfiki proces√≥w AML/CFT Projektowanie i implementacja proces√≥w ETL/ELT umo≈ºliwiajƒÖcych automatyczne zasilanie raport√≥w Pisanie zapyta≈Ñ SQL oraz tworzenie prototyp√≥w skrypt√≥w wspierajƒÖcych proces raportowania Analiza danych w celu identyfikacji mo≈ºliwo≈õci optymalizacji i ulepszania modelu danych Monitorowanie zmian regulacyjnych zwiƒÖzanych z AML oraz dostosowywanie proces√≥w raportowania do nowych wymaga≈Ñ Wspieranie efektywno≈õci proces√≥w AML/CFT poprzez wsp√≥≈Çdzielenie danych i standaryzacjƒô raport√≥w sektorowych w ramach SCU AML W naszej firmie bƒôdziesz m√≥g≈Ç/mog≈Ça liczyƒá na: Pracƒô w organizacji z ugruntowanƒÖ pozycjƒÖ rynkowƒÖ Projekty, w kt√≥rych bƒôdziesz mia≈Ç/mia≈Ça wp≈Çyw na ich rozw√≥j Wsp√≥≈Çpracƒô z ciekawymi klientami biznesowymi z r√≥≈ºnych bran≈º ( m.in .: finanse, bankowo≈õƒá, ubezpieczenia, healthcare, robotyzacja, energetyka, media), Permanentny mentoring zar√≥wno techniczny jak i biznesowo-mened≈ºerski, np. podczas naszych cyklicznych szkole≈Ñ ( m.in . Git, Gitflow, Angular, Docker), czy wew. program√≥w rozwojowych (Primaris x TechTalks, Primaris Leadership Academy) oraz zewnƒôtrznych kurs√≥w.Ju≈º na etapie on-boardingu zapewniamy dostƒôp do naszych wewnƒôtrznych szkole≈Ñ, cyklicznych spotka≈Ñ, kt√≥re serializujemy na Confluence oraz platformy e-learning ≈öwietnƒÖ atmosferƒô pracy, w≈õr√≥d zaanga≈ºowanych ludzi z pasjƒÖ w p≈Çaskiej strukturze z prostymi procesami Kompleksowy pakiet benefit√≥w skrojonych na miarƒô - prywatna opieka medyczna dla Ciebie oraz dla Twojej rodziny, Multisport dla Ciebie i os. towarzyszƒÖcej - Ty decydujesz, co wybierasz! Ca≈Çy proces rekrutacyjny oraz onboarding prowadzony jest zdalnie. Proces rekrutacyjny sk≈Çada siƒô z: rozmowy telefonicznej z osobƒÖ z dzia≈Çu Rekrutacji & HR (do 30 min) zdalnej video rozmowy - weryfikacji techniczno-biznesowej z naszym specjalistƒÖ/specjalistkƒÖ (60-90 min) zdalnego spotkania z liderem po stronie klienta projektu (30-60 min) finalnej decyzji dotyczƒÖcej oferty Primaris Services to ponad 250 ekspert√≥w na pok≈Çadzie i 15 lat do≈õwiadczenia w bran≈ºy IT na rynku polskim oraz zagranicznym.Realizujemy ambitne projekty o wysokiej z≈Ço≈ºono≈õci z r√≥≈ºnych obszar√≥w - m.in . bankowo≈õci, ubezpiecze≈Ñ, funduszy inwestycyjnych czy bran≈ºy logistycznej (mamy ponad 40 aktywnych klient√≥w!). Ro≈õniemy w si≈Çƒô oraz ciƒÖgle poszerzamy portfolio zar√≥wno naszych us≈Çug jak i klient√≥w. Zakres naszej dzia≈Çalno≈õci obejmuje budowƒô system√≥w od zera, ich rozw√≥j oraz utrzymanie, wdro≈ºenia produktowe, alokacje ca≈Çych Zespo≈Ç√≥w, a tak≈ºe pojedynczych Ekspert√≥w w strukturach Klienta. Ponadto od kilku lat dzia≈Çamy bardzo intensywnie jako z≈Çoty Partner firmy UiPath (obszar Robotic Process Automation) budujƒÖc roboty i sprzedajƒÖc licencje u naszych Klient√≥w. Co miesiƒÖc do≈ÇƒÖcza do nas 7 nowych os√≥b! Wierzymy, ≈ºe zgrany zesp√≥≈Ç i ludzie z pasjƒÖ to klucz do naszego wsp√≥lnego sukcesu! W≈Ça≈õnie dlatego ciƒÖgle poszukujemy nowych, zdolnych os√≥b, kt√≥re zasilƒÖ nasze szeregi.","[{""min"": 130, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,520,Data Architect with AWS,Upvanta sp. z o.o.,"Projektowanie i rozw√≥j architektury danych w ≈õrodowisku chmurowym AWS. Wsp√≥≈Çpraca z zespo≈Çami developerskimi, analitykami i interesariuszami biznesowymi. Tworzenie i nadzorowanie modeli danych, przep≈Çyw√≥w ETL/ELT oraz struktur hurtowni danych. Dob√≥r i implementacja narzƒôdzi oraz technologii wspierajƒÖcych zarzƒÖdzanie danymi. Udzia≈Ç w projektach transformacji danych i migracji do chmury. Minimum 5 lat do≈õwiadczenia w pracy z architekturƒÖ danych. Praktyczna znajomo≈õƒá us≈Çug AWS zwiƒÖzanych z danymi (np. Redshift, Glue, S3, Lambda, Athena, EMR). Do≈õwiadczenie w projektowaniu skalowalnych i bezpiecznych rozwiƒÖza≈Ñ data-centric. Umiejƒôtno≈õƒá pracy z du≈ºymi zbiorami danych, integracjƒÖ danych oraz modelowaniem danych. Znajomo≈õƒá SQL, Python i narzƒôdzi ETL. Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (wsp√≥≈Çpraca miƒôdzynarodowa). Certyfikaty AWS (np. AWS Certified Data Analytics, AWS Solutions Architect). Do≈õwiadczenie w ≈õrodowiskach regulowanych (np. finanse, ubezpieczenia). Znajomo≈õƒá dbt, Snowflake, Databricks lub Power BI. Wsp√≥≈Çpracƒô w oparciu o umowƒô B2B. Pracƒô w 100% zdalnƒÖ. D≈Çugoterminowy projekt z du≈ºym zakresem odpowiedzialno≈õci. Mo≈ºliwo≈õƒá wsp√≥≈Çpracy z ekspertami z r√≥≈ºnych kraj√≥w i bran≈º. Atrakcyjne wynagrodzenie zale≈ºne od do≈õwiadczenia.",[],Data Architecture,Data Architecture
Full-time,Mid,Other,Remote,521,Middle Delivery Engineer,MasterBorn Sp. z o.o.,"As a Delivery Engineer, you'll play a pivotal role in ensuring the seamless delivery and optimization of our product for our esteemed retail clientele. Dive into a world of problem-solving and collaboration, where your technical prowess and excellent communication skills will be key in driving the success of our service and application features as well as its quality and performance. Required: 3 years of commercial experience with databases (SQL, database administration, performance tuning), Practical knowledge of database scripting languages (T-SQL, PL/pgSQL), Experience working with large data sets, Experience with system integrations and various data formats (XML, JSON, CSV, Parquet), Practical experience with Python, Familiarity with shell scripting languages, Experience with version control systems (e.g., Git), Proficiency in Microsoft software suite, Experience working directly with clients, Polish C1, English B2/C1 Offer: B2B/UZL, paid days off, 100% remote (or hybrid/onsite - as you prefer), full-time position, long-term contract, polish working hours (10am to 6pm) Perks and Benefits: 750 PLN / quarter for health insurance and sports, Mentoring Program, remote job offer, and more Tools you'll use: MS Office Suite, Azure Devops, Jira, Miro, Windows/iOS, MS SQL, PostgreSQL (Greenplum), Snowflake, Python, Powershell Work with existing retail customers to configure and update the product, executing to the documented requirements of the project Execute the Implementation project through all stages Ingest and map the client‚Äôs source data to a standard data model (ETL and ELT process) Effectively translate complex customer requirements, recommend system solutions, and help formulate detailed specifications Leverage appropriate resources from company and customer, coordinating availability to maximize productivity Gain a deep understanding of how our products work, how they interact with each other, and how to build reliable and reusable processes for support. Act as a problem-solving expert, proactively identifying issues, analyzing root causes, and implementing the solution effectively Create documentation used for an on-going support Highly skilled in writing and optimizing complex SQL queries Knowledgeable in database design, administration, and performance tuning Experienced in handling large datasets and working with various data formats (JSON, XML, CSV, Parquet) Proficient in Python for automation and data processing tasks Familiar with shell scripting and version control systems (e.g., Git) Comfortable using Microsoft Office tools An effective communicator with experience working directly with clients Fluent in Polish (C1) and proficient in English (B2/C1) Strong written and verbal communication skills with the ability to present complex technical information in a clear and concise manner to a variety of audiences Ability to communicate with both technical and non-technical customers on a variety of issues Ability to select and prioritize tasks within a backlog English level B2 minimum - direct communication/documentation/tasks/other reading/speaking/writing Polish level C1 to communicate efficiently within the team Strong problem-solving abilities and detail orientation to diagnose issues, suggest solutions and make decisions based on requirements Experience with AzureDevOps and release pipelines Experience in software implementation or Enterprise SaaS solutions, preferably in retail Familiarity with retail industry and data or hand-on experience working in retail Experience with project management tools (preferably JIRA) Experience working with Snowflake CV review by HR Team Phone call with HR Team (15min) Soft-skills interview with HR Team (1h) Technical interview with Data Team (1h interview + 10min test) Feedback and decision!","[{""min"": 10000, ""max"": 15000, ""type"": ""Net per month - B2B""}, {""min"": 8000, ""max"": 12400, ""type"": ""Gross per month - Mandate""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,522,Cloud Data Engineer with snowflake,Experis Manpower Group,"Requirements: at least 3 years of experience in Big Data or Cloud projects in the areas of processing and visualization of large and/or unstructured datasets (including at least 1 year of hands-on Snowflake experience) understanding of Snowflake's pricing model and cost optimization strategies for managing resources efficiently experience in designing and implementing data transformation pipelines natively with Snowflake or Service Partners familiarity with Snowflake‚Äôs security model practical knowledge of at least one Public Cloud platform in Storage, Compute (+Serverless), Networking and DevOps area supported by commercial project work experience at least basic knowledge of SQL and one of programming languages: Python/Scala/Java/bash very good command of English Tasks: design, develop, and maintain Snowflake data pipelines to support various business functions collaborate with cross-functional teams to understand data requirements and implement scalable solutions optimize data models and schemas for performance and efficiency ensure data integrity, quality, and security throughout the data lifecycle implement monitoring and alerting systems to proactively identify and address issues plan and execute migration from on-prem data warehouses to Snowflake develop AI, ML and Generative AI solution stay updated on Snowflake best practices and emerging technologies to drive continuous improvement Our offer: Remote work Multisport card Private healthcare system Life insurance","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,523,Data Engineer (AWS),Upvanta sp. z o.o.,"Do naszego zespo≈Çu poszukujemy do≈õwiadczonego Data Engineera , kt√≥ry pomo≈ºe nam rozwijaƒá i utrzymywaƒá nowoczesne rozwiƒÖzania danych w chmurze. Je≈ºeli masz do≈õwiadczenie z AWS , Pythonem , Databricks i DBT ‚Äî zapraszamy do aplikacji! Minimum 5 lata do≈õwiadczenia na stanowisku Data Engineer Bardzo dobra znajomo≈õƒá AWS (S3, Lambda, Glue, Redshift lub inne us≈Çugi danych) Znajomo≈õƒá Pythona w kontek≈õcie przetwarzania danych Do≈õwiadczenie z Databricks oraz pracƒÖ z du≈ºymi zbiorami danych (Spark) Praktyczna znajomo≈õƒá DBT (Data Build Tool) Umiejƒôtno≈õƒá projektowania skalowalnych i efektywnych pipeline‚Äô√≥w danych Zrozumienie zasad modelowania danych (np. star schema, snowflake) Dobra znajomo≈õƒá SQL Do≈õwiadczenie z CI/CD w kontek≈õcie danych Znajomo≈õƒá narzƒôdzi orkiestracji (np. Airflow, Dagster) Znajomo≈õƒá narzƒôdzi monitorowania danych (np. Monte Carlo, Great Expectations) Projektowanie, budowa i utrzymanie pipeline‚Äô√≥w danych Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç w ≈õrodowisku chmurowym Wsp√≥≈Çpraca z Data Scientistami, Analitykami i innymi zespo≈Çami produktowymi Udzia≈Ç w projektowaniu i optymalizacji hurtowni danych Dokumentowanie i dbanie o jako≈õƒá danych Pracƒô w dynamicznym i do≈õwiadczonym zespole data Mo≈ºliwo≈õƒá pracy w pe≈Çni zdalnej Elastyczne godziny pracy Wyb√≥r formy zatrudnienia: UoP lub B2B Pakiet benefit√≥w (np. prywatna opieka medyczna, karta sportowa,)",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,524,Senior Data Engineer ‚Äì Graph & Analytics Systems,Holisticon Insight,"Holisticon Insight is a division of http: //nexergroup.com focused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! üëá https: //holisticon.pl/holisticon-insight/ We offer a B2B Contract: 165 ‚Äì 195 PLN net/hour + VAT We're seeking an experienced Senior Data Engineer to lead work on our client's analytics platform. You'll assist in evaluating modern database technologies and designing migration strategies that enable advanced relationship analysis and improve client's ability to track metrics across complex interconnected systems. Responsibilities Design data models to represent events, entity relationships, and patterns Develop ETL pipelines to transform data structures Optimize query performance for complex relationship traversals Integrate new database solutions with existing analytics workflows Create proof-of-concept demonstrations for key use cases Help assessing current architecture and designing database migration strategy Evaluate and recommend appropriate database technologies, with focus on graph databases (Neo4j, Amazon Neptune, ArangoDB, etc.) Key Qualifications 5+ years hands-on experience in similar role Strong understanding of data modeling and query optimization Experience migrating between different database paradigms (document, relational, graph) Proficiency with distributed data processing frameworks ( Apache Spark preferred) Experience working with document databases ( MongoDB) and JSON data structures Experience with large-scale analytics platforms Nice to have: Experience with graph databases and graph query languages (Cypher, Gremlin) Knowledge of event-driven data architectures and batch processing pipelines Familiarity with visualization platforms (Grafana or similar) Experience with both batch and stream processing patterns Ability to evaluate trade-offs between different database technologies Strong communication skills to explain complex concepts to stakeholders Experience with hybrid architectures (combining multiple database types) Understanding of metrics-driven analysis and KPI tracking By joining Holisticon Insight you will get: Life insurance Multisport card Fully remote job Private medical care Flexible working hours B2B or contract of employment Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories)","[{""min"": 27720, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,525,DWH Developer (public/health),Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 5-letnie do≈õwiadczenie zawodowe na stanowisku Projektanta IT lub Starszego Programisty SQL Znajomo≈õƒá zasad i metodologii projektowania hurtowni danych i data mart√≥w analitycznych Do≈õwiadczenie w pracy z systemami relacyjnych baz danych PostgreSQL lub EDB Do≈õwiadczenie w projektowaniu i programowaniu przy wykorzystaniu jƒôzyka PL/SQL Do≈õwiadczenie w tworzeniu modeli logicznych i fizycznych z wykorzystaniem narzƒôdzia Enterprise Architect Do≈õwiadczenie w projektowaniu i programowaniu proces√≥w ETL Znajomo≈õƒá standard√≥w wymiany danych HL7 Znajomo≈õƒá narzƒôdzi s≈Çu≈ºƒÖcych do orkiestracji proces√≥w ETL Znajomo≈õƒá Airflow Znajomo≈õƒá Jira Do≈õwiadczenie w programowaniu w jƒôzyku Python Dobra organizacja pracy w≈Çasnej, orientacja na realizacje cel√≥w Umiejƒôtno≈õci interpersonalne, w szczeg√≥lno≈õci umiejƒôtno≈õƒá planowania, definiowania, realizacji, oraz monitorowania i rozliczania cel√≥w Efektywna komunikacja, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista i proaktywno≈õƒá Zdolno≈õƒá adaptacji i elastyczno≈õƒá, otwarto≈õƒá na sta≈Çy rozw√≥j i gotowo≈õƒá uczenia siƒô Mile widziane Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Do≈õwiadczenie projektowe w obszarze Hurtownia Danych Znajomo≈õƒá s≈Çownik√≥w i rejestr√≥w z obszaru zdrowia np.: ICD9, ICD10, OID, PESEL Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá programowania w jƒôzyku pl/sql (np. PostgreSQL Certification lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy wiedzƒô z zakresu administrowania EDB Certyfikat z obszaru administrowania ≈õrodowiskiem Hadoop Kluczowe zadania Projektowanie hurtowni danych i data mart√≥w analitycznych Tworzenie modeli danych (logicznych i fizycznych) z u≈ºyciem Enterprise Architect Implementacja i optymalizacja proces√≥w ETL z wykorzystaniem narzƒôdzi orkiestracyjnych (np. Airflow) Programowanie w SQL, PL/SQL oraz Pythonie, g≈Ç√≥wnie w ≈õrodowisku PostgreSQL/EDB Praca ze standardami wymiany danych HL7 oraz narzƒôdziami takimi jak Jira i Enterprise Architect Wsp√≥≈Çpraca z zespo≈Çami projektowymi w zakresie analizy wymaga≈Ñ i dostarczania rozwiƒÖza≈Ñ danych","[{""min"": 110, ""max"": 139, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Junior,B2B,Remote,526,Junior Oracle SQL/APEX Developer,Pretius,"W Pretius poszukujemy Junior Oracle SQL/APEX Developera do projektu nowego systemu dla klienta z bran≈ºy us≈Çug doradczych i in≈ºynierskich. Lokalizacja: zdalnie lub Warszawa Wynagrodzenie: 40-60 pln netto/h O projekcie: System zarzƒÖdzania aktywami (asset management) w przestrzeni publicznej oparty o rozwiƒÖzania goespatial Modu≈Çy planowania, bud≈ºetowania inspekcji i inwentaryzacji Wykorzystywany w ponad 120 miastach Rozw√≥j i utrzymanie aktualnego systemu w celu ekspansji nowego rozwiƒÖzania Prace nad nowymi funkcjonalno≈õciami Stack: Oracle APEX, Oracle Cloud, Azure Oczekiwania: Podstawowa znajomo≈õƒá Oracle APEX Jƒôzyk angielski na poziomie B2/C1 Znajomo≈õƒá baz danych i SQL Co oferujemy w Pretius? Stawiamy na d≈Çugofalowe relacje oparte na uczciwych zasadach i rzetelno≈õci Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Mo≈ºliwo≈õƒá pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnƒôtrzne, konferencje, certyfikacje","[{""min"": 40, ""max"": 60, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,527,Starszy Programista Baz danych,DCG,"Jako firma rekrutacyjna jeste≈õmy ≈õwiadomi, ≈ºe ka≈ºdy solidny biznes napƒôdzajƒÖ ludzie z odpowiednio dopasowanymi kompetencjami. Nasz styl zarzƒÖdzania i partnerskie podej≈õcie pozwalajƒÖ nam na elastyczne dostosowanie siƒô do Twoich potrzeb i zapewniajƒÖ pe≈Çne wsparcie podczas wsp√≥≈Çpracy. W zwiƒÖzku z ciƒÖg≈Çym rozwojem i du≈ºƒÖ liczbƒÖ projekt√≥w rekrutacyjnych, jakie prowadzimy dla naszych Partner√≥w, szukamy osoby na stanowisko: Starszy Programista Baz danych Jest to kontynuacja prac w ju≈º dzia≈ÇajƒÖcym projekcie, kt√≥ry jest wdra≈ºany fazowo. W projekcie jest kilka zespo≈Ç√≥w developerski, kt√≥re odpowiadajƒÖ za budowƒô nowych i rozw√≥j istniejƒÖcych funkcjonalno≈õci oraz bugfixing dla zakresu ju≈º wdro≈ºonego. Zespo≈Çy liczƒÖ po kilku developer√≥w, kt√≥rzy mogƒÖ byƒá rotowani miƒôdzy zespo≈Çami. Minimum 5 lat do≈õwiadczenia w pracy z relacyjnymi bazami danych np. PostgreSQL, MySQL, MSSQL, ORACLE Minimum 4 lata do≈õwiadczenia w zakresie wykorzystania jednego z proceduralnych jƒôzyk√≥w programowania np. PL/SQL, PL/PqSQL Znajomo≈õƒá SQL Znajomo≈õƒá technologii bazy danych PostgreSQL Do≈õwiadczenia w strojeniu polece≈Ñ SQL i procedur PL/PgSQL Znajomo≈õƒá zasad zarzƒÖdzania, konfiguracji i optymalizacji bazy danych PostgresSQL Do≈õwiadczenie w migracji danych z system√≥w klasy enterprise Do≈õwiadczenia w analizie i transformacji danych na potrzeby migracji danych Udzia≈Ç w projektach realizowanych w obszarze ochrony zdrowia dla czƒô≈õci bia≈Çej lub zwiƒÖzanej z rejestrami Projektowanie i eksploatacja baz danych systemu Optymalizacja obecnie eksploatowanych baz danych systemu Opiniowanie i przygotowanie rozwiƒÖza≈Ñ projektowych w obszarach bazodanowych Bie≈ºƒÖca wsp√≥≈Çpraca z zespo≈Çem wytw√≥rczym m.in ., analitykami i architektami w tym udzia≈Ç w spotkaniach zespo≈Ç√≥w projektowych wynikajƒÖcych z metodyki prowadzenia projektu Bie≈ºƒÖca wsp√≥≈Çpraca z interesariuszami Projektu, w tym w≈Ça≈õcicielem biznesowym Oferujemy: Pracƒô przy du≈ºych projektach infrastrukturalnych Stabilne zatrudnienie i mo≈ºliwo≈õƒá rozwoju zawodowego Wsp√≥≈Çpracƒô z do≈õwiadczonym zespo≈Çem projektowym",[],Database Administration,Database Administration
Full-time,Senior,Any,Remote,528,GCP Data Engineer,Lingaro,"Growth through diversity, equity, and inclusion. As an ethical business, we do what is right ‚Äî including ensuring equal opportunities and fostering a safe, respectful workplace for each of us. We believe diversity fuels both personal and business growth. We're committed to building an inclusive community where all our people thrive regardless of their backgrounds, identities, or other personal characteristics. Tasks: You will be a part of the team accountable for design, model and development of whole GCP data ecosystem for one of our Client‚Äôs (Cloud Storage, Cloud Functions, BigQuery). Involvement throughout the whole process starting with the gathering, analyzing, modelling, and documenting business/technical requirements will be needed. The role will include direct contact with clients. Modelling the data from various sources and technologies. Troubleshooting and supporting the most complex and high impact problems, to deliver new features and functionalities. Designing and optimizing data storage architectures, including data lakes, data warehouses, or distributed file systems. Implementing techniques like partitioning, compression, or indexing to optimize data storage and retrieval. Identifying and resolving bottlenecks, tuning queries, and implementing caching strategies to enhance data retrieval speed and overall system efficiency. Identifying and resolving issues related to data processing, storage, or infrastructure. Monitoring system performance, identifying anomalies, and conducting root cause analysis to ensure smooth and uninterrupted data operations. Train and mentor less experienced data engineers, providing guidance and knowledge transfer. Requirements: Must have: At least 4 years of experience as a Data Engineer working with GCP cloud-based infrastructure & systems. Deep knowledge of Google Cloud Platform and cloud computing services. Extensive experience in design, build, and deploy data pipelines in the cloud, to ingest data from various sources like databases, APIs or streaming platforms. Proficient in database management systems such as SQL (Big Query is a must), NoSQL. Candidate should be able to design, configure, and manage databases to ensure optimal performance and reliability. Programming skills (SQL, Python, other scripting). Proficient in data modeling techniques and database optimization. Knowledge of query optimization, indexing, and performance tuning is necessary for efficient data retrieval and processing. Knowledge of at least one orchestration and scheduling tool (Airflow is a must). Experience with data integration tools and techniques, such as ETL and ELT Candidate should be able to integrate data from multiple sources and transform it into a format that is suitable for analysis. Knowledge of modern data transformation tools (such as DBT, Dataform). Excellent communication skills to effectively collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders. Ability to convey technical concepts to non-technical stakeholders in a clear and concise manner. Advanced English level. Ability to actively participate/lead discussions with clients to identify and assess concrete and ambitious avenues for improvement. Tools knowledge: Git, Jira, Confluence, etc. Open to learn new technologies and solutions. Experience in multinational environment and distributed teams. Good to have: Certifications in big data technologies or/and cloud platforms. Experience with BI solutions (e.g. Looker, Power BI, Tableau). Experience with ETL tools: e.g. Talend, Alteryx Experience with Apache Spark, especially in GCP environment. Experience with Databricks. Experience with Azure cloud-based infrastructure & systems. We offer: Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,529,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for a Data Engineer to join us to provide value to our customers in line with the Volue mission. In Volue Insight we enable our customers to make data driven decisions ‚Äì spanning from creating pricing models for their energy products, when to buy energy, to invest in renewable power plants or power-consuming industry. In the fuels team, we provide actuals and forecasts for prices, production levels, flows for the gas markets and conventional power plant operators. We build and maintain data pipelines, collect and process data from external sources, craft mathematical models, analyze time series, train machine learning models, build web applications, and enjoy working together. What you will be doing to make a difference: Design, build and maintain flexible and scalable end-to-end data pipelines for forecasting and prediction models. Contribute to our continuous push towards highly scalable and automated data pipelines and forecasting models where performance is monitored, quality is continuously evaluated, and experimentation is easy. Take part in the entire lifecycle of our models, from initial concept to deployment and ongoing maintenance, ensuring reliability and performance. Implement automated tests, participate in peer code reviews, and embrace continuous integration practices to ensure robust, maintainable code. What you need to succeed: A Bachelor‚Äôs or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Reasons to join Volue team and what we offer: Large degree of influence in shaping and developing the role further Great colleagues in one of Europe‚Äôs most exciting green tech companies with innovative and international work environment Flexible working hours and competitive compensation package, which includes a Multisport card, group life insurance, private healthcare, English classes, memorable offsite events, outstanding referral programme and access to various sports groups. In Volue, we cherish each employee‚Äôs competence, ideas and personality. Let your skills and talent be a part of our team ‚Äì and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,530,Senior Data Analyst - Market Data,Bayer Sp. z o.o.,"Senior Data Analyst ‚Äì Market Data Are you ready to make a significant impact in the world of data analytics and data management? We are seeking a talented Data Analyst to become a vital part of our dynamic Data Assets, Analytics, and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in building and maintaining our core data assets across various domains, ensuring their completeness, semantics and quality. You will work closely with data owners, product managers, data engineers, data architects, data stewards, data governors and data scientist to enable our data analytics solutions, enhance the strategic value of our data assets and enable cutting-edge AI solutions and support data-driven decision-making. If you‚Äôre passionate about transforming data into actionable insights and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Collaborate with data owners, architects, engineers, stewards, governors, scientists and product managers to understand objectives and requirements for data assets. Serve as a liaison between technical teams and business stakeholders to ensure data assets meet both business requirements and technical standards. Take ownership of a data asset roadmap. Co-develop data strategy and governance frameworks. Proactively identify new datasets from across the organization and collaborate with data architects and data engineers to integrate them into the core data assets Define transformation logic and enrich data to create valuable KPIs and features in the consumption layer. Develop and maintain clear semantics and metadata for data assets, ensuring that all data is well-documented and easily understandable. Ensure data quality, availability, and completeness through implementation of quality checks, validation processes, and continuous monitoring in collaboration with data architects and data engineers. Serve as the primary point of contact for users of data assets, providing guidance and support to help them understand and utilize the data effectively. Analyze complex datasets to extract actionable insights that streamline the development of analytics and AI solutions. Become a go-to expert for market data. Qualifications & Competencies: Master's degree in Statistics, Computer Science, Data Management, Data Science or a related field. 5+ years of experience as a Data Analyst or Data Steward, preferably within the consumer-packaged goods, FMCG, pharmaceutical or healthcare industry. Strong knowledge of data management principles, data quality frameworks, and metadata management practices and tools. Understanding of data lineage and data cataloging concepts. Business acumen in the area of market and competition analysis. Experience with data manipulation and analysis using Azure Databricks, SQL and Python. Familiarity with relational databases (PostgreSQL, MSSQL) and data modelling. Excellent analytical and problem-solving skills with a keen attention to detail. Strong understanding about data compliance & security standards such as data privacy regulations (GPDR, HIPAA), EU AI Act, management of confidential data, handling of licensed data, and experience with measures to mitigate data risks. Strong communication skills, with the ability to present complex data in a clear and understandable manner. Interest and experience with AI tools supporting data analysis and stewardship is a plus. Experience with preparing data for AI solutions (e.g. traditional machine learning models, AI Agents) is a plus. Experience in IT product management is a plus. Ability to work collaboratively in a team-oriented environment. Fluent in English, both written and spoken. What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (""Wczasy pod gruszƒÖ"") Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn't mean you aren't the right fit for the role. Apply with confidence, we value potential over perfection. WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,531,Head of Technology (Co-founder),Relout,"Relout jest firmƒÖ konsultingowƒÖ, zajmujƒÖcƒÖ siƒô wspieraniem klient√≥w z sektora IT w dynamicznym rozwoju poprzez dostarczanie top talent√≥w i tworzenie zespo≈Ç√≥w developerskich ‚ú® Pomagamy start-upom, software house‚Äôom, firmom marketingowym i przedsiƒôbiorstwom produktowym w skalowaniu i transformacji poprzez nawiƒÖzywanie wsp√≥≈Çpracy z ambitnymi in≈ºynierami posiadajƒÖcymi prawdziwƒÖ pasjƒô do technologii. NaszƒÖ misjƒÖ jest kreowanie warto≈õci poprzez ≈ÇƒÖczenie firm z w≈Ça≈õciwymi profesjonalistami, wspierajƒÖc ich w realizacji cel√≥w i rozwoju. Obecnie dla naszego klienta szukamy do≈õwiadczonej osoby na stanowisko Head of Technology / Co-foundera , kt√≥ra do≈ÇƒÖczy do zespo≈Çu i we≈∫mie aktywny udzia≈Ç w budowaniu niezale≈ºnej sp√≥≈Çki dzia≈ÇajƒÖcej w obszarze konsultingu i zarzƒÖdzania danymi.Firma zajmuje siƒô dzia≈Çalno≈õciƒÖ konsultingowƒÖ w zakresie rozwiƒÖza≈Ñ z szeroko pojƒôtego obszaru zarzƒÖdzania danymi w przedsiƒôbiorstwach . Aktulnie przechodzi istotnƒÖ transformacjƒô organizacyjnƒÖ. Do ko≈Ñca bie≈ºƒÖcego roku dzia≈Çalno≈õƒá firmy w Polsce zostanie wydzielona jako niezale≈ºna sp√≥≈Çka ‚Äì z w≈ÇasnƒÖ strategiƒÖ, to≈ºsamo≈õciƒÖ i kierunkiem rozwoju na rynku polskim. Szukamy partnera ‚Äì lidera technologicznego , kt√≥ry nie tylko wniesie eksperckƒÖ wiedzƒô z zakresu data engineeringu i nowoczesnych technologii, ale bƒôdzie r√≥wnie≈º zaanga≈ºowany operacyjnie (hands-on) i gotowy do prowadzenia oraz wspierania projekt√≥w. Na tym stanowisku bƒôdziesz budowa≈Ç strategiƒô i rozwiƒÖzania techniczne dla klient√≥w, ale tak≈ºe przewodzi≈Ç zespo≈Çowi technicznemu, kierowa≈Ç projektami i odpowiada≈Ç za jako≈õƒá i terminowo≈õƒá ich dostarczenia. üî∑ ObowiƒÖzki Przeprowadzanie audyt√≥w technicznych dla nowych klient√≥w. Projektowanie architektury procesowania danych, analityki i automatyzacji proces√≥w. Wyznaczanie plan√≥w transformacji cyfrowej. Nadz√≥r nad realizacjƒÖ prac wdro≈ºeniowych w projektach, okre≈õlanie potrzeb personalnych i kompetencji cz≈Çonk√≥w zespo≈Çu projektowego oraz zawiƒÖzywanie zespo≈Çu projektowego. ZarzƒÖdzanie jako≈õciƒÖ realizacji i rentowno≈õciƒÖ projekt√≥w. ZarzƒÖdzanie pracƒÖ rozproszonego zespo≈Çu konsultant√≥w, przydzia≈Ç do projekt√≥w os√≥b o w≈Ça≈õciwych kompetencjach, planowanie pracy, okre≈õlanie cel√≥w, weryfikacja rezultat√≥w. Rozw√≥j polskiego zespo≈Çu technicznego i wsparcie mentorskie. Wsp√≥≈Çpraca ze stakeholderami technicznymi i biznesowymi (ze strony klienta) w zakresie jako≈õci i organizacji danych. Pe≈Çnienie roli konsultanta w kluczowych projektach. üéØ Wymagania ostatnie 5 + lat do≈õwiadczenia ""hands-on"" w obszarze zarzƒÖdzania danymi - Data Engineering, BI, analityka, przetwarzanie danych oraz udokumentowane do≈õwiadczenie w zarzƒÖdzaniu zespo≈Çem technicznym. Do≈õwiadczenie w prowadzeniu projektach analitycznych end-to-end - od zbierania, przechowywania, czyszczenia, procesowania, modelowania a≈º po wizualizacjƒô i zaawansowanƒÖ analitykƒô. Bardzo dobra znajomo≈õƒá ≈õrodowiska Azure , w tym jako architekt rozwiƒÖza≈Ñ. Bardzo dobra znajomo≈õƒá Databricks, Python, Spark i SQL. Do≈õwiadczenie w zakresie IaC (Infrastructure as Code). Du≈ºe wyczucie biznesowe i umiejƒôtno≈õƒá wsp√≥≈Çpracy z lud≈∫mi o r√≥≈ºnym typie osobowo≈õci, poziomie wiedzy i do≈õwiadczeniu. Bardzo dobre zdolno≈õci planowania, delegowania, monitorowania efekt√≥w pracy i dowo≈ºenia wysokiej jako≈õci wynik√≥w. Wysoce rozwiniƒôte umiejƒôtno≈õci komunikacyjne, umiejƒôtno≈õƒá dawania i przyjmowania feedbacku. Elastyczno≈õƒá, konsekwencja i wytrwa≈Ço≈õƒá wspierane postawƒÖ ‚Äúwe can do it‚Äù. Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego i jƒôzyka polskiego . Mile widziana znajomo≈õƒá: Snowflake, dbt, AWS, Fabric, PowerBI. ‚≠ê Oferujemy atrakcyjne sta≈Çe wynagrodzenie po≈ÇƒÖczone z pakietem udzia≈Ç√≥w - zaproponuj jaki model i stawka Ciƒô interesuje - jeste≈õmy otwarci na rozmowy pracƒô w polskiej sp√≥≈Çce z du≈ºymi perspektywami na rozw√≥j, kt√≥rej mo≈ºesz byƒá wsp√≥≈Çw≈Ça≈õcicielem du≈ºƒÖ pulƒô klient√≥w sp√≥≈Çki Pracƒô zdalnƒÖ - z okresowymi spotkaniami wsp√≥lnik√≥w elastyczno≈õƒá godzin pracy - w zamian wymagamy 100% zaanga≈ºowania Pracƒô z niewielkim, zgranym i eksperckim zespo≈Çem Pracƒô w lu≈∫nym i dynamicznym start-upowym ≈õrodowisku Nie czekaj! Aplikuj ju≈º teraz : )",[],Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,532,Data Engineer (Databricks),ERGO Technology & Services,"About Us ERGO Technology & Services S.A. (ET&S S.A.) was established in January 2021 following the integration of ERGO Digital IT and Atena into one entity, leveraging both companies‚Äô strengths and best practices. As a part of ERGO Technology & Services Management AG, the technology holding of ERGO Group AG, we support millions of internal and external customers with state-of-the-art IT solutions to everyday problems. In October 2022, ET&S S.A. expanded its scope of operations by creating a Business Services unit to contribute in a new way to the growth of ERGO‚Äôs business. Acting as a co-partner and internal consultant, it adds non-IT value and supports the development of the entire ERGO Group, currently offering skills in reporting, analysis, actuarial, and input management. We are committed to fostering innovation and meeting the evolving needs of our clients worldwide. Discover how we implement AI, IoT, Voice Recognition, Big Data science, advanced mobile solutions, and business-related services to anticipate and address our customers‚Äô future needs. About the role We work in a DevOps approach culture: continuous everything, infrastructure as a code, you write it so you own it. We are in the Azure Cloud with modern architecture using AKS and different server-less apps. How you will get the job done acting as a data ninja: dive deep, flip, slip, and transform data discovering clients‚Äô requirements working with application developers, DevOps, business, and data scientists in an agile setup taking excellent care of the code you write and being open to providing ideas on how we can improve our work working in an international Scrum team Skills and experience you will need fluency in English and Polish good knowledge of cloud solutions, preferably Azure Databricks strong SQL skills knowledge of ETL processes/programming communicative and open-minded person who is not afraid of challenges great understanding of Python & PySpark, and commercial use of it Perks & Benefits Let's be healthy Medical package, sports card, and numerous sports sections ‚Äì these are some of the beneÔ¨Åts that help our employees stay in good shape. Let's be balanced Work-life balance is a key aspect of a healthy workplace. We offer our employees flexible working hours, a confidential employee assistant program, as well as the possibility of remote working. However, staying at home with our in-office gaming room and dog-friendly office in Warsaw won‚Äôt be easy. Let's be smart We organize numerous workshops and training courses. Thanks to hackathons and meetups, our specialists share their expertise with others. Additionally, we have a wide range of digital learning platforms and language courses. Let's be responsible Each year, we participate in several CSR activities, during which, together with our colleagues, we do our best to create a better future. Let's be fun Company-wide bike races and soccer matches, Ô¨Ålm marathons in our cinema room or other engaging team-building activities ‚Äì we got it covered! Let's be diverse Every team member is valued, regardless of gender, nationality, religious beliefs, disability, age, and sexual orientation or identity. Your qualiÔ¨Åcations, experience, and mindset are our greatest beneÔ¨Åt!",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,533,Programista SQL,B2Bnetwork,"Wymagania: Do≈õwiadczenie zawodowe na stanowisku Projektanta IT lub Starszego Programisty SQL, minimum 5 lat Znajomo≈õƒá zasad i metodologii projektowania hurtowni danych i data mart√≥w analitycznych Do≈õwiadczenie w pracy z systemami relacyjnych baz danych PostgreSQL lub EDB Do≈õwiadczenie w projektowaniu i programowaniu przy wykorzystaniu jƒôzyka pl/sql Do≈õwiadczenie w tworzeniu modeli logicznych i fizycznych z wykorzystaniem narzƒôdzia Enterprise Architect Do≈õwiadczenie w projektowaniu i programowaniu proces√≥w ETL Znajomo≈õƒá standard√≥w wymiany danych HL7 Znajomo≈õƒá narzƒôdzi s≈Çu≈ºƒÖcych do orkiestracji proces√≥w ETL Znajomo≈õƒá Airflow Znajomo≈õƒá Jira Znajomo≈õƒá Enterprise Architect Do≈õwiadczenie w programowaniu w jƒôzyku SQL Do≈õwiadczenie w programowaniu w jƒôzyku Python ObowiƒÖzki: Projektowanie i rozw√≥j hurtowni danych oraz data mart√≥w analitycznych, zgodnie z obowiƒÖzujƒÖcymi zasadami i metodologiami. Tworzenie modeli danych (logicznych i fizycznych) przy u≈ºyciu narzƒôdzia Enterprise Architect, zapewniajƒÖc ich sp√≥jno≈õƒá i optymalizacjƒô pod kƒÖtem wydajno≈õci. Projektowanie i implementacja proces√≥w ETL, w tym orkiestracja zada≈Ñ za pomocƒÖ narzƒôdzi takich jak Apache Airflow. Programowanie w jƒôzykach SQL oraz PL/SQL, w szczeg√≥lno≈õci w ≈õrodowiskach PostgreSQL lub EDB, z dba≈Ço≈õciƒÖ o optymalizacjƒô zapyta≈Ñ i stabilno≈õƒá dzia≈Çania baz danych. Wykorzystywanie jƒôzyka Python do wsparcia proces√≥w analitycznych, automatyzacji i integracji danych. Praca z narzƒôdziami wspierajƒÖcymi zarzƒÖdzanie projektami i zespo≈Çem, takimi jak Jira oraz Enterprise Architect. Analiza i integracja danych w standardzie wymiany HL7, w szczeg√≥lno≈õci w kontek≈õcie projekt√≥w z obszaru ochrony zdrowia.",[],Database Administration,Database Administration
Full-time,Senior,B2B,Hybrid,534,Senior Data Analyst,Jit Team,"Salary: 900 - 1100 PLN/day net+vat on B2B Work model: hybrid from Gdynia / Gda≈Ñsk / Warszawa / ≈Å√≥d≈∫ (2 days per week from the office) Why choose this offer? You can expect a flexible work organization The international work environment will give you the opportunity to interact with the English language on a daily basis Scandinavian organizational culture will provide you with work-life balance, you will gain time for additional training (financed by Jit) The Jit community will bring you a nice time during regular integration meetings Project We are looking for a data-driven professional to join a strategic KYC transformation initiative focused on improving data quality, governance, and regulatory compliance across the organization. This is a cross-functional role collaborating with business, IT, and architecture teams to implement a robust data model and reporting capabilities supporting a new Group-wide KYC system. Responsibilities you'll have Analyse existing KYC data to identify gaps, inconsistencies, and areas for improvement Work collaboratively with business and IT partners across business units to develop and implement data standards to ensure data accuracy & completeness Be a part of major projects for delivery of Group KYC Tool Partner with Data Architects and business SMEs to create and maintain a Business Information Model (BIM) for each data domain that is aligned across the enterprise Monitor data quality metrics and report on findings to management Investigate and resolve data quality issues, escalating complex problems as needed Provide consultative services to agile delivery teams and ensure data governance best practices Support the implementation of new KYC systems and processes, ensuring data migration and integration are seamless and accurate Utilize enterprise approved tools to develop reproducible data-flows that ultimately fulfill reporting requirements Implement testing and monitor quality of developed data-flows and reporting outputs, performing upgrades as identified and maintenance as required Work with multiple customers to analyze and visualize complex data in simple and accurate ways Document data definitions, data lineage, and data quality rules Participate in data governance/stewardship communities of practice Contribute to the development and maintenance of data governance policies and procedures Stay up-to-date with industry best practices and regulatory requirements related to KYC data management Proactively identify opportunities to improve data quality and efficiency Expected competences and knowledge Proven experience in data management, data quality, or a related role, preferably within the financial services industry Experience with advanced SQL queries and familiarity with another programming language, e.g. Python, R, Java and Scala Familiar with data taxonomy and capable to build small data models Experience with Power BI to produce reports Excellent stakeholder management is mandatory as you will be working closely with multiple business areas and key business leads across IT, Architecture, Financial Crime, Compliance, Legal - and the Business in order to implement changes Experience within financial crime (AML/CTF, KYC, Transaction Monitoring and Sanctions), working agile and experience with SAFe is an advantage Professional, organised, and structured in approach and work produced to a high quality of standard, and you always strive to find the solution that is best for the bank Excellent analytical and problem-solving skills, strong attention to detail and a commitment to data accuracy Ability to communicate effectively with both technical and non-technical audiences Experience with data governance frameworks and methodologies is a plus Experience with data migration and integration projects is a plus Ability to work independently and as part of a team Fluency in English is a requirement (speaking and writing) Technologies you'll work with SQL Python Power BI Client ‚Äì why choose this particular client from the Jit portfolio? Jit Team has had an over-decade-long relationship with the leading financial group in the Nordic countries, and we are privileged to be our client's premier partner in Poland. At present, over 200 Jit personnel are engaged in the completion of more than 60 projects for this Norwegian major provider of financial services with a global presence and a strong focus on modern technology. Our customer's work atmosphere is epitomized by the Scandinavian culture , which is conducive to people who place emphasis on work-life balance and feedback culture . Furthermore, all projects are executed in international teams, giving constant exposure to the English language. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 18900, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,535,Data Engineer,Link Group,"About the Role We're looking for a Data Engineer to join a growing team working on modern data platforms. You will play a key role in designing, developing, and maintaining scalable data pipelines and infrastructure to support data analytics and reporting initiatives. This is a great opportunity to work with cutting-edge cloud and big data technologies. Design, build, and maintain scalable ETL/ELT pipelines Work with structured and unstructured data from diverse sources Optimize data workflows for performance, reliability, and cost Implement data quality checks and monitoring Collaborate with analysts, architects, and other engineers to support data needs Build data integrations with internal and third-party APIs Support cloud data infrastructure and automation 3+ years of experience as a Data Engineer or similar role Strong knowledge of SQL and data modeling principles Experience with Python or Scala for data processing Hands-on experience with cloud platforms (ideally AWS , but Azure/GCP also valuable) Familiarity with tools like Apache Spark , Airflow , Kafka , or similar Experience with data lakes , data warehouses , or lakehouse architectures Git and CI/CD workflows Strong problem-solving skills and ability to work independently Experience with Snowflake , Redshift , or BigQuery Familiarity with dbt , Terraform , or other IAC tools Background in data governance or security Experience with real-time data processing (e.g., Flink, Kinesis) Exposure to ML pipelines or MLOps","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Office,536,"Strategic Cloud Data Engineer, Global Service Delivery",Google,"Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Wroc≈Çaw, Poland; Warsaw, Poland . Bachelor's degree in Science, Technology, Engineering, Mathematics, or equivalent practical experience. Experience writing code in one or more programming languages (e.g., Python, Java). Experience in solution engineering and in stakeholder management, professional services, or technical consulting. Experience with Oracle, MySQL and MongoDB. Master‚Äôs degree in Engineering, Computer Science, Business, or a related field. Experience in an analytical role such as business intelligence, data analytics, or statistics. Experience working with database technologies (e.g., SQL, NoSQL). Experience with cloud technologies such as architecting, developing, or maintaining cloud solutions in virtualized environments or cloud data engineering. Knowledge and experience in Google Cloud Native Databases like Cloud Spanner, Cloud BigTable and AlloyDB. Proficiency in database design principles, data modeling, and the ability to translate business requirements into efficient database structures. The Google Cloud Platform team helps customers transform and build what's next for their business ‚Äî all with technology built in the cloud. Our products are developed for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. Our teams are dedicated to helping our customers ‚Äî developers, small and large businesses, educational institutions and government agencies ‚Äî see the benefits of our technology come to life. As part of an entrepreneurial team in this rapidly growing business, you will play a key role in understanding the needs of our customers and help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. As a Strategic Cloud Data Engineer, you'll work with customers and partner teams to design,develop,deploy and manage highly scalable and reliable databases on the Google Cloud Platform meeting the Organization‚Äôs storage demands. You will work on data migrations and modernization projects incorporating best practices of data governance and security controls. You will travel to customer sites to deploy solutions and deliver workshops to educate and empower customer teams to adhere to best practices in Database design and operations. Additionally, you'll work closely with Product Management and Product Engineering teams to build and constantly drive excellence in our products that solve the biggest customer challenges. Work with clients to understand their objectives and challenges, identify technical gaps, and surface opportunities for solution reuse or innovations. Design and implement solutions that meet client needs and are compliant with data and legal policies. Understand the nuances of clients within the industry and develop subject matter expertise in trending spaces. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,537,"Senior Data Engineer ‚Äì Python, Spark, AWS",Link Group,"Senior Data Engineer ‚Äì Python, Spark, AWS We‚Äôre looking for a Senior Data Engineer to join a highly skilled team responsible for building a scalable, multitenant data platform that powers data ingestion, transformation, cataloging, and distribution for internal and external stakeholders. This role is central to designing robust data pipelines, integrating modern data lakehouse components, and supporting the company‚Äôs AWS-based infrastructure. üîß What You‚Äôll Do: Design and build scalable data ingestion pipelines in AWS Work extensively with Databricks, including Unity Catalog, streaming, Delta Lake, and data transformation tools Lead technical delivery and implementation of core platform components across raw, normalized, operational, and historical layers Collaborate with engineering managers and cross-functional teams across the US and Poland Drive technical excellence by improving team processes, architecture standards, and engineering best practices Support and consult with stakeholders to ensure successful delivery of data-driven solutions Contribute to the growth and maturity of the team‚Äôs cloud and data engineering capabilities ‚úÖ What We‚Äôre Looking For: 5+ years of experience in Python and strong software engineering skills Solid knowledge of AWS cloud services and best practices Experience building scalable Spark pipelines in PySpark or Scala Practical experience with Spark Streaming for low-latency pipelines Familiarity with Delta Lake and modern data lakehouse architectures Hands-on experience with Kubernetes and container orchestration Experience with MongoDB and other NoSQL data stores Understanding of microservices and event-driven architecture Excellent communication skills and ability to work in a collaborative, global team Commitment to high standards of ethics, quality, and delivery üåü Nice to Have: Familiarity with Machine Learning, AI models, or Data Science workflows","[{""min"": 35000, ""max"": 40000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,538,Product Owner ‚Äì Data & Analytics,ITDS,"Product Owner Join us, and lead innovation through data-driven decision making! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As a Product Owner , you will be working for our client, a global financial institution focused on transforming its data and analytics landscape. You will be leading the development of data-driven products that support critical business decisions across credit, lending, and operational processes. This project aims to optimize investigation outcomes by leveraging advanced analytics and technology solutions, driving measurable value through effective stakeholder collaboration and Agile product development. You will be working in a complex, global environment that requires a strong strategic mindset and hands-on delivery expertise. Your main responsibilities: Defining and evolving the product vision in alignment with business goals Collaborating with stakeholders to prioritize features based on value and business impact Leading Agile ceremonies and maintaining a clear and refined product backlog Supporting the team in decomposing epics into actionable user stories Communicating with cross-functional teams to ensure product alignment and clarity Resolving conflicting stakeholder priorities and negotiating trade-offs Showcasing product iterations and improvements to business stakeholders Promoting and advocating for the product across the broader organization Shaping and validating business outcomes through data analytics and metrics Ensuring consistent delivery of business value through iterative releases You're ideal for this role if you have: Proven experience as a Product Owner in a large , complex organization Strong leadership and communication skills across all seniority levels Expertise in Agile frameworks and tools such as JIRA and Confluence Experience managing product strategy, roadmaps, and value delivery Strong stakeholder management and conflict resolution capabilities Demonstrated ability to deliver business and technology change Understanding of data management and analytics within a corporate context Experience working with global, cross-functional teams Familiarity with credit and lending processes in the financial industry Ability to translate business outcomes into technical requirements It is a strong plus if you have: Hands-on experience with SQL , Big Query , or other data tools Background in data science, analytics, or business analysis Experience with big data technologies or cloud-based platforms Proficiency in Python or PySpark for data processing Knowledge of data visualization tools like Tableau or QlikSense","[{""min"": 17850, ""max"": 24150, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Remote,539,Data Analyst (Finance),Revolut,"About Revolut People deserve more from their money. More visibility, more control, and more freedom. Since 2015, Revolut has been on a mission to deliver just that. Our powerhouse of products ‚Äî including spending, saving, investing, exchanging, travelling, and more ‚Äî help our 55+ million customers get more from their money every day. As we continue our lightning-fast growth,‚Äå 2 things are essential to our success: our people and our culture. In recognition of our outstanding employee experience, we've been certified as a Great Place to Work‚Ñ¢. So far, we have 10,000+ people working around the world, from our offices and remotely, to help us achieve our mission. And we're looking for more brilliant people. People who love building great products, redefining success, and turning the complexity of a chaotic world into the simplicity of a beautiful solution. About the role We approach Data Science at Revolut the same way that we approach everything else. We take complex problems, and create extraordinary solutions that our customers love. Our Data Analysts aren‚Äôt kept in the background, doomed to never see the impact of their work. They‚Äôre some of our best and brightest problem solvers, deployed to the front-lines to work in Product Teams and deliver rock star solutions. We‚Äôre looking for a superstar Data Analysts who don't believe there's a data task that can be too hard. From digging into our complex databases, looking for the root cause of a problem to designing their own solutions and writing their own code to implement them. In this process, they never stop learning, picking up new skills, and delivering value. Up for the challenge? Get in touch. What you‚Äôll be doing Supporting the Finance team in controlling market products, including market making and treasury investments Analysing and optimising financial control and treasury management activities Understanding business processes through financial data and ensuring accuracy from booking to accounting Developing Price Verification Models to ensure accurate valuation of financial products Automating financial controls to improve efficiency and compliance Designing and implementing reconciliation logic for balance accuracy Creating and maintaining dashboards to track key financial metrics Collaborating with Market Making, Treasury, and Financial Control teams to enhance reporting and decision-making What you'll need 5+ years of experience working with Data Analysis and creating impactful solutions Ability to manage a team and stakeholders Excellent background / education in a quantitative discipline Great skills with Python, SQL, or other programming languages Evidence of strong mathematical and statistics knowledge Nice to have Advanced degree in a core STEM subject Previous experience working on a Finance department Ôªø Building a global financial super app isn‚Äôt enough. Our Revoluters are a priority, and that‚Äôs why in 2021 we launched our inaugural D&I Framework, designed to help us thrive and grow everyday. We're not just doing this because it's the right thing to do. We‚Äôre doing it because we know that seeking out diverse talent and creating an inclusive workplace is the way to create exceptional, innovative products and services for our customers. That‚Äôs why we encourage applications from people with diverse backgrounds and experiences to join this multicultural, hard-working team.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,540,LIMS Expert,Sii,"Minimum of 4 years of relevant experience in a regulated environment Hands-on laboratory experience and familiarity with analytical techniques (e.g., HPLC) Knowledge of GxP requirements and global regulatory standards Proficiency with LIMS (LabVantage experience preferred), eQMS, and document management systems Excellent communication and customer service skills, with the ability to translate technical information for diverse audiences Fluency in English (spoken and written) We are looking for a skilled, detail-oriented, and collaborative LIMS Expert to join our team in the medical and pharmaceutical industry, supporting global users of the Laboratory Information Management System. In this role, you will be part of a dynamic group committed to delivering both rapid and long-term solutions to enhance the laboratory user experience and ensure alignment with strategic goals. The ideal candidate will bring strong technical expertise, customer service skills, and a proactive attitude to a highly regulated environment. Support creation and updates of LIMS Master Data (e.g., specifications, sampling plans, LES worksheets) Troubleshoot and resolve user support tickets within the LIMS environment Communicate with QC departments to collect and verify data for LIMS integration Create and manage compliance documentation, including routing for approval Participate in training users and supporting routine QC operations Recommend process improvements and contribute to best practice development Rekrutacja online Jƒôzyk rekrutacji: polski&angielski Start ASAP Praca w pe≈Çni zdalna Darmowa kawa Darmowe ≈õniadanie Bez wymaganego dress code'u Nowoczesne biuro Szkolenia wewnƒôtrzne Pakiet sportowy Miƒôdzynarodowe projekty Prywatna opieka medyczna Bud≈ºet na szkolenia Ma≈Çe zespo≈Çy Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title ‚Äì get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers ‚Äì Power People. Learn more at sii.pl .","[{""min"": 22000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Hybrid,541,Data Analyst,Capco Poland,"CAPCO POLAND *We are looking for Poland based candidates* Joining Capco means joining an organisation that is committed to an inclusive working environment where you‚Äôre encouraged to #BeYourselfAtWork. We celebrate individuality and recognize that diversity and inclusion, in all forms, is critical to success. It‚Äôs important to us that we recruit and develop as diverse a range of talent as we can and we believe that everyone brings something different to the table ‚Äì so we‚Äôd love to know what makes you different. Such differences may mean we need to make changes to our process to allow you the best possible platform to succeed, and we are happy to cater to any reasonable adjustments you may require. You will find the section to let us know of these at the bottom of your application form or you can mention it directly to your recruiter at any stage and they will be happy to help. Capco Poland is a leading global technology and management consultancy, dedicated to driving digital transformation across the financial services industry. Our passion lies in helping our clients navigate the complexities of the financial world, and our expertise spans banking and payments, capital markets, wealth, and asset management. We pride ourselves on maintaining a nimble, agile, and entrepreneurial culture, and we are committed to growing our business by hiring top talent. We also are: Experts in banking and payments, capital markets, wealth and asset management Focused on maintaining our nimble, agile, and entrepreneurial culture Committed to growing our business and hiring the best talent to help us get there ROLE OVERVIEW: We are looking for an experienced Data Analyst to join the Data Management and Analytics team within the Data Analytics Office (DAO). This role is integral to supporting the GRADE (Global Risk Analytics Data Environment) platform, which underpins Wholesale Credit Risk (WCR) model development, validation, and monitoring. You will work with cutting-edge data systems to deliver high-quality analytics, data governance, and reporting solutions, supporting global risk and compliance processes. KEY RESPONSIBILITIES: Data Analysis - collate, test and check independently sourced data and assess its robustness and fitness for purpose. Understanding data requirements from model development & monitoring teams, working on DQ issues, liaising with project teams for new ingests or data assets, working with teams to ensure data governance and compliance. Develop ETL pipelines to make data assets available on GRADE. Contribute to designing roadmaps for data, application and business process architecture to support future state environments. Prepare effective material for dissemination to key business stakeholders at all levels of seniority. Effectively manage relationships across key functional teams. Work with management to prioritize business and information needs. Encourage and drive process improvement opportunities. KEY SKILLS & QUALIFICATIONS: Solid and relevant experience in a similar job role. Bachelor‚Äôs degree in IT or a numerate subject; Master‚Äôs degree preferred. Strong theoretical and practical knowledge of programming tools, including proficiency in at least one of the following tools: SAS, Python, PySpark, SQL, and Hadoop Big Data, experience with Prophecy is a plus. Business Analysis ‚Äì requirements gathering and specification, Ability to comprehend complex data architecture. Strong organizational, analytical, problem-solving, and project management skills. Multitasking ability and the capacity to work under pressure within tight timelines are essential. Working knowledge of JIRA, Confluence Open personality with effective communication skills and the flexibility to work in an international team. Previous experience in IT/Analytics is desirable. Preferable knowledge of banking risk data, systems, and risk models. WHY WORTH JOINING US: Employment contract and/or Business to Business as you prefer Hybrid work Speaking English on a daily basis, mainly in contact with foreign stakeholders and peers Multiple employee benefits packages (MyBenefit Cafeteria, private medical care, insurance) Access to 3,000+ Business Courses Platform (Udemy) Access to required IT equipment Ongoing learning opportunities to help you acquire new skills or deepen existing expertise Being part of the core squad focused on the growth of the Polish business unit A flat, non-hierarchical structure that will enable you to work with senior partners and directly with clients A work culture focused on innovation and creating lasting value for our clients and employees ONLINE RECRUITMENT PROCESS STEPS: Screening call with the Recruiter Competencies interview with Capco Hiring Manager Client‚Äôs interview Feedback/Offer",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Hybrid,542,"Senior Data Engineer ‚Äì Python, Spark, AWS",Link Group,"We‚Äôre looking for a Senior Data Engineer to join a highly skilled team responsible for building a scalable, multitenant data platform that powers data ingestion, transformation, cataloging, and distribution for internal and external stakeholders. This role is central to designing robust data pipelines, integrating modern data lakehouse components, and supporting the company‚Äôs AWS-based infrastructure. Design and build scalable data ingestion pipelines in AWS Work extensively with Databricks , including Unity Catalog, streaming, Delta Lake, and data transformation tools Lead technical delivery and implementation of core platform components across raw, normalized, operational, and historical layers Collaborate with engineering managers and cross-functional teams across the US and Poland Drive technical excellence by improving team processes, architecture standards, and engineering best practices Support and consult with stakeholders to ensure successful delivery of data-driven solutions Contribute to the growth and maturity of the team‚Äôs cloud and data engineering capabilities 5+ years of experience in Python and strong software engineering skills Solid knowledge of AWS cloud services and best practices Experience building scalable Spark pipelines in PySpark or Scala Practical experience with Spark Streaming for low-latency pipelines Familiarity with Delta Lake and modern data lakehouse architectures Hands-on experience with Kubernetes and container orchestration Experience with MongoDB and other NoSQL data stores Understanding of microservices and event-driven architecture Excellent communication skills and ability to work in a collaborative, global team Commitment to high standards of ethics, quality, and delivery Familiarity with Machine Learning , AI models , or Data Science workflows","[{""min"": 35000, ""max"": 40000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,543,Senior Data Engineer,N-iX,"We are seeking a Senior Data Engineer specializing in Databricks to join our global team. You will be instrumental in setting up and maintaining our Databricks platform, building robust data pipelines, and collaborating closely with our solution architects and data scientists. Your expertise will directly support our mission to leverage data and AI effectively within a cutting-edge automotive claims management environment. Key Responsibilities: Design, build, and maintain robust data pipelines within Databricks. Collaborate closely with international teams, including data scientists and architects, to develop scalable data solutions. Debug complex issues in data pipelines and proactively enhance system performance and reliability. Set up Databricks environments on cloud platforms (Azure/AWS). Automate processes using CI/CD practices and infrastructure tools such as Terraform. Create and maintain detailed documentation, including workflows and operational checklists. Develop integration and unit tests to ensure data quality and reliability. Migrate legacy data systems to Databricks, ensuring minimal disruption. Participate actively in defining data governance and management strategies. What We Expect from You (Requirements): 5+ years of proven experience as a Data Engineer. Advanced proficiency in Python for developing production-grade data pipelines. Extensive hands-on experience with Databricks platform. Strong knowledge of Apache Spark for big data processing. Familiarity with cloud environments, specifically Azure or AWS. Proficiency with SQL and experience managing relational databases (MS SQL preferred). Practical experience with Airflow or similar data orchestration tools. Strong understanding of CI/CD pipelines and experience with tools like GitLab. Solid skills in debugging complex data pipeline issues. Proficiency in structured documentation practices. B2 level or higher proficiency in English. Strong collaboration skills, ability to adapt, and eagerness to learn in an international team environment. Nice to have: Experience with Docker and Kubernetes. Familiarity with Elasticsearch or other vector databases. Understanding of DBT (data build tool). Ability to travel abroad twice a year for on-site workshops. Why Join Us Work on impactful projects with cross-functional teams. Opportunity to grow your BI and analytics career in a data-driven organization. Flexible working hours and remote work options. Competitive compensation and benefits. Opportunity to work on presales We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 29162, ""max"": 30256, ""type"": ""Net per month - B2B""}, {""min"": 24059, ""max"": 25152, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,544,Data Engineer,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: Data Engineer Responsibilities: Develop, optimize & maintain big data (ELT/ETL) pipelines for purposes of business intelligence and statistical modelling Ensure & monitor data quality and integrity Set up and configure data storage systems (e.g. SQL databases, data lake) Manage and support production use of connections between key elements of data infrastructure Write and maintain secure, robust, scalable, and efficient code that turns business concepts into tangible solutions Drive engineering best practices like automation, CI/CD, and maintainability Support standardization and automation by following best-practices / common development standards Collaborate with BI analysts, data scientists, ML engineers and core IT in cross-functional teams delivering value through data solutions Requirements: Broad skills in Python, SQL, MDX and bash scripting 2+ years of hands-on experience within Azure data components (Data Factory, Synapse, Datalake, Blob Storage, Sharepoint, Databricks) Experience with data workflow orchestration (ADF, Databricks Jobs, possibly Airflow) Good command of version control and CI/CD pipelines using Azure DevOps or alike Experience with Snowflake and its use with Azure cloud Business-ready English (B2-C1) Familiarity with data governance concepts (e.g., catalogue & meta data, data lineage, master data, security & compliance) Nice to have: Familiarity with SAP Business Warehouse as a data source Azure Data Engineer / Data Analytics certificate Understanding of BI tools, eg. Power BI Experience with PySpark / Scala Offer: Private medical care Co-financing for the sport card Training & learning opportunities Constant support of dedicated consultant Employee referral program","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,545,Data Engineer ‚Äì Informatica with Azure,Sii,"Join a team driving global data transformation by contributing to a project for an international company that supplies components and systems for transporting liquids and gases. We are looking for Data Engineers to help develop a modern integration platform based on Informatica IDMC and Microsoft Azure. The project aims to build an integration layer that consolidates data from both on-premise and cloud-based systems (ERP, HR, procurement) into a centralized data warehouse hosted in Azure. Designing and maintaining data integrations using Informatica Cloud (CDI, CAI) Implementing mass data ingestion processes Integrating with source systems (ERP, HR/payroll, procurement platforms) Managing metadata using CDGC and Metadata Command System Extending platform functionality with Java, where Informatica features are insufficient Integrating with Azure API Gateway Working within an environment based on Azure SQL, Data Lake, and Windows Secure Agents Minimum 3 years of experience in a similar role Proficiency with ETL tools (at least one of the following: Informatica IICS, Boomi, Azure Data Factory) Strong SQL skills and experience working with databases (including schema design) Good knowledge of Microsoft Azure (e.g., managing resources, start/stop Azure hosts) Understanding of API architecture and system integrations Very good command of English ‚Äì minimum B2 level Fluent Polish required Residing in Poland required Great Place to Work since 2015 - it‚Äôs thanks to feedback from our workers that we get this special title and constantly implement new ideas Employment stability - revenue of PLN 2.1BN, no debts, since 2006 on the market We share the profit with Workers - over PLN 60M has already been allocated for this aim since 2022 Attractive benefits package - private healthcare, benefits cafeteria platform, car discounts and more Comfortable workplace ‚Äì class A offices or remote work Dozens of fascinating projects for prestigious brands from all over the world ‚Äì you can change them thanks to Job Changer application PLN 1 000 000 per year for your ideas - with this amount, we support the passions and voluntary actions of our workers Investment in your growth ‚Äì meetups, webinars, training platform and technology blog ‚Äì you choose Fantastic atmosphere created by all Sii Power People 1 Send your CV 2 Talk to us about your expectations 3 Learn more about our projects and choose the best 4 Start your adventure with Sii! Sii is the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We already employ more than 7 500 professionals and implement projects in a variety of industries for clients from many countries around the world. The Great Place to Work title, won 10 times in a row, proves that at Sii we create a friendly work environment. In a survey, as many as 90% of our employees responded that Sii is a great place to work, and 95% of them think we have a great atmosphere here.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Remote,546,Data Engineer,Dataplace.ai,"Kilka s≈Ç√≥w o nas: Jeste≈õmy polsko-portugalskim startupem dzia≈ÇajƒÖcym w obszarze geoanalityki i machine learning. Nasze analizy i technologia pomagajƒÖ podejmowaƒá strategiczne decyzje biznesowe najwiƒôkszym firmom z bran≈ºy retail, mediowej oraz FMCG. WykorzystujƒÖc dane przestrzenne pomagamy naszym klientom optymalizowaƒá sieƒá sprzeda≈ºy oraz okre≈õlaƒá najlepsze lokalizacje do jej rozwoju. Jak to robimy? Codziennie miliony u≈ºytkownik√≥w urzƒÖdze≈Ñ mobilnych raportuje nam o swojej lokalizacji, a dziƒôki dodaniu kontekstu geoprzestrzennego i socjodemograficznego z zewnƒôtrznych baz danych, oraz zaawansowanym modelom predykcyjnym, kt√≥re rozwijamy, jeste≈õmy w stanie w kompleksowy spos√≥b poznaƒá preferencje i ruch klient√≥w w ≈õwiecie offline. Z naszego rozwiƒÖzania ju≈º dzi≈õ korzystajƒÖ najwiƒôksi gracze na rynku, nie osiadamy jednak na laurach i wciƒÖ≈º rozwijamy dataplace.ai oraz naszƒÖ technologiƒô. Kogo szukamy? Do naszego zespo≈Çu zajmujƒÖcego siƒô przetwarzaniem i analizƒÖ danych geoprzestrzennych poszukujemy osoby na stanowisko Data Engineer. Specjalizujemy siƒô w nowoczesnych rozwiƒÖzaniach analitycznych i infrastrukturze danych, pracujƒÖc z ogromnymi zasobami informacji przestrzennych . Je≈õli chcesz tworzyƒá skalowalne systemy do zarzƒÖdzania danymi geograficznymi i usprawniaƒá nasze procesy przetwarzania , zapraszamy do aplikowania! Co bƒôdzie nale≈ºa≈Ço do Twoich zada≈Ñ w dataplace.ai? PracujƒÖc na stanowisku Data Engineer do Twoich obowiƒÖzk√≥w bƒôdzie nale≈ºa≈Ço: Projektowanie, budowa i optymalizacja potok√≥w danych z wykorzystaniem narzƒôdzi takich jak PySpark, Databricks i SQL; ZarzƒÖdzanie i optymalizacja hurtowni danych na platformach takich jak Snowflake oraz AWS S3; Praca z danymi geoprzestrzennymi i ich przetwarzanie z u≈ºyciem Apache Sedona; Implementacja rozwiƒÖza≈Ñ chmurowych dla przetwarzania du≈ºych zbior√≥w danych geoprzestrzennych na platformie AWS; Usprawnianie proces√≥w ETL oraz zapewnienie jako≈õci i bezpiecze≈Ñstwa danych; Udzia≈Ç w pracach B+R ‚Äì uczestnictwo w pracach badawczych nad rozwojem naszej technologii przetwarzania danych przestrzennych. Czego oczekujemy na tym stanowisku? Do≈õwiadczenia (przynajmniej 2 lata) w pracy z du≈ºymi zbiorami danych i narzƒôdziami Big Data (PySpark, Databricks); Bieg≈Çej znajomo≈õci SQL oraz do≈õwiadczenia z hurtowniami danych (preferowane Snowflake); Znajomo≈õci narzƒôdzi i technologii do przetwarzania danych geoprzestrzennych (np. Apache Sedona); Do≈õwiadczenia w pracy z AWS (S3, IAM, EC2, RDS); Umiejƒôtno≈õci optymalizacji i skalowania potok√≥w danych oraz dba≈Ço≈õci o wysokƒÖ jako≈õƒá przetwarzania informacji; Umiejƒôtno≈õci oceny jako≈õci danych i oczyszczania ich z b≈Çƒôd√≥w; Proaktywnej postawy, inicjatywy i biznesowego podej≈õcia; Samodzielno≈õci, zaanga≈ºowania i poczucia odpowiedzialno≈õci za dostarczone dane; Znajomo≈õci jƒôzyka angielskiego pozwalajƒÖcej na swobodnƒÖ komunikacjƒô (w mowie i pi≈õmie). Twoim dodatkowym atutem bƒôdzie: Do≈õwiadczenie w optymalizacji proces√≥w ETL oraz w zarzƒÖdzaniu i monitorowaniu system√≥w Big Data; Znajomo≈õƒá innych narzƒôdzi geospatial i do≈õwiadczenie z danymi przestrzennymi. Co oferujemy w zamian? Wynagrodzenie adekwatne do posiadanego do≈õwiadczenia ‚Äì aplikujƒÖc, podziel siƒô z nami swoimi oczekiwaniami. Preferujemy umowƒô UoP lub UZ Pracƒô z lud≈∫mi, kt√≥rzy lubiƒÖ to, co robiƒÖ i siebie nawzajem : ) Mo≈ºliwo≈õƒá pracy w pe≈Çni zdalnej ‚Äì pracujemy z Portugalii, Wiednia czy Gi≈ºycka, ale je≈õli wolisz przychodziƒá do biura, zapraszamy do naszej warszawskiej lokalizacji; Elastyczne godziny pracy ‚Äì ≈Çapiemy siƒô na statusy, okre≈õlamy cele i terminy, ale to Ty decydujesz, kiedy zaczynasz i ko≈Ñczysz pracƒô (wsp√≥lnie ustalamy zasady); Urlop no limit ‚Äì bierzesz tyle urlopu, ile chcesz, wa≈ºne ≈ºeby≈õmy wsp√≥lnie dowozili zadania, na kt√≥re siƒô um√≥wimy; Pracƒô przy projektach z zakresu ML i Data Science dla najwiƒôkszych firm w Polsce; Pracƒô nad innowacyjnym produktem location intelligence, w kt√≥rego rozwoju bƒôdziesz mia≈Ç/a sw√≥j realny udzia≈Ç; Indywidualnie dopasowany pakiet benefit√≥w; powiedz nam czego oczekujesz, a my powiemy Ci co mo≈ºemy Ci zaoferowaƒá; Mo≈ºliwo≈õƒá sta≈Çego podnoszenia kwalifikacji - Ty wybierasz, jakie szkolenie jest Ci najbardziej potrzebne. Je≈õli fascynujƒÖ Ciƒô dane przestrzenne i chcesz pracowaƒá z nowoczesnymi technologiami, do≈ÇƒÖcz do naszego zespo≈Çu i pom√≥≈º nam rozwijaƒá innowacyjne rozwiƒÖzania w zakresie danych geoprzestrzennych!",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,547,Senior PL SQL Developer,BNP Paribas SA oddzia≈Ç w Polsce,"Description As a part of the FA IT team within Securities Services cluster you will contribute to providing new solutions for operational teams and establishing the link between international stakeholders and development teams. This position will give you the unique opportunity to cooperate with dynamic, cross location squads while you will shape the product vision, manage the backlog, including transversal topics, to ensure continuity and guarantee the quality and sustainability of the application. Senior PL SQL Developer Responsibilities: Organizes development of software that meets development and security standards to support underlying business function. Drives optimization of existing code and determines techniques to improve its maintainability to continuously improve the quality of delivered solutions. Selects software packages and their configuration to lower the cost of delivered solutions Selects and configures product deployment tools to minimize impact on business continuity Drives implementation and creates the test strategy to ensure quality of delivered solutions Clarifies and qualifies the needs and carries out the studies before project launch to assess feasibility Requirements: 7-10+ years of professional experience Extensive knowledge of IT development standards, methods, and tools: Dev tools ‚Äì PL/SQL, SQL Developers OS: Cloud oriented platform AIX is an advantage Oracle feature Database ‚Äì performance tuning, code review, new feature analysis CI/CD DEVOPS Test automation Communication and presentation skills to audiences of different levels of seniority and different culture Awareness of IT quality and security standards, functional and technical architecture rules Knowledge of Fund Admin domain or familiarity with MultiFonds would be an advantage. Proactiveness, accuracy and ability to work under time pressure. Fluency in English as working language. What we offer: Hybrid work mode, 60% working from home within a month Equivalent for remote work expenses (120 PLN per month) Stable employment in the international company Fully paid private medical care for employee Pre-paid lunch card Employee Pension Plan Co-financed Multisport Card MyBenefit Cafeteria Platform Life insurance Car parking availability in the office building Trainings and development opportunities",[],Database Administration,Database Administration
Full-time,Senior,B2B,Remote,548,Senior Data Analyst,ERLI sp√≥≈Çka akcyjna,"ERLI to jedna z czo≈Çowych platform marketplace w Polsce, kt√≥ra co miesiƒÖc przyciƒÖga ponad 9 milion√≥w aktywnych kupujƒÖcych. Wsp√≥≈Çpracujemy z tysiƒÖcami sprzedawc√≥w, wspierajƒÖc ich w maksymalizacji sprzeda≈ºy i rozwoju ich biznes√≥w. Naszym celem jest dostarczanie szerokiej gamy produkt√≥w w konkurencyjnych cenach, zapewniajƒÖc klientom najwy≈ºszy standard obs≈Çugi i satysfakcjƒô z zakup√≥w. Do≈ÇƒÖcz do ERLI jako Senior Data Analyst! Co bƒôdziesz robiƒá? Jako Senior Data Analyst bƒôdziesz kluczowƒÖ postaciƒÖ w naszym zespole, odpowiedzialnƒÖ za dog≈Çƒôbne zrozumienie zachowa≈Ñ u≈ºytkownik√≥w i optymalizacjƒô naszych produkt√≥w z focusem na udoskonalenia wyszukiwarki (search) i konwersji w ≈õcie≈ºkach zakupowych u≈ºytkownik√≥w (CVR). Twoje codzienne zadania bƒôdƒÖ obejmowaƒá: Prowadzenie wszechstronnych analiz ilo≈õciowych i jako≈õciowych , koncentrujƒÖc siƒô na wyszukiwarce, ≈õcie≈ºkach u≈ºytkownik√≥w i innych kluczowych obszarach platformy. Wnioskowanie i formu≈Çowanie konkretnych rekomendacji na podstawie przeprowadzonych analiz, a nastƒôpnie ich wdra≈ºanie wraz z zespo≈Çem w celu poprawy funkcjonalno≈õci platformy i osiƒÖgania za≈Ço≈ºonych metryk. Projektowanie i analizowanie eksperyment√≥w A/B , w tym precyzyjne definiowanie kohort, KPI oraz metryk stra≈ºniczych, a tak≈ºe interpretowanie ich wynik√≥w. Aktywne wspieranie procesu projektowania i realizacji bada≈Ñ ilo≈õciowych . Tworzenie i optymalizowanie metryk jako≈õciowych i ilo≈õciowych dla listingu oraz wynik√≥w wyszukiwania, aby zapewniƒá ich najwy≈ºszƒÖ efektywno≈õƒá. Wspieranie w definiowaniu i monitorowaniu kluczowych wska≈∫nik√≥w efektywno≈õci (KPI) , kt√≥re przek≈ÇadajƒÖ siƒô na wyniki wyszukiwarki i konwersjƒô na ≈õcie≈ºce u≈ºytkownika. Gromadzenie, monitorowanie i interpretowanie danych z r√≥≈ºnorodnych ≈∫r√≥de≈Ç i obszar√≥w, takich jak performance marketing czy ≈õcie≈ºki konwersji . Bycie kluczowym wsparciem w przek≈Çadaniu danych ilo≈õciowych na identyfikacjƒô problem√≥w u≈ºytkownik√≥w , co w efekcie bƒôdzie stanowiƒá podstawƒô do aktywnego wsparcia w kreowaniu skutecznych rozwiƒÖza≈Ñ. Jeste≈õ idealnym kandydatem, je≈õli: Jeste≈õ w pe≈Çni samodzielnym analitykiem z silnym mindsetem produktowym , kt√≥ry doskonale odnajduje siƒô w dynamicznym i zmiennym ≈õrodowisku startupowym. Posiadasz minimum 5-letnie do≈õwiadczenie w pracy z du≈ºymi zbiorami danych, wykorzystujƒÖc narzƒôdzia takie jak BigQuery, Tableau czy Snowflake . Znakomicie pos≈Çugujesz siƒô narzƒôdziami takimi jak: Looker, Grafana, GA4, Firebase, BigQuery, Excel . Posiadasz bardzo dobrƒÖ znajomo≈õƒá metodyki eksperyment√≥w A/B zar√≥wno na poziomie teoretycznym, jak i praktycznym, ze szczeg√≥lnym uwzglƒôdnieniem bada≈Ñ wyszukiwarek, a tak≈ºe potrafisz trafnie interpretowaƒá ich wyniki. Potrafisz samodzielnie wyciƒÖgaƒá konkretne wnioski biznesowe z zachowa≈Ñ u≈ºytkownik√≥w i danych ilo≈õciowych, wspierajƒÖc siƒô w razie potrzeby danymi jako≈õciowymi. Potrafisz odnale≈∫ƒá siƒô w gƒÖszczu danych, narzƒôdzi i proces√≥w, wyciƒÖgajƒÖc z nich nie tylko suche informacje, ale przede wszystkim warto≈õciowe wnioski , kt√≥re pomagajƒÖ osiƒÖgaƒá cele biznesowe. Nie boisz siƒô dynamicznego i zmiennego ≈õrodowiska , gdzie otwarta g≈Çowa i proaktywno≈õƒá w rozwiƒÖzywaniu problem√≥w sƒÖ fundamentem sukcesu. Dobrze znasz bran≈ºƒô e-commerce lub pasjonujesz siƒô rozwojem wyszukiwarek oraz poprawƒÖ efektywno≈õci ≈õcie≈ºek u≈ºytkownik√≥w. Co oferujemy? Mo≈ºliwo≈õƒá indywidualnego, nieograniczonego rozwoju biznesowego w dynamicznie rosnƒÖcej organizacji w bran≈ºy e-commerce; Realny wp≈Çyw na rozw√≥j naszej organizacji - Twoje pomys≈Çy siƒô dla nas liczƒÖ! Dofinansowanie pakietu benefit√≥w (karta sportowa, pakiet medyczny, ubezpieczenie grupowe); R√≥≈ºnorodne zadania i cele ‚Äì wiesz, co masz robiƒá, ale pozostawiamy Ci du≈ºƒÖ swobodƒô dzia≈Çania i jeste≈õmy otwarci na Twoje pomys≈Çy; Zachƒôcamy do aplikowania!","[{""min"": 18000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Remote,549,Data Scientist (Risk),Revolut,"People deserve more from their money. More visibility, more control, and more freedom. Since 2015, Revolut has been on a mission to deliver just that. Our powerhouse of products ‚Äî including spending, saving, investing, exchanging, travelling, and more ‚Äî help our 60+ million customers get more from their money every day. As we continue our lightning-fast growth,‚Äå 2 things are essential to our success: our people and our culture. In recognition of our outstanding employee experience, we've been certified as a Great Place to Work‚Ñ¢. So far, we have 10,000+ people working around the world, from our offices and remotely, to help us achieve our mission. And we're looking for more brilliant people. People who love building great products, redefining success, and turning the complexity of a chaotic world into the simplicity of a beautiful solution. We approach Data Science at Revolut the same way that we approach everything else ‚Äî with class, logical thinking, and lots of style üòé Let‚Äôs break it down: we take the most complex problems and create tailor-made solutions for our customers. If you‚Äôre thinking the Data team is kept in some sort of secret den, doomed to never see the impact of their work, don‚Äôt worry, that‚Äôs not how we do things. They‚Äôre some of our best and brightest problem-solvers, deployed to the front lines to work in product teams and deliver rockstar solutions ü§ò We‚Äôre looking for a next-level Risk Data Scientist who'll help us with effective assessment and management of risk by developing policies, methodologies, models, and systems for risk quantification, reporting, and monitoring. Up for the challenge? Let's get in touch üëá Developing and maintaining methodologies and policies for Credit Risk Modelling Building models for ECL, Capital and Stress Testing for Mortgages Delivering real impact to the product through rigorous data-driven solutions Collaborating with product owners, engineers, and data scientists to continually solve complex data problems A degree in mathematics, statistics, economics, finance, machine learning, computer science, or engineering Experience in Credit Risk Modelling for Mortgages for IRB, Capital, ECL, Stress Testing Excellent knowledge of data science tools, including Python, SQL, and production tools Knowledge in mathematics and statistics Experience building statistical and/or machine learning models Familiarity with Risk Theory and financial products in the Credit Risk area Experience in the Finance sector, having worked in a bank or other financial company A PhD with relevant research experience Building a global financial super app isn‚Äôt enough. Our Revoluters are a priority, and that‚Äôs why in 2021 we launched our inaugural D&I Framework, designed to help us thrive and grow everyday. We're not just doing this because it's the right thing to do. We‚Äôre doing it because we know that seeking out diverse talent and creating an inclusive workplace is the way to create exceptional, innovative products and services for our customers. That‚Äôs why we encourage applications from people with diverse backgrounds and experiences to join this multicultural, hard-working team.",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,550,Senior Data Engineer,emagine Polska,"Summary: The role of the Senior Data Engineer focuses on enhancing a Data Lakehouse platform built on Snowflake and DBT, driving innovation, and ensuring the platform's operational excellence. Start: ASAP Duration: 3 months with possible extension until the end of 2025 Location: 100% Remote Salary: 170-190 z≈Ç/h Main Responsibilities: Design and implement solutions for the data platform using Snowflake and Azure. Enhance the platform with new features and optimize CI/CD pipelines. Automate processes and improve governance and monitoring. Collaborate within an international team and gather requirements from stakeholders. Work within Scrum and Kanban methodologies using Jira. Key Requirements: DBT Snowflake SQL Python or other object-oriented languages (Java, C#, etc.) Experience with relational databases CI/CD operations Version control using Git (preferably GitHub) Data architecture and integration skills English B2/C1 proficiency Nice to Have: PowerBI Other Details: Ability to work in Scrum and Kanban environments using Jira. Experience working independently with international stakeholders. Strong problem-solving skills. Experience working in an international team setting. Project Start Date: As soon as possible Team Composition: Three other data engineers Contract Type: B2B Work Type: Remote (with office options in Poznan and Warsaw) Travel: Unlikely but possible to Poznan, Warsaw, or Copenhagen Working Hours: Flexible; generally available from 9 AM - 4 PM.","[{""min"": 170, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,551,Senior Data Engineer - MarAdTech Advertising,Allegro,"The salary range for this position is (contract of employment): 18 400 - 25 410 PLN in gross terms A hybrid work model requires 1 day a week in the office We are looking for a Senior Data Engineer with a focus on the data processing and preparation, deployment and maintenance of our data projects. Join our team to enhance your skills related to deploying data-based processes, data ops approaches and share the skills within the team. Your main responsibilities: You will be actively responsible for developing and maintaining processes for handling large volumes of data You will be streamlining and developing the data architecture that powers analytical products and work along a team of experienced analysts You will be monitoring and enhancing quality and integrity of the data You will manage and optimize costs related to our data infrastructure and data processing on GCP This is the right job for you if you: Have at least 3 years of experience as Data Engineer and working with large datasets Have experience with cloud providers (GCP preferred) Are highly proficient in SQL Have strong understanding of data modeling and cloud DWH architecture Have experience in designing and maintaining ETL/ELT processes Are capable of optimizing cost and efficiency of data processing Are proficient in Python for working with large data sets (using PySpark or Airflow) Use good practices (clean code, code review, CI/CD) Have a high degree of autonomy and take responsibility for developed solutions Have English proficiency on at least B2 level Like to share knowledge with other team members Nice to have: Experience with rest API What we offer: A hybrid work model that you will agree on with your leader and the team. We have well-located offices (with fully equipped kitchens and bicycle parking facilities) and excellent working tools (height-adjustable desks, interactive conference rooms) Annual bonus up to 10% of the annual salary gross (depending on your annual assessment and the company's results). A wide selection of fringe benefits in a cafeteria plan ‚Äì you choose what you like (e.g. medical, sports or lunch packages, insurance, purchase vouchers). English classes that we pay for related to the specific nature of your job. 16"" or 14"" MacBook Pro with M1 processor and, 32GB RAM or a corresponding Dell with Windows (if you don‚Äôt like Macs) and other gadgets that you may need. Working in a team you can always count on ‚Äî we have on board top-class specialists and experts in their areas of expertise. A high degree of autonomy in terms of organizing your team‚Äôs work; we encourage you to develop continuously and try out new things. Hackathons, team tourism, training budget and an internal educational platform, MindUp (including training courses on work organization, means of communications, motivation to work and various technologies and subject-matter issues). If you want to learn more, check it out This may also be of interest to you: Allegro Tech Podcast ‚Üí https: //podcast.allegro.tech/ Send in your CV and see why it is #dobrzetubyƒá (#goodtobehere)",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,552,Senior Data Scientist,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Senior Data Scientist Dla naszego Klienta, miƒôdzynarodowej organizacji, poszukujemy do≈õwiadczonego Senior Data Scientist, kt√≥ry swobodnie porusza siƒô w ≈õwiecie ML/AI, zna dobre praktyki MLOps i potrafi projektowaƒá skalowalne rozwiƒÖzania. Je≈õli masz mocne zaplecze w Pythonie, ≈õwietnie radzisz sobie z du≈ºymi zbiorami danych i chcesz pracowaƒá nad realnymi wyzwaniami biznesowymi ‚Äì aplikuj. Czego oczekujemy: 5+ lat do≈õwiadczenia jako Data Scientist oraz udokumentowane sukcesy w stosowaniu technik ML do rozwiƒÖzywania rzeczywistych problem√≥w. Bieg≈Ço≈õci w Pythonie i popularnych frameworkach ML (scikit-learn, TensorFlow, PyTorch). Do≈õwiadczenie w projektowaniu i wdra≈ºaniu kompleksowych rozwiƒÖza≈Ñ ML, w tym wstƒôpnego przetwarzania danych, in≈ºynierii cech, budowy i oceny modeli. Znajomo≈õƒá platform chmurowych (AWS, Azure, GCP) oraz praktyk MLOps. Solidne podstawy statystyki i matematyki oraz ich zastosowania w modelowaniu i ocenie modeli. Umiejƒôtno≈õƒá zarzƒÖdzania du≈ºymi, z≈Ço≈ºonymi zbiorami danych oraz przeprowadzania zada≈Ñ takich jak przetwarzanie danych, in≈ºynieria cech i optymalizacja modeli. Silne umiejƒôtno≈õci rozwiƒÖzywania problem√≥w i analitycznego my≈õlenia, pozwalajƒÖce na proponowanie innowacyjnych rozwiƒÖza≈Ñ w zakresie analizy danych. Doskona≈Çe umiejƒôtno≈õci komunikacyjne, w tym zdolno≈õƒá do przedstawiania koncepcji technicznych i wynik√≥w osobom nietechnicznym. Bieg≈Ço≈õƒá w jƒôzyku angielskim (w mowie i pi≈õmie). Mile widziane: Do≈õwiadczenie w pracy z Generative AI i du≈ºymi modelami jƒôzykowymi (LLMs). Znajomo≈õƒá przetwarzania jƒôzyka naturalnego (NLP) lub technik wizji komputerowej. Dodatkowe umiejƒôtno≈õci programistyczne w jƒôzykach takich jak R, SQL, Java itp. Do≈õwiadczenie w pracy z narzƒôdziami Big Data (np. Hadoop, Spark, Kafka). Twoje obowiƒÖzki: Projektowanie, rozwijanie i wdra≈ºanie kompleksowych rozwiƒÖza≈Ñ ML end-to-end. Analiza i przetwarzanie du≈ºych zbior√≥w danych. Budowa, testowanie i wdra≈ºanie modeli ML z wykorzystaniem nowoczesnych algorytm√≥w i technik uczenia maszynowego. Praca z platformami chmurowymi (AWS, Azure, GCP) oraz implementacja rozwiƒÖza≈Ñ zgodnych z praktykami MLOps. Eksperymentowanie z nowymi podej≈õciami w AI, w tym Generative AI i LLMs. Optymalizacja istniejƒÖcych modeli ML pod kƒÖtem wydajno≈õci i dok≈Çadno≈õci. Wsp√≥≈Çpraca z zespo≈Çami biznesowymi i technicznymi, w celu identyfikowania potrzeb i wdra≈ºania rozwiƒÖza≈Ñ opartych na danych. Tworzenie dokumentacji technicznej oraz prezentowanie wynik√≥w analiz osobom nietechnicznym. Co oferujemy? Rozw√≥j kariery w miƒôdzynarodowych projektach, z wykorzystaniem nowoczesnych narzƒôdzi i technologii. Elastyczny model pracy: mo≈ºliwo≈õƒá 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejƒôtno≈õci i do≈õwiadczenia. Wsp√≥≈Çpracƒô w zgranym zespole, kt√≥ry ceni wymianƒô wiedzy oraz otwartƒÖ komunikacjƒô. Realny wp≈Çyw na projekty oraz wdra≈ºane rozwiƒÖzania. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 20000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,B2B,Remote,553,Data Engineer,KUBO,"We‚Äôre hiring a Data Engineer! Designing and building robust ETL/ELT data pipelines in Microsoft Fabric using PySpark and SQL Modeling data using approaches like star schema, data vault, and lakehouse Creating well-structured datasets, reports, and Power BI dashboards focused on business usability and self-service Implementing best practices around data governance, security, and documentation Automating tests, CI/CD workflows, and monitoring using Azure DevOps or GitHub Actions Collaborating closely with product owners, analysts, and fellow engineers in cross-functional teams 3+ years of experience as a Data Engineer or in a similar role Advanced SQL skills ‚Äì query optimization, indexing, partitioning Strong Python programming skills Solid knowledge of Apache Spark for batch & streaming data, Delta Lake Experience with Power BI ‚Äì data modeling, DAX, RLS, deployment pipelines Familiarity with cloud platforms ‚Äì ideally Azure (Data Lake Gen2, Data Factory, Synapse/Databricks) Proficiency with version control and DevOps tools (Git, pull requests, CI/CD basics) Fluency in English (minimum B2+) Work model: 100% remote Rate: 100‚Äì150 PLN/h net (B2B) Benefits: Private medical care, Life insurance, Multisport card","[{""min"": 100, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,554,Data Engineer,Tesco Technology,"Tesco Technology is multi-functional and specialist team that drives operational excellence of services improves scale for our systems and processes globally and creates business leading capabilities. We are an agile team of an industry-leading team of engineers. We create the future continuous integration and delivery tools for Colleague and Customer & Loyalty areas, solving problems, and developing new features through quality, scalable, performant, and maintainable technical solutions. The solutions that we are responsible for will have a global reach, impacting hundreds of thousands of Tesco colleagues worldwide. We operate in a DevOps philosophy. We take responsibility for the software through its entire lifecycle. We practice continuous integration, delivery, and support of our code through to production and beyond. As Tech Hub we cooperate within the group of Tesco Technology Hubs located in the UK, Poland, Hungary, and India. We always welcome conversations about flexible working, so feel free to talk to us during your application about how we can support you.We value connecting, collaborating, and innovating with our colleagues in person. At Tesco Technology, we work in a hybrid model. This role requires you to be based in or near Krak√≥w, as we currently meet in the office three days a week. The Data Engineering department at Tesco Technology is at the forefront of data processing within the retail and technology industry. This vital department handles a range of responsibilities, including: Analyzing order and delivery data to optimize logistics processes and enhance delivery efficiency. Managing critical data related to customer orders, suppliers, and products to ensure the seamless flow of our fulfillment operations. Upholding data integrity and security during the processing of order and delivery-related information. As we continue to expand, we are actively seeking a skilled Data Engineer to join our team of analytics experts. In this role, you will take charge of expanding and refining our data and data pipeline architecture. Additionally, you will be instrumental in optimizing data flow and collection to cater to the needs of cross-functional teams. Our ideal candidate is an experienced data pipeline builder and data enthusiast who relishes the opportunity to optimize data systems and construct them from the ground up. As a Data Engineer, you will collaborate closely with software developers, database architects, data analysts, and data scientists on various data-driven initiatives. You will play a crucial role in ensuring that the optimal data delivery architecture remains consistent across all ongoing projects. This role calls for a high level of self-direction and the ability to effectively support the data requirements of multiple teams, systems, and products. If you are enthusiastic about the prospect of optimizing, and possibly even redesigning, our company's data architecture to support our next generation of products and data initiatives, we encourage you to apply and be part of our dynamic team shaping the future of our data operations! Responsibilities Create and maintain optimal data pipeline architecture Assemble large complex data sets that meet functional / non-functional business requirements. Identify design and implement internal process improvements: automating manual processes optimising data delivery re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction transformation and loading of data from a wide variety of data sources Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition operational efficiency and other key business performance metrics. Work with stakeholders including the Executive Product Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure Create data tools for analytics and data scientist team members that assist them in building and optimising our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Mandatory skills: Data Processing : Apache Spark - Scala or Python Data Storage : Apache HDFS or respective cloud alternative Resource Manager : Apache Yarn or respective cloud alternative Lakehouse : Apache Hive/Kyuubi or alternative Workflow Scheduler : Airflow or alternative Nice to have skills: Functional programming Apache Kafka Kubernetes Stream processing CI/CD Unsure if you fit all the criteria? Apply and give us the chance to evaluate your potential ‚Äì you could be the perfect fit! We value flexibility at Tesco; therefore, this position is also available for candidates who are interested in working part time ‚Äì about 120 hours a month or more. Please let us know what would work for you. Hybrid work We know life looks a little different for each of us. That‚Äôs why at Tesco, we always welcome chats about different flexible working options. Some people are at the start of their careers, some want the freedom to do the things they love. Others are going through life-changing moments like becoming a carer, adapting to parenthood, or something else. So, talk to us throughout your application about how we can support. This role requires you to be based in or near Krak√≥w, as you will spend 60% (3 days) of your week collaborating with colleagues at our office locations or local sites and the rest remotely. Benefits Tesco is a diverse and exciting employer, dedicated to being #aplacetogeton, providing career-defining opportunities to all of our colleagues. If you choose to join our business, we will provide you with (for all): MacBook as your tool for work Learning opportunities - certified technical training and learning platforms like Udemy, Pluralsight and O‚Äôreily Referral Bonus Sports activities with a personal trainer in the office Benefits for colleagues on employment of contract only: Additional 4 days of paid leave to support your well-being and family life Up to 20% yearly salary bonus ‚Äì based on both individual and business performance Private healthcare (LuxMed) Cafeteria & Multisport Supporting those, who are not yet eligible for full holiday entitlement, by expanding their pool from 20 to 25 days Relocation Help IP Tax Deductible Costs If that sounds exciting, then we'd love to hear from you. Tesco is committed to celebrating diversity and everyone is welcome at Tesco. As a Disability Confident Employer, we‚Äôre committed to providing a fully inclusive and accessible recruitment process, allowing candidates the opportunity to thrive and inform us of any reasonable adjustments they may require.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 18500, ""max"": 26000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,555,Senior Data Science/AI Engineer,N-iX,"Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management. Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact. Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company‚Äôs R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 21700, ""max"": 29000, ""type"": ""Net per month - B2B""}, {""min"": 17500, ""max"": 23950, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,556,Expert MS SQL DBA with Guidewire,Experis Manpower Group,"Start date: ASAP / 1 month / flexible Duration: Long term (6 months with further extensions) Work model: 100% remote Type of cooperation: B2B Overview We are looking for an experienced MS SQL Server and Guidewire Database Specialist to join a large-scale, long-term project in the UK insurance sector. The ideal candidate will have deep expertise in SQL performance optimization, Guidewire Insurance Suite, and Azure environments. You will be part of a dynamic, remote-first team working on complex database challenges in a scaled Agile setup. Responsibilities Optimize MS SQL Server queries for performance and resource efficiency, including Guidewire-specific queries Support and maintain databases within the Guidewire Insurance Suite environment Participate in Guidewire version upgrades from a database perspective, including production deployment Implement and support Guidewire Archiving solutions Collaborate with multiple Scrum teams in a large-scale Agile environment Communicate technical concepts effectively to business stakeholders Ensure data integrity and performance across environments hosted in Azure Requirements 5+ years of experience with MS SQL Server 2012 and newer Proven track record in query optimization and performance tuning Minimum 3 years of experience with Guidewire Insurance Suite databases In-depth knowledge of Guidewire table structures and data models Experience with at least 2 full end-to-end Guidewire upgrades, including production deployment Hands-on experience with Guidewire Archiving implementation Experience working in large-scale projects (30 Scrum teams / 300+ people) Strong communication skills, with the ability to tailor messaging to business audiences Previous experience in the UK insurance sector Experience working in Azure environments (certification is a plus) Advanced English proficiency Availability for remote work We offer B2B contract via Experis Access to Medicover healthcare Multisport card E-learning platform for continuous development Group insurance",[],Database Administration,Database Administration
Full-time,Mid,Permanent,Hybrid,557,Salesforce Consultant,Vaillant Group Business Services,"What we achieve together Are you ready to make a meaningful impact by joining our Service2Retention team? You will play a crucial role in enhancing service solutions for the Vaillant Group using Salesforce. In this position, you will have the opportunity to design and implement innovative solutions in collaboration with IT teams, customers, and product owners. Your work will span across Salesforce Service Cloud, Field Service (both Classic and LEX), and Marketing Cloud. Your responsibilities will also include writing and updating platform documentation, as well as conducting user trainings and demos to ensure everyone is equipped with the knowledge they need. You will manage stakeholder requirements and expectations through effective project management, ensuring that all parties are aligned and satisfied with the progress. Finally, you will contribute to a self-organizing and autonomous team, where your input and initiative are valued and encouraged. What makes us successful together Experience : You bring a minimum of 3 years of experience in designing salesforce or service processes. Ideally you designed and implemented such solutions in Salesforce Service Cloud or Field Service Know-how and skills: You have solid understanding of different types of service processes, such as orders, contracts, call center, invoicing, etc. Nice to have : You are familiar with Salesforce Marketing Cloud, including Journey Builder, Email Studio, and Automation Studio, experience in integrating Salesforce products and third-party applications, especially ERP systems. Personality : With your positive attitude and trustworthy personality, you can build strong stakeholder relationships. You are very well organized, have high standards for the quality of your work and communicate proactively and clearly. Language skills: You speak English fluently; Polish or German would be a plus. What makes us special Environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee Package of additional benefits: private medical care, multi-sport card. A fast growing, agile and very dynamic team that challenges established routines and helps transforming the Vaillant Group to a data informed business Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings","[{""min"": 17000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Unclassified,Unclassified
Full-time,Senior,Permanent or B2B,Hybrid,558,Senior Data Engineer,Remodevs,"Please note it's now remote role but later turns into hybrid - so only candidates from Warsaw and surroundings are required. About: We are seeking a highly motivated and self-driven data engineer for our growing data team -who is able to work and deliver independently and as a team. In this role, you will play a crucial part in designing, building and maintaining our ETL infrastructure and data pipelines. Major Responsibilities: ‚óè Design, develop, and deploy Python scripts and ETL processes with Prefect and Airflow to prepare data for analysis. ‚óè Model dimensional and denormalized schemas for optimal performance reporting and discovery. ‚óè Design AI-friendly DB schemas and ontologies. ‚óè Architect cloud ops solutions for data topologies. ‚óè Transform and migrate data with Python, DBT, and Pandas. ‚óè Work with event-based/streaming technologies for real-time ETL. ‚óè Ingest and transform structured, semi-structured, and unstructured data. ‚óè Optimize ETL jobs for performance and scalability to handle big data workloads. ‚óè Monitor and troubleshoot ETL jobs to identify and resolve issues or bottlenecks. ‚óè Implement best practices for data management, security, and governance with Prefect, DBT, and Pandas. ‚óè Write SQL queries, program stored procedures, and reverse engineer existing data pipelines. ‚óè Perform code reviews to ensure fit to requirements, optimal execution pattern,s and adherence to established standards. ‚óè Assist with automated release management and CI/CD processes. ‚óè Validate and cleanse data and handle error conditions gracefully. Skills ‚óè 3+ years of Python development experience, including Pandas ‚óè 5+ years writing complex SQL queries with RDBMSes. ‚óè 5+ years of Experience with developing and deploying ETL pipelines using Airflow, Prefect, or similar tools. ‚óè Experience with cloud-based data warehouses in environments such as RDS, Redshift, or Snowflake. ‚óè Experience with data warehouse design: OLTP, OLAP, Dimensions, and Facts. ‚óè Experience with Cloud-based data architectures, messaging, and analytics. Pluses: Experience with ‚óè Docker ‚óè Kubernetes ‚óè CI/CD automation ‚óè AWS lambdas/step functions ‚óè Data partitioning ‚óè Databricks ‚óè Pyspark ‚óè Cloud certifications","[{""min"": 23694, ""max"": 25517, ""type"": ""Net per month - B2B""}, {""min"": 23694, ""max"": 25517, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,559,AI Data Engineer,ITDS,"AI Data Engineer Join us, and build the future of AI-powered data systems! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As an AI Data Developer you will be working for our client, a global financial institution currently investing in advanced AI-driven solutions to modernize its internal data ecosystems. You will play a key role in a pioneering project that focuses on creating agents to collect and synchronize data from multiple systems, powering Retrieval-Augmented Generation (RAG) pipelines for GenAI models. This is a technically challenging role that involves building scalable data pipelines, designing microservices, and leading a collaborative team of developers within a high-impact cloud-based infrastructure. Designing and implementing scalable data pipelines to support GenAI systems Creating and managing microservices that integrate into a broader AI solution architecture Leading a team of developers and AI specialists across various project phases Developing and maintaining RESTful APIs using FastAPI Managing data sourcing processes for RAG using Python and time-series/analytics databases Overseeing deployment workflows using tools like Ansible and Jenkins Ensuring system reliability and maintainability within a Unix/Linux environment Coordinating with stakeholders to ensure requirements are captured and implemented Writing shell scripts for task automation and process monitoring Documenting technical specifications and system architecture Proven experience developing in Python within Unix/Linux environments Hands-on experience with time-series or analytics databases such as Elasticsearch Proficiency in building RESTful APIs using FastAPI Experience with Azure Cloud infrastructure and services Background in building microservices architectures Strong knowledge of data pipelines for RAG or similar GenAI applications Familiarity with Git/GitHub and automated deployment tools (Ansible, Jenkins) Basic shell-scripting and process automation skills Solid understanding of software development lifecycle (SDLC) practices Ability to work independently with a proactive, team-oriented mindset Experience with Generative AI APIs and frameworks Understanding of big data modeling using both relational and non-relational techniques Familiarity with cloud-native design patterns Excellent communication and documentation skills Willingness to quickly adapt to evolving project requirements We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7227 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 21000, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,Permanent or B2B,Hybrid,560,Technical Engineer with Oracle Forms and German,Eviden,"The future is our choice At Atos, as the global leader in secure and decarbonized digital, our purpose is to help design the future of the information space. Together we bring the diversity of our people‚Äôs skills and backgrounds to make the right choices with our clients, for our company and for our own futures. About the Role In this position, you will be responsible for 2nd level support of applications critical to the client's quality certification processes. You will work closely with users, analyze technical issues, and ensure the reliability of systems based on Oracle technologies and Unix environments. Strong communication skills in German are essential for understanding documentation and collaborating with stakeholders. Your Responsibilities Provide 2nd level support and incident resolution for business-critical applications Perform troubleshooting and root cause analysis using SQL , PL/SQL , and Unix Shell Maintain and enhance Oracle Forms -based solutions Interact with business users and technical teams in German Ensure accurate documentation and support within ITIL-based processes (e.g. incident, change) Contribute to process improvements and application performance tuning Required Skills & Experience Strong working knowledge of: SQL / PL/SQL Oracle Forms Unix and Shell scripting Microsoft Office Suite (Excel, Outlook, Word, PowerPoint) Fluent German ‚Äì both spoken and written (minimum B2‚ÄìC1 level) Nice to Have / Preferred Qualifications Experience in application support or management Basic programming experience (e.g. Bash, Python) Familiarity with: OBIEE Microsoft SharePoint ServiceNow (SMaaS) ITIL practices What‚Äôs in it for you? Wellbeing programs & work-life balance - integration and passion-sharing events Private medical and dental care Benefits platform ‚Äì shopping, cinema, sport etc. Co-funding of sports activities, e.g. Multisport & OK system cards, b-active program Courses and certifications e.g. Google Cloud, AWS, ITIL Conferences and Expert Communities Gift packages for special occasions: Easter, Christmas, Children‚Äôs Day Appreciation for seniority: additional days off, Atos Jubilee gifts Remote working or commuting allowance Charity and eco initiatives What happens next? Quick conversation with HR Interview with a Manager/IT expert/project representative Feedback (1-5 business days after the interview)",[],Unclassified,Unclassified
Full-time,Mid,Permanent,Remote,561,Power BI Developer,EndySoft,"Position Overview: We are seeking a talented Power BI Developer to join our team. The ideal candidate will have expertise in developing and implementing Power BI dashboards, reports, and data visualizations to support data-driven decision-making. This role involves collaborating with various stakeholders to gather requirements and deliver interactive, high-quality BI solutions. MD rate: 16600-20000 PLN Roles and Responsibilities: Design, develop, and deploy Power BI dashboards and reports based on business requirements. Connect to various data sources, including SQL databases , Excel , and cloud services, to create comprehensive BI solutions. Develop and maintain Power BI datasets , dataflows , and queries to ensure efficient and accurate data models. Implement DAX calculations and measures to enable advanced data analysis and insights. Optimize Power BI reports for performance and usability, ensuring quick load times and responsive user experiences. Collaborate with stakeholders to gather requirements and translate them into actionable dashboards and reports. Perform data analysis and create meaningful visualizations to support business strategies and operational decisions. Conduct user training and provide ongoing support for Power BI tools and solutions. Stay updated with the latest Power BI features and best practices to continuously enhance BI solutions. Required Skills and Experience: Proficiency in Power BI development, including report building, data modeling, and visualization. Strong knowledge of DAX (Data Analysis Expressions) for creating complex calculations and measures. Experience with SQL for querying and transforming data. Familiarity with data integration tools and methods for connecting Power BI to various data sources. Strong understanding of data modeling concepts , including star schema and snowflake schema . Experience with Power BI Service for publishing and managing dashboards. Knowledge of data governance and security in Power BI, including row-level security (RLS). Strong analytical and problem-solving skills. Excellent communication and collaboration abilities. Nice to Have: Experience with Power Query for data transformation. Familiarity with Azure Data Services such as Azure Synapse , Azure Data Factory , or Azure SQL Database . Knowledge of Power BI Paginated Reports . Experience with Python or R for advanced analytics within Power BI. Understanding of big data technologies such as Databricks or Spark . Exposure to Agile/Scrum methodologies. Experience with version control tools like Git for Power BI development. Additional Information: This role offers an exciting opportunity to work on impactful BI projects and transform data into actionable insights. If you are passionate about leveraging Power BI to drive business success, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Hybrid,562,Analityk Hurtowni Danych DWH,Upvanta sp. z o.o.,"Stanowisko: Analityk Hurtowni Danych DWH Lokalizacja: Wroc≈Çaw lub okolice (hybrydowo ‚Äì min. 1 dzie≈Ñ w biurze co tydzie≈Ñ lub raz na 2 tygodnie) Szukasz pracy, w kt√≥rej Twoje umiejƒôtno≈õci analityczne i techniczne naprawdƒô majƒÖ znaczenie? Do≈ÇƒÖcz do zespo≈Çu nowoczesnej instytucji finansowej, kt√≥ra inwestuje w rozw√≥j technologii i pracownik√≥w! Obecnie poszukujemy osoby na stanowisko Analityka Hurtowni Danych DWH, kt√≥ra wesprze zesp√≥≈Ç w rozwoju i utrzymaniu rozwiƒÖza≈Ñ opartych na hurtowniach danych. Je≈õli masz do≈õwiadczenie w pracy z du≈ºymi zbiorami danych, znasz SQL jak w≈ÇasnƒÖ kiesze≈Ñ i chcesz mieƒá realny wp≈Çyw na dzia≈Çanie system√≥w bankowych ‚Äì to og≈Çoszenie jest dla Ciebie. Zakres obowiƒÖzk√≥w: Analiza biznesowo-systemowa w kontek≈õcie hurtowni danych oraz projektowanie i implementacja struktur hurtowni. Modelowanie danych i tworzenie oraz interpretacja modeli ERD. Optymalizacja zapyta≈Ñ SQL oraz analiza du≈ºych wolumen√≥w danych (setki milion√≥w transakcji). Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi i biznesowymi w celu dostosowania rozwiƒÖza≈Ñ do wymaga≈Ñ klienta. Tworzenie i wdra≈ºanie po≈ÇƒÖcze≈Ñ miƒôdzy hurtowniƒÖ danych a ≈∫r√≥d≈Çami danych, takimi jak sFTP, Azure DB, onPremDB, QuickBase oraz Power BI. Utrzymanie hurtowni danych i procedur automatyzacji, diagnozowanie problem√≥w wydajno≈õciowych oraz naprawa b≈Çƒôd√≥w w ≈õrodowisku (np. Azure Data Factory, Azure SQL Server, maszyny wirtualne). Wykorzystanie system√≥w kontroli wersji (Git) do zarzƒÖdzania kodem oraz wsp√≥≈Çpracy z zespo≈Çami. Tworzenie dokumentacji technicznej i analitycznej zwiƒÖzanej z implementacjƒÖ i utrzymaniem hurtowni danych. Dodawanie procedur sk≈Çadowanych, tabel, trigger√≥w (T-SQL, Git, Visual Studio) oraz wdra≈ºanie zmian w ≈õrodowiskach produkcyjnych. Wymagania: Wykszta≈Çcenie wy≈ºsze z zakresu informatyki lub pokrewne. Minimum 2 lata do≈õwiadczenia w implementacji i zarzƒÖdzaniu hurtowniami danych. Bardzo dobra znajomo≈õƒá jƒôzyka SQL (T-SQL, PostgreSQL) na poziomie ≈õredniozaawansowanym. Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych (setki milion√≥w transakcji). Umiejƒôtno≈õƒá modelowania danych oraz tworzenia modeli ERD. Zdolno≈õci analityczne oraz umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w w kontek≈õcie hurtowni danych. Znajomo≈õƒá systemu kontroli wersji (Git) i narzƒôdzi do zarzƒÖdzania kodem. Praktyczna znajomo≈õƒá Microsoft Azure oraz Power BI. Mile widziane: Znajomo≈õƒá Oracle. Do≈õwiadczenie w modelowaniu i implementacji hurtowni danych w metodologii Data Vault. Znajomo≈õƒá narzƒôdzi do automatyzacji implementacji hurtowni danych (np. Wherescape). Znajomo≈õƒá technologii SSAS Tabular. Znajomo≈õƒá Python. Je≈õli czujesz, ≈ºe ta rola jest dopasowana do Twojego profilu zawodowego i chcesz pracowaƒá w ≈õrodowisku, kt√≥re stawia na rozw√≥j, wsp√≥≈Çpracƒô i nowoczesne technologie ‚Äì aplikuj ju≈º dzi≈õ! Do≈ÇƒÖcz do zespo≈Çu, kt√≥ry nie tylko analizuje dane, ale wykorzystuje je do tworzenia rozwiƒÖza≈Ñ realnie wspierajƒÖcych decyzje biznesowe.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,563,Senior Cloud Data Engineer (GCP),Future Processing,"Do naszej linii biznesowej Data Solutions poszukujemy osoby na stanowisko Senior Cloud Data Engineer ze znajomo≈õciƒÖ GCP masz min. 5 lat do≈õwiadczenia w IT, w tym min. 3,5 roku w pracy z danymi w chmurze GCP (potwierdzone projektami komercyjnymi wdro≈ºonymi na produkcje), budowa≈Çe≈õ(a≈õ) i utrzymywa≈Çe≈õ(a≈õ) hurtownie/data lake w BigQuery , ≈ÇƒÖczƒÖc wiele ≈∫r√≥de≈Ç danych, korzystasz z SQL i Python na poziomie zaawansowanym, znasz metodyki i stosujesz biegle Git oraz CI/CD , tworzysz i optymalizujesz rozwiƒÖzania przetwarzajƒÖce dane ( ETL , ELT , itp.) poprzedzone projektem technicznym oraz alternatywami rozwiƒÖza≈Ñ, masz praktyczne do≈õwiadczenie w przetwarzaniu danych przy u≈ºyciu Dataflow / Dataproc oraz orkiestracji Airflow (Cloud Composer) , monitoring, diagnostyka oraz rozwiƒÖzywanie problem√≥w w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanowaƒá infrastrukturƒô oraz obliczyƒá jej koszt, pracujesz w duchu DevSecOps & FinOps , znasz koncepcje BigLake / Lakehouse / Data Mesh , znasz architekturƒô SMP oraz MPP wraz z przyk≈Çadami rozwiƒÖza≈Ñ opartych o te architektury, potrafisz zaplanowaƒá infrastrukturƒô GCP, estymowaƒá i optymalizowaƒá jej koszty, masz do≈õwiadczenie w migracji rozwiƒÖza≈Ñ on-premise do chmury i w ochronie danych (IAM, DLP, GDPR), swobodnie wsp√≥≈Çpracujesz z klientem i interdyscyplinarnymi zespo≈Çami, pos≈Çugujesz siƒô j. angielskim na poziomie ≈õredniozaawansowanym (min. B2). odpowiedzialno≈õƒá end-to-end za rozwiƒÖzania data-platformowe tworzone wsp√≥lnie z zespo≈Çem, tworzenie i optymalizacjƒô potok√≥w ETL/ELT w GCP (BigQuery, Dataflow, Dataproc, Cloud Composer), tworzenie i modyfikowanie dokumentacji, budowanie i utrzymywanie katalogu oraz modeli danych zgodnie z najlepszymi praktykami Data Governance, analizƒô wymaga≈Ñ biznesowych i dob√≥r optymalnych rozwiƒÖza≈Ñ technologicznych, analizowanie potencjalnych zagro≈ºe≈Ñ, monitoring, diagnostykƒô i FinOps-owe optymalizacje koszt√≥w chmury.","[{""min"": 135, ""max"": 200, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,564,Data Engineer,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: projekt z bran≈ºy automotive, dotyczy przetwarza danych z maszyn i czƒô≈õci, aby wesprzeƒá klient√≥w na ca≈Çym ≈õwiecie i pom√≥c im w przekszta≈Çcaniu siƒô w pe≈Çni operacyjne, oparte na danych organizacje, projektowanie, rozw√≥j i utrzymanie rozwiƒÖza≈Ñ przetwarzania danych, tworzenie i optymalizacja przep≈Çyw√≥w danych w ≈õrodowisku Azure i Databricks, wsp√≥≈Çpraca z interesariuszami z dzia≈Ç√≥w IT i biznesu w celu dostarczania warto≈õciowych produkt√≥w danych, praca hybrydowa, 1 dzie≈Ñ w tygodniu z biura we Wroc≈Çawiu, stawka do 140 z≈Ç/h przy B2B. Ta oferta jest dla Ciebie, je≈õli: posiadasz minimum 4-letnie do≈õwiadczenie w roli Data Engineer, bardzo dobrze znasz: Databricks, Workflows, Python, Spark, SQL, masz do≈õwiadczenie z DevOps - praca z pipelineami i repozytoriami, dobrze znasz: Azure Data Factory, Azure Key Vault i Power BI, znasz jƒôzyk angielski na poziomie min. B2, cechujesz siƒô otwarto≈õciƒÖ umys≈Çu i zaanga≈ºowaniem. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 22000, ""max"": 23500, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,565,DataBricks Architect,CRODU,"üå¥ Forma pracy: d≈Çugoterminowo, fulltime, 100% zdalnie üëà ‚è∞ Start: ASAP üëà Cze≈õƒá! üëã Dla naszego klienta z USA poszukujemy DataBrick Architect√≥w. Prace dotyczƒÖ dzia≈Ça≈Ñ w obszarach m.in . migracji, zbierania danych i optymalizacji rozwiƒÖza≈Ñ opartych na DataBricks. Klient posiada sta≈Çe zapotrzebowanie na specjalist√≥w. Projekty kt√≥re prowadzƒÖ przewa≈ºnie sƒÖ kr√≥tkoterminowe (spore prawdopodobie≈Ñstwo na przed≈Çu≈ºenia projekt√≥w) i ze wzglƒôdu na sta≈Ço≈õƒá zapotrzebowania klient jest w stanie zaproponowaƒá nowy temat po zako≈Ñczeniu danego projektu. Obecnie poszukiwani sƒÖ specjali≈õci do projektu AI/ML z obszar√≥w healthcare. Projekt dotyczy analizy danych tekstowych i analizy obraz√≥w generowanych przez urzƒÖdzenia medyczne (rentgen, rezonans magnetyczny itp.). Zebrane dane bƒôdƒÖ migrowane do chmurowej bazy opartej na DataBricks. Platforma ma obs≈Çugiwaƒá ca≈Çy cykl ≈ºycia danych w zgodzie z wbudowanymi funkcjonalno≈õciami zapewniajƒÖcymi zgodno≈õƒá z przepisami, mo≈ºliwo≈õƒá przeprowadzania audyt√≥w, tworzenia kohort czy wt√≥rnego wykorzystania modeli. Celem jest rozwiƒÖzanie problem√≥w zwiƒÖzanych z istniejƒÖcymi systemami zarzƒÖdzania danych (rozproszone ≈∫r√≥d≈Ça, rƒôczne procesy, niewystarczajƒÖce bezpiecze≈Ñstwo). Poszukujemy os√≥b, kt√≥re biegle znajƒÖ Pythona. Dla klienta kluczowe jest obycie w ≈õrodowiskach chmurowych oraz znajomo≈õƒá DataBricks i Apache Spark. Projekty prowadzone przede wszystkim dla firm z USA - w wiƒôkszo≈õci przypadk√≥w wymagana jest praca jedynie z niewielkƒÖ zak≈ÇadkƒÖ godzinowƒÖ (np. od 10: 00 do 18: 00) natomiast jeste≈õmy w stanie dogadaƒá siƒô je≈õli chodzi o godziny pracy. Og√≥lny zakres obowiƒÖzk√≥w: üìç Stworzenie ≈õrodowiska i architektury platformy na DataBricks üìç Kontakt z biznesem pod kontem ustale≈Ñ projektowych üìç Zapewnienie bezpiecze≈Ñstwa przechowywanie danych üìç Przetwarzanie i indeksowanie danych DICOM üìç Walidacja danych, tworzenie pipeline'√≥w przetwarzania danych, tworzenie i udostƒôpnianie kohort üìç Zaplanowanie i przeprowadzenie migracji baz danych üìç ≈öcis≈Ça wsp√≥≈Çpraca z zespo≈Çem ( m.in . data engineers, data scientists, informatycy kliniczni, zesp√≥≈Ç wsparcia) Wymagania: ‚ö°Ô∏è Solidne do≈õwiadczenie w pracy w roli data engineera lub pokrewnych rolach (8+ lat) ‚ö°Ô∏è Bardzo dobra znajomo≈õƒá platformy DataBricks oraz Apache Spark ‚ö°Ô∏è Bardzo dobra znajomo≈õƒá Python ‚ö°Ô∏è Do≈õwiadczenie w przeprowadzaniu migracji chmurowych ‚ö°Ô∏è Do≈õwiadczenie w pracy w ≈õrodowisku AWS (Amazon s3) ‚ö°Ô∏è Do≈õwiadczenie w prowadzeniu projekt√≥w zwiƒÖzanych z AI/ ML ‚ö°Ô∏è Umiejƒôtno≈õci interpersonalne i zespo≈Çowe ‚ö°Ô∏è Umiejƒôtno≈õƒá podejmowania inicjatywy i samodzielno≈õƒá ‚ö°Ô∏è Angielski na poziomie umo≈ºliwiajƒÖcym swobodnƒÖ komunikacjƒô w zespole Mile widziane: ‚ö°Ô∏è Do≈õwiadczenie w pracy w ≈õrodowisku innych ≈õrodowiskach chmurowych (np. Azure - Data Factory, Synapse, Logic Apps, Data Lake) ‚ö°Ô∏è Do≈õwiadczenie w projektowaniu i optymalizacji przep≈Çyw√≥w danych za pomocƒÖ, DBT, SSIS, TimeXtender lub podobnych rozwiƒÖza≈Ñ (ETL, ELT) ‚ö°Ô∏è Do≈õwiadczenie z dowolnymi platformami big data lub noSQL (Redshift, Hadoop, EMR, Google Data itp.) Jak dzia≈Çamy i co oferujemy? üéØ Stawiamy na otwartƒÖ komunikacjƒô zar√≥wno w procesie rekrutacji jak i po zatrudnieniu - zale≈ºy nam na klarowno≈õci informacji dotyczƒÖcych procesu i zatrudnienia üéØ Do rekrutacji podchodzimy po ludzku, dlatego upraszczamy nasze procesy rekrutacyjne, ≈ºeby by≈Çy mo≈ºliwie jak najprostsze i przyjazne kandydatowi üéØ Pracujemy w imiƒô zasady ""remote first"", wiƒôc praca zdalna to u nas norma, a wyjazdy s≈Çu≈ºbowe ograniczamy do minimum üéØ Oferujemy prywatnƒÖ opiekƒô medycznƒÖ (Medicover) oraz kartƒô Multisport dla kontraktor√≥w","[{""min"": 200, ""max"": 250, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Mid,Permanent,Hybrid,566,Data Engineer (She/He/They),Accenture,"WHO WE ARE: To find information about Accenture Technology Poland please visit our website: Poland | Let There Be Change | Accenture THE WORK: We carry out projects for clients in the telecommunications, media, and new technologies sectors, focusing on online and batch data processing using Big Data technologies. Among the projects we have completed are the construction and development of Data Hub/Data Lake solutions for a major telecommunications operator and one of the largest social media companies in the world. Flexible: The work location for this role may include a mix of working remotely, onsite at a client or in an Accenture office - depending on specific project circumstances WHAT‚ÄôS IN IT FOR YOU? Work on projects for international clients, expanding your design skill set. Thrive in an exciting workplace that values and celebrates diverse perspectives. Contribute to a creative environment where your ideas are welcomed. Engage in creative projects that foster your development as a designer. Access continuous learning through workshops and skill-building initiatives. WHAT WE OFFER? Individual support of a People Lead and a specific path of professional development, as well as the possibility of a session with a Coach. A wide training package (soft, technical, and language training offer, access to the e-learning platforms, Gallup test, GenAI training, possibility of co-financing courses, we provide full financing for courses and certifications, as well as technical and soft training) Employee Assistance Program - legal, financial, and psychological consultations. Accenture employees eligible for the Employee share purchase plan automatically become eligible for quarterly dividends if they own company shares. Paid employee referral program. Private medical care, life insurance. Access to the MyBenefit platform (possibility of using a wide range of products and services, including the Multisport card). WHAT WE BELIEVE: Accenture does not discriminate employment candidates on the basis of race, religion, color, sex, age, disability, national origin, political beliefs, trade union membership, ethnicity, denomination, sexual orientation or any other basis impermissible under Polish law. All our leaders are committed to building a better, stronger and more durable company for future generations to create positive, long-lasting change. Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and creative, which helps us better serve our clients and our communities. Our position as partner to many of the world‚Äôs leading businesses, organizations and governments affords us both an extraordinary opportunity and a tremendous responsibility to make a difference. Sustainability is one of our greatest responsibilities, which we embed it into everything we do and for everyone we work with. show less Qualifications HERE‚ÄôS WHAT YOU‚ÄôLL NEED: Interest in Big Data or data warehousing and a willingness to learn new tools and cloud technologies. Good knowledge of SQL & Python focused on data transformations Knowledge PySpark, Spark SQL, Presto and Airflow is additional advantage. Engagement in working within an international, interdisciplinary team. Fluency in English, both spoken and written (B2 level).",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,567,"Data Scientist (Python, Node, AI)",Link Group,"Tworzenie i rozwijanie modeli AI z wykorzystaniem Pythona oraz Node.js , w tym integracja z bazami danych wektorowymi oraz zaawansowanymi narzƒôdziami AI. Rozw√≥j API do integracji rozwiƒÖza≈Ñ AI i danych zewnƒôtrznych, z wykorzystaniem FastAPI lub Flask w Pythonie oraz Node.js . Integracja aplikacji w chmurze ‚Äì wdra≈ºanie rozwiƒÖza≈Ñ w chmurze, w tym tworzenie i zarzƒÖdzanie ≈õrodowiskami do trenowania i implementacji modeli AI. Praca z kontenerami Docker w celu uruchamiania aplikacji i modeli AI w izolowanych ≈õrodowiskach. Wykorzystanie LangChain i Prompt Engineering do generowania wynik√≥w i interakcji z modelami AI na poziomie przetwarzania jƒôzyka naturalnego (NLP). ZarzƒÖdzanie bazami danych wektorowymi ‚Äì integracja rozwiƒÖza≈Ñ AI z bazami danych, kt√≥re przechowujƒÖ dane w postaci wektor√≥w (np. FAISS , Pinecone ). Optymalizacja proces√≥w AI, zapewniajƒÖc skalowalno≈õƒá, wydajno≈õƒá i jako≈õƒá wynik√≥w. Analiza wynik√≥w i generowanie raport√≥w wspierajƒÖcych decyzje biznesowe oparte na danych. Minimum 4 lata do≈õwiadczenia w pracy jako Data Scientist z do≈õwiadczeniem w tworzeniu rozwiƒÖza≈Ñ AI. Doskona≈Ça znajomo≈õƒá Pythona oraz Node.js w kontek≈õcie budowania aplikacji i rozwiƒÖza≈Ñ AI. Do≈õwiadczenie w tworzeniu API ‚Äì wykorzystanie FastAPI lub Flask w Pythonie oraz Node.js do budowy i integracji rozwiƒÖza≈Ñ. Znajomo≈õƒá us≈Çug chmurowych ‚Äì Azure lub AWS oraz tworzenie rozwiƒÖza≈Ñ AI w tych ≈õrodowiskach. Do≈õwiadczenie z Dockerem oraz konteneryzacjƒÖ aplikacji i modeli AI. Znajomo≈õƒá LangChain i Prompt Engineering ‚Äì umiejƒôtno≈õƒá optymalizacji interakcji z modelami AI. Dobre umiejƒôtno≈õci analityczne ‚Äì zdolno≈õƒá do pracy z danymi, analizowania wynik√≥w i generowania raport√≥w. Doskona≈Çe umiejƒôtno≈õci komunikacyjne w jƒôzyku angielskim, umo≈ºliwiajƒÖce wsp√≥≈Çpracƒô z miƒôdzynarodowymi zespo≈Çami. Bardzo dobrej komunikacji w jƒôzyku angielskim.","[{""min"": 22000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Hybrid,568,Data Engineer (Spark),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. bran≈ºy finansowej. Wykorzystywany stos technologiczny: Scala, Apache Spark, SQL/HQL, Airflow, Control-M, Jenkins, GitHub, Hive, Databricks, Azure, AWS S3, tworzenie kompleksowych proces√≥w ETL z wykorzystaniem Spark/Scala ‚Äì obejmujƒÖcych transfer danych do/z Data Lake, walidacje techniczne oraz implementacjƒô logiki biznesowej, dokumentowanie tworzonych rozwiƒÖza≈Ñ w narzƒôdziach takich jak: JIRA, Confluence czy ALM, weryfikacja jako≈õci dostarczanego rozwiƒÖzania ‚Äì projektowanie i przeprowadzanie test√≥w integracyjnych, zapewniajƒÖcych jego poprawne dzia≈Çanie i zgodno≈õƒá z innymi komponentami, praca w modelu hybrydowym: 1-2 razy w tygodniu praca z biura w Warszawie, stawka do 180z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz co najmniej 7 lat do≈õwiadczenia w programowaniu, przez minimum 3 lata pracujesz z technologiami Big Data, najlepiej Spark i Scala, znasz podej≈õcie Agile/Scrum, korzystasz z narzƒôdzi Continuous Integration ‚Äì Git, GitHub, Jenkins, swobodnie pos≈Çugujesz siƒô SQL, komunikujesz siƒô w jƒôzyku angielskim na poziomie minimum B2, dodatkowym atutem bƒôdzie: do≈õwiadczenie z technologiami frontendowymi: React, TypeScript, Next.js, Qlik, umiejƒôtno≈õƒá pisania skrypt√≥w w Bash, znajomo≈õƒá narzƒôdzia Control-M, znajomo≈õƒá Docker, Kubernetes, AWS S3, Azure, AWS. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 25200, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,569,Senior Consultant - Data Management & Governance,EPAM Systems,"We are seeking an accomplished, forward-thinking leader in Data Management & Governance to work collaboratively with our client in delivering exceptional business outcomes. In this role, you‚Äôll engage closely with senior executives, shape impactful strategies, and leverage modern data technologies to drive organizational transformation. If you‚Äôre passionate about solving complex challenges and redefining data management excellence, this is the perfect opportunity. Our client is investing heavily in their Data Management & Governance consulting practice, combining efficient data governance with cutting-edge technology to build a dynamic, high-growth environment centered on innovation. With an agile culture, they empower you to tackle complex, high-visibility challenges for leading global organizations. This isn‚Äôt just a job ‚Äì it‚Äôs your chance to be at the forefront of data-driven transformation and make a lasting impact. If you‚Äôre eager to join a high-performing team where your expertise shapes strategic initiatives, we‚Äôd love to hear from you. Responsibilities Engage with senior executives to define and execute enterprise-wide Data Management & Governance strategies, often becoming a ‚Äútrusted advisor‚Äù Lead assessments to help clients advance from current to optimal data governance maturity by developing strategies, roadmaps, and implementation plans Leverage deep business and IT expertise to build Data Management & Governance solutions that drive innovation and create value across client organizations Deliver end-to-end services, from strategy ideation and use case elaboration to MVP implementation and scaling for enterprise adoption Guide architectural teams and lead discussions on modern Data Management architecture, ensuring best practices for data lifecycle management Support business development efforts, including crafting proposals, Statements of Work, and delivering client presentations Develop consulting offerings and reusable templates to enhance delivery efficiency Requirements Bachelor's or Master's degree in information technology or a related field At least 3 years of experience in a consulting role, leading Data Management & Governance initiatives In-depth knowledge in 2-3 of the following areas: Data Governance Operating Models & Policies, Data Quality, Master Data Management, Metadata Management & Data Cataloguing, Data Lineage, or Data Compliance Familiarity with data governance frameworks like DAMA or DCAM, and an understanding of their practical implementation Proven ability to build relationships with directors and C-level stakeholders, understand business needs, and deliver data-driven insights Experience conducting maturity assessments, gap analyses, and developing strategic roadmaps with implementation guidelines Skilled in running client workshops to identify priorities, assess current states, and evaluate readiness for enterprise Data Management & Governance initiatives Knowledge of market trends, technology selection models, and best practices tailored for large-scale application Hands-on experience with the modern data stack, working in agile environments with advanced technologies Ability to work independently, manage small projects with multiple workstreams, or oversee parts of larger engagements Strong verbal and written English skills for leading discussions and producing clear, concise documentation Nice to have Experience in leading data transformation projects Technology expertise with one or several commercial or open-source tools, e.g. Collibra, Alation, Informatica, Ataccama, Azure Purview, Atlas, Talend, Soda Understanding of modern concepts as Data Observability, DataOps and Data Mesh Technology expertise in modern Big Data & Cloud stack: Spark, Snowflake, Databricks, etc Practical expertise in implementation of Data Governance for modern cloud data lakes We offer We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Unclassified,Unclassified
Full-time,Senior,B2B,Remote,570,Data Architect ETRM,INFOPLUS TECHNOLOGIES,"Exciting Opportunity: ETRM Data Architect (Remote, Poland) | 10+ Years Experience | 2000 PLN/day About the Role: As an ETRM Data Architect, you will be the driving force behind creating robust data transformation architectures, working closely with trading systems like Allegro, RightAngle, Endur, and more. Your expertise in Python, Streamlit, and Azure data technologies will enable us to deliver seamless, scalable solutions in the dynamic energy trading domain. What You‚Äôll Do: Lead the design and development of cutting-edge data transformation solutions using Python. Build and optimize interactive web APIs leveraging Streamlit to enhance user engagement. Collaborate with cross-functional teams to translate business needs into high-performance technical solutions. Work extensively with Azure Data Factory, Data Lake, Snowflake, and Databricks to manage complex data workflows. Ensure data quality, integrity, and security throughout all processes. Drive continuous improvements for performance, scalability, and efficiency. Provide technical mentorship to junior team members and promote best practices. Bring your expertise in ETRM systems, especially in Gas and Power markets, to the forefront. What We‚Äôre Looking For: 10+ years of hands-on experience in data engineering, transformation, and architecture Strong proficiency in Python, Kafka, and Streamlit Deep experience with Azure Data Factory, Data Lake, Snowflake, Databricks Well-versed in DevOps practices (CI/CD) Proven ability to design scalable, real-time data processing systems Strong analytical, problem-solving, and communication skills Familiarity with ETRM systems and energy trading concepts (Allegro, RightAngle, Endur) is a big plus Why Join Us? Remote work from anywhere in Poland/EU Lead innovative projects in the fast-evolving energy sector Collaborative environment with passionate professionals","[{""min"": 38000, ""max"": 40000, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Mid,B2B,Remote,571,Programista Baz Danych MS-SQL,People More P.S.A.,"Cze≈õƒá! Nazywamy siƒô People More bowiem traktujemy naszych wsp√≥≈Çpracownik√≥w z szacunkiem ale r√≥wnie≈º dlatego, ≈ºe projekty przy kt√≥rych pracujemy sƒÖ dla ludzi i powinny byƒá ≈Çatwe oraz przyjemne w u≈ºyciu. Jeste≈õmy technologiczni ale patrzymy szerzej : ) People More istnieje ju≈º ponad cztery lata i wywodzi siƒô z jednej z najstarszych agencji interaktywnych w kraju - Insignia. Sp√≥≈Çkƒô tworzƒÖ osoby z ogromnym zapleczem klient√≥w w kraju i zagranicƒÖ dla kt√≥rych budujemy projekty od zera (UX, UI, frontend, backend, mobile) lub w czƒô≈õci. Tworzymy bezpo≈õrednio dla naszych klient√≥w jak r√≥wnie≈º wspieramy naszych partner√≥w w pracy nad w≈Çasnymi rozwiƒÖzaniami. Gwarantujemy tym samym bogaty wachlarz projekt√≥w i mo≈ºliwo≈õƒá ich zmian! Pracujemy przy projektach z ca≈Çego ≈õwiata. Do projektu naszego bezpo≈õredniego klienta poszukujemy specjalisty na stanowisko Programista Baz Danych MS-SQL Zakres obowiƒÖzk√≥w: Projektowanie i rozw√≥j struktur baz danych MS SQL Tworzenie oraz optymalizacja zapyta≈Ñ i procedur sk≈Çadowanych (T-SQL) Wsparcie zespo≈Ç√≥w projektowych w obszarach zwiƒÖzanych z bazami danych ≈öcis≈Ça wsp√≥≈Çpraca z developerami .NET Diagnozowanie problem√≥w i wsparcie w zakresie dzia≈Çania aplikacji oraz modyfikacja danych Generowanie raport√≥w zgodnie z ustalonym harmonogramem Wymagania: Minimum 2 lata do≈õwiadczenia w programowaniu MS SQL, T-SQL Bardzo dobra znajomo≈õƒá SQL oraz procedur sk≈Çadowanych Umiejƒôtno≈õƒá priorytetyzacji zada≈Ñ i dobrej organizacji pracy w≈Çasnej Znajomo≈õƒá ≈õrodowiska .NET Znajomo≈õƒá systemu kontroli wersji GIT Znajomo≈õƒá narzƒôdzi RED GATE (SQL Source Control, SQL Prompt) Umiejƒôtno≈õƒá analizy potrzeb wewnƒôtrznego klienta i dostosowania rozwiƒÖza≈Ñ Znajomo≈õƒá jƒôzyka angielskiego i polskiego O nas / co oferujemy: Jeste≈õmy otwarci, szczerzy i rozwiƒÖzujemy problemy zamiast je generowaƒá. Mo≈ºe to oczywiste ale naprawdƒô szanujemy pracownik√≥w i wsp√≥≈Çpracownik√≥w. My te≈º byli≈õmy programistami i cenimy tƒô pracƒô Miƒôdzynarodowe ≈õrodowisko i projekty PrywatnƒÖ opiekƒô medycznƒÖ Kartƒô sportowƒÖ Praca w 100% zdalna (chyba ≈ºe preferujesz inny system) Mamy biuro w Krakowie, ale je≈õli lubisz pracowaƒá zdalnie, nie ma sprawy. Nie mamy nic przeciwko pracy w pe≈Çni zdalnej. Dla nas mo≈ºesz znajdowaƒá siƒô w dowolnym miejscu : ) Dlaczego warto pracowaƒá z People More? Je≈õli nie jeste≈õ zadowolony ze swojej pracy lub zada≈Ñ, wsp√≥lnie znajdziemy wyj≈õcie! Je≈õli siƒô znudzisz, zaproponujemy Ci nowy produkt i nowe, fascynujƒÖce zadania Wsp√≥lnie popracujemy nad TwojƒÖ markƒÖ: bƒôdziesz mia≈Ç okazjƒô uczestniczyƒá w konferencjach, w tym jako prelegent, pomo≈ºemy Ci publikowaƒá w uznanych czasopismach i online U≈Çatwimy Ci dostƒôp do wyzwa≈Ñ, kt√≥re zazwyczaj sƒÖ trudne do zdobycia. W ka≈ºdej chwili mo≈ºesz porozmawiaƒá bezpo≈õrednio z zarzƒÖdem People More - m√≥wimy Twoim jƒôzykiem, poniewa≈º za≈Ço≈ºyciele firmy sƒÖ programistami i projektantami! Jak wyglƒÖda proces rekrutacji? Przyjazna, zdalna rozmowa wstƒôpna Zdalna rozmowa techniczna Decyzja o podjƒôciu wsp√≥≈Çpracy!","[{""min"": 110, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,B2B,Remote,572,Manager Database Administrator,Link Group,"Role Overview We are looking for an experienced and versatile Senior Database Administrator to step into a managerial role within our growing IT team. This is an exciting opportunity for someone passionate about database technologies who is also ready to lead and develop a team. You will be responsible for ensuring the reliability, performance, and security of our database systems while driving strategic improvements and mentoring your team. Design, implement, and maintain robust, high-performance, and secure database environments. Monitor system performance, proactively troubleshoot issues, and optimize efficiency. Oversee backup strategies, ensure successful recovery processes, and manage disaster recovery plans. Lead, coach, and mentor a team of database professionals, supporting their growth and daily activities. Partner with cross-functional teams to understand and deliver on database requirements aligned with business objectives. Promote a culture of learning, innovation, and continuous improvement. Define and execute database strategies that align with broader IT and organizational goals. Stay updated on emerging technologies and best practices, recommending and implementing upgrades and enhancements. Maintain thorough documentation of database architecture, procedures, and configurations. Ensure adherence to security, data governance, and compliance standards. Provide technical guidance and training to IT staff and end-users. Work closely with development teams to optimize database integration and performance for business-critical applications. Bachelor‚Äôs degree in Computer Science, Information Technology, or a related field. At least 5 years of experience as a DBA, including proven experience in a leadership or managerial role . Strong leadership and team management skills, with ability to mentor and inspire technical teams. Deep expertise with database systems such as Oracle, SQL Server, or MySQL. Strong background in database design, performance tuning, and optimization. Excellent problem-solving, analytical, and decision-making skills. Strong communication and interpersonal abilities to effectively collaborate across teams. Experience working with cloud database platforms (e.g., AWS, Azure). Understanding of data warehousing and business intelligence concepts. Familiarity with database security standards and best practices.","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Database Administration,Database Administration
Full-time,Senior,Permanent,Hybrid,573,Senior + / Staff Big Data Engineer,The Stepstone Group Polska,"At The Stepstone Group, we have a simple yet very important mission: The right job for everyone. Using our data, platform, and technology, we create opportunities for job seekers and companies around the world to find a perfect match, in fair and equitable way. With over 20 brands across 30+ countries, we strive for fair and unbiased hiring. At our Tech Hub, located near Wilanowska Metro, we are here as more than 300 ambitious specialists who work on the development of our IT products. We are proud to be part of The Stepstone Group, a global expert in job-tech platforms and e-recruiting. Join our team of 4,000+ employees and be part of reshaping the labour market and becoming the world‚Äôs leading job-tech platform. The job at a glance We‚Äôre looking for a Staff Big Data Engineer who is passionate about building meaningful, data-driven products that have real-world impact. In this hands-on and strategic role, you‚Äôll design and lead the development of secure, scalable data solutions that power intelligent search, matching, and recommendations across our global platform. Working within the Marketplace Enablement portfolio in the Data Products and Search & Match function, you‚Äôll collaborate with a diverse mix of engineers, data scientists, product managers, and stakeholders to drive innovation and build with purpose. This is a hybrid role based in Warsaw, with flexible remote working options. Your responsibilities Lead the architecture and development of big data solutions within a modern cloud environment (AWS) Design, build, and scale batch and real-time data pipelines that support inclusive, AI-driven features Partner with teams across the business to align data products with user needs and long-term goals Establish best practices in Big Data Engineering, fostering a culture of quality, learning, and collaboration Mentor engineers across teams, helping them grow while raising the overall standard of our work Actively contribute to our big data engineering community, sharing knowledge and supporting others Your skills and qualifications Solid experience designing and deploying scalable, secure big data systems Proficiency in Python, Apache Spark / PySpark, and AWS big data technologies Familiarity with tools like Apache Airflow, AWS Step Functions, or AWS Glue Workflows Understanding of batch and real-time processing; experience with Kafka is a plus Solid understanding of the software development lifecycle, CI/CD, containerization, observability, and security Strong collaboration, communication, and mentoring skills Comfortable working in a cross-functional environment and advocating for best practices A growth mindset, with a passion for inclusive technology and solving meaningful problems Fluency in English; experience in agile environments is a plus Benefits We‚Äôre a community here that cares as much about your life outside work as how you feel when you‚Äôre with us. Because your job shouldn‚Äôt take over your life, it should enrich it. Here are some of the benefits we offer: Medical and dental care Life insurance Benefit platform budget Employee Referral Programme Hackathons, Knowledge Sharing Hours In-house projects Events and integration parties Charity initiatives, 2 extra volunteer days English/German lessons Game room and chillout zone Our commitment Equal opportunities are important to us. We believe that diversity and inclusion at The Stepstone Group are critical to our success as a global company, so we want to recruit, develop, and keep the best talent. We encourage applications from everyone, regardless of background, gender identity, sexual orientation, disability status, ethnicity, belief, age, family or parental status, and any other characteristic.","[{""min"": 18000, ""max"": 30500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,574,Senior Data Engineer,co.brick,"Dla naszego klienta z sektora fintech poszukujemy do≈õwiadczonych Senior Data Engineer√≥w , kt√≥rzy pomogƒÖ zbudowaƒá i rozwinƒÖƒá nowoczesnƒÖ platformƒô danych opartƒÖ na rozwiƒÖzaniach Snowflake i Databricks . Projekt rusza ""na ju≈º"" i zak≈Çada d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô (min. 9‚Äì12 miesiƒôcy). Szukamy specjalist√≥w z do≈õwiadczeniem w pracy z danymi bankowymi, kt√≥rzy potrafiƒÖ zar√≥wno projektowaƒá architekturƒô, jak i implementowaƒá niezawodne, skalowalne rozwiƒÖzania ETL/ELT w ≈õrodowisku chmurowym. W projekcie: Tworzenie i optymalizacja data pipelines (ETL/ELT) Projektowanie architektury data lake / lakehouse Praca z big data frameworks (np. Spark) i rozwiƒÖzaniami cloud DWH (Snowflake, Databricks, Redshift, Synapse) Udzia≈Ç w tworzeniu API oraz integracji us≈Çug danych Rozwijanie dashboard√≥w analitycznych z u≈ºyciem React i BI tools Proaktywne podej≈õcie do technologii i wsp√≥≈Çtworzenie zespo≈Çu od podstaw Kogo szukamy: Min. 5 lat do≈õwiadczenia jako Data Engineer Zaawansowana znajomo≈õƒá Python oraz SQL Bardzo dobra znajomo≈õƒá: Snowflake , Databricks , Spark Praktyka w architekturze data lake / lakehouse Do≈õwiadczenie w ≈õrodowisku finansowym lub bankowym ‚Äì du≈ºy plus Znajomo≈õƒá proces√≥w CI/CD , wersjonowania (Git), REST API Mile widziana znajomo≈õƒá: React, Kafka, Airflow, Docker, Azure/AWS Jƒôzyk angielski ‚Äì C1 (praca w miƒôdzynarodowym zespole) Proaktywno≈õƒá i gotowo≈õƒá do brania odpowiedzialno≈õci za obszary techniczne Co oferujemy: Projekt minimum na 9‚Äì12 miesiƒôcy Zesp√≥≈Ç miƒôdzynarodowy ‚Äì startujemy z 2 osobami, zesp√≥≈Ç docelowo 6+ 100% zdalnie Start: ASAP",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,575,Data Integrations Engineer,dotLinkers,"Position: Data Integrations Engineer Working model: Fully remote Employment type: B2B Salary: 6 000 ‚Äì 9 000 USD B2B/month We‚Äôre working with a fast-growing US tech company revolutionizing how organizations manage commissions, integrations, and operational data. As part of their Professional Services team, you‚Äôll help bridge their product with third-party systems like Workday, Salesforce, and BigQuery‚Äîthrough both hands-on scripting and strategic integrations work. This is a backend-leaning role with some customer interaction, best suited for someone who‚Äôs solid with SQL, APIs, and light Python/JavaScript. If you‚Äôre interested in delivering real business impact ‚Äì this one‚Äôs for you. Responsibilities: Build and maintain managed integrations between third-party platforms and the client‚Äôs system (e.g. Workday, Sage Intacct, MS Dynamics) Use tools like Workato and Snowflake to ingest and process customer data Handle support tickets related to integrations and act as the SME for integration architecture Work directly with enterprise clients and help design scalable data connection flows Support implementations and unblock client-facing solutions when product limitations arise Requirements: Strong command of SQL and REST APIs (including JSON) Working knowledge of Python and JavaScript (for light scripting) Familiarity with CRM/HRIS systems like Salesforce , Workday , Rippling , or MS Dynamics Comfort speaking with customers‚Äîideally 1‚Äì2 years of client-facing or cross-functional experience Nice to have: experience with SSO/SCIM , ETL tools , or integration platforms (e.g. Workato) What‚Äôs in it for you: Fully remote work Work directly with a senior Director in a flat, agile team structure Be part of a well-funded scale-up with IPO ambitions and an internal culture of excellence Working hours: Must have overlap with US Eastern Time (EST) for at least 4‚Äì5 days/week Full overlap expected for first 3 months (onboarding period) Flexibility increases post-ramp-up No formal on-call, but some flexibility is appreciated during launches","[{""min"": 21872, ""max"": 32808, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,B2B,Remote,576,Clinical Database Programmer,Antal Sp. z o.o.,"üìç Location: Poland / Remote‚è∞ Employment Type: Full-timeüìÑ Contract Type: B2Büè¢ Industry: Clinical Research / Pharma / CRO We are seeking a skilled Medidata Rave EDC Study Builder / Clinical Data Programmer to join our clinical data team. In this role, you will be responsible for developing and maintaining clinical trial databases using Medidata Rave EDC , ensuring compliance with Good Clinical Practice (GCP) and Good Documentation Practice (GDP) . You will collaborate closely with Data Managers, Study Teams, Vendors, and Site Staff throughout the clinical study lifecycle. Develop, test, validate, and maintain Medidata Rave EDC clinical trial databases. Create EDC design specifications: data dictionary, event definitions, eConsent, edit checks, branching logic, advanced query rules, calculated fields, dynamic forms, and event rules. Collaborate with Data Managers and Study Teams to design and build the database using global eCRF libraries. Configure and optimize user interfaces for various data collection methods (eCOA, EDC). Develop test scripts and coordinate User Acceptance Testing (UAT) of the EDC system. Coordinate database go-live activities and manage production deployments. Perform batch data imports and support secondary data source transfers (e.g., lab or site data). Support internal and external audits and inspections as a subject matter expert. Ensure adherence to company SOPs, guidelines, and regulatory standards (GCP/GDP). Contribute to the development of EDC design standards to improve quality and efficiency. Medidata Rave EDC Certified Study Builder OR an equivalent combination of: Medidata Rave Study Design and Build Essentials ‚Äì Data Managers Medidata Rave Study Design and Build Essentials ‚Äì Data Validations Hands-on experience building and validating Rave EDC systems in clinical trial settings. Familiarity with GCP, GDP, and regulatory compliance processes. Strong understanding of clinical data flow, SDTM mapping, and EDC best practices. Excellent communication skills and ability to work collaboratively in cross-functional teams. Experience working with SOPs and within a regulated environment. Experience with integration to enterprise data warehouses. Prior exposure to medical device studies (e.g., Boston Scientific). Experience with Rave Migration and Version Management tools. To learn more about Antal, please visit www.antal.pl","[{""min"": 100, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Any,Hybrid,577,Data Modeller,Antal Sp. z o.o.,"Role Description For our Client a Top Leader in Banking & Financial Industry we are seeking a skilled Data Modeller, based in Krak√≥w. The successful candidate will be responsible for designing, developing, and maintaining robust data models to support business and technical requirements. Daily duties will include working with data architects and business stakeholders to translate business needs into efficient data structures, ensuring data quality and integrity, and contributing to data governance initiatives. Tasks also involve the identification and resolution of data-related issues, documentation of data models, and support for database development and optimization activities. Company Description Our Client is one of the world‚Äôs largest banking and financial services organizations, serving millions of customers globally. The core values emphasize open-mindedness, sustainability, and putting the customer at the heart of everything they do. They operate across a diverse range of financial products and services, with a strong focus on responsible business practices and professional development. Requirements Proven experience as a Data Modeller or in a similar data-centric role Strong knowledge of data modelling methodologies (e.g., ER, dimensional, NoSQL models) Proficiency in data modelling tools and database management systems Familiarity with data governance, data quality, and metadata management Excellent analytical and problem-solving skills Effective communication skills in English, both written and verbal Ability to collaborate with technical and non-technical stakeholders Bachelor‚Äôs degree (or higher) in Computer Science, Information Systems, or a related field What We Offer Competitive salary range: Please contact us for details Comprehensive benefits package, including private medical care and life insurance Support for professional development and certification Opportunities for internal mobility and career progression Hybrid work arrangements and flexible working hours Access to employee assistance programmes Inclusive work environment with a focus on work-life balance Recruitment Process Application Review (Nowy): Initial review of applications received Preliminary Acceptance (Wstƒôpny akcept): Shortlisting of qualified candidates Telephone Interview (Rozmowa telefoniczna): Discussion of skills and experience Client Verification (Weryfikacja u klienta): Further assessment with business stakeholders Final Interview (Rozmowa kwalifikacyjna): In-depth interview with the recruitment panel Offer Stage (Oferta): Presentation of the job offer to the selected candidate Screening: Employment verification and onboarding formalities To learn more about Antal, please visit www.antal.pl",[],Data Science,Data Science
Full-time,Mid,B2B,Remote,578,Mid Data Engineer,Cloudfide,"You are Passionate about Cloud and data analytics. Curious and eager to learn new technologies. One that would like to work with a team of like-minded people. Opportunity overview You will work on a project involving modern cloud data lake implementation, leveraging Databricks, CI/CD and cloud services as your daily driver. Your impact zone Implementing, and optimizing modern cloud-based solutions. Implementing, and optimizing modern cloud-based solutions. Building and launching new data models and data pipelines. Building and launching new data models and data pipelines. Implementing best practices in data engineering including data integrity, quality, and documentation. Implementing best practices in data engineering including data integrity, quality, and documentation. Optimization of existing analytical solutions. Optimization of existing analytical solutions. Leading small size teams of engineers and being a role model. Leading small size teams of engineers and being a role model. Qualifications & tech toolbox 2+ years of experience delivering complex data warehouse / data lake / business intelligence solutions. 2+ years of experience delivering complex data warehouse / data lake / business intelligence solutions. 1+ years of experience working with cloud services (Azure / GCP / AWS). 1+ years of experience working with cloud services (Azure / GCP / AWS). Knowledgeable in Python. Knowledgeable in Python. Experience in SQL and data analysis, knowledge of relational databases (preferably SQL Server, PostgreSQL). Experience in SQL and data analysis, knowledge of relational databases (preferably SQL Server, PostgreSQL). Knowledge of public cloud architecture, security, networking concepts and best practices (MS Azure preferred). Knowledge of public cloud architecture, security, networking concepts and best practices (MS Azure preferred). Knowledge of DWH data modeling practices and ETL/ELT development. Knowledge of DWH data modeling practices and ETL/ELT development. Conceptual and analytical skills. Conceptual and analytical skills. Extra stardust for Experience with Apache Spark or Databricks platform. Experience with Apache Spark or Databricks platform. Experience with Azure DevOps environment. Experience with Azure DevOps environment. Experience with Apache Airflow. Experience with Apache Airflow. Here's why you'll love Cloudfide BENEFITS: Regardless of the form of employment - Budget for your professional development - training and certification. MyBenefit cafeteria (with Multisport). Medicover medical care. Team-building meetings and trips. BENEFITS: Regardless of the form of employment - Budget for your professional development - training and certification. MyBenefit cafeteria (with Multisport). Medicover medical care. Team-building meetings and trips. FLEXIBILITY: Enjoy the freedom of working from anywhere, and have a genuine say on our tools, tech, and solutions. FLEXIBILITY: Enjoy the freedom of working from anywhere, and have a genuine say on our tools, tech, and solutions. STABILITY: Stable and long-term employment (employment contract, B2B). STABILITY: Stable and long-term employment (employment contract, B2B). START-UP CULTURE: Open communication, creative problem solving and a flat hierarchy. START-UP CULTURE: Open communication, creative problem solving and a flat hierarchy. GROWTH: Skyrocket your career by exploring new territories ‚Äì you can work on various projects related to Big Data and Cloud. GROWTH: Skyrocket your career by exploring new territories ‚Äì you can work on various projects related to Big Data and Cloud. COLLABORATION: Be part of our diverse, passionate team, where every voice matters. Work in a company full of well-coordinated people who do their work with passion and commitment. COLLABORATION: Be part of our diverse, passionate team, where every voice matters. Work in a company full of well-coordinated people who do their work with passion and commitment. Equal opportunities Cloudfide is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,579,GCP Cloud Infrastructure Engineer,Polkomtel,"Wdra≈ºanie i utrzymanie system√≥w na platformie Google Kubernetes Engine Projektowanie, rozwijanie i wdra≈ºanie rozwiƒÖza≈Ñ na Google Cloud Platform Utrzymanie i rozw√≥j landing zone w GCP Wsp√≥≈Çpraca z zespo≈Çami programist√≥w, kierownikami projekt√≥w i interesariuszami w celu zapewnienia zgodno≈õci rozwiƒÖza≈Ñ technicznych z potrzebami biznesowymi Bieg≈Ça znajomo≈õƒá Google Kubernetes Engine Bieg≈Ça operatywno≈õƒá w technologiach GitOps (ArgoCD) 4-letnie do≈õwiadczenie w przygotowaniu oraz utrzymaniu infrastruktury chmur publicznych pod wdra≈ºane lub migrowane systemy komercyjne Dobra znajomo≈õƒá Terraform; znajmo≈õƒá modu≈Ç√≥w Terraform u≈ºywanych w GCP Znajomo≈õƒá proces√≥w i narzƒôdzi CI/CD (GitLab) Mile widziana znajomo≈õƒá ≈õrodowisk OpenShift, K8S (on-premise) ‚Äì do≈õwiadczenie na stanowisku administratora Mile widziana znajomo≈õƒá ≈õrodowiska Linux (RedHat, Debian) Dobre zdolno≈õci w zakresie komunikacji i wsp√≥≈Çpracy, umiejƒôtno≈õƒá dzielenia siƒô wiedzƒÖ Jƒôzyk angielski na poziomie komunikatywnym, swobodne pos≈Çugiwanie siƒô dokumentacjƒÖ technicznƒÖ Gotowo≈õƒá do pe≈Çnienia dy≈ºur√≥w telefonicznych poza standardowymi godzinami pracy; Wykszta≈Çcenie wy≈ºsze techniczne - preferowane kierunkowe (informatyka) Pracƒô na odpowiedzialnym stanowisku w najwiƒôkszej grupie kapita≈Çowej w kraju, Super atmosferƒô, przyjazne ≈õrodowisko pracy, wsp√≥≈Çpracƒô z osobami otwartymi i chƒôtnie dzielƒÖcymi siƒô wiedzƒÖ, Hybrydowy model pracy, Dostƒôp do platformy szkoleniowej, gdzie z pewno≈õciƒÖ znajdziesz cos dla siebie, Pakiet benefit√≥w (karta Multisport, opieka medyczna w LuxMed, ubezpieczenie na ≈ºycie, liczne oferty pracownicze dedykowane pracownikom Grupy), Poniewa≈º praca to nie wszystko ‚Äì dostƒôp do bezp≈Çatnych sekcji sportowych",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,580,Data Engineer (Azure),Upvanta sp. z o.o.,"Do≈ÇƒÖcz do zespo≈Çu rozwijajƒÖcego nowoczesnƒÖ platformƒô do oceny dojrza≈Ço≈õci organizacji w zakresie cyfryzacji, cyberbezpiecze≈Ñstwa i zarzƒÖdzania ryzykiem. Projektowanie i rozw√≥j rozwiƒÖza≈Ñ bazodanowych w oparciu o Azure SQL i inne us≈Çugi chmurowe, Analiza oraz optymalizacja zapyta≈Ñ i struktur danych pod kƒÖtem wydajno≈õci, Wdra≈ºanie i utrzymanie proces√≥w ETL/ELT, Projektowanie oraz implementacja rozwiƒÖza≈Ñ do wymiany danych w trybie near real-time , Udzia≈Ç w definiowaniu i wdra≈ºaniu architektury danych platformy, Wsp√≥≈Çpraca z zespo≈Çem backendowym i frontendowym w celu zapewnienia sp√≥jno≈õci i efektywno≈õci rozwiƒÖza≈Ñ, RozwiƒÖzywanie zg≈Çaszanych problem√≥w technicznych oraz proponowanie usprawnie≈Ñ, Tworzenie dokumentacji technicznej. Posiada bardzo dobrƒÖ znajomo≈õƒá Azure SQL , Potrafi optymalizowaƒá z≈Ço≈ºone zapytania, Ma do≈õwiadczenie w proponowaniu nowych rozwiƒÖza≈Ñ w odpowiedzi na zg≈Çaszane problemy, Posiada wysokopoziomowe zrozumienie architektury system√≥w, Jest w stanie sugerowaƒá zmiany na poziomie architektury, np. wdro≈ºenie wymiany danych w trybie near real-time. Pracƒô przy innowacyjnym, globalnym projekcie, Realny wp≈Çyw na kierunek rozwoju platformy i architekturƒô rozwiƒÖzania, Wsp√≥≈Çpracƒô z do≈õwiadczonym i zaanga≈ºowanym zespo≈Çem, Elastyczne godziny pracy i mo≈ºliwo≈õƒá pracy zdalnej, Konkurencyjne wynagrodzenie dostosowane do do≈õwiadczenia, D≈ÇugofalowƒÖ wsp√≥≈Çpracƒô i mo≈ºliwo≈õƒá rozwoju w roli eksperta technicznego.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,581,Architekt Danych (K/M),LINK4 TU S.A.,"TW√ìJ ZAKRES OBOWIƒÑZK√ìW: wsparcie Pionu Strategii Ubezpieczeniowej w pozyskiwaniu zmiennych do proces√≥w produktowych i Pricingowych, zarzƒÖdzanie i rozw√≥j miejsc sk≈Çadowania danych na potrzeby analiz i raportowania, wsp√≥≈Çpraca z zespo≈Çami analitycznymi i produktowymi w zakresie projektowania struktur danych, udzia≈Ç w projektach zwiƒÖzanych z rozwojem architektury danych. NASZE WYMAGANIA: znajomo≈õƒá jƒôzyka SQL (tworzenie zapyta≈Ñ, optymalizacja, praca z du≈ºymi zbiorami danych), umiejƒôtno≈õƒá programowania w Pythonie (np. przetwarzanie danych, automatyzacja), zrozumienie zasad modelowania danych i architektury hurtowni danych, umiejƒôtno≈õƒá pracy zespo≈Çowej i komunikatywno≈õƒá. MILE WIDZIANE: do≈õwiadczenie w pracy z platformƒÖ Databricks, znajomo≈õƒá narzƒôdzia Apache Airflow (lub innego orkiestratora proces√≥w ETL), zainteresowanie tematykƒÖ ubezpieczeniowƒÖ i analitykƒÖ danych. W ZAMIAN OFERUJEMY: mo≈ºliwo≈õƒá rozwoju w obszarze architektury danych i nowoczesnych technologii, pracƒô w zespole ekspert√≥w i realny wp≈Çyw na rozw√≥j proces√≥w danych, stabilne warunki zatrudnienia - wsp√≥≈Çpracƒô w oparciu o umowƒô o pracƒô, pracƒô w trybie hybrydowym ‚Äì minimum raz w tygodniu w biurze (ul. Postƒôpu 15 w Warszawie, wdro≈ºenie na stanowisko pracy ‚Äì przeka≈ºemy Ci naszƒÖ wiedzƒô i do≈õwiadczenie, aby≈õ m√≥g≈Ç/a komfortowo rozpoczƒÖƒá pracƒô i rozwijaƒá swoje kompetencje, pakiet benefit√≥w (w przypadku umowy o pracƒô) ‚Äì opiekƒô medycznƒÖ, dofinansowanie do karty Multisport, grupowe ubezpieczenie na ≈ºycie oraz szereg inicjatyw na rzecz naszych pracownik√≥w, kulturƒô organizacyjnƒÖ opartƒÖ na trzech warto≈õciach: klient, mistrzostwo, szacunek ‚Äì w centrum naszego zainteresowania zawsze jest klient, swoimi dzia≈Çaniami dƒÖ≈ºymy do mistrzostwa, a podstawƒÖ naszych relacji jest szacunek,",[],Data Architecture,Data Architecture
Full-time,Senior,Any,Remote,582,Data Engineer (remote),NATEK,"Do≈ÇƒÖcz do NATEK Polska jako Data Engineer i wesprzyj projekty dla jednego z TOP 3 bank√≥w w Polsce. Staniesz siƒô czƒô≈õciƒÖ zespo≈Çu odpowiedzialnego za rozw√≥j narzƒôdzi dla biznesu. ObowiƒÖzki: ‚Ä¢ Projektowanie, implementacja i utrzymanie proces√≥w przetwarzania danych (ETL/ELT) ‚Ä¢ Budowa i rozw√≥j hurtowni danych oraz nowoczesnych platform danych ‚Ä¢ Integracja danych z r√≥≈ºnych system√≥w i ≈∫r√≥de≈Ç ‚Ä¢ Opracowywanie raport√≥w i analiz na podstawie danych przetwarzanych w ≈õrodowisku Databricks ‚Ä¢ Wsp√≥≈Çpraca z zespo≈Çami analitycznymi, developerskimi i biznesowymi ‚Ä¢ Udzia≈Ç w projektowaniu architektury danych i rozwiƒÖza≈Ñ wspierajƒÖcych analizƒô oraz raportowanie ‚Ä¢ Monitorowanie i optymalizacja wydajno≈õci proces√≥w przetwarzania danych ‚Ä¢ Wdra≈ºanie dobrych praktyk w zakresie bezpiecze≈Ñstwa, wersjonowania i dokumentacji danych Wymagania: ‚Ä¢ Minimum 8 lat do≈õwiadczenia jako In≈ºynier w obszarach danych ‚Ä¢ Do≈õwiadczenie projektowe z Databricks (w tym znajomo≈õƒá Data Lake) ‚Ä¢ Umiejƒôtno≈õƒá programowania z wykorzystaniem jƒôzyka Python lub Node.js ‚Ä¢ Bardzo dobra znajomo≈õƒá SQL ‚Ä¢ Do≈õwiadczenie w modelowaniu i przetwarzaniu du≈ºych zbior√≥w danych ‚Ä¢ Znajomo≈õƒá jƒôzyka angielskiego na poziomie swobodnej komunikacji (wsp√≥≈Çpraca z zespo≈Çami miƒôdzynarodowymi) ‚Ä¢ Otwarto≈õƒá na wsp√≥≈Çpracƒô zespo≈ÇowƒÖ oraz z biznesem ‚Ä¢ Umiejƒôtno≈õƒá priorytetyzacji zada≈Ñ oraz proaktywno≈õƒá w znajdowaniu i rozwiƒÖzywaniu problem√≥w Mile widziane: ‚Ä¢ Do≈õwiadczenie z Microsoft Azure ‚Ä¢ Znajomo≈õƒá React Oferujemy: ‚Ä¢ D≈Çugofalowa wsp√≥≈Çpraca oparta o nasze warto≈õci: do≈õwiadczenie, partnerstwo i odpowiedzialno≈õƒá, ‚Ä¢ Praca w projektach dedykowanych dla najwiƒôkszych marek na ≈õwiecie ‚Ä¢ Mo≈ºliwo≈õƒá wymiany do≈õwiadcze≈Ñ w miƒôdzynarodowych ≈õrodowisku i rozwoju w projektach realizowanych globalnie ‚Ä¢ Elastyczne godziny pracy, ‚Ä¢ PrywatnƒÖ opiekƒô medycznƒÖ, ‚Ä¢ Karta Multisport lub My Cafeteria, ‚Ä¢ Ubezpieczenie na ≈ºycie, ‚Ä¢ NATEK loyalty Club, ‚Ä¢ Mo≈ºliwo≈õƒá do≈ÇƒÖczenia do spo≈Çeczno≈õci odpowiedzialnej spo≈Çecznie i uczestniczenia w inicjatywach firmowych, ‚Ä¢ Dla kontraktor√≥w - uprawnienie do 23 dodatkowych dni ≈õwiadczenia us≈Çug w roku, ‚Ä¢ Dodatkowy p≈Çatny dzie≈Ñ wolny na wolontariat dla pracownik√≥w na etacie.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,583,Data Engineer,Link Group,"About the Role We're looking for a Data Engineer to join a growing team working on modern data platforms. You will play a key role in designing, developing, and maintaining scalable data pipelines and infrastructure to support data analytics and reporting initiatives. This is a great opportunity to work with cutting-edge cloud and big data technologies. Design, build, and maintain scalable ETL/ELT pipelines Work with structured and unstructured data from diverse sources Optimize data workflows for performance, reliability, and cost Implement data quality checks and monitoring Collaborate with analysts, architects, and other engineers to support data needs Build data integrations with internal and third-party APIs Support cloud data infrastructure and automation 3+ years of experience as a Data Engineer or similar role Strong knowledge of SQL and data modeling principles Experience with Python or Scala for data processing Hands-on experience with cloud platforms (ideally AWS , but Azure/GCP also valuable) Familiarity with tools like Apache Spark , Airflow , Kafka , or similar Experience with data lakes , data warehouses , or lakehouse architectures Git and CI/CD workflows Strong problem-solving skills and ability to work independently Experience with Snowflake , Redshift , or BigQuery Familiarity with dbt , Terraform , or other IAC tools Background in data governance or security Experience with real-time data processing (e.g., Flink, Kinesis) Exposure to ML pipelines or MLOps","[{""min"": 90, ""max"": 105, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,584,Sr Cloud Engineer - Data Integration Platform,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Sr. Cloud Engineer - Data Integration Platform The Data, Analytics and AI Organization of the Digital Transformation Unit of Bayer Consumer Health operates a global transactional data integration backbone which is used to connect internal and external applications and systems into a uniform experience for our customers, consumers and employees. The backbone (platform) provides composable infrastructure modules, development frameworks, CI/CD automations and developer tooling to internal and external software engineering teams that build integrations on top of the platform. As Sr. Cloud Engineer - Data Integration Platform you are responsible for planning, designing and implementing platform capabilities, steering external software engineering teams and overseeing operations of the cloud infrastructure. To do so, you will engage with Enterprise Architects, Integration Architects and Software Engineers from Bayer and partner companies as well as incorporate industry best practices and trends into your designs. Key Tasks & Responsibilities: Contribute to platform roadmap together with Enterprise and Integration Architects, derive requirements for missing capabilities and actively shape the future of the platform Plan, design and implement capabilities on Amazon Web Services and/or the GitHub ecosystem based on derived requirements in line with security guidelines & non-functional requirements like cost efficiency, elasticity & scalability, maintainability, developer experience and documentation guidelines. Engage and consult with software engineering teams building integrations on the platform on design, best practices and integration patterns Document and share newly built capabilities within the corporate software engineering community Observe and assess platform compliance and implement solutions to ensure platform operations within compliance guidelines such as security or regulatory aspects Engage with technology and implementation partners to overcome issues if required Engage with central cloud teams, network and security teams, cyber security teams to overcome issues if required and follow company best practices Qualifications & Competencies (education, skills, experience): Master‚Äôs degree in computer science, Engineering, or similar Proficient knowledge of distributed systems and their challenges such as eventual consistency, horizontal scaling and parallel processing, consensus, operations and maintenance of a microservices architecture Good knowledge of data integration technologies and concepts such as message brokers, streaming systems, batch processing, API design and - management 3-5 years of working experience in AWS cloud development with focus on serverless services such as AWS Lambda, AWS SQS, AWS DynamoDB, AWS API Gateway and augmenting services such as AWS CloudWatch, AWS IAM. Experience with AWS networking services, AWS DocumentDB (MongoDB) and AWS container services are a plus. Proficient knowledge of infrastructure as code tooling (preferred: Terraform) to automate infrastructure deployment. Additional coding skills such as shell scripting for task automation are a plus. 2-3 years of experience of software development with JavaScript/ECMAScript (preferred: server-side development with NodeJS). Good knowledge of the GitHub ecosystem including the core VCS, GitHub Actions for CI/CD and working with tickets/issues and pull requests Problem solving and analysis skills, combined with profound business judgement as well as good documentation and communication skills (active listening, consulting, challenging). Intercultural awareness and willingness to travel from time to time. Fluent in English, both written & spoken What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,585,IPC Developer,ITDS,"Drive Innovation in Data Warehousing: IPC Developer wanted! Warsaw based opportunity with remote work model (2 days in the office/month). As an IPC Developer , you will be working for our client, a leading player in the online banking sector, on the development and optimization of data warehouse solutions. This includes designing and implementing ETL processes, enhancing data architecture, and supporting the business intelligence environment to ensure effective data reporting and analysis. The project involves using cutting-edge technologies like Informatica Power Center and Oracle-based systems to support complex financial systems. You‚Äôll be part of a dynamic IT team that ensures data integrity, performance, and scalability of large-scale data systems. Your main responsibilities: Design and implement ETL processes using Informatica Power Center Develop and maintain data warehouses and reporting data marts Participate in the implementation and integration of Informatica tools Design, implement and develop BI-class analytical environments Create and maintain Oracle databases and applications Define and document technical specifications and administrative documentation Test and validate software solutions created by others Prepare software installation packages Support the software release and handover to production teams You're ideal for this role if you have: Strong knowledge of Informatica Power Center for ETL process development Solid understanding of RDBMS Oracle 9i/10g and database design Excellent command of SQL and good knowledge of PL/SQL Understanding of information systems engineering and development methodologies At least 6 months of experience designing and implementing ETL solutions At least 1 year of experience with Oracle-based systems in information environments Practical experience designing data warehouses for large-scale institutions Ability to read and write technical documentation in English Strong analytical thinking and problem-solving skills Team-oriented mindset with attention to quality and detail Nice to have: Theoretical knowledge and practical experience with Big Data, Python, and Spark Familiarity with Business Intelligence (BI) concepts Ability to work using Agile methodologies (Scrum, Kanban) Knowledge of tools such as SQL Developer, SVN, GitHub, JIRA, and Confluence We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here . Ref. number 7273","[{""min"": 16800, ""max"": 21500, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,586,BI Inhouse Consultant - Tagetik Focus,Vaillant Group Business Services,"What we achieve together The department ""Business Intelligence - Center of Excellence"" drives the group-wide digitalization and data decision making via state-of-the-art BI solutions and innovations. You will become part of the ""Business Intelligence - Finance, Audit & Human Resources "" team, which consists of 8 BI experts, with diverse backgrounds and focus. To ensure that we provide high-quality, sustainable and innovative solutions, the team makes use of various technologies of data modeling and follows best practices. Our BI landscape is dynamic, utilizing multiple tools, and we constantly challenge the status quo. As a BI Inhouse Consultant, you will explore superior alternatives to actively shape Vaillant's BI transformation. Your primary focus will be on the Corporate Performance Ôªø Management solution CCH Tagetik and its module Budgeting, Planning & Forecasting . You will oversee Tagetik applications, infrastructure, and customer engagement. Your key responsibilities are: Lead the management and optimization of Tagetik applications and infrastructure. Partner with middle management to understand their needs and design bespoke BI solutions. Spearhead the rollout of the Tagetik budget and forecast solution across the Vaillant group, collaborating closely with business users and external software providers. Evaluate current Tagetik applications and customer requirements to ensure alignment with business objectives. Manage, maintain, and support our Tagetik SaaS application, ensuring seamless integration with SAP ERP, SAP BW, and other applications (including VPN tunnels). What makes us successful together Experience : You bring extensive experience with the Tagetik ""Budgeting, Planning & Forecasting"" solution, including cash flow planning. Know-how and skills : You have strong proficiency in SQL & Databases and experience in implementing Tagetik financial planning solutions (including in-depth knowledge of FW and AiHub modeling and ETL development). Nice to have: Basic knowledge of the SAP ecosystem (S/4 & BW). You are familiar with financial processes and reporting requirements, or are willing to deepen your understanding. Personality : With your positive attitude and trustworthy personality, you can build strong stakeholder relationships. You have passion for data integration and modeling. Language skills: You speak English fluently; Polish or German would be a plus. What makes us special Environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee Package of additional benefits: private medical care, multi-sport card. A fast growing, agile and very dynamic team that challenges established routines and helps transforming the Vaillant Group to a data informed business Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings","[{""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,587,Senior Data Architect,N-iX,"N-iX is a software development service company that helps businesses across the globe develop successful software products. During 21 years on the market and by leveraging the capabilities of Eastern Europe talents the company has grown to 2000+ professionals with a broad portfolio of customers in the area of Fortune 500 companies as well as technological start-ups. N-iX has come a long way and increased its presence in nine countries - Poland, Ukraine, Romania, Bulgaria, Sweden, Malta, the UK, the US, and Colombia. The Data and Analytics practice, part of the Technology Office, is a team of high-end experts in data strategy, data governance, and data platforms, and contribute to shaping the future of data platforms for our customers. As Senior Data Architect , you will play a crucial role in designing and overseeing the implementation of our strategic Databricks-based data and AI platforms. You will collaborate with data engineers and data scientists, define architecture standards, and ensure alignment across multiple business units. Your role will be pivotal in shaping the future state of our data infrastructure and driving innovative solutions within the automotive claims management domain. Key Responsibilities: Design scalable and robust data architectures using Databricks and cloud technologies (Azure/AWS) Oversee and guide the implementation of Databricks platforms across diverse business units Collaborate closely with data engineers, data scientists, and stakeholders to define architecture standards and practices Develop and enforce governance strategies, ensuring data quality, consistency, and security across platforms Lead strategic decisions on data ingestion, processing, storage, and analytics frameworks Evaluate and integrate new tools and technologies to enhance data processing capabilities Provide mentorship and guidance to engineering teams, ensuring architectural compliance and effective knowledge transfer Develop and maintain detailed architectural documentation. Requirements: 5+ years of experience as a Solution/Data Architect in complex enterprise environments Extensive expertise in designing and implementing Databricks platforms Strong experience in cloud architecture, preferably Azure or AWS Proficient in Apache Spark and big data technologies Advanced understanding of data modeling, data integration patterns, and data governance Solid background in relational databases (MS SQL preferred) and SQL proficiency Practical knowledge of data orchestration and CI/CD practices (Terraform, GitLab) Ability to articulate complex technical strategies to diverse stakeholders Strong leadership and mentorship capabilities Fluent English (B2 level or higher) Exceptional interpersonal and communication skills in an international team setting. Nice to have: Experience with Elasticsearch or vector databases Knowledge of containerization technologies (Docker, Kubernetes) Familiarity with dbt (data build tool) Willingness and ability to travel internationally twice a year for workshops and team alignment.","[{""min"": 20049, ""max"": 29162, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,Permanent,Hybrid,588,Data & Software Engineer (L5) - Productivity Metrics & System Insights,Netflix,"Netflix is one of the world's leading entertainment services, with 283 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. The Productivity Metrics and System Insights team is crucial to our Developer Experience organization. It focuses on empowering Netflix engineers with comprehensive system insights and metrics to enhance decision-making, optimize workflows, and drive engineering excellence across the organization. This team is responsible for developing solutions that visualize and analyze comprehensive system insights and metrics, supporting engineering excellence and aligning with strategic goals of accelerating innovation and reducing risk. As a Data and Software Engineer, you will play a crucial role in developing solutions that ingest, process, aggregate, visualize, and analyze comprehensive system insights and metrics. Your work will directly impact Netflix's ability to deliver high-quality entertainment experiences to millions of users worldwide. You will collaborate with data scientists, product managers, and other engineers to build robust data pipelines and software solutions that power our analytics, machine learning models, and product features. For this role, we offer relocation support for candidates based outside of Warsaw, Poland. Key Responsibilities: Design, develop, and maintain scalable data pipelines and software applications that support Netflix's core business objectives. Collaborate with cross-functional teams to understand business needs and translate them into technical solutions that drive innovation and efficiency. Implement and optimize distributed processing frameworks to handle large-scale data processing. Develop and integrate software solutions for data analysis, reporting, and experimentation Continuously improve system efficiency and effectiveness, leveraging best practices in data governance and software engineering. Stay informed about industry trends and emerging technologies to keep Netflix at the forefront of innovation. Who You Are: Proficient in Java or Python, Spark, and SQL. Extensive experience working on ETL, aggregation, and reporting. Experienced in building and managing distributed systems and data pipelines. Familiar with big data technologies and comfortable working with large scale datasets. Strong communicator with the ability to explain complex data problems in clear and concise language. Passionate about data quality, software design, and delivering impactful solutions. Comfortable working in a fast-paced, agile environment with ambiguous requirements. A collaborative team player who thrives in a diverse and inclusive culture. Preferred Qualifications: Hands-on experience with batch or streaming data processing. Knowledge of data modeling, data warehousing, and data transformation. Familiarity with software development tools and best practices. Inclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner. We are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.",[],Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,589,Data Engineer,Bayer Sp. z o.o.,"At Bayer we‚Äôre visionaries, driven to solve the world‚Äôs toughest challenges and striving for a world where ,Health for all, Hunger for none‚Äô is no longer a dream, but a real possibility. We‚Äôre doing it with energy, curiosity and sheer dedication, always learning from unique perspectives of those around us, expanding our thinking, growing our capabilities and redefining ‚Äòimpossible‚Äô. There are so many reasons to join us. If you‚Äôre hungry to build a varied and meaningful career in a community of brilliant and diverse minds to make a real difference, there‚Äôs only one choice. Data Analysis and Synthesis Undertake data profiling and source system analysis Present clear insights to colleagues to support the end use of the data Data Development Process Design, build and test data products that are complex or large scale Build teams to complete data integration services Data Innovation Understand the impact on the organization of emerging trends in data tools, analysis techniques and data usage Data Integration Design Select and implement the appropriate technologies to deliver resilient, scalable and future-proofed data solutions and integration pipelines Data Modeling Produce relevant data models across multiple subject areas Explain which models to use for which purpose Understand industry-recognized data modelling patterns and standards, and when to apply them Compare and align different data models Metadata Management Design an appropriate metadata repository and present changes to existing metadata repositories Understand a range of tools for storing and working with metadata Provide oversight and advice to more inexperienced members of the team Problem Resolution Respond to problems in databases, data processes, data products and services as they occur Initiate actions, monitor services and identify trends to resolve problems Determine the appropriate remedy and assist with its implementation, and with preventative measures Programming and Build Use agreed standards and tools to design, code, test, correct and document moderate-to-complex programs and scripts from agreed specifications and subsequent iterations Collaborate with others to review specifications where appropriate Technical Understanding Understand the core technical concepts related to the role, and apply them with guidance Testing Review requirements and specifications, and define test conditions Identify issues and risks associated with work Analyze and report test activities and results WHO YOU ARE: Required: Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Data Science, or a related field. 5+ years of experience in data engineering or related roles, with a strong understanding of data integration and modeling. Proficiency in programming languages such as Python, SQL, or Java, and experience with data processing frameworks (e.g., Apache Spark, Hadoop). Familiarity with data warehousing solutions and ETL (Extract, Transform, Load) processes. Experience with cloud data platforms (e.g., AWS, Azure, Google Cloud) and data storage solutions (e.g., relational databases, NoSQL databases). Strong analytical skills and the ability to present complex data insights clearly to stakeholders. Excellent problem-solving abilities and a proactive approach to identifying and resolving issues. Preferred: Experience with data visualization tools (e.g., Tableau, Power BI) and business intelligence practices. Knowledge of data governance frameworks and best practices. Familiarity with Agile methodologies and project management tools. You feel you do not meet all criteria we are looking for? That doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence, we value potential over perfection! WHAT DO WE OFFER: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure Increased tax-deductible costs for authors of copyrighted works VIP Medical Care Package (including Dental & Mental health) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English WORK LOCATION: WARSAW AL. JEROZOLIMSKIE 158 YOUR APPLICATION: Bayer welcomes applications from all individuals, regardless of race, national origin, gender, age, physical characteristics, social origin, disability, union membership, religion, family status, pregnancy, sexual orientation, gender identity, gender expression or any unlawful criterion under applicable law. We are committed to treating all applicants fairly and avoiding discrimination. Bayer is committed to providing access and reasonable accommodations in its application process for individuals with disabilities and encourages applicants with disabilities to request any needed accommodation(s) using the contact information below. Bayer offers the possibility of working in a hybrid model. We know how important work-life balance is, so our employees can work from home, from the office or combine both work environments. The possibilities of using the hybrid model are each time discussed with the manager.Bayer respects and applies the Whistleblower Act in Poland.","[{""min"": 15000, ""max"": 21000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,590,Data Modeler,emagine Polska,"Informacje o projekcie: Bran≈ºa: finanse/po≈ºyczki Lokalizacja: zdalnie Umowa: B2B Stawka: ~200 pln/h netto + VAT D≈Çugo≈õƒá projektu: d≈Çugoterminowy Poszukujemy do≈õwiadczonego Data Engineera do zespo≈Çu Data Platform, kt√≥ry bƒôdzie modelowaƒá dane oraz implementowaƒá procesy ELT w chmurze. ObowiƒÖzki: Modelowanie struktur bazodanowych w podej≈õciu DDD oraz tworzenie logicznych i fizycznych modeli danych. Data Mapping. Przygotowywanie warstwy Data Contracts (wymaga≈Ñ HD do system√≥w ≈∫r√≥d≈Çowych pod merytorycznƒÖ p≈Çaszczyznƒô kontraktu na dane) na podstawie zamodelowanych uprzednio struktur dla poszczeg√≥lnych domen danych. Wsp√≥≈Çpraca w procesie ingerencji danych z system√≥w ≈∫r√≥d≈Çowych. Implementacja modeli danych dla poszczeg√≥lnych domen w Data Platform (warstwa Bronze, Silver i Gold) w podej≈õciu ELT w ≈õrodowisku Azure Databricks. Wymagania: Do≈õwiadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejƒôtno≈õƒá implementacji proces√≥w ELT. Mile widziane: Umiejƒôtno≈õƒá tworzenia dokumentacji technicznej. Do≈õwiadczenie w mapowaniu danych ze ≈∫r√≥d≈Çowych do docelowych struktur w DWH. Umiejƒôtno≈õƒá interpretacji fizycznego/logicznego modelu danych (ERD, modele relacyjne) i synchronizacji struktur tych≈ºe modeli z wymaganiami ≈õwiata analityki. Do≈õwiadczenie w podej≈õciu do projektowania s≈Çownik√≥w i zarzƒÖdzania nimi (masterdata). Znajomo≈õƒá narzƒôdzia dbdiagram.io. Wiedza na temat zagadnie≈Ñ Data Quality, Data Lineage i zasad zarzƒÖdzania danymi Znajomo≈õƒá narzƒôdzi do zarzƒÖdzania metadanymi (np. Azure Purview).","[{""min"": 160, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,591,Lead Data Engineer with Databricks,emagine Polska,"PROJECT INFORMATION: Industry: Pharma Remote work: 100% Project language: English Business trips: Occasionally to Warsaw and Denmark (every few months) Project length: 12 months contracts + prolongations. Start: ASAP Assignment type: B2B Summary: The Lead Data Engineer plays a critical role in leading transformative data engineering initiatives to optimize data infrastructure while ensuring high-quality data management solutions across the organization. Main Responsibilities: Work collaboratively with teams of data engineers to foster a culture of innovation. Execute data engineering strategies in collaboration with stakeholders. Define and implement best practices and standards for data engineering. Facilitate efficient data management by enabling data pipelines and tools. Ensure delivery of scalable and efficient data management solutions. Collaborate with cross-functional teams to address technical requirements and improve efficiency. Provide capabilities for achieving data quality and system performance goals. Promote a culture of professional growth, diversity, and inclusion in a high-performance team. Key Requirements: A master‚Äôs degree in Data, Computer Science, or a related field. 8+ years experience in software and data engineering, including at least 4 years in Data Pipeline Engineering. Strong proficiency in Databricks. Extensive experience with Python for data manipulation and automation. Proficient in SQL for complex queries and data extraction. Hands-on with DevOps and CI/CD processes. Expertise in data modeling and integration. 4+ years experience with Cloud Technologies like Azure and AWS. Familiarity with life sciences regulatory requirements. Proven leadership and exceptional communication skills. Nice to Have: Certifications in Cloud Technologies. Experience designing scalable data engineering solutions. We offer: Long-term cooperation. Transparently built relations based on trust and fair play. Co-financed benefits: Medicover card, Multisport card.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,592,Automation Specialist,BNP Paribas SA oddzia≈Ç w Polsce,"Description As an Automation Specialist you will be responsible for the maintenance and delivery of EUC (End User Computing) tools and executing solutions for business and functional needs. This includes providing technical solutions to end-users, analysis and design, specification, development, configuration, reporting, testing, documentation, deployment and on-going monitoring and support. Additionally, you will be responsible for implementation of automation tools available within BNP Group, with focus on Alteryx, Power BI, BNP workflow tool and other technologies within the portfolio of Business Automation Team. Automation Specialist Requirements: Experience in a similar role Expert skills of VBA in MS Excel and advanced MS Access skills Advanced understanding of applicable data sources, tables, and data relationships, as well as, design skills for user interface screens, process logic and reports Advanced ability to translate design specifications into functional and efficient code Strong understanding of Atlassian stack (Jira/Confluence) Familiarity with ITIL framework Basic knowledge of financial instruments Knowledge of Fund Administration, including Financial Reporting and Fund Accounting (MFFA/FAML) would be considered a great asset Alteryx / PowerBI / Power Automate / Power Query / SQL / React / Python and other software technologies knowledge would be considered an asset Fluency in written and spoken English (at least B2) Responsibilities: Maintaining and developing End User Computing automation tools, as well as, developing and implementing automation use cases Analysing current business process and its redesigning for highest optimisation and automation utility Collaborating with business representatives and managers to understand user requirements, align with business needs, and provide status updates Designing, developing, testing, debugging and documenting applications according to the functional requirements Providing technical and analytical expertise across multiple areas of application development and suggesting procedural improvements Performing peer reviews, capturing review comments & tracking until closure Upholding and adhering to existing governance and rules We offer: Hybrid work mode, 60% working from home within a month Equivalent for remote work expenses (120 PLN per month) Stable employment in the international company Fully paid private medical care for employee Pre-paid lunch card Employee Pension Plan Co-financed Multisport Card MyBenefit Cafeteria Platform Life insurance Car parking availability in the office building Trainings and development opportunities",[],Unclassified,Unclassified
Full-time,Mid,Permanent,Remote,593,Data Analyst,CCC Group,"Co bƒôdziesz robiƒá? Analiza, projektowanie i implementacja semantycznych modeli danych na platformie Power BI/Azure Analysis Services Analiza wymaga≈Ñ i tworzenie raport√≥w oraz dashboard√≥w. Analiza przep≈Çywu i integracji danych dla wielu system√≥w w organizacji Odpowiedzialno≈õƒá za kontakty z w≈Ça≈õcicielami danych (definicje pojƒôƒá i kompletno≈õƒá danych s≈Çownikowych) ≈öcis≈Ça wsp√≥≈Çpraca z odbiorcami biznesowymi w zakresie utrzymania i rozwoju platformy analitycznej Prowadzenie i utrzymywanie dokumentacji technicznej Rekomendowanie usprawnie≈Ñ w procesach przep≈Çywu, integracji oraz przetwarzania danych w rozwiƒÖzaniach Data Lake House oraz Hurtowni danych Wsp√≥≈Çpraca z wewnƒôtrznymi zespo≈Çami (Data Engineers, Data Science ) oraz dostawcami zewnƒôtrznymi w zakresie budowy rozwiƒÖza≈Ñ Data Lake House i BI Szukamy os√≥b, kt√≥re: MajƒÖ minimum 3-letnie do≈õwiadczenie zawodowe na stanowisku BI Developera / Data Developera. Komercyjne do≈õwiadczenie w Microsoft Azure PosiadajƒÖ wykszta≈Çcenie wy≈ºsze, preferowany kierunek: Informatyka. Praktycznie znajƒÖ SQL/T-SQL, DAX, Power BI, Power Query. PotrafiƒÖ projektowaƒá modele danych. ZnajƒÖ relacyjne bazy danych, hurtowni danych, OLAP oraz koncepcji data lake house. MajƒÖ do≈õwiadczenie w analizie wymaga≈Ñ biznesowych w obszarze hurtowni danych, integracji danych. MajƒÖ praktycznƒÖ wiedzƒô w zakresie zarzƒÖdzania danymi (Data Governance). MajƒÖ do≈õwiadczenie w pracy w ≈õrodowisku Agile i z narzƒôdziami takimi jak JIRA, Confulence, SƒÖ odpowiedzialne, sumienne i terminowe. Co oferujemy: Mo≈ºliwo≈õƒá pracy 100% zdalnie. Umowa o pracƒô lub B2B Konkurencyjne wynagrodzenie oraz stabilne warunki zatrudnienia. Rabaty na zakupy w eobuwie.pl , MODIVO, HalfPrice i CCC. Dostƒôp do nowoczesnych biur w wybranych lokalizacjach. Swobodny dress code oraz partnerskƒÖ atmosferƒô pracy.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,594,Remote Data Engineer,Ework Group,"Ework Group - founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking for Data Engineer with AWS experience - remote work üîπ At our Digital Data & IT, we‚Äôre on a mission to unlock the full potential of data towards helping more patients worldwide. It's an ambitious mission that requires many bright minds to succeed. So, we warmly welcome your data engineering skills to help us unfold smarter solutions that better support the business needs and, in the end, save more lives. Do you want to join a purpose-driven ride? Then read on. ‚úîÔ∏è What you will be doing: As a Data Engineer, you will be an integral member of the Data Harmonisation Layer (DHL) Agile team. DHL is a global platform designed to collect and harmonize of data from various Operational Technology (OT) data sources across company production facilities. DHL makes data easily accessible to develop several front-end Manufacturing Intelligence (MI) products, focusing on improving our production processes. Key technologies used in DHL are Kafka, Ignition, and AWS services. ‚úîÔ∏è Your main responsibilities include: Software development activities i.e., writing, reviewing, refactoring, testing, and documenting the code base Build production ready stable and scalable data pipelines Act as a trusted advisor to provide technical expertise on one or more projects for our business partners Work with the Agile teams to understand and handling enabler work and work towards technical agility Contribute in Agile events such as Program Increment (PI) planning, system demos, and Inspect and Adapt (I&A) We are looking for An ambitious and proactive colleague who can contribute to the team both professionally and personally. ‚úîÔ∏è On a professional level, you have: A master‚Äôs degree in computer science, engineering, chemistry, biology, or other relevant fields Experienced with data engineering principles such as data warehousing, batch processing, data streaming, data lakes, databases, and data modelling Experience with building CI/CD pipelines Coding/scripting skills (Bash, Python, or similar) Hands-on experience in AWS services such as Lambda, Glue, IAM, Kinesis, MKS, Step Functions, DMS, RDS, Managed Grafana, SNS, S3, CloudWatch, etc Hands-on experience with IaC for managing the cloud Strong mathematical, statistical, and problem-solving skills ‚úîÔ∏è On a personal level, you are: Strategic mindset Innovative mindset Strong communicator Systematic ‚úîÔ∏è The team waiting for you... You will be part of the Manufacturing Intelligence (MI) department. Our vision is to enable data using cutting-edge technologies and empower our colleagues in Product Supply to make data-driven decisions to continuously improve manufacturing processes, in short #EnableDataEmpowerDecisions. ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 161, ""max"": 178, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Remote,595,Data Analyst (Engineer),Revolut,"People deserve more from their money. More visibility, more control, and more freedom. Since 2015, Revolut has been on a mission to deliver just that. Our powerhouse of products ‚Äî including spending, saving, investing, exchanging, travelling, and more ‚Äî help our 60+ million customers get more from their money every day. As we continue our lightning-fast growth,‚Äå 2 things are essential to our success: our people and our culture. In recognition of our outstanding employee experience, we've been certified as a Great Place to Work‚Ñ¢. So far, we have 10,000+ people working around the world, from our offices and remotely, to help us achieve our mission. And we're looking for more brilliant people. People who love building great products, redefining success, and turning the complexity of a chaotic world into the simplicity of a beautiful solution. We approach data science at Revolut the same way that we approach everything else. We take complex problems, and create extraordinary solutions that our customers love. Our Data Analysts aren‚Äôt kept in the background, doomed to never see the impact of their work. They‚Äôre some of our best and brightest problem solvers, deployed to the front-lines to work in product teams and deliver rock star solutions. We‚Äôre looking for a Data Analyst who can dig into our complex databases, look for the root cause of a problem, and design their own solutions by writing code to implement them. This is an opportunity to apply your skills while picking up new ones and delivering value for millions of customers around the world. Up to shape the future of finance? Let's get in touch. Understanding our business and its processes through data Applying this understanding and knowledge of data to help product and services teams Developing documentation and data governance Owning the entire ETL process Designing key metrics to measure different aspects of the business Creating and maintaining new aggregated views and tables to simplify data querying Providing clean data sets, modelling data in a way that empowers end users to answer their own questions 5+ years of experience in an analytical role, creating impactful data-driven solutions Excellent background/education in a quantitative discipline Great skills with Python, SQL, or other programming languages Evidence of exceptional mathematical and statistics knowledge An advanced degree in a core STEM subject Solid experience with additional programming languages (Java, Scala, C++, etc.) School/university Olympic medal competitions in physics, maths, economics, or programming Building a global financial super app isn‚Äôt enough. Our Revoluters are a priority, and that‚Äôs why in 2021 we launched our inaugural D&I Framework, designed to help us thrive and grow everyday. We're not just doing this because it's the right thing to do. We‚Äôre doing it because we know that seeking out diverse talent and creating an inclusive workplace is the way to create exceptional, innovative products and services for our customers. That‚Äôs why we encourage applications from people with diverse backgrounds and experiences to join this multicultural, hard-working team.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,596,Senior Data Engineer,co.brick sp. z o.o.,"Do≈õwiadczonych specjalist√≥w na stanowiska: üîπ Data Engineer üîπ Data Architect Projekt realizowany jest dla firmy z Wielkiej Brytanii, z kt√≥rym wsp√≥≈Çpracujemy od wielu lat. ≈örodowisko dojrza≈Çe technologicznie, dobra komunikacja i otwarto≈õƒá na wsp√≥≈Çpracƒô z polskim zespo≈Çem. Oferujemy Lokalizacja: 100% zdalnie Forma wsp√≥≈Çpracy: B2B Stawka: Do ustalenia indywidualnie, w zale≈ºno≈õci od do≈õwiadczenia Start: ASAP / elastyczny Czas trwania: min. 12 miesiƒôcy, z opcjƒÖ przed≈Çu≈ºenia ObowiƒÖzki Migracja danych i dekomisjonowanie starego oprogramowania. Celem jest transfer danych do nowego ≈õrodowiska i standaryzacja proces√≥w. Z≈Ço≈ºony projekt, przy kt√≥rym potrzebni sƒÖ do≈õwiadczeni i odpowiedzialni specjali≈õci. Wymagania Minimum 5 lat do≈õwiadczenia w obszarze Data Engineering / Architecture Bieg≈Çy angielski (C1 lub wy≈ºej) ‚Äì codzienna komunikacja z zespo≈Çem i klientem Doskona≈Çe umiejƒôtno≈õci komunikacyjne i proaktywne podej≈õcie Do≈õwiadczenie w projektach migracyjnych i integracyjnych bƒôdzie du≈ºym atutem Must have: Snowflake DBT Python AWS (preferowane) lub Azure Nice to have: Fivetran (ogromny plus)",[],Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,597,Lead Data Software Engineer (Azure),EPAM Systems,"We are looking for a Lead Data Software Engineer to join one of the projects in our growing Data Practice. Our client is a UK-based global leader in consumer health, hygiene, and nutrition products, with a strong portfolio of well-known brands available in over 200 countries. The company focuses on innovation and sustainability to enhance everyday health and well-being. You can work remotely or join one of our offices, located in Krakow, Wroclaw, Warsaw, Lodz, Gdansk, and Katowice. Responsibilities Design ETL pipelines to ingest, integrate, curate, and refine data Develop and support Common Data Products Review the organization's data landscape, including all data sources and systems Document shared data estates and enforce structured, controlled access through data segregation Perform analysis of business problems and technical environments to design effective solutions Participate in code reviews and test solutions to ensure alignment with best practices Build a high-performance engineering culture, mentor team members, and provide motivation tools Requirements 5+ years of experience in software development with Big Data technologies Background in building and maintaining reliable data pipelines using Databricks Advanced knowledge of SQL, Python, Spark, PySpark Understanding of modern storage formats, including Delta and Iceberg Proficiency in extracting data from various source systems such as databases, APIs, files, and streaming platforms Knowledge of Azure Competency in key data governance and cost optimization principles Familiarity with data architecture concepts such as Data Lakes, Data Warehouses, and Data Lakehouses Nice to have Background in Azure Data Factory We offer We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,598,Analityk Danych,Detable,"Do≈ÇƒÖcz do Detable Sp. Z o.o. - prƒô≈ºnie rozwijajƒÖcej siƒô firmy, kt√≥ra stawia na d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô z do≈õwiadczonymi profesjonalistami.Od ponad 3 lat jeste≈õmy partnerem dla instytucji z sektora publicznego, wsp√≥≈Çpracujemy m.in . z Centrum e-zdrowia, Aplikacjami Krytycznymi Ministerstwa Finans√≥w, G≈Ç√≥wnym Urzƒôdem Nadzoru Budowlanego, Urzƒôdem Do Spraw Cudzoziemc√≥w , Narodowym Funduszem Zdrowia i wieloma innymi. Nasi konsultanci pracujƒÖ nad rozwojem aplikacji i system√≥w, z kt√≥rych korzystajƒÖ miliony Polak√≥w! Posiadasz minimum 5 lata do≈õwiadczenia w pracy na stanowisku Analityka Danych / lub na stanowisku zwiƒÖzanym z analizƒÖ danych lub analizƒÖ biznesowƒÖ w projektach dla klienta masowego; Nie jest Ci obce przetwarzanie i analiza du≈ºych zbior√≥w danych (Big Data); Na co dzie≈Ñ pracujesz z SQL, Python, PySpark; Wykorzystujesz w codziennej procesy ETL/ELT; Mia≈Çe≈õ okazjƒô pracy w Data Quality; Biegle pos≈Çugujesz siƒô relacyjnymi bazami danych; Mo≈ºesz pochwaliƒá siƒô znajomo≈õciƒÖ i do≈õwiadczeniem w obszarze ochrony zdrowia; Posiadasz do≈õwiadczenie w obszarze Hurtowni Danych; Dodatkowym atutem bƒôdzie je≈õli mo≈ºesz pochwaliƒá siƒô jednym z certyfikat√≥w: certyfikat AgilePM, certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Airflow/tworzenie DAG√≥w Airflow/Apache Spark, certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL. Analiza danych i ≈∫r√≥de≈Ç danych dla projekt√≥w IT; Analiza danych pod kƒÖtem ich jako≈õci i sp√≥jno≈õci; Okre≈õlanie przypadk√≥w u≈ºycia system√≥w informatycznych; Zbieranie i specyfikacja wymaga≈Ñ analityczno-raportowych; Projektowanie i dokumentowanie przep≈Çyw√≥w danych (ETL) oraz struktur raportowych; Zbieranie i specyfikacja wymaga≈Ñ analityczno-raportowych; Utrzymywanie modelu proces√≥w biznesowych w zakresie us≈Çug analityczno-raportowych; Zapewnienie sp√≥jno≈õci proces√≥w analizy i projektowania; Utrzymanie i rozw√≥j kanonicznego modelu danych; Ocenianie i analiza zbior√≥w danych; Przygotowywanie analiz i dokumentacji projektowej; Wsp√≥≈Çpraca z zespo≈Çem projektowym, wytw√≥rczym i interesariuszami w celu zapewnienia zgodno≈õci dzia≈Ça≈Ñ z oczekiwaniami i wymaganiami projektowymi w zakresie praktyki ZarzƒÖdzania Danymi; Proponowanie i konsultowanie rozwiƒÖza≈Ñ systemowych ze zleceniodawcami oraz realizatorami zdefiniowanych wymaga≈Ñ; Wsparcie analityczne na etapach projektowania, wytwarzania i testowania system√≥w informatycznych. Konkurencyjne wynagrodzenie w oparciu o kontrakt B2B ( do 140PLN netto/h); D≈ÇugofalowƒÖ wsp√≥≈Çpracƒô opartƒÖ o wzajemny szacunek i partnerstwo; Dedykowanego opiekuna kontraktu po stronie Detable; Mo≈ºliwo≈õƒá 100% pracy zdalnej lub z naszego biura w Bia≈Çymstoku; Mo≈ºliwo≈õƒá podnoszenia swoich kwalifikacji poprzez skorzystanie z bud≈ºetu szkoleniowego; Realny wp≈Çyw na rozw√≥j projektu; Atrakcyjny program polece≈Ñ pracowniczych; Zdalny proces rekrutacji. Rozmowa HR z naszƒÖ IT RekruterkƒÖ; Weryfikacja umiejƒôtno≈õci technicznych przez naszego Lidera technicznego; Spotkanie z klientem; Decyzja i rozpoczƒôcie wsp√≥≈Çpracy","[{""min"": 20160, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Remote,599,Senior Data Engineer (AdTech),Sigma Software,"We are looking for a Senior Data Engineer to join one of our teams and help us build great products for our clients. You‚Äôll be part of a high-performance team where innovation, collaboration, and excellence are at the core of everything we do. As a Senior Data Engineer, you‚Äôll have the chance to design and develop optimized, scalable big data pipelines that power our products and applications we work on. Your expertise will be valued, your voice will be heard, and your career will be supported every step of the way. Does this sound like an interesting opportunity? Keep reading to learn more about your future role! Customer Our client is an international technology company that specializes in developing high-load platforms for data processing and analytics. The company‚Äôs core product helps businesses manage large volumes of data, build models, and gain actionable insights. The company operates globally, serving clients primarily in the Marketing and Advertising domain. They focus on modern technologies, microservices architecture, and cloud-based solutions. Responsibilities: Design, develop, and maintain end-to-end big data pipelines that are optimized, scalable, and capable of processing large volumes of data in real-time and batch modes Collaborate closely with cross-functional stakeholders to gather requirements and deliver high-quality data solutions that align with business goals Implement data transformation and integration processes using modern big data frameworks and cloud platforms Build and maintain data models, data warehouses, and schema designs to support analytics and reporting needs Ensure data quality, reliability, and performance by implementing robust testing, monitoring, and alerting practices Contribute to architecture decisions for distributed data systems and help optimize performance for high-load environments Ensure compliance with data security and governance standards Qualifications: 4+ years of experience in data engineering, big data architecture, or related fields Strong proficiency in Python and PySpark Advanced SQL skills, including query optimization, complex joins, and window functions. Experience using NoSQL databases Strong understanding of distributed computing principles and practical experience with tools, such as Apache Spark, Kafka, Hadoop, Presto, and Databricks Experience designing and managing data warehouses and data lake architectures in cloud environments (AWS, GCP, or Azure) Familiarity with data modeling, schema design, and performance tuning for large datasets Experience working with business intelligence tools, such as Tableau or Power BI for reporting and analytics Strong understanding of DevOps practices for automating deployment, monitoring, and scaling of big data applications (e.g., CI/CD pipelines) At least Upper-Intermediate level of English Personal profile: Excellent communication skills Ability to collaborate effectively within cross-functional and multicultural teams",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,600,Data Engineer,emagine Polska,"PROJECT INFORMATION: Industry: Construction Assignment type: B2B Start: September Work model: Hybrid model (2 days/week in office - Warsaw) Project length: 12 months + extensions Project language: English We are looking for a dedicated Data Engineer/ BI Developer to join our Danish client's BI scrum team of six members, who develop and maintain our BI back-end solutions. The team you will join works in close collaboration with our highly innovative departments and companies, where you identify and build the foundation for their new BI reports. Reports covering everything from working environment, diversity, IOT machine data to the more financial. Our team is at the forefront of digitization and automation, focused on streamlining the way we work by processing and presenting data through Business Intelligence tools. The role involves collaborating with various departments to develop insightful BI reports that enhance decision-making and operational efficiency. Main Responsibilities Develop and maintain BI back-end solutions. Collaborate with various teams to identify and implement new BI reporting frameworks. Analyze data trends and provide actionable insights to colleagues. Facilitate the transition to cloud environments and integrate new data sources. Contribute to the development and execution of our digital strategies. Identify and implement new data sources and improve existing data frameworks. Work closely with cross-functional teams to define requirements for BI initiatives. Analyze complex data sets and translate findings into actionable insights. Contribute to cloud integration and Data Lakehouse projects. Key Requirements +5 years of experience in BI area Experience in Business Intelligence and data analysis. Strong understanding of Python and SQL at an advanced level. Strong Experience with Databricks Basic experience with DevOps practices Knowledge of cloud-based solutions, particularly Azure. Capacity to perform dimensional data modeling. Experience with structured and unstructured data sources (API, SQL). Adept at communicating complex ideas effectively. Good problem-solving skills and a proactive attitude toward technological advancements. Nice to Have Familiarity Data Factory. Knowledge of MS Dynamics and process automation. Familiarity with Power BI and data transformation tools.","[{""min"": 170, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,601,Data Scientist,HR Contact,"Data Scientist ‚Äã You'll take end-to-end ownership of machine learning solutions from concept to production. Join a team where data science fuels smarter e-commerce and marketing automation. Company : a European MarTech ecosystem of technology companies with a product portfolio including AI-powered personalization engines, e-commerce platform and data management solutions. üìç Poland, Remote. üóÉÔ∏è‚Äã B2B Contract. ü§ñ What will you be doing? Owning full model lifecycle: from research, experimentation and prototyping to deployment and monitoring. Building and optimizing machine learning models using Python, e.g., customer lifetime value models, AI modules that support the recommendation engines and the use of a customer data platform. Designing data pipelines and integrating ML solutions into production (search personalization, clickstream analysis, email triggers). Improving and maintaining existing models to ensure performance, scalability, and robustness in production. Collaborating closely with engineering, product and business teams to turn complex data into actionable product features. Upholding high development standards by writing clean, maintainable code, conducting code reviews, and embracing DevOps practices like testing and CI/CD. Contributing to the continuous improvement of the team by mentoring junior colleagues and bringing new perspectives to modeling and methodology discussions. üîß What will help you get the job done? Experience in end-to-end data science projects (preferably in eCommerce, SaaS, MarTech space), with around 5 years of experience. Strong background in Python. Experience deploying models in production environments. Experience with cloud infrastructure (Azure or AWS) would be an advantage Solid understanding of statistics, modeling, and software engineering practices. Analytical mindset with a passion for practical impact, not just research. Excellent English and communication skills. Strong communication skills, with the ability to explain data-driven insights to diverse audiences. ‚ú® Nice to have: Experience with: Azure, DevOps, Docker, Kubernetes, Databricks, Cassandra, SQL. Experience with Delta Lake and Spark for handling large-scale data. Hands-on experience with deep learning (PyTorch, TensorFlow, MXNet). ‚ú® Why is it worth it? An ability to take charge of your own projects. Possibility to develop further in the MLOps space. Flexible B2B Contract Model. Private medical care (Luxmed or Medicover) & MultiSport package fully covered. This is a unique opportunity to be at the heart of product innovation, turning behavioral data into real business impact. Supportive company culture ‚Äì flat structure, strong teamwork and openness to new ideas. Remote-first setup. Do you prefer slippers instead of shoes or maybe you are expecting Amazon delivery, no worries, we‚Äôve got you covered. üëã Meet Your Guide: Hello! I‚Äôm Olga and I am responsible for this recruitment process. I will be happy to meet you! üó∫Ô∏è Recruitment stages: Online interview with Olga - we will talk in more details about the company, the role and your last projects : ) Online interview with a company - you will meet the team. Technical assignment - it is time for you to shine! Online interview - final assessment of your task and decision.","[{""min"": 19000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Part-time,Junior,B2B,Remote,602,Junior Automation Hero,Automation House,"ü§ñ Jak dzia≈Çamy? W Automation House pracujemy w zgranym i zaanga≈ºowanym zespole, kt√≥ry zajmuje siƒô narzƒôdziami no-code oraz low-code i automatyzacjƒÖ proces√≥w w firmach. Nasza misja to ocaliƒá naszych Klient√≥w przed ciƒÖg≈ÇƒÖ i ≈ºmudnƒÖ pracƒÖ manualnƒÖ, wdra≈ºajƒÖc rozwiƒÖzania AI i wprowadzajƒÖc ich w nowƒÖ rzeczywisto≈õƒá. Pomagamy uporzƒÖdkowaƒá procesy i usprawniƒá codziennƒÖ pracƒô, co przynosi ogromnƒÖ satysfakcjƒô, majƒÖc ≈õwiadomo≈õƒá rzeczywistego, pozytywnego wp≈Çywu na funkcjonowanie biznesu naszych Klient√≥w. Stawiamy na pe≈ÇnƒÖ transparentno≈õƒá wsp√≥≈Çpracy i zaanga≈ºowanie, dobierajƒÖc mo≈ºliwie najlepiej pasujƒÖcych specjalist√≥w do profilu firmy. Przygotowali≈õmy dla Ciebie ciekawy i anga≈ºujƒÖcy proces rekrutacji z udzia≈Çem zespo≈Çu Automation House i Tigers! ü§ñ Co cechuje idealnego Junior Automation Hero? Jako Sta≈ºysta bƒÖd≈∫ Junior Automation Hero bƒôdziesz wspieraƒá nasz zesp√≥≈Ç w tworzeniu innowacyjnych rozwiƒÖza≈Ñ automatyzujƒÖcych procesy biznesowe dla naszych klient√≥w. To doskona≈Ça okazja, aby zdobyƒá praktyczne do≈õwiadczenie, rozwinƒÖƒá swoje umiejƒôtno≈õci i staƒá siƒô ekspertem w dziedzinie automatyzacji i AI. Bƒôdziesz pracowaƒá pod okiem do≈õwiadczonego dedykowanego Seniora, kt√≥ry chƒôtnie podzieli siƒô swojƒÖ wiedzƒÖ i do≈õwiadczeniem. üîπWsparcie w projektowaniu i wdra≈ºaniu proces√≥w automatyzacji z wykorzystaniem narzƒôdzi no-code i AI. üîπPomoc w analizie proces√≥w biznesowych klient√≥w i identyfikacji obszar√≥w do optymalizacji. üîπWsp√≥≈Çpraca przy tworzeniu relacyjnych baz danych. üîπUczestnictwo w warsztatach dla klient√≥w i potencjalnych klient√≥w. üîπWsparcie w przygotowywaniu propozycji automatyzacji i ofert. üîπMonitorowanie i analiza wydajno≈õci wdro≈ºonych rozwiƒÖza≈Ñ. Wymagania: üîπZajawka na automatyzacjƒô, AI i nowe technologie. üîπChƒôƒá do nauki i rozwoju w obszarze automatyzacji proces√≥w biznesowych. üîπPodstawowa wiedza na temat baz danych i logiki programowania bƒôdzie dodatkowym atutem. üîπUmiejƒôtno≈õƒá analitycznego my≈õlenia i rozwiƒÖzywania problem√≥w. üîπKomunikatywno≈õƒá i umiejƒôtno≈õƒá pracy w zespole. üîπZnajomo≈õƒá jƒôzyka angielskiego na poziomie min. B2 (mile widziane). ü§ñ Co oferujemy? üîπ B≈Çyskawiczny rozw√≥j ‚Äì u nas w kwarta≈Ç nauczysz siƒô tyle, ile w innych miejscach w rok. üîπ Brak sufitu ‚Äì chcesz i≈õƒá w eksperta czy account managera? Sam zdecyduj! Masz pomys≈Ç na rozw√≥j us≈Çugi? Dostajesz wsparcie, by to zrobiƒá. üîπ Ekspozycja w bran≈ºy ‚Äì posiadamy najmocniejsze kana≈Çy komunikacji w bran≈ºy, w kt√≥rych mo≈ºesz budowaƒá wizerunek eksperta. üîπ Kole≈ºe≈Ñski zesp√≥≈Ç ‚Äì w Automation House szczeg√≥lnƒÖ wagƒô przywiƒÖzujemy do budowania relacji miƒôdzy sobƒÖ! üîπ Doskona≈Ço≈õƒá operacyjna ‚Äì dbamy o Tw√≥j komfort pracy dziƒôki przejrzystym procesom i pe≈Çnej jasno≈õci ‚Äì wiesz co robiƒá i wiesz po co to robisz. üí∏ Wynagrodzenie Bezp≈Çatny sta≈º lub 30,50 z≈Ç (ca≈Çkowity koszt pracodawcy) na Umowie Zlecenie",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,603,Senior Data Scientist,Procter & Gamble,"We are looking for a passionate Senior Data Scientist who is eager to make an impact on their career and take on a new challenge! P&G Scaled Data Science Hub in Warsaw is dynamically growing and making impact across most key algorithmic investments of the Company. We work across problems and algorithmic products related to Retail, Media, Digital commerce, R&D, Supply Chain, Productivity, Knowledge management and others. With the growth, we are happy to offer Senior Data Scientist roles. In your role, you will take ownership of a particular business space with algorithmic model needs. As a Senior Data Scientist, you are expected to own the algorithmic evolution roadmap and be a thought leader on algorithmic enablement of IT products, as well as share knowledge and mentor other data scientists where applicable. We work all across traditional machine learning, deep learning, GenAI, optimization models, statistical models, adopting methods as required for the problem. To do your job well, you will need to build solid business understanding of your problem space. In Warsaw Hub we specialize in developing and bringing algorithms to scale, always keeping impact, value, resilience, and reliability in mind. Our work impacts P&G's 100k+ employees and billions of consumers around the world. You will join a renowned company improving lives of billions of consumers by applying data science. You‚Äôll be welcomed within a 200+ global team of Data Scientists across US, Panama, Switzerland, Belgium, Poland, Singapore, India and China. You will be able to network and learn from scientists with diverse backgrounds and seniorities. On top, in our Warsaw Hub, we do plenty of knowledge sharing on a weekly basis and have a unique job setup which will allow you to experience data science beyond your own key scope of responsibilities. Our best-in-class P&G AI Factory platform you will use is an example taught about by Harvard Business School, and we continuously improve our developer experience, thanks to our strong AI Engineering team. With us you will Partner with product teams and business leaders to fully understand the problem, and AI & data engineering teams to automate & deploy your models into applications Develop your insights to steer the algorithmic evolution roadmap in your area of ownership Analyze and model on big datasets ‚Äì e.g., translating 1.5TB daily consumer touchpoints and 500 million consumers' behaviors to actionable recommendations Answer business questions and propose solutions for business problems by applying machine learning techniques and algorithms Further develop understanding of machine learning, statistics, optimization, GenAI, and other advanced analytical models ‚Äì and how to apply them to real-world problems Further learn what it takes to build and maintain a resilient algorithmic pipeline that passes the test of time, and what it takes for an algorithm to materialize its impact and value Write production-grade code applying best practices Job Qualifications At least a Master's in a quantitative degree (Statistics, Operations Research, Systems Engineering, Computer Science, Applied Math, Economics), Bachelor's/Engineer degree with sufficient consecutive data science experience At least 5 years of experience in Data Science, with at least 3 years of experience with production-grade Data Science / algorithmically enabled applications Solid code writing capabilities - Python and Spark are preferable Strong technical & analytical skills (SQL, optimization, simulation, predictive modeling etc) Proven success in leadership, problem solving,and and prioritising Strong collaboration skills and working comfortably across teams Nice to have Desired experience with Big Data ecosystem: Databricks, Spark, BigQuery Basic understanding of Business Intelligence Tools such as Power BI, Tableau Experience with Agile DevOps, Github, Jira, Confluence What we offer Work in an international Data Science team with global responsibilities (with a large part of the engineering and product teams located in Warsaw) Long-term career with development and growth opportunities Competitive salary and benefits program (private health care, life insurance, P&G stock options, saving plans, lunch subsidy, sports cards, in-office fitness center) Relevant trainings, certifications, and conference participation Internal coaching programs & training Flexible working arrangements",[],Data Science,Data Science
Full-time,Mid,B2B,Remote,604,Data Scientist,in4ge sp. z o.o.,"Zakres obowiƒÖzk√≥w: Opracowywanie innowacyjnych rozwiƒÖza≈Ñ z wykorzystaniem zaawansowanych technologii uczenia maszynowego i/lub sztucznej inteligencji. Analiza i przetwarzanie du≈ºych zbior√≥w danych. Poszukiwanie nowych ≈∫r√≥de≈Ç danych oraz eksploracja zale≈ºno≈õci w danych. Wsp√≥≈Çpraca z zespo≈Çami programist√≥w i analityk√≥w biznesowych. Przekazywanie z≈Ço≈ºonych wynik√≥w analizy w spos√≥b zrozumia≈Çy dla odbiorc√≥w technicznych i nietechnicznych. ≈öledzenie nowych trend√≥w i technologii w obszarze sztucznej inteligencji, machine learning i analizy danych oraz proponowanie ich zastosowania w projektach. Automatyzacja proces√≥w przetwarzania i analizy danych przy u≈ºyciu nowoczesnych narzƒôdzi i bibliotek. Wdra≈ºanie oraz zarzƒÖdzanie modelami w ≈õrodowiskach produkcyjnych z uwzglƒôdnieniem praktyk MLOps. Wymagania: Minimum 3 lata do≈õwiadczenia zawodowego na podobnym stanowisku. Do≈õwiadczenie w pracy z danymi i tworzeniem modeli uczenia maszynowego. Znajomo≈õƒá jƒôzyka Python i bibliotek zwiƒÖzanych z analizƒÖ danych (np. Pandas, NumPy) oraz podstawowe umiejƒôtno≈õci w zakresie SQL. Znajomo≈õƒá narzƒôdzi chmurowych (AWS, Azure, GCP) bƒôdzie dodatkowym atutem. Umiejƒôtno≈õƒá pracy z danymi tekstowymi (NLP) lub danymi obrazowymi (CV) mile widziana. Znajomo≈õƒá podstaw MLOps i narzƒôdzi do wdra≈ºania modeli (Docker, MLflow, CI/CD). Wiedza z zakresu statystyki i podstawowych algorytm√≥w uczenia maszynowego. Umiejƒôtno≈õƒá analitycznego my≈õlenia i rozwiƒÖzywania problem√≥w. Znajomo≈õƒá jƒôzyka angielskiego na poziomie pozwalajƒÖcym na swobodnƒÖ komunikacjƒô. Mile widziane: Do≈õwiadczenie w pracy z du≈ºymi modelami jƒôzykowymi (LLM) i koncepcjami takimi jak Retrieval Augmented Generation, bazy wektorowe czy in≈ºynieria prompt√≥w. Znajomo≈õƒá framework√≥w LLM, takich jak Langchain, LLamaindex czy agentowych framework√≥w. Wiedza z zakresu przetwarzania jƒôzyka naturalnego (NLP). Znajomo≈õƒá dodatkowych jƒôzyk√≥w programowania (np. Java, C#, Go). Znajomo≈õƒá narzƒôdzi i bibliotek zwiƒÖzanych z agentami GenAI (np. Taskweave, Autogen). Co oferujemy? Rozw√≥j kariery w miƒôdzynarodowych projektach, z wykorzystaniem nowoczesnych narzƒôdzi i technologii. Elastyczny model pracy: mo≈ºliwo≈õƒá 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejƒôtno≈õci i do≈õwiadczenia. Wsp√≥≈Çpracƒô w zgranym zespole, kt√≥ry ceni wymianƒô wiedzy oraz otwartƒÖ komunikacjƒô. Realny wp≈Çyw na projekty oraz wdra≈ºane rozwiƒÖzania.","[{""min"": 13000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,605,Data Science Lead (Manager),in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Poszukujemy do≈õwiadczonego i zmotywowanego lidera zespo≈Çu Data Science, kt√≥ry pomo≈ºe w budowaniu innowacyjnych rozwiƒÖza≈Ñ opartych na danych. Jako Data Science Lead (Manager) , bƒôdziesz odpowiedzialny za prowadzenie projekt√≥w, mentoring zespo≈Çu, a tak≈ºe wsp√≥≈Çpracƒô z dzia≈Çami biznesowymi i technicznymi w celu realizacji ambitnych cel√≥w organizacji. Zakres obowiƒÖzk√≥w: Kierowanie zespo≈Çem Data Science, tworzenie strategii i wytycznych w zakresie analiz danych, uczenia maszynowego oraz sztucznej inteligencji. Wsp√≥≈Çpraca z zespo≈Çami produktowymi i in≈ºynierskimi w celu identyfikowania kluczowych problem√≥w biznesowych, kt√≥re mo≈ºna rozwiƒÖzaƒá za pomocƒÖ danych. Projektowanie i wdra≈ºanie algorytm√≥w oraz modeli predykcyjnych, optymalizacyjnych i klasyfikacyjnych. Mentorowanie i rozw√≥j cz≈Çonk√≥w zespo≈Çu ‚Äì wsparcie w rozwoju ich umiejƒôtno≈õci technicznych i zawodowych. Przeprowadzanie analiz danych, interpretowanie wynik√≥w i przedstawianie rekomendacji dla kluczowych interesariuszy. ZarzƒÖdzanie cyklem ≈ºycia projekt√≥w data science, od zbierania wymaga≈Ñ po wdro≈ºenie i monitorowanie wynik√≥w. Przygotowywanie raport√≥w i prezentacji, komunikowanie wynik√≥w dzia≈Ça≈Ñ zespo≈Çu do kierownictwa oraz innych dzia≈Ç√≥w. Wymagania: Minimum 5 lat do≈õwiadczenia w dziedzinie Data Science, z co najmniej 2-letnim do≈õwiadczeniem w zarzƒÖdzaniu zespo≈Çem. Doskona≈Ça znajomo≈õƒá narzƒôdzi i jƒôzyk√≥w programowania (Python, R, SQL, itp.) oraz bibliotek do analizy danych (np. Pandas, Scikit-learn, TensorFlow). Do≈õwiadczenie w pracy z narzƒôdziami do wizualizacji danych (np. Tableau, PowerBI, Matplotlib). Silne umiejƒôtno≈õci w zakresie uczenia maszynowego, analizy predykcyjnej i Big Data. Doskona≈Çe zdolno≈õci komunikacyjne, zar√≥wno w zakresie technicznym, jak i biznesowym. Do≈õwiadczenie w pracy z chmurowymi platformami analitycznymi (AWS, Azure, GCP). Umiejƒôtno≈õƒá podejmowania decyzji strategicznych i zarzƒÖdzania projektami w ≈õrodowisku zwinnej organizacji. Silne umiejƒôtno≈õci rozwiƒÖzywania problem√≥w i analitycznego my≈õlenia, pozwalajƒÖce na proponowanie innowacyjnych rozwiƒÖza≈Ñ w zakresie analizy danych. Doskona≈Çe umiejƒôtno≈õci komunikacyjne, w tym zdolno≈õƒá do przedstawiania koncepcji technicznych i wynik√≥w osobom nietechnicznym. Bieg≈Ço≈õƒá w jƒôzyku angielskim (w mowie i pi≈õmie). Mile widziane: Do≈õwiadczenie w pracy z Generative AI i du≈ºymi modelami jƒôzykowymi (LLMs). Znajomo≈õƒá przetwarzania jƒôzyka naturalnego (NLP) lub technik wizji komputerowej. Dodatkowe umiejƒôtno≈õci programistyczne w jƒôzykach takich jak R, SQL, Java itp. Do≈õwiadczenie w pracy z narzƒôdziami Big Data (np. Hadoop, Spark, Kafka). Co oferujemy? Rozw√≥j kariery w miƒôdzynarodowych projektach, z wykorzystaniem nowoczesnych narzƒôdzi i technologii. Elastyczny model pracy: mo≈ºliwo≈õƒá 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejƒôtno≈õci i do≈õwiadczenia. Wsp√≥≈Çpracƒô w zgranym zespole, kt√≥ry ceni wymianƒô wiedzy oraz otwartƒÖ komunikacjƒô. Realny wp≈Çyw na projekty oraz wdra≈ºane rozwiƒÖzania. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 22000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,Permanent,Remote,606,Changepoint Application Specialist with French,ArcelorMittal BCOE,"About the Role: We are seeking a dedicated and highly skilled Changepoint Application Specialist to oversee the maintenance, support, and evolution of the Changepoint SaaS solution. The role demands a proactive and organized individual who can manage operational responsibilities, serve as the primary connection to business users, and lead transformation initiatives. Key Responsibilities: Provide end-user support for the Changepoint application and maintain close collaboration with the SaaS partner to ensure smooth operations. Ensure timely and accurate execution of monthly financial closures, maintaining high service standards. Act as the main point of contact for business stakeholders, addressing their needs and concerns effectively. Manage transformation projects including product upgrades, reporting improvements, new feature implementations, and scope extensions. Oversee the system architecture, its landscape, and dependencies. Monitor service indicators and dashboards to ensure optimal performance. Manage and maintain the data warehouse (transitioning from SAP BI to Cognos, PowerBI). Essential Skills and Qualifications: Strong analytical skills and an IT technical background. Experience in operational roles for IT applications. Well-developed communication skills, with the ability to convey technical information clearly and actively listen to stakeholders. Self-organized, structured, and decisive in a fast-paced environment. Fluency in English; Fluency in French. Technical Skill: Generic knowledge on IT architecture MS Excel Power Query Soft Skills: Proactive problem-solving mindset. Strong organizational abilities. Collaborative approach to working with diverse teams and stakeholders.",[],Unclassified,Unclassified
Full-time,Senior,B2B,Remote,607,Senior Data Engineer,KMD Poland,"#Data Engineer #Apache Spark #Databricks #Java #Apache Kafka #Batch Processing #Structured Streaming #Azure #SQL #Microservices #CI/CD #Docker #DDD Are you ready to join our international team as a Lead/Senior Data Engineer ? We shall tell you why you should... What product do we develop? We are building an innovative solution, KMD Elements , on Microsoft Azure cloud dedicated to the energy distribution market (electrical energy, gas, water, utility, and similar types of business). Our customers include institutions and companies operating in the energy market as transmission service operators, market regulators, distribution service operators , energy trading, and retail companies. KMD Elements delivers components allowing implementation of the full lifecycle of a customer on the energy market: meter data processing , connection to the network, physical network management, change of operator, full billing process support, payment, and debt management, customer communication, and finishing on customer account termination and network disconnection. The key market advantage of KMD Elements is its ability to support highly flexible, complex billing models as well as scalability to support large volumes of data. Our solution enables energy companies to promote efficient energy generation and usage patterns, supporting sustainable and green energy generation and consumption. We work with always up-to-date versions of: ‚Ä¢ Apache Spark on Azure Databricks ‚Ä¢ Apache Kafka ‚Ä¢ Delta Lake ‚Ä¢ Java ‚Ä¢ MS SQL Server and NoSQL storages like Elastic Search, Redis, Azure Data Explorer ‚Ä¢ Docker containers ‚Ä¢ Azure DevOps and fully automated CI/CD pipelines with Databricks Asset Bundles, ArgoCD, GitOps, Helm charts ‚Ä¢ Automated tests How do we work? #Agile #Scrum #Teamwork #CleanCode #CodeReview #Feedback #BestPracticies ‚Ä¢ We follow Scrum principles in our work ‚Äì we work in biweekly iterations and produce production-ready functionalities at the end of each iteration ‚Äì every 3 iterations we plan the next product release ‚Ä¢ We have end-to-end responsibility for the features we develop ‚Äì from business requirements, through design and implementation up to running features on production ‚Ä¢ More than 75% of our work is spent on new product features ‚Ä¢ Our teams are cross-functional (7-8 persons) ‚Äì they develop, test and maintain features they have built ‚Ä¢ Teams‚Äô own domains in the solution and the corresponding system components ‚Ä¢ We value feedback and continuously seek improvements ‚Ä¢ We value software best practices and craftsmanship Product principles: ‚Ä¢ Domain model created using domain-driven design principles ‚Ä¢ Distributed event-driven architecture / microservices ‚Ä¢ Large-scale system for large volumes of data (>100TB data), processed by Apache Spark streaming and batch jobs powered by Databricks platform Your responsibilities: ‚Ä¢ Develop and maintain the leading IT solution for the energy market using Apache Spark, Databricks, Delta Lake, and Apache Kafka ‚Ä¢ Have end-to-end responsibility for the full lifecycle of features you develop ‚Ä¢ Design technical solutions for business requirements from the product roadmap ‚Ä¢ Maintain alignment with architectural principles defined on the project and organizational level ‚Ä¢ Ensure optimal performance through continuous monitoring and code optimization. ‚Ä¢ Refactor existing code and enhance system architecture to improve maintainability and scalability. ‚Ä¢ Design and evolve the test automation strategy, including technology stack and solution architecture. ‚Ä¢ Prepare reviews, participate in retrospectives, estimate user stories, and refine features ensuring their readiness for development. Personal requirements: ‚Ä¢ Have 4+ years of Apache Spark experience and have faced various data engineering challenges in batch or streaming ‚Ä¢ Have an interest in stream processing with Apache Spark Structured Streaming on top of Apache Kafka ‚Ä¢ Have experience leading technical solution designs ‚Ä¢ Have experience with distributed systems on a cloud platform ‚Ä¢ Have experience with large-scale systems in a microservice architecture ‚Ä¢ Are familiar with Git and CI/CD practice s and can design or implement the deployment process for your data pipelines ‚Ä¢ Possess a proactive approach and can-do attitude ‚Ä¢ Are excellent in English and Polish, both written and spoken ‚Ä¢ Have a higher education in computer science or a related field ‚Ä¢ Are a team player with strong communication skills Nice to have requirements: ‚Ä¢ Apache Spark Structured Streaming ‚Ä¢ Azure ‚Ä¢ Domain Driven Development ‚Ä¢ Docker containers and Kubernetes ‚Ä¢ Message brokers (i.e. Kafka) and event-driven architecture ‚Ä¢ Agile/Scrum Our offer: ‚Ä¢ Contract type: B2B ‚Ä¢ Work Mode : Flexible ‚Äî this role supports on-site , hybrid , and remote arrangements, depending on your individual preferences. ‚Ä¢ Occasional on-site presence may be required ‚Äî for example, onboard new team members, explore new business domains, or refine requirements in close collaboration with stakeholders or team building activities. What does the recruitment process look like? ‚Ä¢ Phone conversation with Recruitment Partner ‚Ä¢ Technical interview with the Hiring Team ‚Ä¢ Cognitive test ‚Ä¢ Offer KMD (an NEC company) is committed to providing equal opportunities. Hence, we invite all qualified interested applicants to apply for career opportunities. At KMD all aspects of employment and cooperation including the decision to hire/cooperate with will be based on merit, competence, performance, and business needs without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other status protected under local anti-discrimination legislation.","[{""min"": 115, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Hybrid,608,Senior Data Integration Engineer,EPAM Systems,"We are looking for a Senior Data Integration Engineer to join the Client‚Äôs security team, help develop and support data products (data pipelines, data integration, data analytics dashboard). Our team is distributed between the US and the EU (Poland, Portugal). This role offers a hybrid model, with 3 days per week working from the client's office in Wroclaw, Gdansk or Krakow. RESPONSIBILITIES Assess Client‚Äôs internal infrastructure, get used to Client‚Äôs services and tools Work closely with the Client to understand their data needs, consult them and develop reporting and analytics solutions Use internal Client‚Äôs tools, build new data pipelines and ETL workflows for various use cases Implement PoCs, perform data integration and migration activities Implement data quality checks and monitor data pipelines to ensure they perform as expected Investigate and support released data products Own the code, plan and do sanity check before initiating code review Develop and maintain technical documentation Participate closely in the refinement and scope definition Present the results and outcomes REQUIREMENTS 2+ years of hands-on experience in SQL Proven expertise in dimensional data modeling, data warehousing Good communication, problem solving, and collaboration skills English: B2 Availability from 8 AM to 10 AM Pacific Time (17: 00 - 18: 00 Poland Time) is required. About once a week, the candidate should be available until 19: 00 Poland Time NICE TO HAVE Experience in programming and scripting languages such as Python, Bash, or Java Data-driven CI/CD development WE OFFER We gather like-minded people: Engineering community of industry professionals Friendly team and enjoyable working environment Flexible schedule and opportunity to work remotely within Poland Chance to work abroad for up to 60 days annually Business-driven relocation opportunities We provide growth opportunities: Outstanding career roadmap Leadership development, career advising, soft skills, and well-being programs Certification (GCP, Azure, AWS) Unlimited access to LinkedIn Learning, Get Abstract, Cloud Guru English classes We cover it all: Stable income (Employment Contract or B2B) Participation in the Employee Stock Purchase Plan Benefits package (health insurance, multisport, shopping vouchers) Strategically located offices featuring entertainment and relaxation zones, table tennis and football, free snacks, fantastic coffee, and more Referral bonuses Corporate, social and well-being events Please, note: The set of bonuses might vary based on the role you apply for ‚Äì specifics will be discussed with our recruiter during the general interview We will reach out to selected candidates exclusively EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,609,BI Consultant,Primaris,"Forma wsp√≥≈Çpracy : kontrakt B2B Tryb: praca zdalna Aktualnie do jednego z projekt√≥w/zespo≈Ç√≥w poszukujemy osoby na stanowisko BI Consultant , kt√≥ra posiada min. 3 lat komercyjnego do≈õwiadczenia. Dlatego je≈õli szukasz pracy z : zaawansowanym SQL, tworzeniem raport√≥w w Power BI, pracƒÖ z Microsoft SQL Server i SSRS, a tak≈ºe chcesz rozwijaƒá swoje umiejƒôtno≈õci w obszarze Business Intelligence ‚Äì chƒôtnie z TobƒÖ porozmawiamy! WiodƒÖcy stack technologiczny: SQL, T-SQL, Power BI, Microsoft SQL Server, SSRS, Report Builder, Salesforce. Projekt kt√≥ry bƒôdziemy gotowi Ci zaproponowaƒá dotyczy : Rozwoju rozwiƒÖza≈Ñ raportowych dla miƒôdzynarodowego systemu do zarzƒÖdzania portfelem projekt√≥w, wykorzystywanego przez globalne firmy z obszaru HR, logistyki i produkcji. Praca obejmuje tworzenie zaawansowanych raport√≥w na bazie produkcyjnej z wykorzystaniem SQL i Power BI, w oparciu o zg≈Çoszenia w systemie Salesforce. Zakres raportowania obejmuje dane projektowe, finansowe i zasoby projektowe. Wymagania: Min. 3 roku komercyjnego do≈õwiadczenia na podobnym stanowisku Bardzo dobra znajomo≈õƒá SQL + tworzenie zapyta≈Ñ do bazy Bieg≈Ça znajomo≈õƒá jƒôzyka angielskiego (C1) Zaawansowana znajomo≈õƒá PowerBI (g≈Ç√≥wnie Desktop) Do≈õwiadczenie w komunikacji i wsp√≥≈Çpracy bezpo≈õrednio z klientem Mile widziane bƒôdzie do≈õwiadczenie pracy z narzƒôdziami raportowymi: Microsoft Report Bulider W naszej firmie bƒôdziesz m√≥g≈Ç/mog≈Ça liczyƒá na: Pracƒô w organizacji z ugruntowanƒÖ pozycjƒÖ rynkowƒÖ Projekty, w kt√≥rych bƒôdziesz mia≈Ç/mia≈Ça wp≈Çyw na ich rozw√≥j Wsp√≥≈Çpracƒô z ciekawymi klientami biznesowymi z r√≥≈ºnych bran≈º ( m.in .: finanse, bankowo≈õƒá, ubezpieczenia, healthcare, robotyzacja, energetyka, media), Permanentny mentoring zar√≥wno techniczny jak i biznesowo-mened≈ºerski, np. podczas naszych cyklicznych szkole≈Ñ ( m.in . Git, Gitflow, Angular, Docker), czy wew. program√≥w rozwojowych (Primaris x TechTalks, Primaris Leadership Academy) oraz zewnƒôtrznych kurs√≥w.Ju≈º na etapie on-boardingu zapewniamy dostƒôp do naszych wewnƒôtrznych szkole≈Ñ, cyklicznych spotka≈Ñ, kt√≥re serializujemy na Confluence oraz platformy e-learning ≈öwietnƒÖ atmosferƒô pracy, w≈õr√≥d zaanga≈ºowanych ludzi z pasjƒÖ w p≈Çaskiej strukturze z prostymi procesami Kompleksowy pakiet benefit√≥w skrojonych na miarƒô - prywatna opieka medyczna dla Ciebie oraz dla Twojej rodziny, Multisport dla Ciebie i os. towarzyszƒÖcej - Ty decydujesz, co wybierasz! Ca≈Çy proces rekrutacyjny oraz onboarding prowadzony jest zdalnie. Proces rekrutacyjny sk≈Çada siƒô z: rozmowy telefonicznej z osobƒÖ z dzia≈Çu Rekrutacji & HR (do 30 min) zdalnej video rozmowy - weryfikacji techniczno-biznesowej z naszym specjalistƒÖ/specjalistkƒÖ (60-90 min) zdalnego spotkania z liderem po stronie klienta projektu (30-60 min) finalnej decyzji dotyczƒÖcej oferty Primaris Services to ponad 250 ekspert√≥w na pok≈Çadzie i 15 lat do≈õwiadczenia w bran≈ºy IT na rynku polskim oraz zagranicznym.Realizujemy ambitne projekty o wysokiej z≈Ço≈ºono≈õci z r√≥≈ºnych obszar√≥w - m.in . bankowo≈õci, ubezpiecze≈Ñ, funduszy inwestycyjnych czy bran≈ºy logistycznej (mamy ponad 40 aktywnych klient√≥w!). Ro≈õniemy w si≈Çƒô oraz ciƒÖgle poszerzamy portfolio zar√≥wno naszych us≈Çug jak i klient√≥w. Zakres naszej dzia≈Çalno≈õci obejmuje budowƒô system√≥w od zera, ich rozw√≥j oraz utrzymanie, wdro≈ºenia produktowe, alokacje ca≈Çych Zespo≈Ç√≥w, a tak≈ºe pojedynczych Ekspert√≥w w strukturach Klienta. Ponadto od kilku lat dzia≈Çamy bardzo intensywnie jako z≈Çoty Partner firmy UiPath (obszar Robotic Process Automation) budujƒÖc roboty i sprzedajƒÖc licencje u naszych Klient√≥w. Co miesiƒÖc do≈ÇƒÖcza do nas 7 nowych os√≥b! Wierzymy, ≈ºe zgrany zesp√≥≈Ç i ludzie z pasjƒÖ to klucz do naszego wsp√≥lnego sukcesu! W≈Ça≈õnie dlatego ciƒÖgle poszukujemy nowych, zdolnych os√≥b, kt√≥re zasilƒÖ nasze szeregi.","[{""min"": 80, ""max"": 100, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,610,Data Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients‚Äô systems, departments and sites. We provide an open technology platform that‚Äôs shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience. Want to be part of it? Job Title: Data Platform Engineer Job Summary : We are seeking a highly skilled and experienced Data Engineer to join our team. The successful candidate will be responsible for developing and maintaining our Data Lake and existing Data Pipelines, as well as continually exploring, analyzing, and proposing improvements to existing processes and tooling. They will also be responsible for ensuring best practices are being adopted and staying up to date with the latest research and trends in Data Engineering. Skills Required: Strong background in Data Engineering, with experience in developing and maintaining Data Pipelines and Data Lakes Proven record of accomplishment of staying up to date with the latest research and best practices in Data Engineering Excellent technical skills in Data Engineering tools and technologies Advanced proficiency in Python and SQL Understanding of AWS cloud technologies, including infrastructure as code (CDK preferred) Effective communication and interpersonal skills, with the ability to work effectively with stakeholders at all levels Strong understanding of information security and data protection principles Experience in driving technical and career development, creating appropriate goals and seeking learning opportunities within the company and the wider software community Good understanding and prior experience of the Agile process (Scrum or Kanban) Fluency with software design patterns Experience working with automotive retail technology would be a distinct advantage Key Responsibilities: Maintain and develop the Data Lake and existing Data Pipelines to support the product and data teams‚Äô requirements Continuously explore, analyze, and propose improvements to existing processes and tooling Stay up to date with the latest research, trends and best practices in Data Engineering Support the Business Intelligence team and wider Company in querying centralized data stores, including the Data Lake Work within department to maintain an ongoing understanding of the company‚Äôs data strategy and roadmap Proactively report on issues and problems Work independently, manage day-to-day workload and priorities, and take accountability for direction and output Drive your own technical and career development, create appropriate goals, and seek learning opportunities within the company and the wider software community Support colleagues on calls or in meetings with clients, partners, and suppliers as required Maintain systems under the team‚Äôs control, including user and access management Support colleagues and HR with onboarding as well as offboarding processes Ensure information security, data protection and support the business in complying with any legal obligations imposed upon it through positive actions Technologies: Python SQL: Trino, Spark-SQL, Hive, TSQL AWS Cloud services (including: s3, step functions, glue, CDK) Terraform Linux Windows Why join us? We‚Äôre on a journey to become market leaders in our space ‚Äì and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We‚Äôre committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles ‚Äì not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn‚Äôt require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply .","[{""min"": 18000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Remote,611,Data Architect (Vault 2.0 and Snowflake),Upvanta sp. z o.o.,"Assess the client's existing data platform and define a clear and actionable transformation roadmap Define the platform strategy and long-term vision aligned with business needs Lead stakeholder engagement and manage change processes (change management) Coordinate and plan delivery efforts across multiple teams Potential to lead a technical delivery team Proven experience with Data Vault 2.0 modeling and architecture Strong hands-on experience with Snowflake Practical knowledge of DBT (Data Build Tool) for data transformation workflows Familiarity with AWS cloud services (e.g., S3, Lambda, Glue) Solid background in stakeholder management and delivering data transformation projects Team leadership experience is a plus, or a willingness to take on a managerial role",[],Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,612,Senior Automation Hero,Automation House,"ü§ñ Jak dzia≈Çamy? W Automation House pracujemy w zgranym i zaanga≈ºowanym zespole, kt√≥ry zajmuje siƒô narzƒôdziami no-code oraz low-code i automatyzacjƒÖ proces√≥w w firmach. Nasza misja to ocaliƒá naszych Klient√≥w przed ciƒÖg≈ÇƒÖ i ≈ºmudnƒÖ pracƒÖ manualnƒÖ, wdra≈ºajƒÖc rozwiƒÖzania AI i wprowadzajƒÖc ich w nowƒÖ rzeczywisto≈õƒá. Pomagamy uporzƒÖdkowaƒá procesy i usprawniƒá codziennƒÖ pracƒô, co przynosi ogromnƒÖ satysfakcjƒô, majƒÖc ≈õwiadomo≈õƒá rzeczywistego, pozytywnego wp≈Çywu na funkcjonowanie biznesu naszych Klient√≥w. Stawiamy na pe≈ÇnƒÖ transparentno≈õƒá wsp√≥≈Çpracy i zaanga≈ºowanie, dobierajƒÖc mo≈ºliwie najlepiej pasujƒÖcych specjalist√≥w do profilu firmy. Przygotowali≈õmy dla Ciebie ciekawy i anga≈ºujƒÖcy proces rekrutacji z udzia≈Çem zespo≈Çu Automation House i Tigers! ü§ñ Za co bƒôdziesz odpowiedzialny? üîπ Projektujesz, wdra≈ºasz i utrzymujesz procesy automatyzacji przy u≈ºyciu narzƒôdzi no-code i AI. üîπ Budujesz relacje oraz utrzymujesz sta≈Çy kontakt ze swoimi klientami. üîπ Prowadzisz warsztaty dla potencjalnych i obecnych klient√≥w firmy. üîπ Prowadzisz analizƒô biznesowƒÖ pod kƒÖtem optymalizacji proces√≥w (mapowanie proces√≥w) üîπOdpowiadasz za projektowanie relacyjnych baz danych üîπ Przygotowujesz propozycje automatyzacji i uczestniczysz w ofertowaniach. üîπ Bierzesz udzia≈Ç w dzia≈Çaniach majƒÖcych na celu promocjƒô firmy (eventy, warsztaty). üîπ Monitorujesz i analizujesz wydajno≈õƒá wdro≈ºonych rozwiƒÖza≈Ñ oraz wprowadzasz niezbƒôdne usprawnienia. ü§ñ Co cechuje idealnego Senior Automation Hero? üîπ Min. 5-letnie do≈õwiadczenie w zakresie optymalizacji i automatyzacji proces√≥w. üîπ Doskona≈Ça znajomo≈õƒá narzƒôdzi Make (dawniej Integromat) oraz Airtable oraz ClickUp. üîπ Mile widziana znajomo≈õƒá n8n oraz innych narzƒôdzi automatyzacyjnych. üîπ Umiejƒôtno≈õƒá analizy i rozumienia proces√≥w biznesowych klienta oraz mapowania tych proces√≥w. üîπ Zainteresowanie nowymi technologiami i AI üîπ Do≈õwiadczenie w pracy z biznesem, klientami B2B. üîπ Znajomo≈õƒá jƒôzyka angielskiego na poziomie min. B2. üîπ Dostƒôpno≈õƒá full-time. ü§ñ Co oferujemy? üîπ B≈Çyskawiczny rozw√≥j ‚Äì u nas w kwarta≈Ç nauczysz siƒô tyle, ile w innych miejscach w rok. üîπ Brak sufitu ‚Äì chcesz i≈õƒá w eksperta czy account managera? Sam zdecyduj! Masz pomys≈Ç na rozw√≥j us≈Çugi? Dostajesz wsparcie, by to zrobiƒá. üîπ Ekspozycja w bran≈ºy ‚Äì posiadamy najmocniejsze kana≈Çy komunikacji w bran≈ºy, w kt√≥rych mo≈ºesz budowaƒá wizerunek eksperta. üîπ Kole≈ºe≈Ñski zesp√≥≈Ç ‚Äì w Automation House szczeg√≥lnƒÖ wagƒô przywiƒÖzujemy do budowania relacji miƒôdzy sobƒÖ! üîπ Doskona≈Ço≈õƒá operacyjna ‚Äì dbamy o Tw√≥j komfort pracy dziƒôki przejrzystym procesom i pe≈Çnej jasno≈õci ‚Äì wiesz co robiƒá i wiesz po co to robisz. üí∏ Wynagrodzenie Stawka godzinowa na tym stanowisku to 100z≈Ç/h netto na fakturze (B2B).",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Hybrid,613,Senior Data Architect,KUBO,"We are looking for a Senior Data Architect to join our global Data & Analytics team and lead the design and implementation of scalable, reusable, and high-performing data and AI solutions. This role is critical to delivering analytical products that empower data-driven decision-making across various business domains. As a Senior Data Architect, you will take ownership of architecture and implementation, ensuring solutions are scalable, cost-efficient, and aligned with data governance and security standards. Key responsibilities: Lead the technical implementation of analytical and AI solutions in e.g. finance, controlling Own the development of harmonized, enriched, and reusable data models and KPIs based on data from multiple sources Translate business and functional requirements into technical architecture, considering scalability, reusability, performance, security, and cost efficiency Lead and mentor internal and external development teams Design and orchestrate end-to-end data pipelines, implement data quality and governance processes, and ensure delivery of production-ready data assets Ideal candidate profile: At least 5+ years of experience in Data & Analytics or AI, ideally in supply chain Proven experience in leading technical teams Strong technical knowledge in data architecture and engineering using technologies such as Azure Data Lake, Azure Synapse, Databricks Experience with BI tools like Tableau or Power BI and understanding of data cataloging and data quality management Fluent English Conditions: Work model: Hybrid ‚Äì 1 day per week in the Warsaw office Salary: 20 000-28 000 PLN gross/month Employment type: Full-time employment contract (UoP) directly with the client Business trips to Germany 1-2 times a year Benefits: VIP Medical Care Package, Life & Travel Insurance, Company Bonus, Holiday allowance, Co-financed sport card *Relocation package and full support with relocation to Warsaw Recruitment steps: Phone call with a Recruiter (20 - 30 min.) First interview with a Manager (1h) Second interview with a technical team (1h) Feedback and decision","[{""min"": 20000, ""max"": 28000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,Data Architecture
Full-time,Senior,Permanent,Hybrid,614,Senior Data Engineer,Harvey Nash Technology,"3+ years of strong Python development experience and solid engineering practices Expertise in microservices frameworks and event-driven architectures Advanced Spark skills (PySpark, Scala) with experience in scalable, maintainable pipelines Experience with Spark Streaming and Delta Lake Familiarity with Kubernetes and MongoDB Proven AWS cloud experience Design, build, and enhance data pipelines for streaming and batch processing Extend and support our AWS cloud data platform Develop features using Databricks pipelines, Unity Catalog, and Spark Streaming Lead and mentor the team, promoting best practices and process improvements Gather requirements from stakeholders and deliver high-quality solutions Partner with customers to ensure project success What You‚Äôll Bring 3+ years of strong Python development experience and solid engineering practices Expertise in microservices frameworks and event-driven architectures Advanced Spark skills (PySpark, Scala) with experience in scalable, maintainable pipelines Experience with Spark Streaming and Delta Lake Familiarity with Kubernetes and MongoDB Proven AWS cloud experience Strong communication and a commitment to high ethical standards","[{""min"": 32000, ""max"": 40000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,615,Database Administrator,Upvanta sp. z o.o.,"Instalacja, konfiguracja i utrzymanie system√≥w zarzƒÖdzania bazami danych. Monitorowanie wydajno≈õci baz danych oraz wdra≈ºanie optymalizacji poprawiajƒÖcych efektywno≈õƒá dzia≈Çania. Zapewnienie integralno≈õci i bezpiecze≈Ñstwa danych poprzez stosowanie odpowiednich ≈õrodk√≥w ochronnych. Regularne wykonywanie kopii zapasowych baz danych oraz operacji odzyskiwania danych. Wsp√≥≈Çpraca z zespo≈Çem programist√≥w w zakresie projektowania i optymalizacji struktur baz danych. Diagnozowanie problem√≥w i udzielanie wsparcia technicznego u≈ºytkownikom baz danych. Prowadzenie dokumentacji dotyczƒÖcej konfiguracji i procedur zarzƒÖdzania bazƒÖ danych. ≈öledzenie najnowszych trend√≥w i najlepszych praktyk w dziedzinie baz danych. Wykszta≈Çcenie wy≈ºsze w dziedzinie Informatyki, Technologii Informacyjnych lub pokrewnej. Do≈õwiadczenie na stanowisku Administratora Baz Danych lub w podobnej roli. Znajomo≈õƒá system√≥w zarzƒÖdzania bazami danych takich jak Oracle, SQL Server lub MySQL. Dobra znajomo≈õƒá projektowania baz danych oraz modelowania danych. Umiejƒôtno≈õƒá wykonywania procedur backupu i odzyskiwania danych. Zdolno≈õci analityczne, dok≈Çadno≈õƒá oraz umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w. Komunikatywno≈õƒá i umiejƒôtno≈õƒá pracy w zespole. Do≈õwiadczenie z rozwiƒÖzaniami baz danych w chmurze. Znajomo≈õƒá hurtowni danych oraz proces√≥w ETL. Wiedza z zakresu bezpiecze≈Ñstwa baz danych.",[],Database Administration,Database Administration
Full-time,Senior,Permanent or B2B,Hybrid,616,DataOps Engineer,ALTEN Polska,"‚ñ∂Ô∏è Lokalizacja: Warszawa / Krak√≥w / Gda≈Ñsk ‚ñ∂Ô∏è Bran≈ºa : bankowo≈õƒá ‚ñ∂Ô∏è Do≈õwiadczenie: 6+ lat ALTEN ‚Äì ≈öwiatowy lider us≈Çug in≈ºynieryjnych i IT Grupa ALTEN, za≈Ço≈ºona w 1988 roku we Francji, jest wiodƒÖcym partnerem technologicznym, obs≈ÇugujƒÖcym najwiƒôkszych klient√≥w na ≈õwiecie. W 2023 roku osiƒÖgnƒô≈Ça przychody przekraczajƒÖce 4,07 miliarda euro i zatrudnia ponad 57 000 pracownik√≥w, w tym 88% in≈ºynier√≥w i ekspert√≥w IT. ALTEN Polska dzia≈Ça od 13 lat, wspierajƒÖc innowacje, badania i rozw√≥j oraz systemy IT swoich klient√≥w. Posiada biura w Krakowie, Warszawie, Wroc≈Çawiu, Gda≈Ñsku i Poznaniu. Obs≈Çuguje kluczowe bran≈ºe, takie jak lotnicza, kosmiczna, przemys≈Çowa, motoryzacyjna, kolejowa, energetyczna, finansowa, telekomunikacyjna i sprzeda≈ºowa. Zakres ekspertyzy : analiza biznesowa, cyberbezpiecze≈Ñstwo, sztuczna inteligencja, analiza danych i rozwiƒÖzania chmurowe, rozw√≥j aplikacji, testowanie oprogramowania, zarzƒÖdzanie projektami, oprogramowanie wbudowane, zarzƒÖdzanie jako≈õciƒÖ, industrializacja, transformacja cyfrowa oraz in≈ºynieria przemys≈Çowa. Wiƒôcej informacji na http: //www.altenpolska.pl Opis projektu: Realizacja projektu zwiƒÖzanego z rozwojem i utrzymaniem system√≥w danych oraz platform przetwarzania danych w bran≈ºy bankowej / finansowej. Zakres odpowiedzialno≈õci: Wdra≈ºanie nowych funkcji i system√≥w, aktualizacji i konfiguracji Monitorowanie wydajno≈õci i dostƒôpno≈õci system√≥w Analiza b≈Çƒôd√≥w, obs≈Çuga incydent√≥w, wsp√≥≈Çpraca z innymi zespo≈Çami Projektowanie i rozw√≥j platform danych (ETL, SQL, dbt, Airbyte, Dagster, Snowflake) Optymalizacja zapyta≈Ñ, automatyzacja proces√≥w i wsparcie analityczne Wymagania: Do≈õwiadczenie na poziomie seniora (6‚Äì8 lat) Bardzo dobra znajomo≈õƒá SQL/NoSQL, baz danych i rozwiƒÖza≈Ñ ETL Znajomo≈õƒá ≈õrodowisk chmurowych (AWS, Azure, GCP) oraz narzƒôdzi DevOps (CI/CD, kontrola wersji) Praktyka w pracy z Dockerem i Kubernetesem Znajomo≈õƒá jƒôzyka angielskiego na poziomie min. B2 Gotowo≈õƒá do dy≈ºur√≥w on-call (sporadycznie) Umiejƒôtno≈õƒá pracy zespo≈Çowej w metodykach zwinnych (SCRUM) DNA ALTEN ≈ÇƒÖczy w sobie ludzkie warto≈õci, kulturƒô doskona≈Ço≈õci i do≈õwiadczenie w s≈Çu≈ºbie swoim klientom. Oferujemy indywidualne podej≈õcie do ka≈ºdego pracownika w duchu work-life balance. K≈Çadziemy nacisk na odpowiedzialny, zr√≥wnowa≈ºony rozw√≥j i proekologiczne rozwiƒÖzania. Oferujemy: ‚ñ™ Umowƒô na pe≈Çen etat z mo≈ºliwo≈õciƒÖ wyboru formy zatrudnienia (UoP/B2B) ‚ñ™ StabilnƒÖ i d≈ÇugoterminowƒÖ wsp√≥≈Çpracƒô ‚ñ™ Jasno zdefiniowanƒÖ ≈õcie≈ºkƒô kariery i mo≈ºliwo≈õƒá rozwoju ‚ñ™ Udzia≈Ç w konferencjach bran≈ºowych, szkoleniach i warsztatach oraz spotkaniach integracyjnych ‚ñ™ Mo≈ºliwo≈õƒá odbycia kurs√≥w i zdobycia certyfikacji ‚ñ™ Mo≈ºliwo≈õƒá relokacji w ramach lokalnych oddzia≈Ç√≥w ALTEN Polska BENEFITY: ‚ñ™ Prywatna opieka dentystyczna Medicover ‚ñ™ Platforma zakupowa Medicover Benefits / karta sportowa Medicover ‚ñ™ Program polece≈Ñ pracownik√≥w ALTEN Talenty ‚ñ™ Wyprawka dla nowonarodzonego dziecka pracownika ‚ñ™ Ubezpieczenie grupowe na ≈ºycie ‚ñ™ Mo≈ºliwo≈õƒá udzia≈Çu w programie emerytalnym Do≈ÇƒÖcz do naszego zespo≈Çu i razem z nami tw√≥rz przysz≈Ço≈õƒá technologii ju≈º dzi≈õ! KlikajƒÖc w przycisk ‚Äû Wy≈õlij ‚Äù zgadzasz siƒô na przetwarzanie przez Alten Polska sp. z o.o. z siedzibƒÖ w Warszawie Twoich danych osobowych zawartych w zg≈Çoszeniu rekrutacyjnym w celu prowadzenia rekrutacji na stanowisko wskazane w og≈Çoszeniu. W ka≈ºdym momencie mo≈ºesz cofnƒÖƒá zgodƒô, kontaktujƒÖc siƒô z nami pod adresem ALTEN_PL_RODO@alten.com , co oznacza zako≈Ñczenie rekrutacji i nieuwzglƒôdnienie Twojej kandydatury przy przysz≈Çych rekrutacjach. Pe≈ÇnƒÖ informacjƒô odno≈õnie przetwarzania Twoich danych osobowych znajdziesz https: //docs.altenpolska.pl/do_pobrania/Klauzula_informacyjna_dla_potencjalnych_pracownikow.pdf UWAGA: Uprzejmie informujemy, ≈ºe skontaktujemy siƒô z wybranymi kandydatami",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,617,Data Engineer,Tooploox,"We are Tooploox üíé , an AI software development company offering custom AI solutions and services. We help innovative companies and startups design and build digital products with generative AI, mobile, and web technologies. Our team, consisting of nearly 200 experts including our R&D team of over 40 engineers, many with PhDs, has pioneered AI solutions across industries like healthcare, fashion, and e-commerce. We‚Äôve published over 15 research papers in top conferences like NeurIPS and ICML. We're on the lookout for a Data Engineer üìä to take on a pivotal role in our team. You'll be at the heart of working with data, focusing on scalable batch and streaming data pipelines . If you're someone who loves to merge traditional software development with innovative AI technologies, this role is tailor-made for you. Design, develop, and maintain scalable batch and streaming data pipelines. Work with Python to transform, process, and integrate data. Handle a mix of structured and unstructured data, including work with NoSQL and vector databases. Optimize performance across big data workflows , including tuning Hive and Spark jobs. BS/BA in Data Engineering/Computer Science + 2 years of experience or related field or 5 years of relevant experience. Extensive expertise with Apache Spark (especially PySpark), Hadoop, and Apache Hive with a proven track record of optimizing large-scale data systems. Strong programming skills in Python. Comprehensive understanding of database concepts, including experience with NoSQL databases (e.g., MongoDB, Redis) and ideally vector databases. Proven hands-on experience with stream processing, preferably using Apache Flink. In-depth knowledge of distributed computing, data warehousing, and performance optimization techniques. Exceptional problem-solving and communication skills, with experience working in cross-functional teams. Fluency in Polish and English. Experience with LLMs, prompt engineering, or machine learning workflows (we use this in conjunction with vector DBs). Experience in Java or Scala - useful for deeper Spark optimization or contributing to broader engineering projects. Familiarity with Spring Boot for building and deploying data applications. üèñÔ∏è 26 days of annual service break. ü§í An additional pool of 14 days per year paid at 80% of your standard rate . üá¨üáß English lessons once a week or more frequently, depending on your needs. üìö Access to a curated library of books and e-books, regularly updated based on employee suggestions, plus recurring knowledge-sharing initiatives . üè° Flexible hours and the option to work 100% remotely or from one of our offices in Wroc≈Çaw or Warsaw . üíª Top-quality equipment ‚Äì we provide MacBooks, new monitors, noise-cancelling headphones, and any additional gear you may need to work comfortably. üè• Group insurance with Warta and private medical care with Enel-Med for just 1 PLN. üß† Mental health support ‚Äì we offer access to a psychologist with fully anonymous consultations if needed. üèãÔ∏è‚Äç‚ôÇÔ∏è Multisport card (we cover most of the cost ‚Äì your contribution is currently no more than 45 PLN, or less depending on the selected package), access to gyms in our Wroc≈Çaw and Warsaw offices , and sports initiatives like the annual Bike 2 Work Challenge . üçïüéÆüï∫üèª We host team lunches, webinars, game nights, and social events . We enjoy the occasional barbecue, dance party, time on the terrace, foosball, or PlayStation session.","[{""min"": 18000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Part-time,Senior,B2B,Remote,618,Senior BI Consultant / Analityk Power BI,Ness Solution,"üéØ Poszukiwany Senior BI Consultant / Analityk Power BI Lokalizacja: 100% zdalnie opcjonalnie praca hybrydowa (Warszawa) Umowa B2B | 0,5 FTE Tech stack: Power BI, SQL, R (opcjonalnie), Tableau, Azure üíº O roli Szukamy do≈õwiadczonego konsultanta BI, kt√≥ry po≈ÇƒÖczy kompetencje analityczne, deweloperskie i konsultingowe, aby wspieraƒá kluczowe inicjatywy raportowe oraz strategiczne projekty IT i transformacji operacyjnej w jednej z najwiƒôkszych instytucji finansowych w Polsce. üß∞ Twoje zadania Tworzenie i rozw√≥j zaawansowanych dashboard√≥w w Power BI (DAX, M Query) Projektowanie warstw danych (Data Marts, KPI Engines) Wsp√≥≈Çpraca z zespo≈Çem IT i raportingu zarzƒÖdczego Automatyzacja proces√≥w raportowych (ETL, API, data flows) Udzia≈Ç w definiowaniu metryk, KPI, SLA oraz tworzeniu raport√≥w zarzƒÖdczych (Opcjonalnie) wsparcie w analizach w jƒôzyku R i skryptach w Pythonie / Linuxie ‚úÖ Wymagania Min. 4‚Äì5 lat do≈õwiadczenia na stanowiskach analitycznych, BI lub konsultingowych Bardzo dobra znajomo≈õƒá Power BI / Tableau Praktyczna znajomo≈õƒá SQL ‚Äì umiejƒôtno≈õƒá pisania zapyta≈Ñ, ≈ÇƒÖczenia tabel, optymalizacji Do≈õwiadczenie w pracy z danymi biznesowymi i przygotowywaniu raport√≥w dla interesariuszy Komunikatywno≈õƒá i umiejƒôtno≈õƒá pracy w ≈õrodowisku biznesowym (kontakt z zespo≈Çem i zarzƒÖdem) üí° Mile widziane Znajomo≈õƒá R (np. dplyr , ggplot2 , shiny ) ‚Äì projekt nie wymaga R, ale mo≈ºe siƒô przydaƒá Znajomo≈õƒá Azure Dev, Python , Linux (bash, crontab) üí¨ Oferujemy Wsp√≥≈Çpraca B2B w wymiarze 0,5 etatu (ok. 20h tygodniowo) Mo≈ºliwo≈õƒá elastycznego rozplanowania czasu (2‚Äì3 dni/tydzie≈Ñ lub podzia≈Ç godzinowy) Praca w zespole z do≈õwiadczonymi analitykami i architektami BI Projekt z du≈ºym wp≈Çywem na decyzje zarzƒÖdcze Je≈õli oferta jest dla Ciebie interesujƒÖca, prze≈õlij swoje CV!","[{""min"": 20160, ""max"": 26880, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Hybrid,619,"Analityk Hurtowni Danych (DWH) - SQL, Azure",Upvanta sp. z o.o.,"Poszukujemy Analityka Hurtowni Danych (DWH), kt√≥ry bƒôdzie wspiera≈Ç rozw√≥j, utrzymanie i optymalizacjƒô hurtowni danych wykorzystywanej w analizach biznesowych na du≈ºƒÖ skalƒô. Je≈õli masz do≈õwiadczenie z SQL, Azure i Power BI, swobodnie poruszasz siƒô w ≈õwiecie modelowania danych, a jednocze≈õnie potrafisz ≈ÇƒÖczyƒá potrzeby techniczne z biznesowymi ‚Äì ta rola mo≈ºe byƒá w≈Ça≈õnie dla Ciebie. Lokalizacja: Wroc≈Çaw lub okolice (praca hybrydowa ‚Äì minimum 1 dzie≈Ñ w biurze co tydzie≈Ñ lub raz na 2 tygodnie) Zakres obowiƒÖzk√≥w: Analiza biznesowo-systemowa w kontek≈õcie hurtowni danych oraz projektowanie i implementacja struktur hurtowni. Analiza biznesowo-systemowa w kontek≈õcie hurtowni danych oraz projektowanie i implementacja struktur hurtowni. Modelowanie danych i tworzenie oraz interpretacja modeli ERD. Modelowanie danych i tworzenie oraz interpretacja modeli ERD. Optymalizacja zapyta≈Ñ SQL oraz analiza du≈ºych wolumen√≥w danych (setki milion√≥w transakcji). Optymalizacja zapyta≈Ñ SQL oraz analiza du≈ºych wolumen√≥w danych (setki milion√≥w transakcji). Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi i biznesowymi w celu dostosowania rozwiƒÖza≈Ñ do wymaga≈Ñ klienta. Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi i biznesowymi w celu dostosowania rozwiƒÖza≈Ñ do wymaga≈Ñ klienta. Tworzenie i wdra≈ºanie po≈ÇƒÖcze≈Ñ miƒôdzy hurtowniƒÖ danych a ≈∫r√≥d≈Çami danych, takimi jak sFTP, Azure DB, onPremDB, QuickBase oraz Power BI. Tworzenie i wdra≈ºanie po≈ÇƒÖcze≈Ñ miƒôdzy hurtowniƒÖ danych a ≈∫r√≥d≈Çami danych, takimi jak sFTP, Azure DB, onPremDB, QuickBase oraz Power BI. Utrzymanie hurtowni danych i procedur automatyzacji, diagnozowanie problem√≥w wydajno≈õciowych oraz naprawa b≈Çƒôd√≥w w ≈õrodowisku (np. Azure Data Factory, Azure SQL Server, maszyny wirtualne). Utrzymanie hurtowni danych i procedur automatyzacji, diagnozowanie problem√≥w wydajno≈õciowych oraz naprawa b≈Çƒôd√≥w w ≈õrodowisku (np. Azure Data Factory, Azure SQL Server, maszyny wirtualne). Wykorzystanie system√≥w kontroli wersji (Git) do zarzƒÖdzania kodem oraz wsp√≥≈Çpracy z zespo≈Çami. Wykorzystanie system√≥w kontroli wersji (Git) do zarzƒÖdzania kodem oraz wsp√≥≈Çpracy z zespo≈Çami. Tworzenie dokumentacji technicznej i analitycznej zwiƒÖzanej z implementacjƒÖ i utrzymaniem hurtowni danych. Tworzenie dokumentacji technicznej i analitycznej zwiƒÖzanej z implementacjƒÖ i utrzymaniem hurtowni danych. Dodawanie procedur sk≈Çadowanych, tabel, trigger√≥w (T-SQL, Git, Visual Studio) oraz wdra≈ºanie zmian w ≈õrodowiskach produkcyjnych. Dodawanie procedur sk≈Çadowanych, tabel, trigger√≥w (T-SQL, Git, Visual Studio) oraz wdra≈ºanie zmian w ≈õrodowiskach produkcyjnych. Wymagania: Wykszta≈Çcenie wy≈ºsze z zakresu informatyki lub pokrewne. Wykszta≈Çcenie wy≈ºsze z zakresu informatyki lub pokrewne. Minimum 2 lata do≈õwiadczenia w implementacji i zarzƒÖdzaniu hurtowniami danych. Minimum 2 lata do≈õwiadczenia w implementacji i zarzƒÖdzaniu hurtowniami danych. Bardzo dobra znajomo≈õƒá jƒôzyka SQL (T-SQL, PostgreSQL) na poziomie ≈õredniozaawansowanym. Bardzo dobra znajomo≈õƒá jƒôzyka SQL (T-SQL, PostgreSQL) na poziomie ≈õredniozaawansowanym. Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych (setki milion√≥w transakcji). Do≈õwiadczenie w pracy z du≈ºymi wolumenami danych (setki milion√≥w transakcji). Umiejƒôtno≈õƒá modelowania danych oraz tworzenia modeli ERD. Umiejƒôtno≈õƒá modelowania danych oraz tworzenia modeli ERD. Zdolno≈õci analityczne oraz umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w w kontek≈õcie hurtowni danych. Zdolno≈õci analityczne oraz umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w w kontek≈õcie hurtowni danych. Znajomo≈õƒá systemu kontroli wersji (Git) i narzƒôdzi do zarzƒÖdzania kodem. Znajomo≈õƒá systemu kontroli wersji (Git) i narzƒôdzi do zarzƒÖdzania kodem. Praktyczna znajomo≈õƒá Microsoft Azure oraz Power BI. Praktyczna znajomo≈õƒá Microsoft Azure oraz Power BI. Mile widziane: Znajomo≈õƒá Oracle. Znajomo≈õƒá Oracle. Do≈õwiadczenie w modelowaniu i implementacji hurtowni danych w metodologii Data Vault. Do≈õwiadczenie w modelowaniu i implementacji hurtowni danych w metodologii Data Vault. Znajomo≈õƒá narzƒôdzi do automatyzacji implementacji hurtowni danych (np. Wherescape). Znajomo≈õƒá narzƒôdzi do automatyzacji implementacji hurtowni danych (np. Wherescape). Znajomo≈õƒá technologii SSAS Tabular. Znajomo≈õƒá technologii SSAS Tabular. Znajomo≈õƒá Python. Znajomo≈õƒá Python. Brzmi interesujƒÖco?Je≈õli chcesz pracowaƒá z du≈ºymi zbiorami danych, mieƒá realny wp≈Çyw na architekturƒô hurtowni i rozwijaƒá siƒô w ≈õrodowisku opartym na nowoczesnych technologiach, czekamy na TwojƒÖ aplikacjƒô.",[],Database Administration,Database Administration
Full-time,Mid,B2B,Remote,620,Data Scientist,DCV Technologies,"Data Scientist üìå We are looking for Data Scientist on behalf of our client to join to the project. üìç Remote from Bulgaria ‚óªÔ∏è B2B contract 2 years of experience (7-10 years preferred) working in a Data Scientist role with proven track record of using knowledge of data analysis, statistics, machine learning, and predictive modeling to leverage AutoML tools and build custom solutions to support initiatives Demonstrated and advanced proficiency with programming languages (Python, PySpark, R) and developing custom scripts that execute analytics pipelines using large data sets spanning various levels of complexity and curation (raw data to semantic views) Strong knowledge of machine learning algorithms, including unsupervised (clustering, dimension reduction, association, recommendation systems, graph), supervised (classification, linear regression, decision trees, random forests), deep learning (CNN/GNN/RNN), and reinforcement learning Expertise on best practices related to techniques, methods to use, benefits/limitations of each, and performance improvement options through hyperparameter optimization Demonstrated proficiency with cloud data platforms at an enterprise level and strong knowledge of SQL; preferably Google BigQuery (including BigQuery ML, BigQuery Studio) Additionally, C2H will be an option. üì© If you‚Äôre interested and meet the qualifications, please send your CV to Alina Pchelnikova at alina.pchelnikova@dcvtechnologies.co.uk",[],Data Science,Data Science
Full-time,Senior,B2B,Remote,621,DWH Architect (public/healthcare),Britenet,"Projekt dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Oczekiwania: Minimum 5 lat do≈õwiadczenia zawodowego na stanowisku Architekta IT; Do≈õwiadczenie zawodowe w zakresie projektowania architektur system√≥w zorientowanych na us≈Çugi, system√≥w w architekturze wielowarstwowej, system√≥w o wysokiej wydajno≈õci i niezawodno≈õci; Do≈õwiadczenie projektowe w szacowaniu pracoch≈Çonno≈õci prac programistycznych i architektonicznych; Do≈õwiadczenie projektowe w szacowaniu z≈Ço≈ºono≈õci aplikacji/ rozwiƒÖzania (ilo≈õƒá komponent√≥w, wielko≈õƒá komponent√≥w), skalowanie aplikacji horyzontalne i wertykalne; Do≈õwiadczenie projektowe w zakresie badania i oceny bezpiecze≈Ñstwa informacji w systemach teleinformatycznych; Do≈õwiadczenie w realizacji architektury rozwiƒÖza≈Ñ zawierajƒÖcych elementy hurtowni danych i narzƒôdzi analitycznych; Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z architekturƒÖ, projektowaniem i integracjƒÖ system√≥w IT; Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z architekturƒÖ zorientowanƒÖ na us≈Çugi (SOA) oraz mikroserwisy; Znajomo≈õƒá wzorc√≥w projektowych i architektonicznych, relacyjnych baz danych, serwer√≥w aplikacyjnych oraz integracji system√≥w IT; Znajomo≈õƒá SQL oraz proces√≥w ETL; Do≈õwiadczenie w programowaniu w jƒôzyku Python; Znajomo≈õƒá baz danych PostgreSQL/EDB/MySQL/ MongoDB/Oracle; Znajomo≈õƒá Enterprise Architect; Dobra organizacja pracy w≈Çasnej, orientacja na realizacje cel√≥w; Umiejƒôtno≈õci interpersonalne, w szczeg√≥lno≈õci umiejƒôtno≈õƒá planowania, definiowania, realizacji, oraz monitorowania i rozliczania cel√≥w; Efektywna komunikacja, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista i odporno≈õƒá na stres, proaktywno≈õƒá; Zdolno≈õƒá adaptacji i elastyczno≈õƒá, otwarto≈õƒá na sta≈Çy rozw√≥j i gotowo≈õƒá uczenia siƒô. Mile widziane: Do≈õwiadczenie projektowe w obszarze ochrony zdrowia; Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny); Certyfikat potwierdzajƒÖcy umiejƒôtno≈õci z obszaru projektowania architektury rozwiƒÖza≈Ñ IT (np.. TOGAF¬Æ EA Foundation lub r√≥wnowa≈ºny); Certyfikat potwierdzajƒÖcy wiedzƒô z zakresu administrowania EDB (np. EDB Certification - PostgreSQL Essentials 15 lub r√≥wnowa≈ºny); Certyfikat z obszaru administrowania ≈õrodowiskiem Hadoop (np. Cloudera Certified Administrator for Hadoop (CCAH), Hortonworks Certified Apache Hadoop Administrator (HCAHA) lub r√≥wnowa≈ºny) Kluczowe zadania: Tworzenie koncepcji i projekt√≥w architektury system√≥w DWH zgodnych z wymaganiami biznesowymi i technologicznymi. Wyb√≥r odpowiednich technologii oraz rozwiƒÖza≈Ñ integracyjnych (ETL, bazy danych, narzƒôdzia analityczne). Projektowanie modeli danych (w tym modeli logicznych i fizycznych). Okre≈õlanie podzia≈Çu komponent√≥w systemu, ich zale≈ºno≈õci oraz rozmiaru (szacowanie z≈Ço≈ºono≈õci rozwiƒÖzania). Projektowanie przep≈Çyw√≥w danych oraz proces√≥w ekstrakcji, transformacji i ≈Çadowania. Zapewnienie wydajno≈õci i niezawodno≈õci proces√≥w przetwarzania danych. Okre≈õlanie zakresu i koszt√≥w prac architektonicznych i programistycznych. Ocena skali rozwiƒÖzania oraz rekomendowanie sposob√≥w skalowania (wertykalne/horyzontalne). Identyfikowanie ryzyk zwiƒÖzanych z bezpiecze≈Ñstwem informacji i rekomendowanie ≈õrodk√≥w zaradczych. Wsp√≥≈Çpraca z zespo≈Çami ds. bezpiecze≈Ñstwa w celu wdra≈ºania mechanizm√≥w zabezpieczajƒÖcych. Projektowanie integracji z innymi systemami IT (systemy ≈∫r√≥d≈Çowe, API, hurtownie danych). Wsp√≥≈Çpraca z zespo≈Çami integracyjnymi przy wdra≈ºaniu rozwiƒÖza≈Ñ. Konsultowanie rozwiƒÖza≈Ñ z analitykami, programistami, testerami i interesariuszami biznesowymi. Udzia≈Ç w planowaniu i przeglƒÖdach technicznych. Tworzenie i aktualizacja dokumentacji architektonicznej w narzƒôdziach takich jak Enterprise Architect. Utrzymywanie zgodno≈õci z wewnƒôtrznymi i zewnƒôtrznymi standardami architektonicznymi. Ocena efektywno≈õci wdro≈ºonych rozwiƒÖza≈Ñ i rekomendowanie zmian. ≈öledzenie trend√≥w technologicznych i proponowanie innowacji. Tworzenie lub przeglƒÖd skrypt√≥w i komponent√≥w automatyzujƒÖcych przetwarzanie danych. Zapewnienie jako≈õci kodu i jego zgodno≈õci z architekturƒÖ systemu.","[{""min"": 25000, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Hybrid,622,Senior Looker Specialist (LookML),Spyrosoft,"Requirements: Minimum 2 years of experience as a Data Engineer working with Google Cloud Platform (GCP) and cloud-based infrastructure. Hands-on experience with LookML and Looker modelling language. Proven experience in transforming and migrating data models from SSAS and Power BI Semantic Models to LookML. Ability to translate DAX expressions into LookML formulas. Deep understanding of GCP services and cloud computing architecture. Strong background in designing, building, and deploying cloud-based data pipelines, including ingestion from various data sources (e.g., relational databases). Proficiency in data modelling and database optimisation, including query tuning, indexing, and performance optimisation for efficient data processing and retrieval. Nice to Have: Experience with at least one orchestration and scheduling tool ‚Äì Airflow is strongly preferred. Familiarity with ETL/ELT processes and the ability to integrate data from multiple sources into a usable analytical format. Working knowledge of modern data transformation tools such as DBT and Dataform. Strong communication skills to collaborate effectively with cross-functional teams (data scientists, analysts, business stakeholders). Ability to translate technical concepts into business-friendly language and present findings. Experience leading or actively contributing to discussions with stakeholders to identify business needs and improvement opportunities. Relevant certifications in big data technologies and/or cloud platforms (GCP, Azure). Main responsibilities: Design and develop data models in LookML, adhering to best practices. Support business users in building Looker dashboards. Migrate existing data structures and models to LookML. Support and mentor team members in designing and managing LookML-based data models. Build efficient and optimised aggregations and calculations for analytics purposes. Optimise and continuously improve existing data models to enhance performance and usability.","[{""min"": 120, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,623,Data Engineer,Hays Poland,"Data Engineer - SQL Developer Refactor and re-architect solutions to get rid of legacy Handle incidents and support daily operations Help design and build data pipelines and integrations Work on data integration, transformation, and storage Collaborate with business users to improve existing systems Suggest process improvements (aligned with GxP standards) Document your work clearly and consistently Support cloud-native data services and APIs Join project planning and execution for new and existing apps Provide on-call support for key data services when needed Skills in SQL, T-SQL, C#, SSIS, and VBA Understanding of ETL/ELT, data modeling, and API integration Familiarity with DevOps, version control, and CI/CD pipelines Awareness of data governance and compliance (GxP, GDPR) A sharp eye for detail and a problem-solving mindset Great communication skills and a team-first attitude Career in an Organization with Scandinavian Culture and Values: Experience a work environment that emphasizes equality, work-life balance, and sustainability. Work in a hybrid model (3 days in the office per week). Potential for a permanent contract after an initial 3-month period. Comprehensive medical coverage through Medicover. Sports Card: Access to various sports facilities and activities. Life insurance coverage for added security. Opportunity to be involved in the transition of processes. Attractive and competitive salary.","[{""min"": 11000, ""max"": 17000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent or B2B,Remote,624,Data Engineer,Transition Technologies MS,"Join our AI Emerging Technology & External Collaborations team, a strategic group focused on delivering AI-enabled solutions and harnessing emerging technologies to advance the Pharma and DIA Partnering business. We are at the forefront of innovation - building scalable data infrastructure, applying advanced data engineering practices, and integrating cutting-edge external capabilities to accelerate decision-making and unlock business value.Our work spans across multiple high-impact initiatives, from developing intelligent data products to enabling end-to-end AI workflows. Whether it's structuring complex data ecosystems or collaborating with external research institutions, this is a unique opportunity to help shape the future of data and AI at company by directly supporting key strategic decisions across our global Partnering organization. As a Data Engineer, you will play a critical role in architecting and implementing the data infrastructure to support our AI initiatives. You will collaborate with data scientists, business stakeholders, and external collaborators to enable clean, accessible, and well-modeled datasets that fuel advanced analytics and machine learning solutions. Design and Build Scalable Data Pipelines: Develop and optimize pipelines to ingest, transform, and curate structured and unstructured data from both internal and external sources. Data Profiling, Mapping & Standardization: Profile data, identify quality issues, and align disparate datasets. Define data models and standardization frameworks to support scalable, reusable, and AI/ML-ready data products. Data Product Engineering & API Development: Build, manage, and document secure, reusable data assets and APIs to power advanced analytics and machine learning use cases. Architect and Operationalize Data Infrastructure: Contribute to the architecture and implementation of scalable data platforms, including the Partnering Data Insight Hub, leveraging AWS-native services. AI/ML Enablement: Collaborate with data scientists and AI/ML engineers to ensure data solutions are optimized for downstream AI applications, including support for LLM and AI agent workflows. Metadata Management & Data Governance: Integrate data lineage, governance, and metadata management practices into all solutions to ensure compliance and traceability. Monitoring and Event Frameworks: Design and implement event-driven monitoring systems to track changes in key datasets, enabling real-time alerts for critical updates (e.g., clinical trial data, research releases). Container and Workflow Orchestration: Deploy and manage scalable, portable data processing workloads using containerization (e.g., Docker) and orchestration frameworks such as Amazon EKS. Support orchestration of AI workflows using tools like Google Agent Development Kit (ADK) and similar frameworks. Continuous Improvement: Evaluate and evolve data engineering tools and practices to improve performance, maintainability, and scalability of our data solutions. Basic Qualifications: Bachelor's or Master's degree in Computer Science, Engineering, Data Science, or a related technical field.5+ years of experience in data engineering or a similar role, preferably in a complex enterprise environment.Proven experience building and maintaining scalable data pipelines and architecture in the cloud (preferably AWS). Strong proficiency in SQL, Python, and data modeling. Solid understanding of data quality, integration, transformation, and curation best practices. Experience with structured and unstructured data sources and working with APIs. Strong communication and collaboration skills ‚Äì able to work across teams and functions. Solid understanding of data governance, data security, and data privacy principles. Preferred Qualifications: Experience with defining data standardization and data modeling for complex data ecosystems.Hands-on experience with AWS data services (e.g., Glue, Redshift, Lake Formation, Lambda, S3, Athena, etc ‚Ä¶). Familiarity with data cataloging, metadata management, and governance frameworks. Experience working in support of AI/ML pipelines and data science workflows. Understanding of healthcare/life sciences or partnership/business development domains is a plus. Experience working in an Agile development environment. Interesting and challenging projects Flexible working hours Friendly, non-corporate atmosphere Stable working conditions (CoE or B2B) Possibility for self-development and promotion in the company Rich benefits package Possibility to work remotely We reserve the right to contact the selected candidates.",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,625,Programista PowerBI,Eyzee S.A.,"Poszukujemy Programisty BI z dog≈ÇƒôbnƒÖ znajomo≈õciƒÖ Power BI, kt√≥ry do≈ÇƒÖczy do naszego zespo≈Çu i bƒôdzie odpowiedzialny za rozwijanie, utrzymanie oraz optymalizacjƒô zaawansowanych rozwiƒÖza≈Ñ Business Intelligence. Je≈õli posiadasz do≈õwiadczenie w pracy z du≈ºymi zbiorami danych, tworzeniu raport√≥w i dashboard√≥w zarzƒÖdczych, a tak≈ºe cenisz sobie samodzielno≈õƒá i ciƒÖg≈Çy rozw√≥j, to ta oferta jest dla Ciebie! Tworzymy przyjazne miejsce pracy i rozwoju dla specjalist√≥w w bran≈ºy IT, zapewniamy ciekawe wyzwania, dbajƒÖc o dobrƒÖ komunikacjƒô i atmosferƒô w zespole. Z nami przyspieszysz rozw√≥j swojej kariery! Praca zdalna, pe≈Çen etat. Zadania dla Ciebie: rozw√≥j i wdra≈ºanie rozwiƒÖza≈Ñ w oparciu o Power BI projektowanie i budowanie proces√≥w ETL tworzenie zaawansowanych raport√≥w i dashboard√≥w zarzƒÖdczych konfiguracja, rozwiƒÖzywanie problem√≥w i wdra≈ºanie hurtowni danych z wykorzystaniem Power BI praca z relacyjnymi bazami danych i programowanie w SQL oraz Python zapewnienie bezpiecze≈Ñstwa danych i implementacja row-level-security optymalizacja wydajno≈õci istniejƒÖcych rozwiƒÖza≈Ñ Power BI Wymagania: min. 5 lat do≈õwiadczenia na stanowisku programisty system√≥w raportowych w obszarze wizualizacji danych do≈õwiadczenie w rozwijaniu rozwiƒÖza≈Ñ Power BI praktyczna znajomo≈õƒá architektury ≈õrodowiska Power BI umiejƒôtno≈õƒá budowania raport√≥w w Power BI oraz projektowania dashboard√≥w zarzƒÖdczych znajomo≈õƒá jƒôzyka DAX oraz technik optymalizacji w Power BI do≈õwiadczenie w budowaniu proces√≥w ETL umiejƒôtno≈õƒá pracy z relacyjnymi bazami danych znajomo≈õƒá zagadnie≈Ñ bezpiecze≈Ñstwa danych w Power BI, w tym umiejƒôtno≈õƒá budowania funkcjonalno≈õci row-level-security. znajomo≈õƒá technik agregacji i przetwarzania danych dobra znajomo≈õƒá jƒôzyka Python Mile widziane: do≈õwiadczenie w bran≈ºy medycznej z systemami przetwarzajƒÖcymi du≈ºe wolumeny danych certyfikat z obszaru znajomo≈õci Power BI Co oferujemy? stabilne zatrudnienie w oparciu o kontrakt B2B s≈Çu≈ºbowy laptop i monitor dofinansowanie prywatnej opieki medycznej sportowƒÖ kartƒô Multisport nauka jƒôzyka angielskiego omawianie postƒôp√≥w i rozwoju co p√≥≈Ç roku transparentna komunikacja z pracownikami mo≈ºliwo≈õƒá zaanga≈ºowania siƒô w rozw√≥j organizacji chƒôtnie dzielimy siƒô wiedzƒÖ - do≈ÇƒÖcz do Akademii Eyzee mocny kompetencyjnie zesp√≥≈Ç sk≈ÇadajƒÖcy siƒô w wiƒôkszo≈õci z senior√≥w praca z narzƒôdziami JIRA, Confluence, BitBucket dbamy o integracje i chƒôtnie wsp√≥lnie spƒôdzamy czas Kim jeste≈õmy? Jeste≈õmy polskƒÖ firmƒÖ specjalizujƒÖcƒÖ siƒô w realizacji z≈Ço≈ºonych projekt√≥w informatycznych oraz doradczych dla firm z sektora finansowego, telekomunikacyjnego i publicznego. Stanowimy zgrany zesp√≥≈Ç konsultant√≥w z wiedzƒÖ i wieloletnim do≈õwiadczeniem w tworzeniu i utrzymywaniu rozwiƒÖza≈Ñ. Wa≈ºne dla nas sƒÖ: doprecyzowanie wymaga≈Ñ przed napisaniem kodu, jako≈õƒá tworzonego kodu, testowanie oraz CI/CD. Nasze projekty to g≈Ç√≥wnie tworzenie nowych mikroserwis√≥w lub nowych funkcjonalno≈õci do istniejƒÖcych rozwiƒÖza≈Ñ. Dodatkowo rozwijamy w≈Çasne aplikacje i plugin‚Äôy, kt√≥re nie tylko usprawniajƒÖ pracƒô, ale te≈º pozwalajƒÖ rozwinƒÖƒá nasze do≈õwiadczenie. Jeden z nich mo≈ºesz pobraƒá tutaj (eZee Worklog). Jeste≈õmy partnerem Atlassian.","[{""min"": 15000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Hybrid,626,Data Engineer,PSE Innowacje sp. z o.o.,"PSE Innowacje jest sp√≥≈ÇkƒÖ powsta≈ÇƒÖ w 2012 na zlecenie operatora systemu przesy≈Çowego - PSE S.A. Od 2012 roku realizujemy takie zadania jak: prowadzenie analiz i bada≈Ñ, w tym analiz techniczno-ekonomicznych, prowadzenie prac badawczo-rozwojowych, budowa nowych oraz rozw√≥j i modernizacja istniejƒÖcych system√≥w informatycznych wspierajƒÖcych prowadzenie ruchu sieciowego. NaszƒÖ misjƒÖ jest dba≈Ço≈õƒá o niezawodnƒÖ i efektywnƒÖ pracƒô systemu elektroenergetycznego w Polsce oraz jego sta≈Çy rozw√≥j. Jeste≈õmy jednostkƒÖ do innowacyjnych zada≈Ñ specjalnych w bran≈ºy elektroenergetycznej. Projektowanie i budowanie architektury danych na potrzeby organizacji. Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç, zar√≥wno batch, jak i stream. Automatyzacja proces√≥w ETL oraz orkiestracja przep≈Çyw√≥w danych. Wsparcie dla zespo≈Ç√≥w analitycznych, data scientist√≥w oraz u≈ºytkownik√≥w biznesowych w dostƒôpie do danych. Zapewnienie jako≈õci dostarczanego kodu, wsparcie w analizie, testowaniu i usuwaniu b≈Çƒôd√≥w w oprogramowaniu. Wsp√≥≈Çtworzenie oraz rozw√≥j platformy danych (Data Lakehouse) z naciskiem na przetwarzanie danych oraz eksploracjƒô danych. Praca w interdyscyplinarnym zespole w podej≈õciu Agile. Do≈õwiadczenia na podobnym stanowisku, z naciskiem na przetwarzanie du≈ºych zbior√≥w danych oraz budowƒô stabilnych rozwiƒÖza≈Ñ produkcyjnych. Minimum 3-letniego do≈õwiadczenia w programowaniu w Python, g≈Ç√≥wnie w kontek≈õcie przetwarzania danych (np. PySpark, pandas) i automatyzacji proces√≥w ETL/ELT. Bardzo dobrej znajomo≈õci SQL, praktycznej pracy z danymi oraz znajomo≈õci technik modelowania danych, w tym SCD (Slowly Changing Dimensions). Do≈õwiadczenia w pracy z rozwiƒÖzaniami Data Lake / Lakehouse (np. Apache Iceberg lub podobne technologie). Znajomo≈õci narzƒôdzi do budowania potok√≥w przetwarzania danych (np. Airflow, NiFi, dbt). Znajomo≈õci framework√≥w lub bibliotek do przetwarzania danych (np. Apache Spark, Flink). Znajomo≈õci dobrych praktyk CI/CD oraz pracy z repozytoriami kodu (np. GitLab pipelines). Do≈õwiadczenia z konteneryzacjƒÖ (Docker, Podman, Kubernetes lub OpenShift). Umiejƒôtno≈õci pracy z narzƒôdziami eksploracji danych / BI (np. Superset, Dremio). üíé Mile widziane: Znajomo≈õƒá praktyk i narzƒôdzi MLOps lub DevSecOps. Udzia≈Ç w ciekawych projektach majƒÖcych strategiczny wp≈Çyw na sektor energetyczny w Polsce i Europie. Pracƒô w przyjaznej atmosferze i wsparcie zespo≈Çu nastawionego na dzielenie siƒô wiedzƒÖ oraz do≈õwiadczeniami. KlarownƒÖ ≈õcie≈ºkƒô rozwoju zawodowego oraz szkolenia. Wewnƒôtrzny program mentoringowy, wspierajƒÖcy zar√≥wno nowych, jak i obecnych pracownik√≥w. Bogaty pakiet benefit√≥w: opieka medyczna, karta sportowa, ubezpieczenie na ≈ºycie. Program onboardingowy, pozwalajƒÖcy na szybkƒÖ i przyjaznƒÖ adaptacjƒô do pracy.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,627,Big Data Developer,Altimetrik Poland,"2-3 day per week you need to be available until 9: 00 PM for meetings with the US team. Altimetrik Poland is a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are seeking a skilled Senior Data Engineer for Airbnb- our customer, an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. Together with them, we are building a world-class payments platform that moves billions of dollars, in 191 countries, with 75 currencies, through a complex ecosystem of payments partners. They are also reinventing how to serve users to improve performance, scalability and extensibility. Responsibilities: Proactively and effectively manages communications, ensuring all stakeholders are kept informed about project progress, changes, and updates. Debugs complex issues without assistance. Navigates ambiguous product requirements to build eloquent solutions. Manages multiple projects at once. Collaborates effectively with cross-functional teams such as Data Science, Product Engineering, Advanced Analytics, and other stakeholders to design and implement data solutions. And if you possess.. Solid understanding of Spark and ability to write, debug and optimize Spark code with Python. Strong knowledge of Python, and expertise with data processing technologies and query authoring (SQL). Nice to have: Expertise in data modeling, warehousing, and working with columnar databases (e.g., Redshift, BigQuery). Extensive experience designing, building, and operating robust distributed data platforms (e.g., Spark, Kafka, Flink) and handling data at the petabyte scale. ‚Ä¶ then we are looking for you! We work 100% remotely or from our hub in Krak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 25000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Hybrid,628,BI Developer / SQL Developer,Beesafe,"About Us: Join our trailblazing team as we expand our digital horizons. From our beginnings as visionary InsurTech to becoming a key player in the Polish digital insurance market, we are part of the esteemed Vienna Insurance Group. We're redefining the rules in the insurance industry with our innovative approach. Our hybrid working model supports both the collaborative energy of office work and the flexibility of remote work. About the Role: We're seeking passionate BI/SQL Developer to join our team. You'll be at the heart of developing and implementing high-quality application software, using state-of-the-art tools and technologies. This is your chance to make a significant impact on one of the most exciting and unique products in the Polish digital insurance market. Why it is worth to work with us? You‚Äôll be contributing to the reporting solution and data model design of our Data Platform with close cooperation with our Data Engineers You‚Äôll gather business requirements and work closely with business stakeholders You‚Äôll deliver end-to-end business intelligence/reporting solutions using SQL/PowerBI What you need to start the adventure with us: +1 year of commercial experience in data extraction, ETL, and report development Proficiency in SQL Understanding of Relational Database Management System and Business Intelligence concepts Business and collaboration skills, and responsive to service needs and operational demands Domain knowledge gained across the insurance or financial sector Nice to have: Experience with BI tools (Power BI would be a plus) Experience with cloud solutions (we use Azure) Familiarity with code version control systems such as GIT Understanding of the principles of Agile and Scrum (we work in Scrum) Enthusiastic approach to coffee breaks (we love informal discussions with a cup of favorite coffee or tea) Why Join Us? Be part of a dynamic team driving digital innovation in the insurance and eCommerce sectors Opportunity to work in a collaborative and forward-thinking environment Contract options: B2B cooperation Engage in meaningful work that directly impacts business success Join a company that values work-life balance and fosters a positive team culture Comprehensive onboarding, including a dedicated Buddy program Remote work flexibility with hybrid office visits Flexible working hours Access to the latest tools and cloud-native solutions A comprehensive benefits package, including health insurance and MultiSport card Employee discounts on insurance products Referral program and sports club memberships Sounds interesting? Join us and help shape the future! üöÄ","[{""min"": 9000, ""max"": 12000, ""type"": ""Net per month - B2B""}, {""min"": 9000, ""max"": 12000, ""type"": ""Gross per month - Permanent""}]",Database Administration,Database Administration
Full-time,Senior,Permanent or B2B,Remote,629,Senior Data Engineer AWS&Snowflake,Lingaro,"We are seeking an experienced Cloud Data Engineer proficient with Snowflake and AWS. The ideal candidate will have hands-on experience in designing and implementing data pipelines, data warehousing solutions, and ETL workflows Designing and implementing data processing systems using distributed frameworks like Snowflake. This involves writing efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data. Building data pipelines to ingest data from various sources such as databases, APIs, or streaming platforms. Integrating and transforming data to ensure its compatibility with the target data model or format. Designing and optimizing data storage architectures, including data lakes, data warehouses, or distributed file systems. Implementing techniques like partitioning, compression, or indexing to optimize data storage and retrieval. Identifying and resolving bottlenecks, tuning queries, and implementing caching strategies to enhance data retrieval speed and overall system efficiency. Designing and implementing data models that support efficient data storage, retrieval, and analysis. Collaborating with data scientists and analysts to understand their requirements and provide them with well-structured and optimized data for analysis and modeling purposes. Collaborating with cross-functional teams including data scientists, analysts, and business stakeholders to understand their requirements and provide technical solutions. Communicating complex technical concepts to non-technical stakeholders in a clear and concise manner. Independence and responsibility for delivering a solution. Ability to work under Agile and Scrum development methodologies. 6+ years of professional experience in the Data & Analytics area. Very good level knowledge of Snowflake (min. 4years experience). Vert good knowledge of AWS cloud services (S3, RDS, Redshift, etc.), knowledge of other clouds is an asset. Proven level knowledge of Python, SQL, and PySpark. Very good level of communication, the ability to convey information clearly and specifically, experience in working with business users. Experience working in the Agile (Scrum) methodology. English at least at B2 level, ideally C1. Capable of working both independently and in a team-oriented, collaborative, cross-functional and cross-cultural environment. Passion for the Data & Analytics solutions in the Cloud environment. Missing one or two of these qualifications? We still want to hear from you! If you bring a positive mindset, we'll provide an environment where you feel valued and empowered to learn and grow. Stable employment. On the market since 2008, 1500+ talents currently on board in 7 global sites. ‚ÄúOffice as an option‚Äù model. You can choose to work remotely or in the office. Workation. Enjoy working from inspiring locations in line with our workation policy. Great Place to Work¬Æ certified employer. Flexibility regarding working hours and your preferred form of contract. Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Social fund benefits for everyone. All Lingarians can apply for social fund benefits, such as vacation co-financing. Plenty of opportunities to donate to charities and support the environment. Modern office equipment. Purchased for you or available to borrow, depending on your location.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,630,Lead Data Engineer with Google Cloud (GCP),emagine Polska,"Work model: remote Assignment type: B2B Start: ASAP Project length: 6 months + possible extension Project language: English Industry: Banking The project is focused on decommissioning a legacy CRM system and successfully migrating to a new platform by the end of the year. The main function of the Lead Data Engineer is to support the development of robust data pipelines and ensure smooth data migration and integration. Main Responsibilities Be a self-starter and proactive, experienced Technical Lead. Exhibit excellent communication, interpersonal, and decision-making skills. Participate and influence in regular planning and status meetings. Provide input to the development process through involvement in Sprint reviews and retrospectives. Contribute to system architecture and design. Utilize good experience with SQL and NoSQL databases. Engage in software design, Python & GCP development , and automated testing of new and existing components in an Agile, DevOps, and dynamic environment. Promote development standards, conduct code reviews, and facilitate knowledge sharing. Build data pipelines using technologies such as BigQuery, DataProc, Python, Composer, and Spark. Demonstrate knowledge and experience of the GCP ecosystem and data management frameworks. Have CI/CD experience . Possess knowledge of Software Development Life Cycle (SDLC) and methodologies like DevOps, Agile, Scrum, Waterfall, and Iterative process. Show good knowledge of Data Warehouse concepts. Implement tools and processes while handling performance, scale, availability, accuracy, and monitoring. Key Requirements Experience in a Technical Lead position. Strong background in SQL and NoSQL databases. Proficiency in Python and GCP development. Experience with CI/CD processes. Solid understanding of Agile and DevOps methodologies. Good knowledge of Data Warehouse concepts. Nice to Have Knowledge and experience of Hadoop. Familiarity with PySpark, Scala, Hive, and Java.",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,631,Senior Azure Data Engineer with Databricks,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: Senior Azure Data Engineer with Databricks Responsibilities: Being responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems Building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies Evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards Driving creation of re-usable artifacts Establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation Working closely with analysts/data scientists to understand impact to the downstream data models Writing efficient and well-organized software to ship products in an iterative, continual release environment Contributing and promoting good software engineering practices across the team Communicating clearly and effectively to technical and non-technical audiences Defining data retention policies Monitoring performance and advising any necessary infrastructure changes Requirements: 3+ years‚Äô experience with Azure Data Factory and Databricks 5+ years‚Äô experience with data engineering or backend/fullstack software development Strong SQL skills Python scripting proficiency Experience with data transformation tools - Databricks and Spark Experience in structuring and modelling data in both relational and non-relational forms Experience with CI/CD tooling Working knowledge of Git English level: B2, C1 Nice to have: Experience with Azure Event Hubs, CosmosDB, Spark Streaming, Airflow Experience in Aviation Industry and Copilot Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,632,Data Scientist,Altimetrik Poland,"Altimetrik Poland is a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. For our client from FinTech UK we are looking for an experienced Data Scientist with a specialization in time series modeling and the application of machine learning and deep learning techniques to temporal data. The candidate will focus on building predictive models and extracting insights from time-dependent datasets using both statistical and modern ML/DL approaches. Responsibilities: Design and train models for time series analysis. Develop ML and DL models specifically for time-based data. Analyze time series data for forecasting, classification, and pattern recognition. Collaborate with the wider team to support the deployment of models into production environments. If you possess... Hands-on experience with time series modeling (e.g., ARIMA, Prophet, LSTM). Proven use of ML and DL methods applied to temporal datasets. Strong skills in working with time-stamped data from preprocessing through to model evaluation. We work 100% remotely or from our hub in Krak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 25000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Senior,Permanent,Remote,633,Senior/Lead Data Scientist,Link Group,"Do≈ÇƒÖcz do zespo≈Çu, kt√≥ry wykorzystuje dane, by ulepszaƒá procesy logistyczne. Szukamy osoby o analitycznym umy≈õle, kt√≥ra potrafi przekszta≈Çcaƒá liczby w konkretne dzia≈Çania i wspieraƒá decyzje operacyjne. Twoje zadania: Oczyszczanie i przygotowanie danych do analiz i test√≥w Budowa i doskonalenie modeli uczenia maszynowego Weryfikacja modeli, wyciƒÖganie wniosk√≥w i prezentowanie wynik√≥w Wsp√≥≈Çpraca z zespo≈Çami biznesowymi i technologicznymi Proponowanie nowych sposob√≥w usprawnienia proces√≥w na podstawie danych Czego oczekujemy: Umiejƒôtno≈õci programowania w Pythonie lub R Znajomo≈õci SQL oraz ≈õrodowisk danych, takich jak Databricks lub Snowflake Do≈õwiadczenia w pracy z modelami ML i ich wdra≈ºaniem Podstaw matematycznych (algebra, statystyka, rachunek r√≥≈ºniczkowy) Zdolno≈õci analitycznych i praktycznej znajomo≈õci takich metod jak regresje, drzewa decyzyjne, prognozowanie szereg√≥w czasowych Umiejƒôtno≈õci wizualizacji danych (np. z matplotlib, seaborn, ggplot2) Nice to have: Do≈õwiadczenie w ≈õrodowisku produkcyjnym ML Wiedzƒô z zakresu ≈Ça≈Ñcucha dostaw lub logistyki","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Senior,B2B,Remote,634,Senior Data Engineer,CLOUDFIDE,You are,[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Hybrid,635,Data and Cloud Architect,B2Bnetwork,"We are looking for an experienced Data and Cloud Architect to support transformation initiatives in the areas of data, cloud, and AI. You will be working in the Data & Analytics domain within the Financial Crime Prevention space, helping to drive solution design and ensure architectural governance. The role involves close collaboration with both business and technology stakeholders. Responsibilities: Design and implement solution architecture for cloud, data, and AI initiatives Create and maintain architecture documentation and blueprints Support architectural governance and ensure alignment with enterprise standards Collaborate with business and IT teams to define scalable, modern architecture solutions Contribute to large-scale cloud and AI transformation efforts Requirements (Must-Have): Strong experience in integration architecture and data integration patterns (batch, real-time, near-real-time) Hands-on experience with cloud-based big data platforms, preferably AWS Background in AI and ML architecture (cloud and on-prem), including GenAI solutions Familiarity with big data technologies (e.g., Hadoop, Spark) and related toolchains Knowledge of enterprise solution architecture implementation and governance Experience with modern tech stacks (open-source and vendor-based) Solid understanding of data architecture, data modeling, and data management technologies Nice-to-Have: Experience in the banking sector Knowledge of Financial Crime Prevention processes or technologies Location: Gdynia, Gda≈Ñsk or Warsaw (hybrid model ‚Äì 2 days/week in the office) Contract Type: B2B Project Duration: Long-term","[{""min"": 170, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,636,Senior Azure Data Engineer with Databricks,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: üåç100% remote from Poland üìùlong-term üíªworking hours - 7: 00-15: 00 üï∞Ô∏èASAP project start Responsibilities: Being responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems Building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies Evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards Driving creation of re-usable artifacts Establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation Working closely with analysts/data scientists to understand impact to the downstream data models Writing efficient and well-organized software to ship products in an iterative, continual release environment Contributing and promoting good software engineering practices across the team Communicating clearly and effectively to technical and non-technical audiences Defining data retention policies Monitoring performance and advising any necessary infrastructure changes Requirements: 3+ years‚Äô experience with Azure Data Factory and Databricks 5+ years‚Äô experience with data engineering or backend/fullstack software development Strong SQL skills Python scripting proficiency Experience with data transformation tools - Databricks and Spark Experience in structuring and modelling data in both relational and non-relational forms Experience with CI/CD tooling Working knowledge of Git English level: B2-C2 Nice to have: Experience with Azure Event Hubs, CosmosDB, Spark Streaming, Airflow Experience with Airflow Experience in Aviation Industry and Copilot Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Manager / C-level,B2B,Remote,637,Data Center Engineering Shift Manager,Link Group,"Data Center Engineering Shift Manager As a Data Center Engineering Shift Manager, you will be responsible for overseeing critical infrastructure operations during your assigned shift to ensure optimal performance, uptime, and safety within the data center environment. You will lead a team of technicians and engineers, coordinate maintenance activities, respond to incidents, and ensure compliance with operational standards and procedures. Key responsibilities: Manage day-to-day operations of data center facilities during shift hours Supervise technical staff and coordinate tasks related to power, cooling, and hardware infrastructure Ensure adherence to safety protocols, SLAs, and escalation procedures Support incident response and perform root cause analysis Collaborate with cross-functional teams for infrastructure improvements and planning Ideal candidate profile: Experience in data center or critical infrastructure environments Strong leadership and troubleshooting skills Knowledge of electrical and mechanical systems Willingness to work in a 24/7 shift rotation",[],Unclassified,Unclassified
Full-time,Mid,Permanent,Hybrid,638,Identity Management Automation Engineer,Beiersdorf Shared Services Poland,"Beiersdorf Shared Services in Pozna≈Ñ IT Hub plays a critical role in enabling global Beiersdorf business success. We work closely with our major well-known brands (such as NIVEA), functions, and markets to deliver innovative, reliable, and efficient information and data technologies. In the IT Operations department of Beiersdorf we are responsible for delivering IT infrastructure services on a global scale to the entire Beiersdorf Group, which comprises more than 20,000 employees worldwide. Our primary focus is to ensure uninterrupted IT operations through efficient automation. Within the department, the IT Identity Management Team specifically handles the management of technical users, onboarding and offboarding processes, and shaping services related to user access.As a team member, you will be involved in designing and implementing automated processes, playing a crucial role in delivering a seamless user experience for these services. Are you a passionate Identity Management Engineer and would like to further develop your skills and career?We have the right opportunity for you! Our tech & application stack: Microsoft Identity Manager (MIM), Nexis 4\ Microsoft Azure, Active Directory, Azure Active Directory Service Now, Azure DevOps MS SQL Server, Power BI, SSRS Node.js, .NET (C#), PowerShell Design & Implementation of IAM Processes : You will be involved in designing and implementing identity and access management (IAM) processes. This includes defining workflows, roles, and permissions, as well as creating automation scripts to streamline user provisioning, onboarding, and offboarding processes. Integration of new Systems into Microsoft Identity Manager : You will be responsible for integrating new systems and applications into the Microsoft Identity Manager (MIM) environment. This involves configuring connectors, mapping attributes, and ensuring seamless integration with existing identity management processes. Implementation of an end-to-end monitoring of the Identity Management landscape and processes: You will develop and implement monitoring solutions to ensure the optimal performance of the identity management landscape and processes. This includes monitoring the execution of processes, jobs, and infrastructure components to identify any issues or bottlenecks and proactively address them. Improvement & extension of current IAM Processes: You will continuously assess and improve existing IAM processes to enhance efficiency, security, and user experience. This involves identifying areas for optimization, implementing process enhancements, and staying updated with industry best practices. Managing the application & infrastructure ensuring availability, right sizing, performance, and integration into central infrastructure services: You will be responsible for managing the application and infrastructure components of the identity management system. This includes ensuring high availability, appropriate resource allocation (right sizing), optimal performance, and seamless integration with central infrastructure services. Troubleshooting technical & procedural issues. Core Requirements: Bachelor's degree in an IT-related field of study, an apprenticeship in an IT profession, or relevant work experience Fluency in English (both verbal and written) Technical Skills: Strong understanding of identity management concepts, principles, and best practices is crucial. Familiarity with the features and functionality of Microsoft Identity Manager (MIM) is essential, as it is the specific application being used Proficiency in working with Active Directory is essential, as it is commonly integrated with identity management solutions. Knowledge of AD structure, user and group management, authentication, and authorization processes is necessary Ability to design user access control, user lifecycle and provisioning processes is vital Proficiency in scripting languages like PowerShell is valuable for automating identity management tasks and creating custom workflows; experience with workflow and automation tools can also be beneficial. Solid experiences with SQL and relational Database technologies You can understand and modify existing code. Experiences with .NET (C#) is beneficial Professional Skills: Critical thinking and data-driven problem-solving abilities Innovative mindset with a focus on improving existing systems Continuous learning, growth mindset, and adaptability to new technologies and techniques Strong communication and collaboration skills in cross-functional and cross-cultural teams Ability to energize and mobilize people around an idea, creating energy and momentum in the face of change Passion for automation and data, and willingness to share knowledge and best practices with colleagues",[],Unclassified,Unclassified
Full-time,Senior,Permanent,Remote,639,Executive Azure Data Engineer - Zesp√≥≈Ç Data Intelligence Solutions,KPMG,"Zesp√≥≈Ç Data Intelligence Solutions zajmuje siƒô dostarczaniem Klientom us≈Çug z zakresu szeroko pojƒôtej analityki danych, modelowania platform danych i Business Intelligence. Wykorzystujemy nowoczesne technologie Microsoft i Databricks, jednocze≈õnie stale rozwijamy siƒô i poszukujemy nowych sposob√≥w, by zaspokoiƒá wymagania rynku.Aktualnie poszukujemy do≈õwiadczonego developera z kompetencjami w zakresie budowy platform danych (Data Lakehouse), w tym wdra≈ºania infrastruktury chmurowej, tworzenia proces√≥w przetwarzania danych ELT oraz projektowania modeli danych. Projektowanie nowoczesnych platform danych w ≈õrodowisku Azure (Data Warehouse, Data Lakehouse) z uwzglƒôdnieniem wydajno≈õci, skalowalno≈õci i niezawodno≈õci; Projektowanie, utrzymanie oraz wdra≈ºanie przep≈Çyw√≥w danych wsadowych w≈ÇƒÖczajƒÖc w to procesy ELT, oraz integracje pomiƒôdzy systemami chmurowymi i systemami on-premises; Projektowanie, utrzymanie oraz wdra≈ºanie przep≈Çyw√≥w danych strumieniowych; Automatyzacja wdro≈ºe≈Ñ w ramach proces√≥w CI/CD; Wsparcie klienta w zakresie analizy wymaga≈Ñ i identyfikacji ≈∫r√≥de≈Ç danych; Analiza zebranych danych, oraz zarzƒÖdzanie nimi w hurtowniach, oraz relacyjnych bazach danych; Doradztwo w obszarze wyboru optymalnych rozwiƒÖza≈Ñ, dostosowanych do potrzeb klienta; ≈öcis≈Ça wsp√≥≈Çpraca z w≈Ça≈õcicielem biznesowym, kierownikiem projektu, ekspertami obszar√≥w biznesowych w celu efektywnej realizacji projekt√≥w; Opracowywanie i utrzymywanie dokumentacji projektowej. Min. 4 lata do≈õwiadczenia w dostarczaniu rozwiƒÖza≈Ñ z zakresu in≈ºynierii danych w ≈õrodowisku konsultingowym lub w ramach projekt√≥w dla wielu firm; Bardzo dobra znajomo≈õƒá platformy Azure (Databricks, Fabric, Data Lake, Synapse Analytics, Data Factory, Logic Apps); Do≈õwiadczenie komercyjne we wdra≈ºaniu platform danych w technologiach Databricks lub Synapse Analytics lub Microsoft Fabric; Bardzo dobra znajomo≈õƒá SQL i Python; Do≈õwiadczenie w projektowaniu test√≥w wdra≈ºanych rozwiƒÖza≈Ñ analitycznych; Wspieranie i promowanie dobrych praktyk in≈ºynierii oprogramowania w ramach zespo≈Çu; Do≈õwiadczenie w samodzielnej wsp√≥≈Çpracy z klientem w zakresie identyfikacji i analizy wymaga≈Ñ oraz realizacji projekt√≥w wdro≈ºeniowych; Do≈õwiadczenie w sporzƒÖdzaniu podsumowa≈Ñ wynik√≥w test√≥w, analiz i dokumentacji w jƒôzykach polskim i angielskim; Wykszta≈Çcenie wy≈ºsze ‚Äì preferowane: informatyka, metody ilo≈õciowe lub pokrewne; Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego w mowie i pi≈õmie; Doskona≈Çe umiejƒôtno≈õci komunikacyjne i zdolno≈õƒá do wsp√≥≈Çpracy zar√≥wno z osobami technicznymi, jak i interesariuszami biznesowymi. Praktyczna znajomo≈õƒá administracji platformƒÖ Azure; Do≈õwiadczenie w automatyzacji wdra≈ºania rozwiƒÖza≈Ñ chmurowych oraz tworzenia infrastruktury w postaci kodu (DevOps, CI/CD, Infrastructure as a Code) oraz wykorzystaniu narzƒôdzia Azure DevOps; Do≈õwiadczenie we wdra≈ºaniu rozwiƒÖza≈Ñ opartych o Microsoft Fabric; Wdra≈ºanie rozwiƒÖza≈Ñ opartych o przetwarzanie danych streamingowych; Znajomo≈õƒá system√≥w wersjonowania Git; Znajomo≈õƒá PowerShell. Mo≈ºliwo≈õƒá wsp√≥≈Çpracy z rynkowymi liderami zar√≥wno w Polsce jak i za granicƒÖ; Udzia≈Ç w projektach wdro≈ºeniowych w najnowszych technologiach i frameworkach (data lakehouse, Azure, zaawansowana analityka oparta o Power Platform, Data Science); Atrakcyjny system premiowy; Mo≈ºliwo≈õƒá samorealizacji oraz rozwoju osobistego poprzez udzia≈Ç w dedykowanych szkoleniach, w tym z technologii Microsoft i Databricks; Szeroki dostƒôp do platform e-learningowych, takich jak: Degreed, LinkedIn Learning, Pluralsite i Databricks Academy; Zaplanowany czas na przygotowanie do egzamin√≥w, oraz bud≈ºet na zdobycie certyfikat√≥w; Mo≈ºliwo≈õƒá wyboru technologii, oraz wp≈Çyw na kierunki rozwoju zespo≈Çu; Pe≈ÇnƒÖ wyzwa≈Ñ pracƒô w wiodƒÖcej na rynku polskim oraz miƒôdzynarodowym firmie wdra≈ºajƒÖcej rozwiƒÖzania cyfrowej transformacji i us≈Çug chmurowych; Szansƒô na zdobycie cennego do≈õwiadczenia zawodowego; Atrakcyjny pakiet socjalny, w tym kartƒô MultiSport, bilety do kina, teatru, vouchery i zni≈ºki, bony okoliczno≈õciowe; PrywatnƒÖ opiekƒô medycznƒÖ, dodatkowe ubezpieczenie i program wellbeing; PrzyjaznƒÖ atmosferƒô pracy, w tym m.in . wyjazdy integracyjne i zespo≈Çowe spotkania okoliczno≈õciowe; Program mentoringowy ‚Äì wsparcie w zaplanowaniu ≈õcie≈ºki kariery; Program polece≈Ñ ‚Äì szansƒô na dodatkowy bonus finansowy za skutecznƒÖ rekomendacjƒô znajomego do pracy; Dni wolne na wolontariat - mo≈ºliwo≈õƒá zaanga≈ºowania siƒô w r√≥≈ºnorodne formy wolontariatu pracowniczego oraz program grantowy wspierajƒÖcy zg≈Çaszane przez pracownik√≥w inicjatywy; Mo≈ºliwo≈õƒá: pracy zdalnej / hybrydowej / w biurze.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,640,Data Analyst,Vasco Electronics,"Miejsce pracy: Krak√≥w Tryb pracy: Hybrydowy, 3 dni z biura, 2 dni zdalnie Etat: Full time Rodzaj umowy: Umowa o Pracƒô, Umowa Zlecenie, B2B Wynagrodzenie: UoP (brutto): 9900 - 12000 PLN UZ (brutto): 59 - 71 PLN/h B2B (netto/h FV): 73 - 93 PLN/h Zakres obowiƒÖzk√≥w Kompleksowa analiza danych (marketingowych, sprzeda≈ºowych, finansowych, magazynowych) z wykorzystaniem Google BigQuery i SQL ≈ÅƒÖczenie, agregowanie, por√≥wnywanie i weryfikowanie danych z r√≥≈ºnych ≈∫r√≥de≈Ç Projektowanie, tworzenie i utrzymywanie interaktywnych dashboard√≥w oraz raport√≥w w Power BI T≈Çumaczenie wymaga≈Ñ biznesowych na specyfikacje techniczne, przygotowywanie analiz i prezentowanie rekomendacji dla interesariuszy ≈öcis≈Ça wsp√≥≈Çpraca z zespo≈Çem Data Engineer√≥w oraz dba≈Ço≈õƒá o jako≈õƒá i sp√≥jno≈õƒá dokumentacji Wykorzystywanie LLM√≥w do automatyzacji swojej pracy Nasze oczekiwania 3 - 5 lat do≈õwiadczenia Zaawansowana, praktyczna znajomo≈õƒá SQL oraz ≈õrodowiska bazodanowego Google BigQuery lub innych baz danych Praca z wersjonowaniem kodu ( Dataform , dbt lub podobne) Bieg≈Ço≈õƒá w wizualizacji danych i budowaniu raport√≥w w Power BI lub podobne Do≈õwiadczenie w samodzielnym prowadzeniu analiz, od ekstrakcji danych po prezentacjƒô wniosk√≥w Wysoko rozwiniƒôte umiejƒôtno≈õci komunikacyjne i zdolno≈õƒá do efektywnej wsp√≥≈Çpracy z odbiorcami biznesowymi i technicznymi Proaktywne podej≈õcie, umiejƒôtno≈õƒá jasnego formu≈Çowania rekomendacji i otwarto≈õƒá na kulturƒô feedbacku Jƒôzyk angielski na poziomie B2 Mile widziane Do≈õwiadczenie w pracy z narzƒôdziami analitycznymi i do wizualizacji danych, takimi jak BigQuery, Power BI, Dataform oraz Python Oferujemy ≈örodowisko oparte na warto≈õciach i przyjaznƒÖ, nieformalnƒÖ atmosferƒô ‚Äì bez nadƒôcia, z fajnymi lud≈∫mi i dobrƒÖ kawƒÖ Du≈ºy wp≈Çyw na kszta≈Çt pracy zespo≈Çu Bud≈ºet do wykorzystania na platformie Worksmile, kt√≥ra oferuje dostƒôp do takich benefit√≥w jak m.in. Multisport, Allianz, Luxmed, PZU oraz wiele innych Elastyczny czas pracy Inicjatywy rozwojowe Dofinansowanie okular√≥w korekcyjnych 800 z≈Ç Czƒôste integracje Atrakcje w biurze tj. PS5 + VR2, biuro przyjazne zwierzƒôtom, przekƒÖski i owoce w biurze Parking przy biurze","[{""min"": 9900, ""max"": 12000, ""type"": ""Gross per month - Permanent""}, {""min"": 73, ""max"": 93, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Remote,641,üëâ Data & AI Enterprise Architect,Xebia sp. z o.o.,"üü£ You will be: leading and inspiring cross-functional architecture efforts across our client‚Äôs organization, filling an internal strategic role designed to bring added value to the client by aligning data architecture with business innovation and long-term growth, acting as a thought leader and enabler, supporting dedicated architects and teams across multiple domains, including: AI & ML Projects (40‚Äì60%), B2B & B2C E-commerce Platforms (20%), DataOps & Data Engineering (20%). üü£ Your profile: proven experience as an Enterprise or Lead Data Architect in complex, enterprise-scale environments, deep expertise in Microsoft Azure and Databricks, strong understanding of data architecture principles, data governance, and modern data platforms, ability to work across business and technical domains, with a focus on value creation and business impact, willingness to occasionally work on site in Amsterdam, very good command of English (min. C1). üü£ Nice to have: familiarity with technologies used across the client‚Äôs ecosystem, such as: Salesforce, Event Hubs / Kafka, Contentful or other headless CMS platforms, experience in customer-facing roles, pre-sales, or innovation consulting.in. Work from the European Union region and a work permit are required. üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Technical Interview ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 43500, ""type"": ""Net per month - B2B""}, {""min"": 26850, ""max"": 35500, ""type"": ""Gross per month - Permanent""}]",Data Architecture,Data Architecture
Full-time,Mid,B2B,Hybrid,642,Data Engineer (Scala),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: rozwijanie platformy DataHub do przechowywania i przetwarzania danych. Wykorzystywany stos technologiczny: Spark, Scala, TDD, Hadoop, Hive, DataBricks, CI/CD, Git, GitHub, Jenkins, Sonar, Nexus, Jira, SQL, PostgreSQL, rozw√≥j, testowanie i wdra≈ºanie specyfikacji technicznych i funkcjonalnych przygotowanych przez Solution Designer√≥w, Architekt√≥w Biznesowych oraz Analityk√≥w Biznesowych, zapewnienie poprawnego dzia≈Çania opracowanych rozwiƒÖza≈Ñ, dbanie o zgodno≈õƒá z wewnƒôtrznymi standardami jako≈õci, praca w modelu hybrydowym: 1-2 razy w tygodniu praca z biura w Warszawie, stawka do 170 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz minimum 2 lata do≈õwiadczenia z Apache Spark i ScalƒÖ, znasz wzorce projektowe i pracujesz w podej≈õciu Test-Driven Development (TDD), masz do≈õwiadczenie z technologiami Big Data ‚Äì Spark, Hadoop, Hive (znajomo≈õƒá Azure Databricks bƒôdzie dodatkowym atutem), masz do≈õwiadczenie w pracy w metodyce SCRUM/Agile, masz do≈õwiadczenie w integracji i zarzƒÖdzaniu du≈ºymi wolumenami danych, dobrze znasz narzƒôdzia CI/CD i DevOps: Git, GitHub, Jenkins, Sonar, Nexus, Jira, posiadasz znajomo≈õƒá struktur baz danych (PostgreSQL, SQL, Hive), biegle komunikujesz siƒô w jƒôzyku angielskim (min. B2), mile widziane do≈õwiadczenie z Bash, Control-M, Docker, Kubernetes, OS3, Azure, AWS. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 25200, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,643,Middle Data Engineer (Healthcare domain),Sigma Software,"Sigma Software is looking for a motivated Data Engineer to join our expanding engineering team. If you want to work in a close-knit team of Data Engineers solving complex problems using advanced data collection, transformation, analysis, and monitoring, then this opportunity is for you. We look forward to having you on our team! Our client is a leading medical technology company. The portfolio of products, services, and solutions is at the center of clinical decision-making and treatment pathways. Patient-centered innovation has always been and will always be, at the core of the company. The client is committed to improving patient outcomes and experiences, regardless of where patients live or what they face. The client is innovating sustainably to provide healthcare for everyone, everywhere. The project‚Äôs mission is to enable healthcare providers to increase their value by equipping them with innovative technologies and services in diagnostic and therapeutic imaging, laboratory diagnostics, molecular medicine, and digital health and enterprise services. Experience in data engineering and with cloud computing services solutions in the area of data and analytics, preferably with Azure Conceptual knowledge of data analysis fundamentals, e.g., dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data Knowledge of SQL and experience with the Python programming language Excellent communication skills and fluency in business English Understanding Big Data databases, such as Snowflake, BigQuery, etc. Snowflake is preferred Experience with database development and data modeling, ideally with Databricks or Spark Implement architecture based on Azure cloud platforms (Data Factory, Databricks, Event Hub) Design, develop, optimize, and maintain squad-specific data architecture and pipelines that adhere to defined ETL and Data Lake principles Discover, understand, and organize disparate data sources and structure them into clean data models with clear, understandable schemas Contribute to evaluating new tools for analytical data engineering or data science Suggest and contribute to training and improvement plans for analytical data engineering skills, standards, and processes",[],Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,644,Manager Database Administrator,Upvanta sp. z o.o.,"Instalacja, konfiguracja oraz administracja systemami baz danych (np. Oracle, SQL Server, MySQL). Monitorowanie wydajno≈õci baz danych i wdra≈ºanie usprawnie≈Ñ w celu optymalizacji dzia≈Çania system√≥w. Zapewnienie integralno≈õci, bezpiecze≈Ñstwa i sp√≥jno≈õci danych poprzez wdra≈ºanie odpowiednich procedur i mechanizm√≥w zabezpiecze≈Ñ. ZarzƒÖdzanie kopiami zapasowymi oraz procesami odzyskiwania danych. Wsp√≥≈Çpraca z zespo≈Çami developerskimi przy projektowaniu struktur danych oraz optymalizacji zapyta≈Ñ. RozwiƒÖzywanie problem√≥w zwiƒÖzanych z funkcjonowaniem baz danych oraz udzielanie wsparcia technicznego u≈ºytkownikom. Tworzenie i aktualizacja dokumentacji technicznej dotyczƒÖcej konfiguracji i procedur zarzƒÖdzania bazami danych. ≈öledzenie trend√≥w rynkowych, technologii i najlepszych praktyk w obszarze baz danych. Wykszta≈Çcenie wy≈ºsze w zakresie Informatyki, Technologii Informacyjnych lub pokrewne. Do≈õwiadczenie zawodowe na podobnym stanowisku (Database Administrator lub r√≥wnowa≈ºne). Praktyczna znajomo≈õƒá system√≥w DBMS: Oracle, SQL Server, MySQL. Dobra znajomo≈õƒá projektowania baz danych i modelowania danych. Wiedza z zakresu backup√≥w, recovery oraz bezpiecze≈Ñstwa baz danych. Umiejƒôtno≈õƒá rozwiƒÖzywania problem√≥w, dba≈Ço≈õƒá o szczeg√≥≈Çy. Wysoko rozwiniƒôte zdolno≈õci komunikacyjne i umiejƒôtno≈õƒá pracy zespo≈Çowej. Do≈õwiadczenie w pracy z bazami danych w chmurze (np. AWS, Azure, Google Cloud). Znajomo≈õƒá proces√≥w hurtowni danych (data warehousing) i narzƒôdzi ETL. Znajomo≈õƒá najlepszych praktyk w zakresie bezpiecze≈Ñstwa baz danych.",[],Database Administration,Database Administration
Full-time,Senior,B2B,Remote,645,Data Engineer with Snowflake & DBT,Holisticon Insight,"Holisticon Insight is a division of http: //nexergroup.com focused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! üëáhttps: //holisticon.pl/holisticon-insight/ üöÄ We are looking for a Senior Data Engineer who is skilled in Snowflake and DBT to work on a project in a team of our client, a Swedish-based leading provider of transport solutions. In the role of Data Engineer, you will oversee and drive the backend development of our client's BI and analytics tool in the Finance Department. You might be the perfect match if you possess the following competencies: 5+ years of previous commercial experience in similar role - a strong background in data engineering, data modeling and database management Proven experience with Snowflake and DBT Experience in setting CI/CD pipelines for seamless code integration Experience working with complex projects with Finance reports. Proactive communication skills and ability to communicate with different stakeholders Excellent English communication skills Location in Poland or EU üôå Nice to have: Experience Migrating from Onprem to Cloud. Experience working with AWS or other cloud By joining Holisticon Insight you will get : Life insurance Multisport card Fully remote job Private medical care Flexible working hours B2B or contract of employment Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories)","[{""min"": 26500, ""max"": 31900, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,646,Mid Data Engineer (Python/Spark/Databricks),7N,"Nasz klient - jedna z najwiƒôkszych firm na ≈õwiecie zajmujƒÖcych siƒô analizƒÖ danych finansowych ‚Äì poszukuje do≈õwiadczonych specjalist√≥w do pracy nad nowoczesnym projektem transformacji danych. W ramach projektu bƒôdziesz odpowiadaƒá za przekszta≈Çcenie istniejƒÖcych makr Excelowych w nowoczesne rozwiƒÖzania dzia≈ÇajƒÖce w ≈õrodowisku Databricks przy wykorzystaniu Pythona i Apache Spark . Lokalizacja: 100% zdalnie Start: wrzesie≈Ñ/pa≈∫dziernik Min. 4-letnie do≈õwiadczenie jako Data / Python Engineer Bardzo dobra znajomo≈õƒá Pythona i Apache Spark Do≈õwiadczenie w pracy z Databricks Umiejƒôtno≈õƒá analizy i przekszta≈Çcania makr Excelowych Dobra orientacja w zagadnieniach in≈ºynierii danych P≈Çynny j. angielski i do≈õwiadczenie we wsp√≥≈Çpracy w miƒôdzynarodowych ≈õrodowiskach (np. USA) Mile widziane: Znajomo≈õƒá SQL Do≈õwiadczenie w pracy z chmurƒÖ ( AWS, Azure ) Znajomo≈õƒá Git lub innych system√≥w kontroli wersji Analiza i zrozumienie logiki dzia≈Çania istniejƒÖcych makr Excelowych Konwersja makr na kod w Pythonie (Databricks/Spark) Wsp√≥≈Çpraca z in≈ºynierami danych przy integracji kodu z pipeline‚Äôami Optymalizacja kodu Spark i zapewnienie jego wydajno≈õci Dokumentowanie rozwiƒÖza≈Ñ oraz udzia≈Ç w przeglƒÖdach kodu Sta≈Çe wsparcie osobistego agenta , dbajƒÖcego o TwojƒÖ ciƒÖg≈Ço≈õƒá projektowƒÖ, kontakt z klientem, niezbƒôdne formalno≈õci, komfort pracy oraz rozw√≥j Consultant Development Program ‚Äì doradztwo w planowaniu kariery w oparciu o najnowsze trendy i potrzeby rynku IT, obejmujƒÖce m.in . konsultacje z agentami i mentorami kariery Dostƒôp do 7N Learning & Development ‚Äì platformy rozwojowo-edukacyjnej z webinarami, bibliotekƒÖ artyku≈Ç√≥w i raport√≥w bran≈ºowych oraz regularnymi zaproszeniami na jednorazowe i cykliczne wydarzenia rozwojowe ‚Äì techniczne, biznesowe oraz life-stylowe Spektakularne eventy integracyjne , zar√≥wno dla Ciebie (np. coroczny wyjazd Kick-Off , imprezy ≈õwiƒÖteczne czy sportowe Letnie Igrzyska), jak i dla Twoich bliskich (np. pikniki rodzinne, premiery filmowe) Rozw√≥j zawodowy nie tylko podczas projektu ‚Äì mo≈ºesz zaanga≈ºowaƒá siƒô w przekazywanie wiedzy innym w ramach oferty 7N Services kierowanej do klient√≥w 7N Relacje i dostƒôp do wiedzy najbardziej do≈õwiadczonych ekspert√≥w IT na rynku ‚Äì ≈õredni sta≈º zawodowy naszego Konsultanta w Polsce to ponad 10 lat Pakiet benefit√≥w zaplanowany od A do Z , czyli dofinansowanie do opieki medycznej, ubezpieczenia na ≈ºycie, karty sportowej dla Ciebie i Twoich bliskich, a tak≈ºe zni≈ºki do sklep√≥w w Polsce i za granicƒÖ. CiƒÖg≈Çe poszukiwanie projekt√≥w, trudne negocjacje stawek, brak wsparcia w rozwoju ‚Äì brzmi znajomo? W 7N zyskujesz nie tylko stabilno≈õƒá kontrakt√≥w, ale te≈º zaanga≈ºowanie osobistego opiekuna dbajƒÖcego o Tw√≥j komfort zawodowy i sta≈Çy dostƒôp do inicjatyw rozwojowych. Naszym celem jest zapewnienie Ci stabilnej i komfortowej wsp√≥≈Çpracy, kt√≥ra przyczyni siƒô do sukcesu Twojego jako eksperta IT oraz naszych klient√≥w. Budujemy trwa≈Çe relacje, bazujƒÖc na skandynawskich warto≈õciach i 30-letnim do≈õwiadczeniu w tworzeniu rozwiƒÖza≈Ñ IT dla ponad 200 organizacji.","[{""min"": 21840, ""max"": 25200, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,647,Senior Data Engineer,HSBC Service Delivery,"Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity If you‚Äôre looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. HSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions. We are currently seeking an experienced professional to join our team in the role of Senior Data Engineer. What you‚Äôll do Establish best practices for data engineering and ensure team adherence. Perform code testing and conduct reviews with team members to ensure high-quality, efficient, and well-tuned code. Design and implement data-driven solutions in collaboration with stakeholders. Develop and maintain data pipelines and CI/CD pipelines, optimizing data integration processes for accuracy and efficiency. Own & review release process. What you need to have to succeed in this role Fluency in English, both verbal and written. Previous experience of an international and multi-cultural context is a plus. 5 years + experience as PL/SQL-Unix Developer. Time management and prioritization skills as well as the ability to work autonomously. Knowledge and previous experience in Business Finance and/or Data Engineering contexts will be appreciated. Technical skills (must have): Oracle PL/SQL, Procedures and packages development, SQL query optimization, Advanced error handling, Unix shell. Other technical skills: Perl, Jenkins / Ansible, Agile methodologies, Github, JIRA, Confluence, Vue.js, Spring Boot, ControlM. What we offer Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Nursery discounts Financial support with trainings and education Social fund Flexible working hours Free parking Intellectual property benefit If your CV meets our criteria, you should expect the following steps in the recruitment process: Online assessment. Telephone screen (for external candidates only). Job interview with the hiring manager. We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,648,Data Collaboration SME,HSBC Service Delivery,"Data Collaboration SME Some careers shine brighter than others. If you‚Äôre looking for a career that will help you stand out, join HSBC, and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further. Your career opportunity Cybersecurity Technology & Engineering is responsible for fielding solutions that help defend HSBC against a wide range of threats to the business as well as its customers, clients, partners, and staff. The team works in concert, with partner teams across HSBC, to implement novel defensive capabilities that are effective and adaptable against a constantly evolving threat landscape. The function operates under the vision: ‚ÄúEnabling HSBC to be safely successful everywhere the Firm chooses to do business‚Äù. The role will report to the Head of Collaboration Platforms Data Protection, and will be responsible for defining, implementing and maintaining Security Controls across multiple collaboration tooling technologies, to protect sensitive data, while ensuring compliance with both HSBC Data Security controls and global data protection regulations and industry best practice. You will collaborate with a wide range of stakeholders including Cybersecurity project teams, Control Owners and the business, across all regions and entities, and be a subject matter expert (SME) in the relevant technologies and support areas to implement and support business requirements and function goals. This role requires a customer-focused mindset, resilience, a strong technical background, and an understanding of risk controls for data security in highly regulated environments. The role holder will support the strategic direction for the team and ensure adequate solutions are utilised by effectively managing competing and shifting priorities. Support the Collaboration Platform Data Protection Owner: by conducting technical assessments, validation and definition activities, across multiple local and global collaboration tooling technologies, supporting all global regions (including those with any local regulations) and in line with the Data Protection strategy (including M365 Apps, SharePoint, OneDrive, Teams, Zoom, Confluence, Jira etc) Facilitate the delivery of the Technology Security Controls via change, through clear strategy, operational planning and effective communication to all affected business functions (including Microsoft, Skyhigh, ProofPoint, Symantec, Data Insights, BigID etc) Investigate and embrace innovation to gain a competitive advantage, thinking differently to achieve the best business outcomes, and thinking ahead to identify and overcome potential issues Fully leverage automation and platform integration, where possible, to deliver the best possible end user experience and frictionless data protection Govern risk responsibly, across all regions and business areas, showing integrity whilst promoting and managing relevant monitoring and reporting requirements. Support GB/Fs/Markets/CTO: by providing technical consultancy, as part of a business requestable service, or ah-hoc ask, e.g. in response to a regulator exam. Support Control Owners: when designing controls, Technical Control Requirements, key indicators and during respective threshold setting. Contribute to/authors/owns capability related security standards (e.g. Virtual Data Rooms), architectural patterns and technology configuration definition. 3+ years of experience of Cybersecurity SME/Product Owner/Engineering/Strategy areas A background in information systems, technology, architecture, design, and service delivery of defense-in-depth capabilities. Strong stakeholder management skills, with experience of understanding and meeting the needs of multiple stakeholders. An ability to communicate complex and technical issues to diverse audiences, orally and in writing, in an easily understood, authoritative and actionable manner. Experience in data protection, cybersecurity, and risk management within the financial services industry. Customer centric consultancy approach. Understanding of data protection controls, regulation, and compliance requirements on a global scale. Excellent communication, and interpersonal skills. Experience working in a highly regulated, large multi-national environment. Understanding and knowledge of common industry cyber security frameworks, standards and methodologies Experience working within Cloud, SaaS and emerging cloud use-cases for web/mobile and enterprise companies. Strong commercial background and knowledge of cybersecurity offerings Expertise and experience in the design and delivery of existing collaboration tooling and the relevant DLP technology solutions M365 Apps, SharePoint, OneDrive, Teams, Zoom, Confluence, Jira Microsoft, Skyhigh, ProofPoint, Symantec, Data Insights, BigID Competitive salary Annual performance-based bonus Additional bonuses for recognition awards Multisport card Private medical care Life insurance One-time reimbursement of home office set-up (up to 800 PLN). Corporate parties & events CSR initiatives Financial support with trainings and education Nursery discounts Social fund Flexible working hours Free parking Online behavioural test Telephone screen Job interview with the hiring manager We are looking to hire as soon as possible so don‚Äôt wait and apply now! You'll achieve more when you join HSBC.",[],Unclassified,Unclassified
Full-time,Senior,Permanent or B2B,Hybrid,649,Oracle Database Support Specialist,ASTEK Polska,"ASTEK Polska We are a part of ASTEK Group, which has been gathering experience in the global consulting and engineering services market since 1988. ASTEK Group is a global player in engineering and technology consulting, present on 5 continents. What do we learn from other Group entities in our daily work? First and foremost: inspiration, objectives, good practices, innovative activities and values. In 2020, 2021, 2022 and 2023 we received the Great Place to Work certificate, and found ourselves among the 15 Best Workplaces in Poland in the category: large companies. About the role: We are looking for a Database Support Specialist to help improve the support and performance of Oracle database infrastructure. You will work closely with internal engineering teams and external support providers to reduce incidents, optimize resources, and ensure high-quality support for IT operations and development teams. Your day-to-day responsibilities include: Collaborate with Database Engineering and external support teams Reduce number and duration of major incidents Optimize resources (people, servers, licenses) Support RCA (Root Cause Analysis) and BCI processes for escalated issues Perform performance tuning and investigations on critical Oracle databases Provide strategic input on ticketing and bug-fixing Ensure compliance with architecture, security, and governance standards You‚Äôre ideal for this role if you have: Proven experience managing large-scale Oracle environments Strong skills in daily DBA operations and incident response Experience with query plan analysis and database monitoring tools Solid SQL development and tuning (DML/DDL) Understanding of l ock ing, contention, rollback handling Experience in performance testing and capacity planning Familiarity with IT operations standards (patching, hardening, monitoring) Proficiency in English (B2+) Your personality: Assertive, proactive skills Detail-oriented with good time management skills Strong technical analytical skills Added value for you: Long-term cooperation Possibility to choose preferred type of cooperation (regular job contract with all benefits or flexible B2B contract) Technical trainings, certificates and upskilling Competence Center mentoring - you will be a member of CC community from the first day of your work. You‚Äôll have a chance to develop your skills, participate in various conferences and share your knowledge and experience with people who face the same challenges in their daily work Clear career path Employee benefits package Friendly work atmosphere, social events and team-building meetings It‚Äôs not about you? Recommend us your friend and get a bonus up to 7,000 PLN! Link: https: //astek.pl/system-rekomendacji/ No ref: AO188887","[{""min"": 16000, ""max"": 19500, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 14500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,650,Data Engineer,Innobo Sp. z o. o.,"For Client from BigPharma sector we are looking for Data Engineer. As a Data Engineer, you will be an integral member of the Data Harmonisation Layer (DHL) Agile team. DHL is a global platform designed to collect and harmonize of data from various Operational Technology (OT) data sources across company production facilities. DHL makes data easily accessible to develop several front-end Manufacturing Intelligence (MI) products, focusing on improving our production processes. Key technologies used in DHL are Kafka, Ignition, and AWS services. You will be part of the Manufacturing Intelligence (MI) department. Our vision is to enable data using cutting-edge technologies and empower our colleagues in Product Supply to make data-driven decisions to continuously improve manufacturing processes, in short #EnableDataEmpowerDecisions. Responsibilities: Software development activities i.e., writing, reviewing, refactoring, testing, and documenting the code base Build production ready stable and scalable data pipelines Act as a trusted advisor to provide technical expertise on one or more projects for our business partners Work with the Agile teams to understand and handling enabler work and work towards technical agility Contribute in Agile events such as Program Increment (PI) planning, system demos, and Inspect and Adapt (I&A) Requirements: A master‚Äôs degree in computer science, engineering, chemistry, biology, or other relevant fields Experienced with data engineering principles such as data warehousing, batch processing, data streaming, data lakes, databases, and data modelling Experience with Databricks is a must Experience with building CI/CD pipelines Coding/scripting skills (Bash, Python, or similar) Hands-on experience with IaC for managing the cloud Strong mathematical, statistical, and problem-solving skills Nice to have: Experience with various Operational Technology (OT) data sources across the company's production facilities Hands-on experience in AWS services such as Lambda, Glue, IAM, Kinesis, MKS, Step Functions, DMS, RDS, Managed Grafana, SNS, S3, CloudWatch, etc Experience with Azure","[{""min"": 24360, ""max"": 26880, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Any,Office,651,Specjalista / Specjalistka ds. System√≥w Analitycznych i IT,ECO RUN,"Tw√≥j zakres obowiƒÖzk√≥w Tworzenie zestawie≈Ñ i analiz danych przy u≈ºyciu MS Excel oraz narzƒôdzi klasy Business Intelligence (BI). Rozw√≥j i obs≈Çuga system√≥w CRM dedykowanych dla bran≈ºy OZE. Tworzenie i optymalizacja kalkulator√≥w (np. do wyceny instalacji PV, pomp ciep≈Ça, magazyn√≥w energii). Bie≈ºƒÖca obs≈Çuga techniczna: rozwiƒÖzywanie zg≈Çosze≈Ñ, konfigurowanie kont e-mail oraz praca z systemami typu Autenti. Budowa i aktualizacja dashboard√≥w oraz raport√≥w wspomagajƒÖcych procesy decyzyjne w firmie. Koordynowanie inicjatyw zwiƒÖzanych z wdro≈ºeniami technologicznymi. Wsp√≥≈Çpraca z dzia≈Çami wewnƒôtrznymi i zespo≈Çem IT przy integracji nowych narzƒôdzi i system√≥w. Do≈õwiadczenie w pracy na podobnym stanowisku. Bardzo dobra znajomo≈õƒá MS Excel, w szczeg√≥lno≈õci: tabele przestawne, zaawansowane formu≈Çy, analiza danych. Znajomo≈õƒá zaawansowanych makr w Excelu (VBA). Do≈õwiadczenie w pracy z platformami BI (np. Power BI, Tableau lub podobnymi). Znajomo≈õƒá dzia≈Çania system√≥w CRM oraz do≈õwiadczenie we wspieraniu ich wdra≈ºania i utrzymania. Podstawowe kompetencje techniczne, umo≈ºliwiajƒÖce rozwiƒÖzywanie prostych problem√≥w IT i wspieranie u≈ºytkownik√≥w w codziennej pracy. Dobra organizacja pracy w≈Çasnej, odporno≈õƒá na presjƒô czasu i wielozadaniowo≈õƒá. Zatrudnienie na pe≈Çny etat w oparciu o umowƒô o pracƒô, umowƒô zlecenie lub kontrakt B2B. Mo≈ºliwo≈õƒá zdobycia atrakcyjnego do≈õwiadczenia zawodowego, rozwoju osobistego, poszerzenia wiedzy i umiejƒôtno≈õci. Wynagrodzenie adekwatne do posiadanych umiejƒôtno≈õci, kwalifikacji wraz z systemem motywacyjnym uzale≈ºnionym od realizacji cel√≥w biznesowych. Mo≈ºliwo≈õƒá korzystania z pakietu socjalnego. Dofinansowanie do karty Multisport. Narzƒôdzia pracy niezbƒôdne do rozpoczƒôcia wsp√≥≈Çpracy.",[],Unclassified,Unclassified
Full-time,Senior,B2B,Remote,652,AI/ML Principal Software Engineer,Sii,"Minimum of 7 years of experience building and deploying complex, production-grade software systems Proficient in backend technologies (e.g., Python, Java, Node.js) and frameworks (e.g., Django, Flask, Spring Boot) Skilled in frontend frameworks such as React or Angular Strong background in containerization (Docker) and orchestration (Kubernetes) Proven expertise with AWS and scalable cloud-based architectures Fluency in English, both spoken and written Residing in Poland required Join our client's team in the medical industry as an AI/ML Principal Software Engineer, and help build cutting-edge software solutions powered by machine learning for real-world applications. In this role, you will play a key part in designing and building scalable, high-performance systems that bring machine learning models into production environments. This is an exciting opportunity to lead critical initiatives at the intersection of software engineering and AI, while working remotely within a collaborative and forward-thinking team. Take ownership of designing and developing reliable, scalable software solutions that incorporate machine learning models Work closely with data scientists and ML engineers to transform experimental prototypes into fully operational, production-grade systems Contribute to key architectural decisions, infrastructure planning, and the selection and implementation of development tools Assist in building and maintaining cloud-native environments and solutions on AWS Apply industry best practices for automated testing, continuous integration and delivery (CI/CD), and system observability Develop and sustain backend infrastructures and user-facing applications with a strong focus on performance, security, and maintainability Start ASAP Praca w pe≈Çni zdalna Darmowe ≈õniadanie Bez wymaganego dress code'u Darmowa kawa Szkolenia wewnƒôtrzne Nowoczesne biuro Pakiet sportowy Bud≈ºet na szkolenia Miƒôdzynarodowe projekty Ma≈Çe zespo≈Çy Prywatna opieka medyczna Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title ‚Äì get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers ‚Äì Power People. Learn more at sii.pl .","[{""min"": 24000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Science,Data Science
Full-time,Mid,Permanent or B2B,Remote,653,Senior Data Engineer,XTB,"Tworzymy XTB ‚Äì globalnƒÖ firmƒô inwestycyjnƒÖ, oferujƒÖcƒÖ innowacyjne rozwiƒÖzania technologiczne, kt√≥re pozwalajƒÖ naszym klientom skutecznie zarzƒÖdzaƒá swoimi finansami na wiele sposob√≥w. Wszystko to w jednej intuicyjnej aplikacji XTB, z kt√≥rej korzysta ju≈º ponad milion u≈ºytkownik√≥w na ca≈Çym ≈õwiecie! Jeste≈õmy certyfikowanƒÖ firmƒÖ Great Place to Work. Poszukujemy osoby, kt√≥ra do≈ÇƒÖczy do naszego zespo≈Çu Data Platform w roli Data Engineer. G≈Ç√≥wnym zadaniem zespo≈Çu jest utrzymanie i rozw√≥j hurtowni danych on premise i jej docelowa migracja na ≈õrodowisko chmurowe. Wsp√≥≈Çpracujemy z zespo≈Çami produktowymi w celu dostarczenia danych potrzebnych do podejmowania kluczowych decyzji biznesowych. Pracujemy w frameworku Scrum. Do Twoich codziennych obowiƒÖzk√≥w bƒôdzie nale≈ºa≈Ço: Projektowanie i utrzymywanie hurtowni danych oraz migracja danych i proces√≥w na ≈õrodowisko chmure, Tworzenie i utrzymywanie modeli danych do wspierania kluczowych decyzji biznesowych, Integracja danych w celu wytworzenia wymaganych modeli danych, Wdra≈ºanie kontroli jako≈õci danych i proces√≥w walidacji, aby zapewniƒá dok≈Çadno≈õƒá, sp√≥jno≈õƒá i kompletno≈õƒá, Wsp√≥≈Çpraca z innymi zespo≈Çami, aby tworzyƒá zestawy danych spe≈ÇniajƒÖce potrzeby raportowania i rozwiƒÖzujƒÖce problemy systemowe, Projektowanie i wykonywanie test√≥w wydajno≈õciowych i integracyjnych, Raportowanie kluczowych wska≈∫nik√≥w w firmie w narzƒôdziu BI Wymagania: Co najmniej 4-letnie do≈õwiadczenie w pracy na stanowisku SQL Developer / Data Engineer Znajomo≈õƒá Python, SQL w tym T-SQL, pisanie z≈Ço≈ºonych procedur sk≈Çadowanych, optymalizacja wydajno≈õci, Umiejƒôtno≈õƒá tworzenia proces√≥w ETL (SSIS, Airflow), Do≈õwiadczenie zawodowe w eksploracji danych, analizie i modelowaniu z≈Ço≈ºonych zbior√≥w danych na du≈ºƒÖ skalƒô; Do≈õwiadczenie w pracy z narzƒôdziami do zarzƒÖdzania kodem ≈∫r√≥d≈Çowym, takimi jak GIT Dobra znajomo≈õƒá zasad standard√≥w integracyjnych: REST, gRPC Umiejƒôtno≈õƒá tworzenia rozwiƒÖza≈Ñ w oparciu o serwisy w Snowflake Do≈õwiadczenie w tworzeniu, wdra≈ºaniu i rozwiƒÖzywaniu problem√≥w z aplikacjami danych na platformie Microsoft Azure Znajomo≈õƒá rozwiƒÖza≈Ñ chmurowych (Azure, GCP), Silne umiejƒôtno≈õci rozwiƒÖzywania problem√≥w i dba≈Ço≈õƒá o szczeg√≥≈Çy. Umiejƒôtno≈õƒá skutecznej komunikacji i wsp√≥≈Çpracy w zespole. Chƒôƒá uczenia siƒô i dostosowywania do nowych technologii i koncepcji. Rozumienie zasad Agile i Scrum (pracujemy w Scrumie) Dodatkowe atuty: Do≈õwiadczenie w budowaniu skalowalnych, dzia≈ÇajƒÖcych w czasie rzeczywistym rozwiƒÖza≈Ñ typu Data Lake, Do≈õwiadczenie ze strumieniowym przesy≈Çaniem danych (Kafka), Do≈õwiadczenie w pracy z Kubernetes Znajomo≈õƒá koncepcji Data Mesh. Znajomo≈õƒá podej≈õcia DevOps Oferujemy Realny wp≈Çyw na rozw√≥j firmy i produktu Pracƒô w do≈õwiadczonym zespole, kt√≥ry chƒôtnie dzieli siƒô wiedzƒÖ JasnƒÖ wizjƒô rozwoju dziƒôki regularnym feedbackom i klarownym ≈õcie≈ºkom karier Bud≈ºet szkoleniowy na interesujƒÖce Ciƒô kursy i konferencje Dodatkowy dzie≈Ñ wolny z okazji Twoich urodzin Dodatkowy dzie≈Ñ wolny dla rodzic√≥w Sprzƒôt dopasowany do Twoich potrzeb PrywatnƒÖ opiekƒô medycznƒÖ i ubezpieczenie grupowe Dostƒôp do platformy e-learningowej do nauki jƒôzyka angielskiego oraz platformy benefitowej Dostƒôp do platformy wellbeingowej i mo≈ºliwo≈õƒá skorzystania z warsztat√≥w oraz prywatnych sesji terapeutycznych Pracƒô zdalnƒÖ, z biura w Warszawie lub z coworku w Twoim mie≈õcie Regularne spotkania integracyjne","[{""min"": 15000, ""max"": 19000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,B2B,Remote,654,Spatial Data Engineer,Spyrosoft,"Project Description Are you passionate about geospatial data and eager to work on cutting-edge GIS solutions? Join our team as a Spatial Data Engineer and help our clients implement and maintain advanced spatial data processing systems using ESRI products or Open-Source equivalents. Key Responsibilities Process and manage geospatial data from diverse sources Design and maintain ETL pipelines Integrate spatial data from multiple systems Develop automation solutions for data workflow Build GIS applications (web and mobile) Technical Requirements Solid understanding of spatial data models and best practices in spatial data management Proficiency in Python at an intermediate level, especially with ArcGIS API for Python and arcpy Strong experience with ESRI products , particularly: ArcGIS Pro ArcGIS Online (including: Experience Builder, Field Maps, Hub, Map Viewer, Survey123, Workflow Manager) ArcGIS Notebooks Managing and administering ArcGIS Online/Portal Automating spatial data processing (using ESRI or Open-Source tools) Integrating cloud solutions (Azure, AWS) with GIS environments Working in Azure DevOps Familiarity with data sharing methodologies and cloud-based environments Nice to have: Experience with FME Form , FME Flow , ArcGIS Velocity , or Sweet for ArcGIS Ôªø üí° Soft Skills Passion for spatial data and strong analytical thinking Excellent communication skills and business-oriented mindset Fluency in English, both written and spoken","[{""min"": 70, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,Permanent,Hybrid,655,Senior Data Scientist (Consumer Inspiration),Allegro,"A hybrid work model requires 1 day a week in the office (Warsaw). In the Consumer area, we pursue the best possible customer experience, starting from main site visits, through browsing, searching and comparing listings, to the purchasing process and post-purchase services on all platforms (web, mobile web, iOS and Android). We are responsible for external traffic that involves thousand of requests per second. We have hundreds of millions of records in our database, microservice architecture on the back-end, and micro-frontends. We conduct dozens of experiments, test hundreds of variants, analyse results and draw conclusions that we convert into specific tasks. We systematically take care of the fast and smooth operations of our sites. We monitor and solve issues related to efficiency, developing an optimum code to provide clients with the best user experience at Allegro. We have nearly 30 teams made up of engineers that have various skills. We also cooperate with designers, researchers and analysts. As a Senior Data Scientist within the Consumer area you will be working on new recommendation and personalization models based on customer behavior and purchases. You'll prepare automated algorithms used to serve personalized content and placements on the home page. Technologies you‚Äôll encounter on the job are (among others): Python, Airflow, Big Query, GCP, Spark, VertexAI, Looker Studio. Why is it worth working with us? We are a team that is not afraid of hard problems and is constantly looking for development opportunities. We have a track record of deploying our models at a large scale and focus on bringing the impact on production. With a wide variety of projects you‚Äôll never be short of interesting challenges. You will use some of the most interesting datasets on the market that are just waiting for you to get even more business value out of them. You‚Äôll work with tabular, natural language, image and time series data. You will combine multiple data sources, domain knowledge, and advanced ML techniques to deliver high-quality models. Our employees regularly attend conferences in Poland and abroad (Europe & US), and each team has its own budget for training and study aids. If you want to keep growing and share your knowledge, we‚Äôve got you covered. Our offer is addressed to people who: Graduated with a degree in Mathematics, Economics, Computer Science or a similar major Have at least 4-5 years of experience in data science, with some models launched to production Have experience with large scale models and data processing Can translate business challenges into ML problems and tackle them using novel approaches Are excellent communicators, able to convey technical information to semi and non-technical audiences Fluently program in Python and use development tools with ease Have an advanced knowledge of SQL Understand statistical and machine learning methods, especially algorithms based on decision trees What we offer: High impact and opportunity to design the ML backbone of largest eCommerce platform in Poland and one of largest in Europe Multidisciplinary nature of work at the intersection of business and technology, development opportunities in designing cutting edge solutions at scale that bring real value to customers Possibility to implement ML solutions which are unique on the market Support of experienced Data Scientists and Engineers - there is always someone to exchange ideas with because we have the best specialists and experts in their field on board Well-located office (with fully equipped kitchens and bicycle parking facilities) and excellent working tools (height-adjustable desks, interactive conference rooms) A wide selection of fringe benefits in a cafeteria plan ‚Äì you choose what you like (e.g. medical, sports or lunch packages, insurance, purchase vouchers) Annual bonus of 10% of the gross annual salary (depending on your end-year assessment and the company's results) Long-term discretionary incentive plan based on Allegro.eu shares Fully sponsored English classes, related to the specific nature of your job",[],Data Science,Data Science
Full-time,Mid,Any,Hybrid,656,Technology Specialist - Azure,Heineken,"Digital & Technology Team (D&T) is an integral division of HEINEKEN Global Shared Services Center . We are committed to making Heineken the most connected brewery. That includes digitalizing and integrating our processes, ensuring best-in-class technology, and embedding a data-driven culture. By joining us you will work in one of the most dynamic and innovative teams and have a direct impact on building the future of Heineken ! Would you like to meet the Team, see our office and much more? Visit our website: Heineken ( heineken-dt.pl ) The Technology Specialist - Azure together with the agile DevOps team work of delivering a releasable increment of the data solutions and platform product at the end of each sprint. The role is part of the Global Planning Data & Analytics Product team . Within HEINEKEN our Analytical Solutions are created on a central Analytics Platform. This central platform is based on Azure cloud technology ranging from data lake, data bricks, SQL/Synapse, ML Studio, PowerBI and related Azure components to build complete analytics solutions. Your responsibilities would include: implementing data products and solutions based on business & IT requirements on the Analytics Enablement platform building data pipelines, implementing data integration solutions, and data models following AEP and Heineken standards in Azure. You are a good candidate if you have: 4-5 years of experience required in a similar role independently used Azure Databricks for transformations & processing data extensive knowledge of Azure Databricks, Spark and clusters advanced knowledge of Azure SQL & Synapse for modelling and serving data axperience with Azure Functions and Logic Apps knowledge of Delta Lake and Delta Live Tables experience in and expert knowledge of ETL and transformation technology across the different technologies knowledge in Python and SQL, preferably also other programming languages experience in bringing analytics & reporting solutions from idea and experiment to scalable production and supportable deployments strong problem solving and root cause identification skills experience working on complex systems, handling multiple solutions, and participating in large-scale projects ability to work effectively and build relationships at all levels in an organisation team player skills, proactivity, and the ability to take initiative very good time management skills and ability to prioritise a can-do and Hands-on attitude while dealing with some uncertainty in requirements & features to deliver MVP solutions an Agile mind-set and operate as part of an Agile Devops team. At HEINEKEN Krak√≥w, we take integrity and ethical conduct seriously. If someone has concerns about a possible violation of legal regulations indicated in Polish Whistleblowing Act or our Code of Business Conduct, we encourage them to speak up . Cases can be reported to global team or locally (in line with the local HGSS Whistleblowing procedure) by selecting proper option in this tool or by communicating it on hotline. üè† Flexible Work from Home scheme üí∏ Attractive Performance Bonus üöó Parking Space for Employees ‚è∞ Flexible working hours üí≥ Sodexo Card ‚òÇ Life Insurance ‚ûï Employee Referral Programme üåê Job Opportunities within HEINEKEN ü©∫ Private Medical Healthcare ‚≠ê Social Events",[],Data Engineering,Data Engineering
Full-time,Mid,B2B,Hybrid,657,Data Engineer (B2B),Reply Polska Sp. z o. o.,"Responsibilities Data System Design : Design and implement robust, scalable data processing systems: this involves selecting appropriate storage technologies, designing schemas, and planning integration strategies. Data Integration and ETL Development : Develop and maintain pipelines for data transformation, integration, and ETL processes. Ensure data quality and accessibility Performance Optimization : Monitor, tune, and optimize data applications and database performance. Address any issues that may affect data processing speeds or analytics capabilities Consulting and Strategy : Provide expert advice and consultancy services to clients on data strategies, architecture choices, and technological advancements Analytics and Business Intelligence Support : Assist in developing analytics platforms and business intelligence solutions, ensuring that data can be effectively transformed into actionable insights Client Interaction : Work closely with clients to understand their business needs and technical requirements. Translate these requirements into effective data engineering solutions. Qualifications Educational Background: Bachelor‚Äôs or master‚Äôs degree in CS, Engineering, IT, or a related field. Very good knowledge of English and Polish. Programming Language: Strong programming skills in languages such as Java, Scala or Kotlin. Cloud: Experience with public cloud providers AWS/Azure Database Technologies: Knowledge of SQL and NoSQL databases. Expertise in Big Data Technologies: Familiarity with big data frameworks and tools like Apache Hadoop, Spark, Kafka, Flink or others, formats such as Apache Iceberg. Data Modeling and Warehousing: Familiarity with data modeling and warehousing techniques. Strong Analytical Skills: Ability to analyze complex data structures and derive insights to provide strategic guidance. Excellent Communication: Strong interpersonal and communication skills to effectively collaborate with team members and clients. Problem Solving: Strong problem-solving skills and the ability to propose creative, efficient solutions to complex problems. Plus: Any additional experience with Docker, Kubernetes, DevOps practices, CI/CD, DBT. Availability to work in a hybrid mode with at least 2 visits in the office per month. About Data Reply Data Reply, as part of the Reply Group, offers a wide range of services to help customers to become data driven. The team is active in various industries and business areas and works closely with clients to enable them to achieve meaningful results through the effective use of data. Data Reply offers many years of experience in transformation projects on the topic of ""Data Driven Enterprise"". Our experts focus on the development of Streaming and Event-Driven applications, Data Platforms and Machine Learning solutions - automated, efficient and scalable - without compromising IT security.","[{""min"": 90, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent,Hybrid,658,Digital Analyst,PAYBACK,"You design Digital Analytics at PAYBACK together with the Digital Analytics team and the stakeholders from the Business Units You create functional and technical concepts for implementing tracking and ensure the tracking data quality for all digital PAYBACK channels, in particular the PAYBACK App You support PAYBACK Product and Development departments in everything relating to the tracking of the digital PAYBACK channels You provide support for Adobe Analytics as a self-service tool and create standardized Dashboards for Business Units in all 11 PAYBACK countries You lead Analytics projects, e.g. for the further improvement of methodical approaches for tracking digital channels You have successfully completed a degree in (Business) Computer Science, Statistics, Mathematics, Business Administration, Economics, or something comparable You have professional experience in the field of Digital Analytics, especially in practical work with tracking tools such as Adobe Analytics or Google Analytics (must have). Some Python, SQL or GCP knowledge is highly valued. You have experience in setting up and optimizing tracking implementations and in operational cooperation with Product and Development departments You have a good overview of current Digital Analytics trends, especially innovative approaches for optimal tracking of the digital channels Web and App You like solving complex tasks which require logical thinking and have an open-minded approach to new challenges. You are a team player with very good communication skills and are willing to take ownership of your task areas You have a structured and solution-oriented way of working Your knowledge of English is very good You are open for hybrid work: 3 days from the office in Warsaw Employment contract? Of course. With us you do not have to worry about stable employment. Benefits? We have them! Among other: corporate incentive program, sport card, private medical care. Working in a hybrid model? üè† Of course! You work with us 3 days a week from the office, 2 days a week from home. Work wherever you want?üå¥ In PAYBACK you have the opportunity. Working 100% remotely, also from European countries for 15 days a year. ‚ÄéFlexible working hours? Sounds great! We start working between 7 to 10. Trainings? Of course. We provide training to develop hard and soft skills. Convenient location? Sure! We invite you to our new office at Rondo Daszy≈Ñskiego, but we are currently also working remotely. Dress code We definitely say no. There are no rigid dress code rules in our company, sneakers are more than welcome. Friendly atmosphere at work? Yes! In PAYBACK, people are the most important asset‚Äé. ‚ÄéSomething is missing? Open communication is our priority, so dare to ask!‚Äé",[],Data Analysis & BI,Data Analysis & BI
Full-time,Mid,B2B,Remote,659,Data Engineer (Palantir+Snowflake),Onwelo Sp. z o.o.,"Poznaj Onwelo: Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Do≈ÇƒÖcz do zespo≈Çu Data, kt√≥ry wspiera jednƒÖ z najwiƒôkszych grup technologicznych na ≈õwiecie w budowie centralnej platformy danych. W projektach wykorzystujemy platformƒô Palantir Foundry jako warstwƒô aplikacyjnƒÖ oraz Snowflake jako warstwƒô magazynowania i przetwarzania danych. Pracujemy nad integracjƒÖ danych z r√≥≈ºnych ≈∫r√≥de≈Ç (ERP, CRM, dane laboratoryjne) i tworzymy zaawansowane pipeline'y danych, modele analityczne i aplikacje wspierajƒÖce decyzje biznesowe. Tworzyƒá i utrzymywaƒá pipeline‚Äôy danych w platformie Palantir Foundry Projektowaƒá i wdra≈ºaƒá procesy przetwarzania danych oraz modele danych w Snowflake Integrowaƒá dane z r√≥≈ºnych ≈∫r√≥de≈Ç ‚Äì ERP, CRM, dane laboratoryjne Zapewniaƒá jako≈õƒá danych, monitorowaƒá i optymalizowaƒá procesy Wsp√≥≈Çpracowaƒá z architektami i analitykami danych przy projektach w r√≥≈ºnych obszarach biznesowych (produkcja, finanse, logistyka, R&D) Masz min. 3‚Äì5 lat do≈õwiadczenia jako Data Engineer lub Developer Pracowa≈Çe≈õ(a≈õ) z platformƒÖ Palantir Foundry (np. Code Workbook, Pipelines, Ontology) Bardzo dobrze znasz Snowflake i jƒôzyk SQL Programujesz w Python i/lub PySpark Masz do≈õwiadczenie w integracji danych z system√≥w klasy ERP/CRM (np. SAP, Salesforce, MS Dynamics) Znasz rozwiƒÖzania chmurowe, szczeg√≥lnie AWS Pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie min. B2 (miƒôdzynarodowe ≈õrodowisko) Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 900, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Data Engineering,Data Engineering
Full-time,Senior,B2B,Remote,660,Data Solution Architect,Sii,"Do naszego Centrum Data & AI poszukujemy do≈õwiadczonego Data Solution Architekta, kt√≥ry wesprze nas w budowie i optymalizacji nowoczesnych platform danych. Je≈õli masz do≈õwiadczenie w projektowaniu skalowalnych rozwiƒÖza≈Ñ dla miƒôdzynarodowych klient√≥w, znasz technologie chmurowe, a tak≈ºe posiadasz zdolno≈õci analityczne i umiejƒôtno≈õƒá pracy zespo≈Çowej ‚Äì zapraszamy do aplikowania! Projektowanie, wdra≈ºanie oraz optymalizacja nowoczesnych i skalowalnych platform danych Dob√≥r odpowiednich narzƒôdzi i technologii (Azure, AWS, Databricks, Snowflake, itp.) w oparciu o wymagania klienta Wsparcie procesu ofertowania, estymacja oraz nadzorowanie techniczne prac projektowych Wsp√≥≈Çpraca z zespo≈Çem analityk√≥w i developer√≥w w celu osiƒÖgniƒôcia optymalnych wynik√≥w dla klient√≥w Transformacja wymaga≈Ñ biznesowych na sp√≥jne rozwiƒÖzania technologiczne Minimum 3 lata do≈õwiadczenia w architekturze platform danych, szczeg√≥lnie w zakresie pracy z klientem, zbierania wymaga≈Ñ oraz projektowania rozwiƒÖza≈Ñ analitycznych na platformach chmurowych oraz wdro≈ºenia i optymalizacji Data Lake i hurtowni danych w oparciu o metodologie takie jak Kimball czy Data Vault Ekspercka wiedza w zakresie przynajmniej jednej z technologii chmurowych w obszarze Data Platform: Azure, AWS, Databricks lub Snowflake Zdolno≈õƒá do holistycznego spojrzenia na dane i procesy oraz zrozumienia strategicznych cel√≥w biznesowych organizacji, umiejƒôtno≈õƒá analizy danych w kontek≈õcie ca≈Çego ekosystemu firmy Efektywne ≈ÇƒÖczenie perspektywy technicznej z wiedzƒÖ biznesowƒÖ Doskona≈Çe umiejƒôtno≈õci komunikacyjne oraz zdolno≈õƒá do efektywnej wsp√≥≈Çpracy w zespole wielofunkcyjnym Znajomo≈õƒá jƒôzyka polskiego i angielskiego na poziomie swobodnej komunikacji Znajomo≈õƒá narzƒôdzi do wizualizacji danych (np. Power BI) oraz technologii wspierajƒÖcych przetwarzanie danych w czasie rzeczywistym, analityki AI Certyfikaty zwiƒÖzane z chmurƒÖ (np. AWS Certified Solutions Architect, Microsoft Azure Solutions Architect) Tytu≈Ç Great Place to Work od 2015 roku - to dziƒôki opiniom pracownik√≥w otrzymujemy tytu≈Ç i wdra≈ºamy nowe pomys≈Çy Stabilno≈õƒá zatrudnienia ‚Äì 2,1 MLD PLN przychodu, brak d≈Çug√≥w, od 2006 roku na rynku Dzielimy siƒô zyskiem z pracownikami - od 2022 roku przeznaczyli≈õmy na ten cel ju≈º ponad 60 milion√≥w PLN Bogaty pakiet benefit√≥w - prywatna opieka zdrowotna, platforma kafeteryjna, zni≈ºki na samochody i wiƒôcej Komfortowe miejsce pracy - pracuj w naszych biurach klasy A lub zdalnie DziesiƒÖtki fascynujƒÖcych projekt√≥w dla presti≈ºowych marek z ca≈Çego ≈õwiata ‚Äì mo≈ºesz je zmieniaƒá dziƒôki aplikacji Job Changer 1 000 000 PLN rocznie na Twoje pomys≈Çy - takƒÖ kwotƒÖ wspieramy pasje i akcje wolontariackie naszych pracownik√≥w Stawiamy na Tw√≥j rozw√≥j - meetupy, webinary, platforma szkoleniowa i blog technologiczny ‚Äì Ty wybierasz Fantastyczna atmosfera stworzona przez wszystkich Sii Power People 1 Wy≈õlij do nas swoje CV 2 We≈∫ udzia≈Ç w rozmowie rekrutacyjnej 3 Poznaj projekty dopasowane do Twoich potrzeb 4 Podejmij decyzjƒô i zacznij swojƒÖ przygodƒô z Sii! Sii to czo≈Çowy dostawca doradztwa technologicznego, transformacji cyfrowej oraz us≈Çug in≈ºynieryjnych i biznesowych w Polsce.Zatrudniamy ju≈º ponad 7 500 specjalist√≥w i realizujemy projekty z r√≥≈ºnorodnych bran≈º dla klient√≥w z wielu kraj√≥w na ca≈Çym ≈õwiecie. Tytu≈Ç Great Place to Work zdobyty 10 razy z rzƒôdu to dow√≥d na to, ≈ºe w Sii tworzymy przyjazne ≈õrodowisko pracy. W badaniu a≈º 90% naszych pracownik√≥w odpowiedzia≈Ço, ≈ºe Sii jest ≈õwietnym miejscem pracy, a 95% z nich sƒÖdzi, ≈ºe panuje tu ≈õwietna atmosfera.",[],Data Architecture,Data Architecture
Full-time,Senior,B2B,Remote,661,Data Engineer with Palantir,Link Group,"About the Role We are looking for a Data Engineer experienced with Palantir Foundry to join a cross-functional team working on large-scale data integration, modeling, and analytics platforms. The ideal candidate is hands-on, proactive, and capable of navigating complex data ecosystems in an enterprise environment. Design and build data pipelines and models using Palantir Foundry Integrate multiple data sources (structured and unstructured) into usable, high-quality data assets Collaborate with data scientists, analysts, and business stakeholders to support advanced analytics initiatives Apply data governance, lineage, and cataloging principles within Foundry Develop and maintain Foundry ‚ÄúObjects‚Äù, Code Workbooks, and other tooling Ensure quality, performance, and scalability of the implemented data solutions Support and document platform usage and development best practices 3+ years of experience in Data Engineering Hands-on experience with Palantir Foundry in a commercial or enterprise setting Proficiency in SQL , Python , and data transformation techniques Good understanding of data modeling (dimensional, relational, and graph-based) Familiarity with data governance and metadata management Experience working in cloud-based environments (AWS, GCP, or Azure) Excellent communication skills and ability to work with cross-functional teams Previous experience in highly regulated industries (finance, pharma, defense, etc.) Experience integrating Foundry with external tools and systems via APIs Knowledge of CI/CD , Git , and software engineering best practices Exposure to tools like Airflow , dbt , Databricks , Snowflake , etc. Experience with data privacy regulations (GDPR, HIPAA, etc.)","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Hybrid,662,üí°AI Specialist üí°,LSI Software,"Chcesz tworzyƒá inteligentne rozwiƒÖzania i pracowaƒá z nowoczesnymi technologiami?Do≈ÇƒÖcz do naszego zespo≈Çu i miej realny wp≈Çyw na innowacyjne projekty. Szukamy pasjonat√≥w, kt√≥rzy chcƒÖ rozwijaƒá AI i kszta≈Çtowaƒá przysz≈Ço≈õƒá. üöÄ Zapraszamy Ciebie, je≈õli: Masz wykszta≈Çcenie wy≈ºsze z zakresu informatyki, matematyki, analizy danych lub pokrewnych, Posiadasz komercyjne do≈õwiadczenie w pracy na stanowisku programistycznym, Rozumiesz mo≈ºliwo≈õci i ograniczenia AI oraz kluczowe pojƒôcia z tej dziedziny, Masz podstawowƒÖ wiedzƒô o bazach danych i potrafisz pracowaƒá z danymi, np. w SQL, CechujƒÖ ciƒô doskona≈Çe umiejƒôtno≈õci komunikacyjne i zdolno≈õƒá do wsp√≥≈Çpracy, Potrafisz wyszukiwaƒá, analizowaƒá i weryfikowaƒá informacje, Masz proaktywne podej≈õcie do nauki i rozwijasz siƒô wraz ze zmieniajƒÖcymi siƒô technologiami, Znasz jƒôzyk angielski na poziomie B2. Mile widziane: Znajomo≈õƒá jednego z jƒôzyk√≥w programowania (.Net, PHP, C#, Angular). Bƒôdziesz odpowiadaƒá za: Implementacjƒô rozwiƒÖza≈Ñ AI w istniejƒÖcych rozwiƒÖzaniach firmy, Wyb√≥r odpowiednich narzƒôdzi i technologii AI do realizacji konkretnych projekt√≥w, Samodzielne wdra≈ºanie rozwiƒÖza≈Ñ AI oraz wsparcie w bardziej zaawansowanych projektach, Identyfikacja obszar√≥w, w kt√≥rych AI mo≈ºe zwiƒôkszyƒá efektywno≈õƒá i automatyzowaƒá procesy, Monitorowanie trend√≥w i nowo≈õci w dziedzinie sztucznej inteligencji. Praca w LSI to: ü§ù Wsparcie do≈õwiadczonych ekspert√≥w - dostƒôp do profesjonalnej wiedzy i wsparcia naszych najlepszych specjalist√≥w ‚öôÔ∏è Autorskie, innowacyjne rozwiƒÖzania - robotyka, AI, automatyzacja to nasza codzienno≈õƒá ‚ù§Ô∏è Dba≈Ço≈õƒá o Tw√≥j komfort i zdrowie - prywatna opieka medyczna Medicover, karta sportowa Medicover sport, ubezpieczenie Allianz/PZU ‚ò∫Ô∏è Rewelacyjna atmosfera - wyj≈õcia zespo≈Çowe, w≈Çasna sala kinowa, wyjazdy integracyjne üìà Realny wp≈Çyw - kszta≈Çtujesz wizerunek firmy i jako≈õƒá produkt√≥w üèùÔ∏è Kr√≥tsze piƒÖtki i jeszcze wiƒôcej work life balance - naszym pracownikom zapewniamy elastyczne godziny pracy oraz mo≈ºliwo≈õƒá pracy hybrydowej, a dwa piƒÖtki w miesiƒÖcu pracujemy tylko 4h üõ°Ô∏è Stabilno≈õƒá i bezpiecze≈Ñstwo - jeste≈õmy ponad 30 lat na rynku",[],Data Science,Data Science
Part-time,Mid,B2B,Remote,663,Automation Hero (N8N),Automation House,"ü§ñ Jak dzia≈Çamy? W Automation House pracujemy w zgranym i zaanga≈ºowanym zespole, kt√≥ry zajmuje siƒô narzƒôdziami no-code oraz low-code i automatyzacjƒÖ proces√≥w w firmach. Nasza misja to ocaliƒá naszych Klient√≥w przed ciƒÖg≈ÇƒÖ i ≈ºmudnƒÖ pracƒÖ manualnƒÖ, wdra≈ºajƒÖc rozwiƒÖzania AI i wprowadzajƒÖc ich w nowƒÖ rzeczywisto≈õƒá. Pomagamy uporzƒÖdkowaƒá procesy i usprawniƒá codziennƒÖ pracƒô, co przynosi ogromnƒÖ satysfakcjƒô, majƒÖc ≈õwiadomo≈õƒá rzeczywistego, pozytywnego wp≈Çywu na funkcjonowanie biznesu naszych Klient√≥w. Stawiamy na pe≈ÇnƒÖ transparentno≈õƒá wsp√≥≈Çpracy i zaanga≈ºowanie, dobierajƒÖc mo≈ºliwie najlepiej pasujƒÖcych specjalist√≥w do profilu firmy. G≈Ç√≥wnym celem tego stanowiska jest projektowanie i wdra≈ºanie zaawansowanych automatyzacji w n8n, kt√≥re bezpo≈õrednio przek≈ÇadajƒÖ siƒô na cele biznesowe naszych klient√≥w ‚Äì od zwiƒôkszenia efektywno≈õci po redukcjƒô koszt√≥w operacyjnych. Bƒôdziesz kluczowƒÖ osobƒÖ odpowiedzialnƒÖ za transformacjƒô manualnych proces√≥w w zautomatyzowane, niezawodne systemy. Przygotowali≈õmy dla Ciebie ciekawy i anga≈ºujƒÖcy proces rekrutacji z udzia≈Çem zespo≈Çu Automation House i Tigers! ü§ñ Za co bƒôdziesz odpowiedzialny? üîπ Projektowanie i budowanie przep≈Çyw√≥w pracy (workflows) w n8n w oparciu o analizƒô potrzeb klienta. üîπ Integracja n8n z r√≥≈ºnorodnymi aplikacjami i us≈Çugami poprzez API. üîπ Wykorzystywanie JavaScript do tworzenia niestandardowych funkcji i nod√≥w w n8n. üîπ Testowanie, debugowanie i optymalizacja istniejƒÖcych automatyzacji w celu zapewnienia ich stabilno≈õci i wydajno≈õci. üîπ Tworzenie dokumentacji technicznej dla wdro≈ºonych rozwiƒÖza≈Ñ. üîπ Wsp√≥≈Çpraca z klientami w celu zrozumienia ich proces√≥w i proponowania najlepszych rozwiƒÖza≈Ñ automatyzacyjnych. üîπ Aktywne ≈õledzenie nowo≈õci w ekosystemie n8n i proponowanie usprawnie≈Ñ w naszych projektach. ü§ñ Co cechuje idealnego Automation Hero? üîπ Bardzo dobra, praktyczna znajomo≈õƒá platformy n8n i jej mo≈ºliwo≈õci. üîπ Podstawowa znajomo≈õƒá JavaScript, umo≈ºliwiajƒÖca pisanie prostych skrypt√≥w i funkcji. üîπ Zrozumienie zasad dzia≈Çania REST API oraz formatu danych JSON. üîπ Analityczne my≈õlenie i umiejƒôtno≈õƒá przek≈Çadania proces√≥w biznesowych na logikƒô automatyzacji. üîπ Zainteresowanie nowymi technologiami i AI üîπ Do≈õwiadczenie w pracy z biznesem, klientami B2B. üîπ Samodzielno≈õƒá i proaktywne podej≈õcie do rozwiƒÖzywania problem√≥w technicznych. üîπ Mile widziana znajomo≈õƒá innych narzƒôdzi no-code/low-code (np. Make, Zapier). ü§ñ Co oferujemy? üîπ B≈Çyskawiczny rozw√≥j ‚Äì u nas w kwarta≈Ç nauczysz siƒô tyle, ile w innych miejscach w rok. üîπ Brak sufitu ‚Äì chcesz i≈õƒá w eksperta czy account managera? Sam zdecyduj! Masz pomys≈Ç na rozw√≥j us≈Çugi? Dostajesz wsparcie, by to zrobiƒá. üîπ Ekspozycja w bran≈ºy ‚Äì posiadamy najmocniejsze kana≈Çy komunikacji w bran≈ºy, w kt√≥rych mo≈ºesz budowaƒá wizerunek eksperta. üîπ Kole≈ºe≈Ñski zesp√≥≈Ç ‚Äì w Automation House szczeg√≥lnƒÖ wagƒô przywiƒÖzujemy do budowania relacji miƒôdzy sobƒÖ! üîπ Doskona≈Ço≈õƒá operacyjna ‚Äì dbamy o Tw√≥j komfort pracy dziƒôki przejrzystym procesom i pe≈Çnej jasno≈õci ‚Äì wiesz co robiƒá i wiesz po co to robisz. üí∏ Wynagrodzenie Stawka godzinowa na tym stanowisku to 60z≈Ç/h netto na fakturze (B2B) lub ca≈Çkowity koszt pracodawcy na UZ.",[],Unclassified,Unclassified
Full-time,Mid,Permanent or B2B,Remote,664,PowerBI Developer,UNIVIO,"#JoinUteam ü¶Ü Cze≈õƒá, jeste≈õmy Univio ‚Äì partner cyfrowej transformacji handlu. Ju≈º od ponad 25 lat pracujemy z globalnymi klientami, realizujƒÖc setki projekt√≥w dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocze≈õnie lu≈∫nƒÖ, niezobowiƒÖzujƒÖcƒÖ atmosferƒô. Nasza organizacja opiera siƒô na kulturze otwarto≈õci i dzielenia siƒô wiedzƒÖ. Wsp√≥lnie pracujemy, rozwijamy siƒô i inspirujemy. üíú Do Twoich zada≈Ñ nale≈ºeƒá bƒôdzie: Projektowanie i implementowanie zmian w systemie kontrolingowym Univio Projektowanie i implementowanie zmian w systemie kontrolingowym Univio Analizowanie, modyfikowanie i budowanie zapyta≈Ñ SQL Analizowanie, modyfikowanie i budowanie zapyta≈Ñ SQL Implementowanie proces√≥w ETL / ELT Implementowanie proces√≥w ETL / ELT Projektowanie i budowanie modeli danych i miar w jƒôzyku DAX Projektowanie i budowanie modeli danych i miar w jƒôzyku DAX Wspieranie Product Ownera w analizowaniu potrzeb u≈ºytkownik√≥w i proponowaniu rozwiƒÖza≈Ñ Wspieranie Product Ownera w analizowaniu potrzeb u≈ºytkownik√≥w i proponowaniu rozwiƒÖza≈Ñ Budowanie raport√≥w pomagajƒÖcych uzyskiwaƒá odpowiedzi na pytania zadawane przez mened≈ºer√≥w Budowanie raport√≥w pomagajƒÖcych uzyskiwaƒá odpowiedzi na pytania zadawane przez mened≈ºer√≥w üíú≈öwietnie siƒô u nas odnajdziesz, je≈õli: Rozumiesz i sprawnie u≈ºywasz DAX do budowania miar, tabel i kolumn obliczeniowych Rozumiesz i sprawnie u≈ºywasz DAX do budowania miar, tabel i kolumn obliczeniowych Sprawnie pos≈Çugujesz siƒô jƒôzykiem SQL (umiejƒôtno≈õƒá tworzenia z≈Ço≈ºonych zapyta≈Ñ) Sprawnie pos≈Çugujesz siƒô jƒôzykiem SQL (umiejƒôtno≈õƒá tworzenia z≈Ço≈ºonych zapyta≈Ñ) Nie masz problemu z obs≈ÇugƒÖ Power Query w szerokim zakresie dostƒôpnych ≈∫r√≥de≈Ç danych Nie masz problemu z obs≈ÇugƒÖ Power Query w szerokim zakresie dostƒôpnych ≈∫r√≥de≈Ç danych Znasz dobre praktyki przy budowie czytelnych, intuicyjnych w u≈ºyciu raport√≥w Znasz dobre praktyki przy budowie czytelnych, intuicyjnych w u≈ºyciu raport√≥w Wiesz, co to jest Power BI Service i masz podstawy z zakresu administracji portalem Wiesz, co to jest Power BI Service i masz podstawy z zakresu administracji portalem Jeste≈õ osobƒÖ z inicjatywƒÖ, nastawionƒÖ na rozumienie potrzeb u≈ºytkownik√≥w i aktywne szukanie rozwiƒÖza≈Ñ Jeste≈õ osobƒÖ z inicjatywƒÖ, nastawionƒÖ na rozumienie potrzeb u≈ºytkownik√≥w i aktywne szukanie rozwiƒÖza≈Ñ üíúDodatkowym plusem bƒôdzie, je≈õli: Potrafisz samodzielnie projektowaƒá obiekty bazodanowe w modelu relacyjnym (tabele, widoki, normalizacja) Potrafisz samodzielnie projektowaƒá obiekty bazodanowe w modelu relacyjnym (tabele, widoki, normalizacja) Praca na komponentach Power BI Premium nie sprawia Ci problemu (przep≈Çywy danych, potoki wdra≈ºania) Praca na komponentach Power BI Premium nie sprawia Ci problemu (przep≈Çywy danych, potoki wdra≈ºania) Masz do≈õwiadczenie w zakresie dzia≈Çania Power Platform (Power Automate i Power Apps) oraz narzƒôdzi Microsoft 365 (np. Sharepoint) Masz do≈õwiadczenie w zakresie dzia≈Çania Power Platform (Power Automate i Power Apps) oraz narzƒôdzi Microsoft 365 (np. Sharepoint) Sprawd≈∫, dlaczego warto z nami pracowaƒá! üëáüëáüëá S≈Çynne ‚Äûowocowe czwartki‚Äù te≈º mamy, ale co wa≈ºniejsze, mo≈ºesz liczyƒá na: üëÖ Grupowe zajƒôcia jƒôzyka angielskiego üèä‚Äç‚ôÄÔ∏è Multisport ü©∫PrywatnƒÖ opiekƒô medycznƒÖ üí∞Ubezpieczenie na ≈ºycie üìôFirmowƒÖ bibliotekƒô (ksiƒÖ≈ºka warta uwagi ‚Äì zamawiasz jƒÖ u nas i wszyscy zyskujemy nowƒÖ wiedzƒô!) üôãProgram rekomendacji pracownik√≥w (ty polecasz, my p≈Çacimy) Przebieg procesu rekrutacyjnego: 1Ô∏è‚É£ Aplikujesz na og≈Çoszenie - Wysy≈Çasz swoje CV, a my nie mo≈ºemy siƒô doczekaƒá, ≈ºeby je zobaczyƒá. 2Ô∏è‚É£ Rozmowa z rekruterem - Nasza rekruterka, Patrycja , zadzwoni na ok. 30 minut do Ciebie po weryfikacji CV, ≈ºeby lepiej Ciƒô poznaƒá i opowiedzieƒá o wszystkim, co musisz wiedzieƒá. Po tym spotkaniu r√≥wnie≈º otrzymasz zadanie do wykonania, na podstawie kt√≥rego dostaniesz informacjƒô o kolejnych etapach. 3Ô∏è‚É£ Spotkanie z managerem i tech-check ‚Äì Oko≈Ço tydzie≈Ñ od rozmowy bƒôdziesz mia≈Ç(a) okazjƒô porozmawiaƒá z osobƒÖ, kt√≥ra mo≈ºe byƒá Twoim przysz≈Çym liderem, a tak≈ºe bƒôdziesz m√≥g≈Ç porozmawiaƒá z Tech Leaderem o aspektach technicznych projektu. Takie spotkanie ≈õrednio trwa godzinƒô. 4Ô∏è‚É£ Spotkanie z Hiring Managerem - To 20-minutowa rozmowa, kt√≥rej celem jest wzajemne poznanie siƒô i om√≥wienie Twojego dotychczasowego do≈õwiadczenia. 5Ô∏è‚É£ Decyzja - Po wszystkim, do dw√≥ch tygodni wr√≥cimy do Ciebie z odpowiedziƒÖ. Trzymamy kciuki! ü§û","[{""min"": 13400, ""max"": 16800, ""type"": ""Net per month - B2B""}, {""min"": 9900, ""max"": 12300, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Mid,Permanent,Remote,665,Data Analyst (Product),Revolut,"People deserve more from their money. More visibility, more control, and more freedom. Since 2015, Revolut has been on a mission to deliver just that. Our powerhouse of products ‚Äî including spending, saving, investing, exchanging, travelling, and more ‚Äî help our 60+ million customers get more from their money every day. As we continue our lightning-fast growth,‚Äå 2 things are essential to our success: our people and our culture. In recognition of our outstanding employee experience, we've been certified as a Great Place to Work‚Ñ¢. So far, we have 10,000+ people working around the world, from our offices and remotely, to help us achieve our mission. And we're looking for more brilliant people. People who love building great products, redefining success, and turning the complexity of a chaotic world into the simplicity of a beautiful solution. We approach Data Science at Revolut the same way that we approach everything else. We take complex problems, and create extraordinary solutions that our customers love. Our Data Analysts aren‚Äôt kept in the background, doomed to never see the impact of their work. They‚Äôre some of our best and brightest problem solvers, deployed to the front-lines to work in Product Teams and deliver rock star solutions ü§ò We‚Äôre looking for a superstar Data Analyst who does not believe there is a data task that can be too hard ü¶¨ From digging into our complex databases, looking for the root cause of a problem to designing their own solutions and writing their own code to implement them. In this process they never stop learning, picking up new skills and delivering value. Up for the challenge? Get in touch üëá Understanding our business and its processes through our data Applying this understanding and knowledge of data to help product and service teams Designing key metrics to measure different aspects of the business Creating and maintaining new aggregated views and tables to simplify data querying Maintaining and creating new dashboards to track metrics and visualise insights Promoting data literacy across the company, organising and holding workshops Previous experience in an analytical role, creating impactful solutions A strong background/education in a quantitative discipline 5+ years of experience with SQL Great skills with Python or other programming languages Evidence of strong mathematical and statistics knowledge Building a global financial super app isn‚Äôt enough. Our Revoluters are a priority, and that‚Äôs why in 2021 we launched our inaugural D&I Framework, designed to help us thrive and grow everyday. We're not just doing this because it's the right thing to do. We‚Äôre doing it because we know that seeking out diverse talent and creating an inclusive workplace is the way to create exceptional, innovative products and services for our customers. That‚Äôs why we encourage applications from people with diverse backgrounds and experiences to join this multicultural, hard-working team.",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent or B2B,Remote,666,Senior Data Engineer,N-iX,"#3071 We are seeking a proactive Senior Data Engineer to join our vibrant team. As a Senior Data Engineer, you will play a critical role in designing, developing, and maintaining sophisticated data pipelines, Ontology Objects, and Foundry Functions within Palantir Foundry. The ideal candidate will possess a robust background in cloud technologies, data architecture, and a passion for solving complex data challenges. Key Responsibilities: Collaborate with cross-functional teams to understand data requirements, and design, implement and maintain scalable data pipelines in Palantir Foundry, ensuring end-to-end data integrity and optimizing workflows. Gather and translate data requirements into robust and efficient solutions, leveraging your expertise in cloud-based data engineering. Create data models, schemas, and flow diagrams to guide development. Develop, implement, optimize and maintain efficient and reliable data pipelines and ETL/ELT processes to collect, process, and integrate data to ensure timely and accurate data delivery to various business applications, while implementing data governance and security best practices to safeguard sensitive information. Monitor data pipeline performance, identify bottlenecks, and implement improvements to optimize data processing speed and reduce latency. Troubleshoot and resolve issues related to data pipelines, ensuring continuous data availability and reliability to support data-driven decision-making processes. Stay current with emerging technologies and industry trends, incorporating innovative solutions into data engineering practices, and effectively document and communicate technical solutions and processes. Tools and skills you will use in this role: Palantir Foundry Python PySpark SQL TypeScript Required: 5+ years of experience in data engineering, preferably within the pharmaceutical or life sciences industry; Strong proficiency in Python and PySpark; Proficiency with big data technologies (e.g., Apache Hadoop, Spark, Kafka, BigQuery, etc.); Hands-on experience with cloud services (e.g., AWS Glue, Azure Data Factory, Google Cloud Dataflow); Expertise in data modeling, data warehousing, and ETL/ELT concepts; Hands-on experience with database systems (e.g., PostgreSQL, MySQL, NoSQL, etc.); Proficiency in containerization technologies (e.g., Docker, Kubernetes); Effective problem-solving and analytical skills, coupled with excellent communication and collaboration abilities; Strong communication and teamwork abilities; Understanding of data security and privacy best practices; Strong mathematical, statistical, and algorithmic skills. Nice to have: Certification in Cloud platforms, or related areas; Experience with search engine Apache Lucene, Webservice Rest API; Familiarity with Veeva CRM, Reltio, SAP, and/or Palantir Foundry; Knowledge of pharmaceutical industry regulations, such as data privacy laws, is advantageous; Previous experience working with JavaScript and TypeScript.","[{""min"": 18226, ""max"": 19320, ""type"": ""Net per month - B2B""}, {""min"": 14581, ""max"": 15310, ""type"": ""Gross per month - Permanent""}]",Data Engineering,Data Engineering
Full-time,Mid,Permanent or B2B,Remote,667,Power BI Developer,Transition Technologies PSC,"Firma Algomine, nale≈ºƒÖca do grupy TTPSC, poszukuje Power BI Developer'a Zakres obowiƒÖzk√≥w Tworzenie, obs≈Çuga i optymalizacja raport√≥w i pulpit√≥w nawigacyjnych Power BI Wsp√≥≈Çpraca z interesariuszami biznesowymi, aby zrozumieƒá ich wymagania i prze≈Ço≈ºyƒá je na skuteczne wizualizacje danych Wykonywanie proces√≥w wyodrƒôbniania, przekszta≈Çcania i ≈Çadowania danych (ETL) Tworzenie modeli danych i zarzƒÖdzanie nimi Pisanie z≈Ço≈ºonych zapyta≈Ñ DAX, aby tworzyƒá kolumny obliczeniowe, miary i tabele niestandardowe Zapewnienie dok≈Çadno≈õƒá i integralno≈õƒá danych w raportach i dashboardach Wdra≈ºanie zabezpiecze≈Ñ na poziomie wiersza (RLS) i zarzƒÖdzania nimi Przeprowadzanie, dostrajanie wydajno≈õci i optymalizacja rozwiƒÖza≈Ñ Power BI Tworzenie i utrzymywanie dokumentacji modeli danych, raport√≥w i proces√≥w Poszukiwane kompetencje Minimum 3 lata do≈õwiadczenia jako programista Power BI lub na podobnym stanowisku Bieg≈Ça znajomo≈õƒá Power BI, w tym Power Query, DAX, Power BI Service ‚Äì przep≈Çywy danych, raporty podzielone na strony Zrozumienie koncepcji aspekt√≥w zwiƒÖzanych z administracjƒÖ, bezpiecze≈Ñstwem i zarzƒÖdzaniem platformƒÖDo≈õwiadczenie w modelowaniu danych i koncepcjach projektowania baz danych Dobra znajomo≈õƒá jƒôzyka SQL i do≈õwiadczenie w pracy z relacyjnymi bazami danych Istotna bƒôdzie dla nas do≈õwiadczenie w pracy z systemami bazodanowymi takimi jak MS SQL Server, Azure Synapse czy Azure SQL oraz praktyczna znajomo≈õƒá narzƒôdzi i technologii: Power BI, DAX, SSIS, SSRS, ADF, Azure Data Lake, Azure SQL, Azure Synapsa Znajomo≈õƒá proces√≥w i narzƒôdzi ETL Doskona≈Çe umiejƒôtno≈õci analityczne i rozwiƒÖzywania problem√≥w Umiejƒôtno≈õƒá tworzenia jasnej i zwiƒôz≈Çej dokumentacji Silne umiejƒôtno≈õci komunikacyjne i interpersonalne Umiejƒôtno≈õƒá pracy zar√≥wno samodzielnie, jak i w zespole Mile widziane: Znajomo≈õƒá pozosta≈Çych komponent√≥w ≈õrodowiska Microsoft Fabric Znajomo≈õƒá hurtowni danych i architektury BI Do≈õwiadczenie w pracy z metodykami Agile/Scrum Odpowiednie certyfikaty (np. Microsoft Certified: Data Analyst Associate) Oferujemy Nowoczesny stack technologiczny i otwarto≈õƒá na wdra≈ºanie nowych rozwiƒÖza≈Ñ Udzia≈Ç w zr√≥≈ºnicowanych i zaawansowanych technologicznie projektach analitycznych Prywatna opieka medyczna i karta sportowa Elastyczny czas pracy i mo≈ºliwo≈õƒá 100% pracy zdalnej Spotkania integracyjne oraz niezapomniane imprezy firmowe",[],Data Analysis & BI,Data Analysis & BI
Full-time,Senior,Permanent,Hybrid,668,Senior Data Scientist - ML&AI,VISA,"Company Description Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose ‚Äì to uplift everyone, everywhere by being the best way to pay and be paid. Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa. Visa‚Äôs Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world‚Äôs most sophisticated processing networks capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. While working with us you‚Äôll get to work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cyber security, and B2C platforms. The Opportunity: We are looking to hire a Data Scientist to lead the AI/ML initiatives in the Cybersecurity Products team. This position will be based out of Warsaw, Poland. In this role, you will: Be responsible for driving, designing, and building cutting edge innovation in the space of cybersecurity through Artificial Intelligence and Machine Learning - current scope of problems includes behavior biometrics, risk-based authentication, Account Takeover protection, advanced threat detection, smart incident response, AI model threat analysis, and many more. Drive the continued innovation and engineering of our existing behavior-based adaptive authentication product and bot/fraud protection product with a high-performance team of data scientists and engineers. Build innovative solutions and collaborate with engineering and product partners across the global Visa organization that help secure Visa against a variety of threats and attacks. Provide consultation to more experienced leaders in order to recommend solutions which solve security & other business challenges. You must have strong technical depth and experience in application of Machine Learning, Deep Learning, and Data Science techniques. On top of that, you should also have a genuine interest in cybersecurity and a desire to build solutions that deliver real impacts to the world. We will rely on your leadership to establish a roadmap and vision as this team engages in existing and new emerging areas. Support transfer technical knowledge to facilitate implementation of the business solution provided. Document all projects developed, including clear and efficient coding, and write other documentation as needed. Identify relevant market trends by country, based on a deep analysis of payment industry information. Interacting with several internal and external stakeholders for the strategic definition of analysis and initiatives. Continuously develop and present innovative ideas to improve current business practices within Visa. Essential Functions: Cyber Analytics Product: Research innovation in digital authentication using behavior biometrics capabilities, build applied AI based models and engineer them into the product called Visa Behavior Analytics. You will engage in data science and applied AI related activities for a Visa engineered product to protect against account takeover related threats, continuously enhancing it to combat threats in the secure authentication and perimeter defense space. Cyber/AI R&D: Research innovation in applying AI to the more general field of cybersecurity, including the protection with and against AI driven technologies, as well as the AI models themselves. You will be using your core competencies around AI and data science and help drive the teams to build models and solutions that work at scale, harnessing Petabytes of data while applying it to products that need to respond with cyber analytics in milliseconds. Influence & Collaborate: Be able to present results to a cross section of employees, including C-Level and other senior leadership at Visa. You will engage with internal technology, and cyber teams along with global product orgs. In addition, you will collaborate with colleagues in technology and product offices to establish effective, productive business relationships. This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs. Basic Qualifications: 2+ years of relevant work experience and a Bachelor‚Äôs degree, OR 5+ years of relevant work experience. Preferred Qualifications: 3 or more years of work experience with a Bachelor‚Äôs Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD,MD). 3+ years of experience in modern data mining and data science techniques(e.g., regressions, decision trees, ensemble algorithms, neural networks, time series analytics, clustering, anomaly detection, text analytics, etc.). Candidates with a PhD in a quantitative field, such as Statistics, Mathematics, Operational Research, Computer Science, Economics, or engineering preferred. Experience in developing and deploying products using Docker, Kubernetes, and the containerization technology stack. Experience in development of advanced machine learning and deep learning models such as RNN, LSTM, Graph Neural Networks. Experience and proficiency in working with large language models (LLMs), and familiarity with the associated solution architecture and infrastructure, including Nvidia GPUs. Experience in leading, building, and supporting scalable and reliable AI/ML-powered systems, enabling rapid prototyping and advanced analytics using modern big data and AI/ML technologies (e.g., Spark, Kafka, TensorFlow, PyTorch) in an agile environment. Software Engineering background: The candidate must be proficient in Python and at least one object-oriented programming language (Java, Golang, C++,etc.) Golang experience is desired. Candidate must possess software engineering skills and be able to take end-to-end ownerships of analytical models. Data Wrangling: The candidate must have proficient data wrangling skills with Python, SQL, and other data processing tools/scripts. Experience with the end-to-end machine learning lifecycle and MLOps, including data preprocessing and feature extraction, model training and evaluation, deployment, and monitoring of AI models in production environments. Experience working with Docker in both development and deployment workflows, ensuring smooth transitions from development to production environments. Distributed Systems: practical experience with NoSQL data platforms (e.g., Cassandra, Lakehouse, DynamoDB) and caching technologies like Redis is a plus. A solid understanding of the Linux networking subsystem, contributing to the stability and performance of deployed AI/ML systems. A solid understanding of the web applications and APIs, contributing to the front-end accessibility and integration of AI-driven solutions. Cloud domain: Familiarity with infrastructure and analytics services on cloud(e.g., AWS, Azure) is a plus. Domain Knowledge - Candidate with background in one or multiple of the following domains is a plus: Cybersecurity, AI security/privacy research and Biometrics Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","[{""min"": 16000, ""max"": 26000, ""type"": ""Gross per month - Permanent""}]",Data Science,Data Science
Full-time,Mid,B2B,Hybrid,669,GenAI Productivity Analyst,ITDS,"GenAI Productivity Analyst Join us, and lead the charge in AI-powered developer tools! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As a GenAI Productivity Analyst, you will be working for our client, a global financial services leader pioneering developer productivity enhancements through innovative data science projects. Your role focuses on advancing the initial proof-of-concept linking GenAI coding assistant usage to developer productivity, operationalizing these insights, and identifying actionable patterns and use cases. Collaborating with data scientists and programme teams, you will help unlock industry-leading knowledge that supports developer performance improvements across thousands of users, setting new standards in GenAI adoption and value realization within a complex, fast-paced environment. Your main responsibilities: Reviewing and enhancing the existing GenAI CA Productivity Analysis proof-of-concept Developing a future roadmap and delivery strategy for GenAI CA productivity insights Identifying specific use cases and technical patterns where GenAI CAs impact productivity most Highlighting challenges, accelerators, and best practices for GenAI CA adoption Contributing to a data science strategy for measuring GenAI CA benefits Gathering, analyzing, and interpreting SDLC, Deployment, and DORA metrics data Collaborating with data analysts and scientists to refine analytical models and methodologies Defining delivery strategy, phased plans, and MVP implementation objectives Communicating actionable insights clearly to senior stakeholders Supporting delivery teams with inputs to develop initiatives improving GenAI CA benefits for developers You're ideal for this role if you have: Proven experience in data analysis within software development contexts Strong understanding of SDLC, Deployment, and DORA metrics and their impact on developer productivity Experience translating data analysis into actionable business or operational insights Familiarity with GenAI technologies, specifically coding assistants like GitHub Copilot Proficiency in data analytics and processing tools such as Python, R, SQL, or Jupyter notebooks Solid knowledge of data modeling concepts Experience working in global matrix organizations and cross-cultural environments Excellent written and verbal communication skills in English, including report and presentation creation Ability to work independently in a fast-paced, dynamic environment with tight deadlines Strong interpersonal and influential communication skills It is a strong plus if you have: Exposure to machine learning libraries such as Scikit-learn, XGBoost, Keras, or PyTorch Track record in knowledge acquisition, transfer, and community building processes Experience rewriting or refining English content authored by non-native speakers Willingness to explore and implement emerging technologies Prior experience contributing to developer productivity or adoption programs We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7159 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 23100, ""max"": 29400, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,Data Analysis & BI
Full-time,Senior,B2B,Remote,670,Senior Data Engineer,Innobo Sp. z o. o.,"For Client from BigPharma sector we are looking for Senior Data Engineer. Right now, we are looking for two Senior Data engineers to join our successful Haystack platform, where we model financial date for valuable reporting. We live in Azure, with Databricks and Fabric being our main tools of trade, but experience and capabilities in working with Python, AAS and SQL/Sequel are highly appreciated. You could be an expert in backend or frontend design and engineering, or maybe a hybrid, even covering Purview/PowerApps or LogAnalytics? If you have strong capabilities in these areas, let‚Äôs hear from you! The position is based in Warsaw, Poland and at our site in √òrestad, which is in Geater Copenhagen, Denmark. You will be working remotely in a global environment. Nevertheless, you must expect that, core meetings or workshops occasionally take place in Danish or Polish office. You will be part of the Global Data & Artificial Intelligence area, in the Data Engineering department of Data Management. Data Management is globally distributed and has for mission to harness the power of Data and Artificial Intelligence, integrating it seamlessly into the fabric of organization's operations. We provide the foundation to integrate the areas of Data and Artificial Intelligence throughout the whole organization, empowering organization to realize its strategic ambitions to create a connected data landscape. We drive the innovation, process definition, and platform implementation in the areas of data governance, data engineering design, master data management, metadata management, data marketplace, and data quality aligned with business priorities. The organisation values flexibility in ways of working to support various life situations. Employees are recognised for their unique qualities and skills, and the environment fosters development and collaboration. The broader mission includes improving the lives of millions of patients globally through innovation and dedication to chronic disease care. There is a commitment to becoming not just the best company in the world, but the best company for the world. This vision can only be achieved through the contributions of talented employees with diverse backgrounds and perspectives. An inclusive culture is fostered that celebrates diversity across employees, patients, and communities. Responsibilities: Dedicated engineering prioritized by the team Product Manager. Support the DK based core team in developing new product/models. Take lead in tasks given by Product Manager, aligning with stakeholders. Plan and execute your work in Azure DevOps. Document all work to be transparent for the rest of the team. Ensure the delivery of high-quality work. Trust in data is our most valuable currency. Collaborate with cross-functional teams to understand and address technical requirements in addition to implementing efficiency improvements. Requirements: A master‚Äôs degree in data, Computer Science, Information Management, or a related field. Experience of 5+ years in the domain of software and data engineering, with relevant years in Data Pipeline Engineering and Integration. Strong proficiency in Databricks and/or Fabric and a deep understanding of its core functions and tools to optimize data workflows, date modelling and analytics. Extensive experience with programming languages such as Python, especially for data manipulation and automation. Proficient in SQL for complex query development, and data extraction across varied databases. Hands-on experience with DevOps, ensuring smooth and reliable software delivery and code deployments.. Expertise in data modelling, data integration, metadata management, and data governance to establish robust and scalable data architectures. Strong experience of 4+ years with Cloud Technologies, such as Azure and AWS. Certifications are a plus.","[{""min"": 25200, ""max"": 29400, ""type"": ""Net per month - B2B""}]",Data Engineering,Data Engineering
