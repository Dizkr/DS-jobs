Type of work,Experience,Employment Type,Operating mode,Job ID,Job Title,Employer Name,Job Description,Salaries,Category,min,max,type
Full-time,Mid,B2B,Hybrid,0,System Engineer,Radpoint,"Poszukujemy In≈ºyniera ds. Wdro≈ºe≈Ñ, kt√≥ry bƒôdzie odpowiedzialny za wdra≈ºanie z≈Ço≈ºonych i zintegrowanych system√≥w informatycznych w przedsiƒôbiorstwach i instytucjach. Oferujemy pracƒô w dynamicznym ≈õrodowisku, w kt√≥rym Twoje umiejƒôtno≈õci i do≈õwiadczenie bƒôdƒÖ doceniane. Je≈õli spe≈Çniasz poni≈ºsze wymagania zachƒôcamy do aplikowania! Wymagania: Co najmniej rok pracy jako In≈ºynier odpowiedzialny za wdro≈ºenia z≈Ço≈ºonych, zintegrowanych system√≥w informatycznych w przedsiƒôbiorstwach lub instytucjach. Umiejƒôtno≈õƒá prowadzenia szkole≈Ñ aplikacyjnych. Praktyczne do≈õwiadczenie w integracji system√≥w informatycznych w tym analiza i tworzenie specyfikacji technicznych Znajomo≈õƒá Excela, umiejƒôtno≈õƒá analizy danych oraz podstawowa znajomo≈õƒá jƒôzyka SQL Jako dodatkowy atut: Znajomo≈õƒá obszaru ochrony zdrowia oraz do≈õwiadczenie w radiologii i diagnostyce obrazowej Znajomo≈õƒá i do≈õwiadczenie w pracy z MIRTH, HL7, DICOM oraz standard√≥w IHE Praktyczna znajomo≈õƒá podstawowych zagadnie≈Ñ zwiƒÖzanych z wdro≈ºeniami w sektorze publicznym- protoko≈Çy i planowanie Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z rozliczeniami NFZ Do≈õwiadczenie w zarzƒÖdzaniu danymi Zadania: Przeprowadzanie szkole≈Ñ aplikacyjnych. Wykonywanie integracji i test√≥w integracyjnych z udzia≈Çem wielu zewnƒôtrznych dostawc√≥w system√≥w IT. Planowanie i prowadzenie test√≥w integracyjnych. Dokonywanie wdro≈ºe≈Ñ w przedsiƒôbiorstwach i instytucjach na terenie Polski. Oferujemy: Stabilne zatrudnienie w nowoczesnym ≈õrodowisku pracy Mo≈ºliwo≈õƒá rozwoju zawodowego i zdobycia do≈õwiadczenia w projektach r√≥≈ºnego rodzaju core hours 9: 00-15: 00 Mo≈ºliwo≈õƒá zdalnego ≈õwiadczenia us≈Çug, jednak chƒôtnie zobaczymy Ciƒô w biurze üòâ Wynagrodzenie adekwatne do umiejƒôtno≈õci i do≈õwiadczenia Jak rekrutujemy? Pierwszy etap to techniczna rozmowa online trwajƒÖca oko≈Ço 1-1,5h Etap drugi to spotkanie w biurze/online sk≈ÇadajƒÖce siƒô ze swobodnej rozmowy i czƒô≈õci negocjacyjnej, gdy ju≈º wiemy, ≈ºe chcemy wsp√≥≈Çpracowaƒá","[{""min"": 6800, ""max"": 9000, ""type"": ""Net per month - B2B""}]",Data Engineering,6800,9000,Net per month - B2B
Full-time,Senior,B2B,Remote,1,Senior Database Administrator / Lead,Experis Manpower Group,"We‚Äôre seeking a highly skilled and experienced Senior Database Administrator (DBA) to join our dynamic IT team. This position requires a deep understanding of database technologies, strong problem-solving skills, and the ability to lead a team effectively. Responsibilities: Database Management: ‚Ä¢ Design, implement, and maintain database systems to ensure high availability, performance, and security ‚Ä¢ Monitor database performance and troubleshoot issues to optimise system efficiency ‚Ä¢ Perform regular database backups, recovery, and disaster recovery planning Leadership and Team Management: ‚Ä¢ Lead and mentor a team of database administrators, providing guidance and support in their professional development ‚Ä¢ Collaborate with cross-functional teams to define database requirements and ensure alignment with organisational goals ‚Ä¢ Foster a culture of continuous improvement and innovation within the database team Strategic Planning: ‚Ä¢ Develop and implement database strategies that align with overall IT strategy ‚Ä¢ Stay current with emerging database technologies and trends, making recommendations for upgrades and enhancements Documentation and Compliance: ‚Ä¢ Maintain comprehensive documentation of database configurations, procedures, and policies ‚Ä¢ Ensure compliance with data governance and security policies User Support: ‚Ä¢ Provide technical support and training to end-users and other IT staff as needed ‚Ä¢ Collaborate with application developers to optimize database performance for applications Requirements: ‚Ä¢ Bachelor's degree in Computer Science, Information Technology, or a related field ‚Ä¢ Minimum of 5 years of experience as a Database Administrator, with a focus on leadership roles ‚Ä¢ Strong knowledge of database management systems (e.g., Oracle, SQL Server, MySQL) ‚Ä¢ Experience with database design, performance tuning, and optimisation ‚Ä¢ Proven leadership and team management skills ‚Ä¢ Excellent problem-solving and analytical abilities ‚Ä¢ Strong communication and interpersonal skills ‚Ä¢ Experience with cloud-based database solutions (e.g., AWS, Azure) ‚Ä¢ Knowledge of data warehousing and business intelligence concepts ‚Ä¢ Familiarity with database security best practices Our offer: ‚Ä¢ B2B via Experis ‚Ä¢ 100% remote work ‚Ä¢ MultiSport Plus ‚Ä¢ Group insurance ‚Ä¢ Medicover Premium ‚Ä¢ e-learning platform","[{""min"": 144, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Database Administration,144,160,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,7,Programista PL/SQL ze znajomo≈õciƒÖ integracji,4IT Solutions,"Poszukujemy Programisty PL/SQL ze znajomo≈õciƒÖ integracji dla naszego klienta ‚Äì globalnego lidera w dziedzinie baz danych, rozwiƒÖza≈Ñ chmurowych i oprogramowania biznesowego. To wyjƒÖtkowa okazja do pracy dla jednej z wiodƒÖcych firm tworzƒÖcych kompleksowe oprogramowanie bazodanowe oraz nowoczesne rozwiƒÖzania chmurowe oparte na sztucznej inteligencji. Dziƒôki ciƒÖg≈Çemu rozwojowi technologii oraz inwestycjom w nowoczesne rozwiƒÖzania, firma pozostaje kluczowym graczem w sektorze IT, dostarczajƒÖc innowacyjne produkty dla firm na ca≈Çym ≈õwiecie. Dynamiczny rozw√≥j oraz globalna spo≈Çeczno≈õƒá ekspert√≥w IT sprawiajƒÖ, ≈ºe jest to idealne miejsce dla ambitnych specjalist√≥w. Na poczƒÖtek konkrety: Mo≈ºliwa forma wsp√≥≈Çpracy: B2B Stawka: 130-150 PLN/h Tryb pracy: hybryda Warszawa (1-2x w tygodniu w biurze) Wymiar pracy: 1FTE O projekcie: Projekt dotyczy rozwoju kodu w PL/SQL oraz integracji system√≥w w architekturze SOA dla klienta z sektora publicznego. Zakres obowiƒÖzk√≥w: Modelowanie danych oraz tworzenie struktur danych na potrzeby integracji system√≥w w architekturze SOA Rozw√≥j i optymalizacja kodu PL/SQL, w tym implementacja zapyta≈Ñ, procedur sk≈Çadowych i interfejs√≥w us≈Çug z naciskiem na wydajno≈õƒá i skalowalno≈õƒá Analiza i optymalizacja zapyta≈Ñ SQL na du≈ºych zbiorach danych Praca zgodnie z wymaganiami SLA (Service Level Agreement) Wymagania: Minimum 5-letnie do≈õwiadczenie na podobnym stanowisku Bardzo dobra znajomo≈õƒá SQL i PL/SQL Do≈õwiadczenie w pracy z du≈ºymi zbiorami danych Umiejƒôtno≈õƒá optymalizacji zapyta≈Ñ w bazie danych Oracle Kompetencje w projektowaniu oraz implementacji interfejs√≥w us≈Çug w architekturze SOA Praktyczne do≈õwiadczenie w wykorzystywaniu SOAP, REST, XQuery, XSLT w projektach integracyjnych Mile widziane: Certyfikaty Oracle PL/SQL Znajomo≈õƒá Oracle Service Bus, Oracle SOA Suite, Oracle Weblogic ServerZa Oferujemy: Atrakcyjna lokalizacja biura Brak dress code‚Äôu Mo≈ºliwo≈õƒá pracy zdalnej Przyjazna niekorporacyjna atmosfera Spotkania integracyjne Opis procesu rekrutacji: Wstƒôpna rozmowa telefoniczna z naszym przedstawicielem - 4IT Solutions (ok 20min) Zdalna rozmowa techniczna z osobami z zespo≈Çu naszego Klienta Spotkanie zapoznawcze z Managerem w biurze ... i finalizujemy rozmowy","[{""min"": 130, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,130,150,Net per hour - B2B
Full-time,Mid,B2B,Remote,8,Data Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients‚Äô systems, departments and sites. We provide an open technology platform that‚Äôs shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience.Want to be part of it? Job Title: Data Platform Engineer Job Summary: We are seeking a highly skilled and experienced Data Engineer to join our team. The successful candidate will be responsible for developing and maintaining our Data Lake and existing Data Pipelines, as well as continually exploring, analyzing, and proposing improvements to existing processes and tooling. They will also be responsible for ensuring best practices are being adopted and staying up to date with the latest research and trends in Data Engineering. Skills Required: Strong background in Data Engineering, with experience in developing and maintaining Data Pipelines and Data Lakes Proven record of accomplishment of staying up to date with the latest research and best practices in Data Engineering Excellent technical skills in Data Engineering tools and technologies Advanced proficiency in Python and SQL Understanding of AWS cloud technologies, including infrastructure as code (CDK preferred) Effective communication and interpersonal skills, with the ability to work effectively with stakeholders at all levels Strong understanding of information security and data protection principles Experience in driving technical and career development, creating appropriate goals and seeking learning opportunities within the company and the wider software community Good understanding and prior experience of the Agile process (Scrum or Kanban) Fluency with software design patterns Experience working with automotive retail technology would be a distinct advantage Key Responsibilities: Maintain and develop the Data Lake and existing Data Pipelines to support the product and data teams‚Äô requirements Continuously explore, analyze, and propose improvements to existing processes and tooling Stay up to date with the latest research, trends and best practices in Data Engineering Support the Business Intelligence team and wider Company in querying centralized data stores, including the Data Lake Work within department to maintain an ongoing understanding of the company‚Äôs data strategy and roadmap Proactively report on issues and problems Work independently, manage day-to-day workload and priorities, and take accountability for direction and output Drive your own technical and career development, create appropriate goals, and seek learning opportunities within the company and the wider software community Support colleagues on calls or in meetings with clients, partners, and suppliers as required Maintain systems under the team‚Äôs control, including user and access management Support colleagues and HR with onboarding as well as offboarding processes Ensure information security, data protection and support the business in complying with any legal obligations imposed upon it through positive actions Technologies: Python SQL: Trino, Spark-SQL, Hive, TSQL AWS Cloud services (including: s3, step functions, glue, CDK) Terraform Linux Windows Why join us? We‚Äôre on a journey to become market leaders in our space ‚Äì and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We‚Äôre committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles ‚Äì not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn‚Äôt require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply.","[{""min"": 18000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Engineering,18000,23000,Net per month - B2B
Full-time,Mid,Permanent or B2B,Hybrid,9,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for aData Engineerto join us to provide value to our customers in line with the Volue mission. In Volue Insight we enable our customers to make data driven decisions ‚Äì spanning from creating pricing models for their energy products, when to buy energy, to invest in renewable power plants or power-consuming industry. In the fuels team, we provide actuals and forecasts for prices, production levels, flows for the gas markets and conventional power plant operators. We build and maintain data pipelines, collect and process data from external sources, craft mathematical models, analyze time series, train machine learning models, build web applications, and enjoy working together. What you will be doing to make a difference: Design, build and maintain flexible and scalable end-to-end data pipelines for forecasting and prediction models. Contribute to our continuous push towards highly scalable and automated data pipelines and forecasting models where performance is monitored, quality is continuously evaluated, and experimentation is easy. Take part in the entire lifecycle of our models, from initial concept to deployment and ongoing maintenance, ensuring reliability and performance. Implement automated tests, participate in peer code reviews, and embrace continuous integration practices to ensure robust, maintainable code. What you need to succeed: A Bachelor‚Äôs or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Reasons to join Volue team and what we offer: Large degree of influence in shaping and developing the role further Great colleagues in one of Europe‚Äôs most exciting green tech companies with innovative and international work environment Flexible working hours and competitive compensation package, which includes a Multisport card, group life insurance, private healthcare, English classes, memorable offsite events, outstanding referral programme and access to various sports groups. In Volue, we cherish each employee‚Äôs competence, ideas and personality. Let your skills and talent be a part of our team ‚Äì and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14000,22000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,10,Senior Data Engineer,Winged IT,"Do you want to play a key role in revolutionizing the future of finance? Join our Client - a global leader in deferred payments, with over 85 million active users and 2.5 million transactions processed daily! As a pioneer in modern payment solutions, our Client is developing innovative methods that streamline the shopping experience, enhance transaction security, and expand the availability of purchasing options. We're seeking individuals eager to achieve remarkable results and share their bold vision to redefine the future of payments and fintech. Send us your CV - we can‚Äôt wait to meet you! Your role is: -> To design and implement robust, scalable data pipelines to collect, integrate, and analyze large volumes of data from various sources including Jira, AWS S3, and external websites via APIs; -> To develop and maintain databases and data tables that are optimized for performance and scalability within Klarna‚Äôs cloud environment, leveraging AWS services; -> To collaborate with different teams to understand data needs and deliver solutions that support business objectives; -> To ensure data integrity and compliance with data governance and security policies; -> To provide technical leadership and mentorship to data analysts in the team; Stay current with industry trends and evaluate new technologies for continuous improvements in data architecture and processing; -> To work closely with other teams to leverage Klarna‚Äôs internal systems for optimized data management and operations; -> To assist DevOps or Full Stack engineers, when necessary, to achieve team objectives. Your skills and experiences: -> 5+ years of experience in data engineering, particularly in designing and developing data pipelines; -> Strong programming skills in Python and experience with API integrations; -> Extensive experience with AWS Cloud Services (e.g., S3, EC2, RDS, Lambda, Glue Jobs, EMR) and understanding of best practices in cloud security; -> Experience with Terraform and infrastructure as a code; -> Proficiency in SQL and experience with relational and NoSQL databases; -> Experience with graph and vector databases; -> Capability to handle multiple high-priority tasks simultaneously and meet tight deadlines without sacrificing detail or accuracy; -> Excellent interpersonal skills to engage with colleagues across various teams, contributing to fast-paced projects; -> Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Engineering, or a related field; -> Language proficiency: Advanced English (minimum B2 level). Nice to have: + DevOps experience and skills will be viewed as a huge plus; + FullStack or Development experience and skill will be viewed as a huge plus; + Familiarity with Klarna‚Äôs internal systems, such as C2C and Data Platform services, is highly preferred. Our client offers: + Great opportunity for personal development in a stable and friendly large multinational company; + Start-up mentality, small agile teams; + Global Reach: Impact millions with seamless shopping and payments; + Career growth and additional education. Ôªø","[{""min"": 170, ""max"": 195, ""type"": ""Net per day - B2B""}]",Data Engineering,170,195,Net per day - B2B
Full-time,Mid,B2B,Remote,12,Data Engineer,Britenet,"Naszym klientem jest znana, globalna firma z bran≈ºy retail. Zostali≈õmy zaanga≈ºowani przez klienta jako zesp√≥≈Ç Ekspert√≥w ds. danych do kluczowego projektu wdro≈ºeniowego. Oczekiwania: Minimum 3 lata do≈õwiadczenia w IT (preferowane w zespole Data/Software) Bardzo dobra znajomo≈õƒá Pythona, wykorzystywanego do tworzenia aplikacji opartych na mikroserwisach i us≈Çugach REST API Do≈õwiadczenie w projektowaniu i budowaniu aplikacji w architekturze mikroserwisowej Praktyczne do≈õwiadczenie z Google Cloud Platform (GCP) Znajomo≈õƒá narzƒôdzi do orkiestracji i automatyzacji proces√≥w danych (np. Apache Airflow) Dobra znajomo≈õƒá SQL oraz relacyjnych baz danych (np. MS SQL, PostgreSQL) Do≈õwiadczenie w pracy z Google BigQuery Znajomo≈õƒá i umiejƒôtno≈õƒá pracy z Dockerem Do≈õwiadczenie w analizie i integracji danych z r√≥≈ºnych ≈∫r√≥de≈Ç, w tym niestandardowych (API, pliki, systemy zewnƒôtrzne) Umiejƒôtno≈õƒá samodzielnego rozwijania i utrzymywania aplikacji o istotnym znaczeniu biznesowym Znajomo≈õƒá jƒôzyka angielskiego na poziomie komunikatywnym (minimum B2) Mile widziane: Umiejƒôtno≈õƒá uruchamiania i konfigurowania infrastruktury chmurowej oraz CI/CD (np. z u≈ºyciem Azure Pipelines) Znajomo≈õƒá Microsoft Azure Zadania: Tworzenie i rozw√≥j aplikacji po≈õredniczƒÖcych (middleware) miƒôdzy systemami e-commerce (np. Ocado Smart Platform) a wewnƒôtrznymi systemami firmy (np. kasy, fakturowanie, logistyka) Budowa i rozw√≥j mikroserwis√≥w backendowych w Pythonie Integracja danych z system√≥w magazynowych i e-commerce do ekosystemu firmy Projektowanie i wdra≈ºanie przep≈Çyw√≥w danych w chmurze (BigQuery, GCP, Azure) Rozw√≥j ekosystemu danych ‚Äì wsparcie dla Data Science, analityk√≥w i raportowania Utrzymanie i rozw√≥j istniejƒÖcych aplikacji oraz wdra≈ºanie nowych funkcjonalno≈õci Wsp√≥≈Çpraca z analitykami, kierownikami projekt√≥w i zespo≈Çem Data (4‚Äì5 os√≥b), w tym przejmowanie wiedzy domenowej Praca z dokumentacjƒÖ technicznƒÖ i architekturƒÖ danych","[{""min"": 100, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,100,150,Net per hour - B2B
Full-time,Senior,B2B,Remote,13,Remote Data Engineer/ Analytics Engineer,Ework Group,"Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking forSenior Analytics Engineerüîπ ‚úîÔ∏èThe Position Join our Global Commercial IT team as aSenior Analytics Engineerand take part in transforming our Common Data Platform (CDP), which integrates and delivers business-critical data across 43 European and Canadian affiliates. You‚Äôll be joining a product team responsible for building and maintaining our AWS-based Data Lake, DBT Cloud transformation pipelines, and Snowflake data warehouse, enabling high-quality commercial and engagement insights across channels. This role goes beyond coding ‚Äì we‚Äôre looking for someone with strong technical expertise, a proactive mindset, and the ability to build foundations where there are none, improve data quality and reliability, and collaborate closely with business-facing product owner. ‚úîÔ∏è Key Responsibilities: Design, build, and maintain robust ETL/ELT pipelines in DBT Cloud, ingesting data from sources like OCE IQVIA, Adobe Analytics, Qualtrics, Salesforce CIAM, and others Contribute to the migration of legacy Snowflake and Qlik environments into the unified EUCAN CDP instance Implement data quality validation frameworks (e.g., DBT tests, freshness checks, anomaly detection) Support and improve our RUN Management process: monitoring, reruns, root-cause analysis and preventive actions Maintain and document model ownership, schema logic and transformations within our CDW_CORE > CDW_STAGE > CDW_PROD architecture Collaborate closely with Product Owner, Data Architect, and affiliate stakeholders to ensure delivery of trusted, scalable and governed data products Mentor junior engineers and act as a point of escalation for complex data issues ‚úîÔ∏è Tech Stack & Environment: DBT Cloud (core ELT engine) Snowflake (Data Warehouse) AWS S3 (Data Lake, ingestion zone) GitHub, Azure DevOps Data sources: IQVIA OCE, Salesforce Marketing Cloud, Adobe AEM/Analytics, Shopify, Qualtrics, Accutics, Facebook/LinkedIn/Instagram ‚úîÔ∏è Must-Have Qualifications: 5+ years of experience as a Data Engineer working with cloud-based ELT pipelines Expert-level SQL and DBT modeling skills Deep understanding of data warehousing concepts (dimensional modeling, incremental loading, performance optimization) Experience with Snowflake and cloud data architecture Strong communication and documentation skills ‚Äì must work well in distributed teams Proactive attitude: ownership over data quality, incident handling and delivery discipline ‚úîÔ∏è Nice-to-Haves: Experience in the pharmaceutical or commercial and Sales data domains (CRM, HCP, field force) Exposure to GDPR and data privacy requirements in commercial data ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 170, ""max"": 196, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,170,196,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,15,Data Engineer,Hays Poland,"Data Engineer - SQL Developer Refactor and re-architect solutions to get rid of legacy Handle incidents and support daily operations Help design and build data pipelines and integrations Work on data integration, transformation, and storage Collaborate with business users to improve existing systems Suggest process improvements (aligned with GxP standards) Document your work clearly and consistently Support cloud-native data services and APIs Join project planning and execution for new and existing apps Provide on-call support for key data services when needed Skills in SQL, T-SQL, C#, SSIS, and VBA Understanding of ETL/ELT, data modeling, and API integration Familiarity with DevOps, version control, and CI/CD pipelines Awareness of data governance and compliance (GxP, GDPR) A sharp eye for detail and a problem-solving mindset Great communication skills and a team-first attitude Career in an Organization with Scandinavian Culture and Values: Experience a work environment that emphasizes equality, work-life balance, and sustainability. Work in a hybrid model (3 days in the office per week). Potential for a permanent contract after an initial 3-month period. Comprehensive medical coverage through Medicover. Sports Card: Access to various sports facilities and activities. Life insurance coverage for added security. Opportunity to be involved in the transition of processes. Attractive and competitive salary.","[{""min"": 11000, ""max"": 17000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,11000,17000,Gross per month - Permanent
Full-time,Mid,B2B,Remote,16,Data Vault Architect with Snowflake,Link Group,"Opis stanowiska: Poszukujemy do≈õwiadczonegoData Vault Architecta, kt√≥ry wesprze naszego klienta w strategicznym przeglƒÖdzie, planowaniu i transformacji istniejƒÖcej platformy danych. Osoba na tym stanowisku bƒôdzie odpowiedzialna nie tylko za ocenƒô obecnej architektury, ale r√≥wnie≈º za opracowanie wizji docelowej, zarzƒÖdzanie interesariuszami oraz nadz√≥r nad realizacjƒÖ technicznƒÖ i zespo≈Çem wdro≈ºeniowym. Projekt ma kluczowe znaczenie dla poprawy efektywno≈õci i skalowalno≈õci system√≥w danych opartych oSnowflake, DBT oraz AWS. PrzeglƒÖd i ocena istniejƒÖcej platformy danych klienta oraz zaproponowanie optymalnej ≈õcie≈ºki rozwoju. Tworzenie strategii i wizji docelowej dla nowoczesnej platformy danych z wykorzystaniem Data Vault 2.0. Koordynacja dostarczania rozwiƒÖza≈Ñ, planowanie dzia≈Ça≈Ñ oraz nadz√≥r nad ich realizacjƒÖ. ZarzƒÖdzanie zmianƒÖ oraz skuteczna wsp√≥≈Çpraca z interesariuszami biznesowymi i technicznymi. Mo≈ºliwo≈õƒá objƒôcia roli mened≈ºerskiej oraz budowania zespo≈Çu technicznego. Wsp√≥≈Çpraca z zespo≈Çami in≈ºynieryjnymi, analitycznymi oraz DevOps. Must-have: Do≈õwiadczenie w projektowaniu i wdra≈ºaniu architektury danych w modeluData Vault 2.0. Bardzo dobra znajomo≈õƒá platformySnowflake. Praktyczne do≈õwiadczenie z narzƒôdziemDBT (Data Build Tool). Znajomo≈õƒá i do≈õwiadczenie z infrastrukturƒÖAWS. Umiejƒôtno≈õƒá zarzƒÖdzania interesariuszami oraz prowadzenia inicjatyw transformacyjnych. Zdolno≈õƒá do definiowania strategii technicznej i architektonicznej.","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Architecture,160,170,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,21,Data Engineer,Vaillant Group Business Services,"What we achieve together We are looking for an experienced Data Engineer to join our innovative team at Vaillant. In this role, you will be instrumental in advancing our data infrastructure, driving analytics excellence, and leveraging cutting-edge technologies to transform data into actionable insights. If you are passionate about making a meaningful impact and eager to collaborate with a team that appreciates your expertise, we encourage you to apply. You will design, build, and maintain robust infrastructure and programs for efficient data extraction, transformation, loading, and serving. You will leverage Azure Cloud services, Spark technologies, and DataOps practices to handle large data volumes from diverse sources such as IoT, CRM, ERP, and PLM. You will play a pivotal role in supporting the design, development, maintenance, and automation of data products on our data platform, contributing significantly to data governance and security compliance. Ensuring data quality and accuracy is your priority, and you will implement validation and cleansing processes to achieve this. You will collaborate with a dynamic team of DevOps engineers, data scientists, and data engineers, and you will engage in advanced analytics of machine data, business process-related data, and customer data, enhancing Vaillant Group‚Äôs digital service portfolio. Your effective communication skills will shine as you work with cross-functional colleagues, sharing innovative ideas and fostering a collaborative environment. Agile methodologies are your preferred approach, allowing you to work seamlessly with the team and stakeholders. What makes us successful together Qualification: You hold a university degree, preferably in Computer Science or a related field, with a focus on Data Science, Data Mining, or Data Analytics. Experience: You bring at least 3 years of relevant experience in data analytics, particularly within a Big Data context. Know-how and skills: Your proficiency in Python (PySpark) and SQL is exceptional, and you are adept at working with cloud services and related components, ideally within the Microsoft Azure ecosystem and Databricks. Nice to have: Experience with additional data platforms and tools will be a plus. Personality: You thrive in a team setting, showcasing a high degree of initiative and motivation to excel in an agile and interdisciplinary environment. Language skills: Fluency in English is essential, enabling effective communication and collaboration across our global teams. What you can count on Hybrid work and environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee. Package of additional benefits: private medical care, multi-sport card. Onboarding: our clearly structured onboarding process, including an Onboarding App, enables us to integrate new employees into Vaillant Group quickly and in a targeted manner. Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings.","[{""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16000,20000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,22,DWH Architect (public/healthcare),Britenet,"Projekt dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Oczekiwania: Minimum 5 lat do≈õwiadczenia zawodowego na stanowisku Architekta IT; Do≈õwiadczenie zawodowe w zakresie projektowania architektur system√≥w zorientowanych na us≈Çugi, system√≥w w architekturze wielowarstwowej, system√≥w o wysokiej wydajno≈õci i niezawodno≈õci; Do≈õwiadczenie projektowe w szacowaniu pracoch≈Çonno≈õci prac programistycznych i architektonicznych; Do≈õwiadczenie projektowe w szacowaniu z≈Ço≈ºono≈õci aplikacji/ rozwiƒÖzania (ilo≈õƒá komponent√≥w, wielko≈õƒá komponent√≥w), skalowanie aplikacji horyzontalne i wertykalne; Do≈õwiadczenie projektowe w zakresie badania i oceny bezpiecze≈Ñstwa informacji w systemach teleinformatycznych; Do≈õwiadczenie w realizacji architektury rozwiƒÖza≈Ñ zawierajƒÖcych elementy hurtowni danych i narzƒôdzi analitycznych; Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z architekturƒÖ, projektowaniem i integracjƒÖ system√≥w IT; Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z architekturƒÖ zorientowanƒÖ na us≈Çugi (SOA) oraz mikroserwisy; Znajomo≈õƒá wzorc√≥w projektowych i architektonicznych, relacyjnych baz danych, serwer√≥w aplikacyjnych oraz integracji system√≥w IT; Znajomo≈õƒá SQL oraz proces√≥w ETL; Do≈õwiadczenie w programowaniu w jƒôzyku Python; Znajomo≈õƒá baz danych PostgreSQL/EDB/MySQL/ MongoDB/Oracle; Znajomo≈õƒá Enterprise Architect; Dobra organizacja pracy w≈Çasnej, orientacja na realizacje cel√≥w; Umiejƒôtno≈õci interpersonalne, w szczeg√≥lno≈õci umiejƒôtno≈õƒá planowania, definiowania, realizacji, oraz monitorowania i rozliczania cel√≥w; Efektywna komunikacja, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista i odporno≈õƒá na stres, proaktywno≈õƒá; Zdolno≈õƒá adaptacji i elastyczno≈õƒá, otwarto≈õƒá na sta≈Çy rozw√≥j i gotowo≈õƒá uczenia siƒô. Mile widziane: Do≈õwiadczenie projektowe w obszarze ochrony zdrowia; Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny); Certyfikat potwierdzajƒÖcy umiejƒôtno≈õci z obszaru projektowania architektury rozwiƒÖza≈Ñ IT (np.. TOGAF¬Æ EA Foundation lub r√≥wnowa≈ºny); Certyfikat potwierdzajƒÖcy wiedzƒô z zakresu administrowania EDB (np. EDB Certification - PostgreSQL Essentials 15 lub r√≥wnowa≈ºny); Certyfikat z obszaru administrowania ≈õrodowiskiem Hadoop (np. Cloudera Certified Administrator for Hadoop (CCAH), Hortonworks Certified Apache Hadoop Administrator (HCAHA) lub r√≥wnowa≈ºny) Kluczowe zadania: Tworzenie koncepcji i projekt√≥w architektury system√≥w DWH zgodnych z wymaganiami biznesowymi i technologicznymi. Wyb√≥r odpowiednich technologii oraz rozwiƒÖza≈Ñ integracyjnych (ETL, bazy danych, narzƒôdzia analityczne). Projektowanie modeli danych (w tym modeli logicznych i fizycznych). Okre≈õlanie podzia≈Çu komponent√≥w systemu, ich zale≈ºno≈õci oraz rozmiaru (szacowanie z≈Ço≈ºono≈õci rozwiƒÖzania). Projektowanie przep≈Çyw√≥w danych oraz proces√≥w ekstrakcji, transformacji i ≈Çadowania. Zapewnienie wydajno≈õci i niezawodno≈õci proces√≥w przetwarzania danych. Okre≈õlanie zakresu i koszt√≥w prac architektonicznych i programistycznych. Ocena skali rozwiƒÖzania oraz rekomendowanie sposob√≥w skalowania (wertykalne/horyzontalne). Identyfikowanie ryzyk zwiƒÖzanych z bezpiecze≈Ñstwem informacji i rekomendowanie ≈õrodk√≥w zaradczych. Wsp√≥≈Çpraca z zespo≈Çami ds. bezpiecze≈Ñstwa w celu wdra≈ºania mechanizm√≥w zabezpieczajƒÖcych. Projektowanie integracji z innymi systemami IT (systemy ≈∫r√≥d≈Çowe, API, hurtownie danych). Wsp√≥≈Çpraca z zespo≈Çami integracyjnymi przy wdra≈ºaniu rozwiƒÖza≈Ñ. Konsultowanie rozwiƒÖza≈Ñ z analitykami, programistami, testerami i interesariuszami biznesowymi. Udzia≈Ç w planowaniu i przeglƒÖdach technicznych. Tworzenie i aktualizacja dokumentacji architektonicznej w narzƒôdziach takich jak Enterprise Architect. Utrzymywanie zgodno≈õci z wewnƒôtrznymi i zewnƒôtrznymi standardami architektonicznymi. Ocena efektywno≈õci wdro≈ºonych rozwiƒÖza≈Ñ i rekomendowanie zmian. ≈öledzenie trend√≥w technologicznych i proponowanie innowacji. Tworzenie lub przeglƒÖd skrypt√≥w i komponent√≥w automatyzujƒÖcych przetwarzanie danych. Zapewnienie jako≈õci kodu i jego zgodno≈õci z architekturƒÖ systemu.","[{""min"": 25000, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Architecture,25000,32000,Net per month - B2B
Full-time,Senior,B2B,Remote,23,Lead Business Intelligence (Power BI) Engineer,CLOUDFIDE,"You are Lead Power BI Developer who excels in transforming complex data into clear, actionable insights. As a motivated and detail-oriented professional, you will lead a team of developers, drive business growth, and maintain strong communication with our customers. Your creativity and curiosity will help fuel smart decisions and innovative solutions. Opportunity overview This global project is centered around empowering a vast number of business analysts to navigate large-scale data hubs effectively. As the Lead Power BI Developer, you will not only work on translating business logic into DAX, optimizing user-written measures, and training users for enhanced self-sufficiency but also manage and mentor a team of developers. Additionally, you will play a crucial role in business development by identifying new project opportunities and building strong relationships with clients. In this role, you will enhance your problem-solving skills, gain substantial experience in data modeling, and drive the strategic direction of our BI initiatives. Your leadership will be key in ensuring the delivery of high-quality solutions while fostering a culture of innovation and continuous improvement within the team. Your impact zone Translating business requirements into technical solutions. Working closely with stakeholders and business users. Design and implement modern cloud-based solutions. Build and launch new data models. Optimize existing data models and reports, elevating the efficiency and impact of our data intelligence. Implement best practices in data engineering to maintain data integrity, quality, and documentation ‚Äì your work will increase data discoverability and understanding. Take part in sharing your knowledge and engage in training activities to promote a learning culture Key responsibilities 5+ years of experience in delivering complex BI solutions ‚Äì it‚Äôs your time to excel! Proven track record of leading and managing BI development teams. Strong business acumen and ability to develop and present compelling business cases. Excellent communication and interpersonal skills, with the ability to build and maintain relationships with customers and stakeholders. Hands-on experience (3+ years) with Microsoft BI stack (Power BI, SSAS/AAS). Mastery of DAX, M, SQL, Microsoft SQL Server, and PostgreSQL and query performance tuning ‚Äì you make data dance! Proficiency in Microsoft Fabric and data integration techniques. Practical know-how in implementing row-level security (RLS) and Data Lake/Warehouse architectures. Python coding experience ‚Äì you‚Äôre our code whisperer! Familiarity with public cloud architecture, security, networking concepts (MS Azure preferred) ‚Äì we like our clouds secure and efficient. Strong conceptual and analytical skills ‚Äì you‚Äôre a whiz at defining and documenting complex requirements. Fluent English communication ‚Äì you can articulate tech in plain language. Qualifications & tech toolbox 5+ years of experience in delivering complex BI solutions ‚Äì it‚Äôs your time to excel! Proven track record of leading and managing BI development teams. Strong business acumen and ability to develop and present compelling business cases. Excellent communication and interpersonal skills, with the ability to build and maintain relationships with customers and stakeholders. Hands-on experience (3+ years) with Microsoft BI stack (Power BI, SSAS/AAS). Mastery of DAX, M, SQL, Microsoft SQL Server, and PostgreSQL and query performance tuning ‚Äì you make data dance! Proficiency in Microsoft Fabric and data integration techniques. Practical know-how in implementing row-level security (RLS) and Data Lake/Warehouse architectures. Python coding experience ‚Äì you‚Äôre our code whisperer! Familiarity with public cloud architecture, security, networking concepts (MS Azure preferred) ‚Äì we like our clouds secure and efficient. Strong conceptual and analytical skills ‚Äì you‚Äôre a whiz at defining and documenting complex requirements. Fluent English communication ‚Äì you can articulate tech in plain language. Extra stardust for Experience with Databricks, Azure Synapse, Azure Data Factory, and Azure DevOps ‚Äì these are your secret weapons! Here's why you'll love Cloudfide BENEFITS: Regardless of the form of employment - Budget for your professional development - training and certification. MyBenefit cafeteria (with Multisport). Medicover medical care. Team-building meetings and trips. FLEXIBILITY: Enjoy the freedom of working from anywhere, and have a genuine say on our tools, tech, and solutions. STABILITY: Stable and long-term employment (employment contract, B2B). START-UP CULTURE: Open communication, creative problem solving and a flat hierarchy. GROWTH: Skyrocket your career by exploring new territories ‚Äì you can work on various projects related to Big Data and Cloud. COLLABORATION: Be part of our diverse, passionate team, where every voice matters. Work in a company full of well-coordinated people who do their work with passion and commitment. Equal opportunities CLOUDFIDE is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.","[{""min"": 20000, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20000,30240,Net per month - B2B
Full-time,Senior,B2B,Hybrid,24,Senior Big Data Engineer,Grid Dynamics Poland,"We are seeking a highly skilled Big Data Engineer with a strong background in data engineering, real-time streaming, and large-scale distributed data processing. The engineer will be a part of the team responsible for building critical risk and fraud detection applications. The ideal candidate will have expertise in big data technologies, real-time data pipelines, and experience developing solutions that enhance fraud prevention and risk mitigation in the payments domain. Responsibilities: Design, develop, and implement scalable batch and real-time data processing systems for fraud detection and risk mitigation. Build and optimize real-time streaming data pipelines using Kafka, Spark Streaming, ELK, and related technologies. Develop ETL/ELT pipelines, including data quality checks, anomaly detection, and notification systems. Work with Hadoop, MapReduce, Hive, Spark, NoSQL, and relational databases (e.g., MySQL) to create data-driven solutions. Architect high-availability, low-latency, and strongly consistent distributed data processing systems. Collaborate with cross-functional teams to create fraud prevention and risk detection solutions that add business value. Follow Agile development methodologies, incorporating CI/CD best practices for continuous integration and delivery. Present complex data-driven insights in a clear and concise manner. Requirements: Proven experience in big data engineering and data-driven business solutions. Strong background in Hadoop, Spark, MapReduce, Hive, and NoSQL databases. Hands-on experience in real-time data streaming using Kafka, Spark Streaming, and ELK. Expertise in ETL/ELT pipeline development, data quality checks, and anomaly detection. Experience building distributed data processing systems with high availability and low latency. Proficiency in Java or Python for data engineering and application development. Familiarity with Agile methodologies, CI/CD, and best development practices. Strong communication skills and ability to present complex ideas clearly. We offer: Opportunity to work on bleeding-edge projects Work with a highly motivated and dedicated team Competitive salary Flexible schedule Benefits package - medical insurance, sports Corporate social events Professional development opportunities Well-equipped office About us: Grid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India. Collapse","[{""min"": 22000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,30000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,25,Big Data Engineer,Grid Dynamics Poland,Description: ,"[{""min"": 17000, ""max"": 18300, ""type"": ""Net per month - B2B""}]",Data Engineering,17000,18300,Net per month - B2B
Full-time,Senior,B2B,Remote,27,Senior Data Engineer (Python i Django),Lumicode Sp. z o.o. (Pentacomp Group),"Lumicode Sp. z o.o. jest czƒô≈õciƒÖ Grupy Pentacomp, kt√≥ra jest producentem rozwiƒÖza≈Ñ informatycznych i dostawcƒÖ profesjonalnych us≈Çug IT dla du≈ºych przedsiƒôbiorstw i sektora publicznego. Jako Pentacomp tworzymy rozwiƒÖzania informatyczne, kt√≥re ≈ÇƒÖczƒÖ innowacyjno≈õƒá z wieloletnim do≈õwiadczeniem - a mamy go ca≈Çkiem sporo. Dzia≈Çamy na rynku od prawie 30 lat i mo≈ºemy pochwaliƒá siƒô wieloma zrealizowanymi projektami. Aktualnie poszukujemy Data Engineera ze znajomo≈õciƒÖ Pythona i Django do klienta z bran≈ºy ubezpieczeniowej. Oferujemy: Pracƒô w 100% zdalnƒÖ Pracƒô w pe≈Çnym wymiarze godzin; Forma wsp√≥≈Çpracy: B2B Stawka do 170 pln/h netto + VAT B2B w zale≈ºno≈õci od do≈õwiadczenia; Mo≈ºliwo≈õƒá korzystania z prywatnej opieki medycznej i karty sportowej; Wymagania na stanowisko: Min. 5 lat do≈õwiadczenia jako Data Engineer/Python Developer Znajomo≈õƒá Python & Django Bardzo dobra znajomo≈õƒá SQL Znajomo≈õƒá Snowflake'a Mile widziana znajomo≈õƒá Flyway Mile widziana znajomo≈õƒá RDS SQL Database Mile widziana znajomo≈õƒá AWS CDK Mile widziana znajomo≈õƒá TypeScript, React oraz Dash Jƒôzyk angielski na poziomie min. B2 Proces rekrutacyjny: Rozmowa z rekruterem z Lumicode Rozmowa wstƒôpna Rozmowa techniczna Decyzja, oferta oraz doprecyzowanie warunk√≥w kontraktu Zapraszamy do aplikowania!","[{""min"": 140, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,140,170,Net per hour - B2B
Full-time,Mid,Mandate,Hybrid,30,Celonis Process Mining Specialist (Maternity Cover),Vaillant Group Business Services,"What we achieve together In this role you lead and drive the Digital Twin implementation process while delivering exceptionally high levels of service to ensure the optimal solution for internal customers with the Celonis technology. You design, optimize and monitor data models and data connections (ETL) to build the best possible and real-time capable data architecture. In addition, you support the implementation of process mining solutions across enterprise end-to-end processes (Design-to-Operate, Procure-to-Pay, Order-to-Cash). You translate business requirements into technical requirements, assess the feasibility, plan the technical implementation and ensure the overall quality of implementation. You also design and implement innovative analyses and execution apps and enrich them with Machine Learning algorithms (Python) or Task Mining to make the customer's processes transparent and automated. Within your responsibilities is also the analysis of the data to identify process inefficiencies, while monitoring compliance and data security measures. What makes us successful together Experience : You have at least 3 years of commercial experience in IT-Consulting, Management Consulting, Process Improvement/Excellence or a similar area. You also bring a solid track record in successfully developing and shipping data driven solutions in Celonis. Know-how and skills: You have proficiency in SQL, other programming languages (Python, R, Matlab) as a plus and a strong interest in Big Data, Data Analytics, Data Mining, Process Mining and Digital Twin. Personality : With your positive attitude and trustworthy personality, you can build strong stakeholder relationships. You understand and interpret business processes and communicate proactively and clearly. In addition, you have Excellent analytical skills, well organized and known for being a quick learner. Language skills : You speak English fluently; German language skills would be a plus. What makes us special Environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee. Package of additional benefits: private medical care, multi-sport card. A fast growing, agile and very dynamic team that challenges established routines and helps transforming the Vaillant Group to a data informed business. Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings.","[{""min"": 15000, ""max"": 20000, ""type"": ""Gross per month - Mandate""}]",Data Analysis & BI,15000,20000,Gross per month - Mandate
Full-time,Senior,B2B,Remote,31,Data Engineer (Databricks),Remodevs,"Role We are looking for an enthusiastic Senior Data Engineer to join our team. In this role, you will collaborate with skilled professionals dedicated to providing insights into user behavior. Your work will help the organization understand when, where, and how users engage with our digital platforms. As a Senior Data Engineer, you will take charge of developing and improving advanced data-tracking solutions, ensuring seamless platform integration and transforming data into valuable insights that drive key business decisions. We are a global manufacturing company based in Scandinavia, with offices and operations around the world. 4+ years of experience with Python Knowledge of Databricks Experience with Snowplow or other tracking solutions Skills in building and maintaining data pipelines with dbt Proficiency with cloud platforms (preferably Azure) Strong SQL skills for data processing and transformation Fluent English Collaborate with developers and product managers Design, build, and manage data pipelines using dbt to process bronze, silver, and gold data layers in Databricks Work with cloud infrastructure, particularly Azure Use SQL and Python to create reliable data solutions Ensure data quality, security, and compliance throughout its lifecycle","[{""min"": 20000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,20000,28000,Net per month - B2B
Full-time,Senior,B2B,Remote,34,Senior/ Lead Data Science Engineer,N-iX,"We are looking for a Data Scientist with a strong analytical mindset and a passion for solving real-world supply chain problems. You will join a cross-functional team focused on optimizing the flow of units from warehouses to stores. This is a high-impact role where your insights and models will directly influence key business operations. Responsibilities: Prepare and clean datasets to enable reliable experimentation Develop and fine-tune machine learning models and algorithms Test models, analyze outcomes, and generate actionable insights Communicate findings to stakeholders through reports and visualizations Propose data-driven solutions and optimization strategies Requirements: Proficiency in Python and/or R Experience with SQL Data Platforms & Tools Hands-on experience with Azure Databricks or Snowflake Familiarity with building and deploying machine learning pipelines is a strong advantage Java experience is a plus Education - Bachelor's or Master's degree in Computer Science, Mathematics, Engineering, or a related field Mathematics & Statistics: Solid understanding of multivariable calculus and linear algebra Applied knowledge of statistical distributions and hypothesis testing Machine Learning Practical knowledge of ML techniques: kNN, decision forests, regressions, MLE, time series Understanding of model evaluation metrics and performance tuning Data Handling & Visualization Skilled in managing missing or inconsistent data Experience with tools like matplotlib, seaborn, or ggplot2 Nice to Have: Experience with deploying ML models into production environments Previous work in supply chain or logistics domains","[{""min"": 23642, ""max"": 28445, ""type"": ""Net per month - B2B""}]",Data Science,23642,28445,Net per month - B2B
Full-time,Senior,B2B,Hybrid,36,Product Owner ‚Äì Data & Analytics,ITDS,"Product Owner Join us, and lead innovation through data-driven decision making! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As aProduct Owner, you will be working for our client, a global financial institution focused on transforming its data and analytics landscape. You will be leading the development of data-driven products that support critical business decisions across credit, lending, and operational processes. This project aims to optimize investigation outcomes by leveraging advanced analytics and technology solutions, driving measurable value through effective stakeholder collaboration and Agile product development. You will be working in a complex, global environment that requires a strong strategic mindset and hands-on delivery expertise. Your main responsibilities: Defining and evolving the product vision in alignment with business goals Collaborating with stakeholders to prioritize features based on value and business impact Leading Agile ceremonies and maintaining a clear and refined product backlog Supporting the team in decomposing epics into actionable user stories Communicating with cross-functional teams to ensure product alignment and clarity Resolving conflicting stakeholder priorities and negotiating trade-offs Showcasing product iterations and improvements to business stakeholders Promoting and advocating for the product across the broader organization Shaping and validating business outcomes through data analytics and metrics Ensuring consistent delivery of business value through iterative releases You're ideal for this role if you have: Proven experience as aProduct Ownerin alarge,complex organization Strong leadership and communication skills across all seniority levels Expertise in Agile frameworks and tools such asJIRAandConfluence Experience managing product strategy, roadmaps, and value delivery Strong stakeholder management and conflict resolution capabilities Demonstrated ability to deliver business and technology change Understanding of data management and analytics within a corporate context Experience working with global, cross-functional teams Familiarity with credit and lending processes in the financial industry Ability to translate business outcomes into technical requirements It is a strong plus if you have: Hands-on experience withSQL, BigQuery, or other data tools Background in data science, analytics, or business analysis Experience with big data technologies or cloud-based platforms Proficiency inPythonor PySpark for data processing Knowledge of data visualization tools likeTableauorQlikSense","[{""min"": 17850, ""max"": 24150, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,17850,24150,Net per month - B2B
Full-time,Senior,B2B,Remote,38,Senior Data Engineer (Databricks),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition byForbesas one of the top 10 AI consulting companies. As aSenior Data Engineer, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of a universal data platform for global aerospace companies.This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Data Platform Transformation for energy management association body.This project addressed critical data management challenges, boosting user adoption, performance, and data integrity. The team is implementing a comprehensive data catalog, leveraging Databricks and Apache Spark/PySpark, for simplified data access and governance. Secure integration solutions and enhanced data quality monitoring, utilizing Delta Live Table tests, established trust in the platform. The intermediate result is a user-friendly, secure, and data-driven platform, serving as a basis for further development of ML components. Design of the data transformation and following data ops pipelines for global car manufacturer.This project aims to build a data processing system for both real-time streaming and batch data. We‚Äôll handle data for business uses like process monitoring, analysis, and reporting, while also exploring LLMs for chatbots and data analysis. Key tasks include data cleaning, normalization, and optimizing the data model for performance and accuracy. üöÄ Your main responsibilities: Design and optimize scalable data processing pipelines for both streaming and batch workloads using Big Data technologies such as Databricks, Apache Airflow, and Dagster. Architect and implement end-to-end data platforms, ensuring high availability, performance, and reliability. Lead the development of CI/CD and MLOps processes to automate deployments, monitoring, and model lifecycle management. Develop and maintain applications for aggregating, processing, and analyzing data from diverse sources, ensuring efficiency and scalability. Collaborate with Data Science teams on Machine Learning projects, including text/image analysis, feature engineering, and predictive model deployment. Design and manage complex data transformations using Databricks, DBT, and Apache Airflow, ensuring data integrity and consistency. Translate business requirements into scalable and efficient technical solutions while ensuring optimal performance and data quality. Ensure data security, compliance, and governance best practices are followed across all data pipelines. üéØ What you‚Äôll need to succeed in this role: At least 5 years of commercial experienceimplementing, developing, or maintaining Big Data systems. Strong programming skills inPython: writing a clean code, OOP design. StrongSQLskills, including performance tuning, query optimization, and experience withdata warehousing solutions. Experience in designing and implementing data governance and data management processes. Deep expertise in Big Data technologies, includingApache Airflow, Dagster, Databricks, and other modern data orchestration and transformation tools. Experience implementing and deploying solutions in cloud environments (with a preference forAzure). Knowledge of how to build and deployPower BI reports and dashboards for data visualization. Excellent understanding of dimensional data and data modeling techniques. Consulting experience and the ability to guide clients through architectural decisions, technology selection, and best practices. Ability to work independently and take ownership of project deliverables. Familiarity withSpark, Azure Event HuborKafka. Master‚Äôs or Ph.D. in Computer Science, Big Data, Mathematics, Physics, or a related field. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage withtop-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you towork remotelyor from modern offices and coworking spaces. Accelerate your professional growth throughcareer paths,knowledge-sharinginitiatives,languageclasses, and sponsoredtrainingorconferences, including a partnership withDatabricks, which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days offavailable for B2B contractors and individuals under contracts of mandate. Participate inteam-building eventsand utilize theintegration budget. Celebratework anniversaries, birthdays,andmilestones. Accessmedicalandsports packages, eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you canboostyourpersonal brandby speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website (career page) and social media (Facebook,LinkedIn,Instagram).","[{""min"": 21000, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,28560,Net per month - B2B
Full-time,Mid,B2B,Remote,40,Revalize CPQ Product Support Specialist,Link Group,"Codzienne wsparcie u≈ºytkownik√≥w i obs≈Çuga zg≈Çosze≈Ñ w ramach CPQ (BAU) Diagnozowanie i rozwiƒÖzywanie incydent√≥w technicznych Wprowadzanie usprawnie≈Ñ i modyfikacji funkcjonalno≈õci systemu Konfiguracja i utrzymanie proces√≥w ofertowania w CPQ Wsp√≥≈Çpraca z zespo≈Çami technicznymi i biznesowymi po stronie klienta Dokumentowanie zmian i konfiguracji Minimum 2 lata do≈õwiadczenia w pracy z platformami CPQ Praktyczna znajomo≈õƒá przynajmniej jednej z platform: Revalize CPQ Oracle CPQ (BigMachines) Infor CPQ Tacton CPQ Do≈õwiadczenie w obs≈Çudze zg≈Çosze≈Ñ BAU, incydent√≥w oraz konfiguracji i optymalizacji CPQ Umiejƒôtno≈õƒá pracy w ≈õrodowisku miƒôdzynarodowym (projekt prowadzony po angielsku)","[{""min"": 85, ""max"": 100, ""type"": ""Net per hour - B2B""}]",Unclassified,85,100,Net per hour - B2B
Full-time,Senior,B2B,Remote,43,Data Engineer (Snowflake),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: udzia≈Ç w miƒôdzynarodowym projekcie realizowanym dla bran≈ºy logistycznej. Wykorzystywany stos technologiczny w projekcie: Snowflake (Cortex AI), AWS, DataLake, Data Warehouse, ETL, Tableau, PowerBI, Python, Docker, Kubernetes, projektowanie i rozwijanie skalowalnych pipeline'√≥w danych, hurtowni danych oraz data lake'√≥w w oparciu o Snowflake, integracja i automatyzacja proces√≥w przetwarzania danych (ETL/ELT) z r√≥≈ºnych ≈∫r√≥de≈Ç, wykorzystywanie us≈Çug AWS oraz Cortex AI do budowy inteligentnych rozwiƒÖza≈Ñ opartych na chmurze, rozwijanie aplikacji analitycznych i prototyp√≥w AI z u≈ºyciem Pythona i frameworka Streamlit, konteneryzacja aplikacji i ich uruchamianie przy pomocy Docker oraz Kubernetes, wsp√≥≈Çpraca z zespo≈Çami analitycznymi i interesariuszami biznesowymi w celu dostarczania odpowiednich rozwiƒÖza≈Ñ danych, monitorowanie oraz optymalizacja pipeline'√≥w danych i proces√≥w przetwarzania, udzia≈Ç w projektach z obszaru AI i uczenia maszynowego, w tym tworzenie rozwiƒÖza≈Ñ konwersacyjnych (np. chatboty), praca 100% zdalna, stawka do 150 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz min. 5-letnie do≈õwiadczenie na stanowisku Data Engineer, posiadasz do≈õwiadczenie w projektowaniu i utrzymywaniu hurtowni danych oraz pipeline‚Äô√≥w w Snowflake, biegle pos≈Çugujesz siƒô Cortex AI i narzƒôdziami AWS, programujesz swobodnie w Pythonie, znasz framework Streamlit, pracowa≈Çe≈õ/a≈õ z technologiami konteneryzacji, takimi jak Docker i Kubernetes, masz do≈õwiadczenie w tworzeniu i optymalizacji workflow√≥w ETL/ELT, dobrze rozumiesz procesy analizy danych i potrafisz rozwiƒÖzywaƒá problemy w z≈Ço≈ºonym ≈õrodowisku, potrafisz efektywnie wsp√≥≈Çpracowaƒá w miƒôdzynarodowym zespole, pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie p≈Çynnym - min. B2+, mile widziane do≈õwiadczenie w projektach opartych na conversAI oraz machine learning. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 19200, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Engineering,19200,24000,Net per month - B2B
Full-time,Senior,B2B,Remote,44,Senior MS Data Engineer (Azure SQL DB / ETL / PySpark),1dea,"Poszukujemy do≈õwiadczonegoSenior MS Data Engineer‚Äôaz pasjƒÖ do tworzenia innowacyjnych rozwiƒÖza≈Ñ. Idealny kandydat powinien posiadaƒá do≈õwiadczenie zAzure SQL, silne umiejƒôtno≈õci woptymalizacji zapyta≈Ñoraz zdolno≈õƒá proponowanianowych implementacjiw odpowiedzi na zg≈Çaszane problemy. Dodatkowo, kluczowe jestsolidne zrozumienie architektury, powiniene≈õ byƒá w stanie sugerowaƒá zmiany, takie jak wdro≈ºeniewymiany danych niemal w czasie rzeczywistym. Informacje organizacyjne: Bran≈ºa: IT Consulting Wakaty: 2 Lokalizacja: praca w 100% zdalnie Start: ASAP (max 2msc okresu wypowiedzenia) Stawka: do ustalenia w zakresie 155 - 170 PLN netto + VAT / h Warunki zaanga≈ºowania: B2B (outsourcing przez 1dea), full-time, long-term Proces rekrutacyjny (w pe≈Çni zdalny): Kr√≥tka rozmowa telefoniczna z rekruterem 1dea - o projekcie i warunkach zaanga≈ºowania: (~10 minut) Przedstawienie Twojej kandydatury Klientowi Rozmowa techniczno-projektowa z Klientem (wideo) (~1-1.5 h) (Opcjonalnie) rozmowa podsumowujƒÖca proces z Managerem Technicznym ze strony naszego Klienta (~30 minut) Podjƒôcie decyzji o wsp√≥≈Çpracy Projektowanie i wdra≈ºanie zaawansowanych rozwiƒÖza≈Ñ in≈ºynierii danych na platformie Azure Optymalizacja zapyta≈Ñ Proponowanie nowych implementacji w odpowiedzi na zg≈Çaszane problemy Sugerowanie zmian, takich jak wdro≈ºeniewymiany danych niemal w czasie rzeczywistym Optymalizacja i utrzymanie istniejƒÖcych proces√≥w ETL Budowa skalowalnych i wydajnych pipeline‚Äô√≥w danych Wsp√≥≈Çpraca z zespo≈Çami biznesowymi w celu zrozumienia wymaga≈Ñ i prze≈Ço≈ºenia ich na rozwiƒÖzania techniczne Zapewnienie wysokiej jako≈õci, bezpiecze≈Ñstwa i zgodno≈õci danych z obowiƒÖzujƒÖcymi przepisami Udzia≈Ç w projektach zwiƒÖzanych z rozwojem platformy danych Minimum 3 lata do≈õwiadczenia w in≈ºynierii danych, z naciskiem na platformƒô Azure Solidna znajomo≈õƒá Azure SQL Databases, Databricks oraz innych kluczowych us≈Çug Azure Praktyczna umiejƒôtno≈õƒá wykorzystania Power Query, Python i PySpark do transformacji i manipulacji danymi Umiejƒôtno≈õƒá modelowania danych, projektowania i optymalizacji proces√≥w ETL Silne umiejƒôtno≈õci analityczne i zdolno≈õƒá do rozwiƒÖzywania problem√≥w Umiejƒôtno≈õƒá pracy w metodyce Agile Znajomo≈õƒá jƒôzyka angielskiego pozwalajƒÖca na swobodnƒÖ komunikacjƒô w mowie i pi≈õmie (B2+) Mile widziane: Certyfikaty Microsoft Data Engineering (DP 600, DP 203) Do≈õwiadczenie w pracy z narzƒôdziami BI, takimi jak Power BI Znajomo≈õƒá praktyk DevOps D≈Çugotrwa≈Çy kontrakt B2B (od razu podpisujemy umowƒô na czas nieokre≈õlony / bezterminowo - Klient nastawia siƒô tylko i wy≈ÇƒÖcznie na d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô) Zosta≈Ñ czƒô≈õciƒÖ firmy o silnej pozycji na rynku Nowoczesny sprzƒôt i oprogramowanie (zapewnia nasz Klient) Elastyczne godziny pracy: ciesz siƒô swobodƒÖ efektywnego zarzƒÖdzania swoim czasem Pracƒô w 100% zdalnie Kultura wsp√≥≈Çpracy: Cenimy pracƒô zespo≈ÇowƒÖ, otwarto≈õƒá, szacunek i wzajemne wsparcie w rozwoju umiejƒôtno≈õci. Kreatywno≈õƒá mile widziana: Twoje pomys≈Çy i sugestie w realizacji projektu bƒôdƒÖ uwzglƒôdniane i brane pod uwagƒô : -)","[{""min"": 155, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Database Administration,155,170,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,46,Data Privacy and Compliance Specialist,OChK,"Data Privacy and Compliance Specialist Miejsce pracy: Warszawa / hybrydowo Poziom stanowiska: Intermediate 10.000 - 13.000 brutto UoP lub umowa B2B Tw√≥j zakres obowiƒÖzk√≥w: udzia≈Ç w negocjacjach i opiniowaniu um√≥w z klientami oraz dostawcami, w zakresie zapis√≥w dotyczƒÖcych ochrony danych i bezpiecze≈Ñstwa informacji, wsparcie w ocenie i monitorowaniu bezpiecze≈Ñstwa ≈Ça≈Ñcucha dostaw, ze szczeg√≥lnym uwzglƒôdnieniem aspekt√≥w zwiƒÖzanych z przetwarzaniem danych osobowych, udzia≈Ç w kontrolach zgodno≈õci, audytach wewnƒôtrznych i zewnƒôtrznych oraz wdra≈ºanie dzia≈Ça≈Ñ korygujƒÖcych i zapobiegawczych, przygotowywanie, aktualizacja i rozw√≥j dokumentacji z zakresu bezpiecze≈Ñstwa informacji (m.in. polityki, procedury, klauzule, rejestry), wsparcie zespo≈Ç√≥w biznesowych w analizie ryzyka oraz ocenie i zapewnianiu zgodno≈õci planowanych dzia≈Ça≈Ñ z obowiƒÖzujƒÖcymi regulacjami, weryfikacja zgodno≈õci przetwarzania danych osobowych w procesach biznesowych i systemach IT, prowadzenie szkole≈Ñ i dzia≈Ça≈Ñ edukacyjnych dla pracownik√≥w w obszarze bezpiecze≈Ñstwa informacji, w tym ochrony danych, monitorowanie zmian w przepisach prawa i regulacjach oraz inicjowanie i wdra≈ºanie niezbƒôdnych dzia≈Ça≈Ñ dostosowawczych. Nasze wymagania: wykszta≈Çcenie wy≈ºsze (preferowane: prawnicze, cyberbezpiecze≈Ñstwo), doskona≈Ça znajomo≈õƒá przepis√≥w oraz norm i standard√≥w dotyczƒÖcych bezpiecze≈Ñstwa informacji oraz ochrony danych osobowych, z uwzglƒôdnieniem zagadnie≈Ñ dotyczƒÖcych chmury obliczeniowej oraz sztucznej inteligencji, znajomo≈õƒá jƒôzyka angielskiego na poziomie minimum B2, co najmniej 3-letnie do≈õwiadczenie na stanowisku zwiƒÖzanym z bezpiecze≈Ñstwem informacji/ochronƒÖ danych osobowych, certyfikaty po≈õwiadczajƒÖce wiedzƒô z zakresu bezpiecze≈Ñstwa informacji oraz ochrony danych osobowych ‚Äì mile widziane. W OChK: pracujemy zadaniowo w trybie hybrydowym (nowoczesne biuro przy ul. Grzybowskiej), dzia≈Çamy w zwinnym ≈õrodowisku, z wykorzystaniem aplikacji zwiƒôkszajƒÖcych efektywno≈õƒá (m. in. Google Workspace, Slack, GitHub, Jira), inwestujemy w Tw√≥j rozw√≥j poprzez finansowanie szkole≈Ñ i cert√≥w, a od pierwszego dnia pracy udostƒôpniamy platformy edukacyjne Google i Microsoft, oferujemy prywatne ubezpieczenie medyczne, preferencyjne warunki ubezpieczenia grupowego oraz kartƒô Multisport organizujemy i wsp√≥≈Çfinansujemy naukƒô jƒôzyka angielskiego, udostƒôpniamy program polece≈Ñ, dziƒôki kt√≥remu pracownicy zyskujƒÖ dodatkowe bonusy za skutecznƒÖ rekomendacjƒô kandydat√≥w do pracy, cenimy proaktywno≈õƒá i inicjatywƒô w≈ÇasnƒÖ, dlatego wspieramy autonomiƒô w podejmowaniu decyzji, budujemy kulturƒô organizacyjnƒÖ na warto≈õciach takich jak profesjonalizm, wsp√≥≈Çodpowiedzialno≈õƒá i wzajemny szacunek, przyk≈Çadamy du≈ºƒÖ wagƒô do efektywnego onboardingu, podczas kt√≥rego w lu≈∫nej atmosferze i ze wsparciem Twojego CloudBuddiego poznajesz zesp√≥≈Ç, firmƒô i swoje obowiƒÖzki, stawiamy na integracjƒô zespo≈Ç√≥w podczas r√≥≈ºnorodnych inicjatyw, zar√≥wno firmowych jak i oddolnych, kt√≥re pomagajƒÖ nam lepiej siƒô poznawaƒá oraz budowaƒá i utrzymywaƒá dobrƒÖ atmosferƒô wsp√≥≈Çpracy.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 13000, ""type"": ""Gross per month - Permanent""}]",Unclassified,10000,13000,Net per month - B2B
Full-time,Mid,B2B,Remote,50,Data Analyst,Haddad Brands USA,"Along with dynamic company development, we are looking for talented and experienced Data Analyst. Self-motivated and team player. Long term contract only. As a Data Analyst, you will be responsible for developing and maintaining Power BI dashboards, analyzing complex data sets, and collaborating with cross-functional teams to provide actionable insights. The ideal candidate should have a strong analytical mindset, advanced proficiency in Power BI, and a deep understanding of data modeling, visualization, and business intelligence reporting. Key Responsibilities: Develop, maintain, and enhance interactive Power BI dashboards and reports. Transform raw data into meaningful insights using Power BI to support business decisions. Work closely with business stakeholders to understand their requirements and translate them into analytical solutions. Collaborate with data engineering and IT teams to ensure data accuracy, integrity, and consistency. Perform data analysis using DAX, Power Query, and other advanced Power BI features. Optimize dashboard performance and recommend best practices for data visualization. Support ad-hoc reporting and analytical requests from various departments. Identify trends, patterns, and key insights through complex data analysis. Ensure the security and governance of Power BI reports according to company standards. Train and support users in the effective use of Power BI reports and analytics tools. Required Qualifications: Bachelor‚Äôs degree in Statistics, data science, Computer Science, or a related field. 3+ years of hands-on experience in data analysis and business intelligence using Power BI. Proficiency in Power BI Desktop, Power BI Service, DAX, and Power Query. Strong experience with data modeling, ETL processes, and SQL. Knowledge of database management systems (e.g., SQL Server, Azure, etc.). Ability to work with large datasets and perform data cleaning, transformation, and analysis. Excellent communication skills with the ability to present complex data in a simple, actionable format. Strong problem-solving skills and attention to detail. Ability to work in a fast-paced environment and manage multiple projects simultaneously. Preferred Qualifications: Knowledge of Python or R for data analysis. Power Automate experience Web scraping experience Familiarity with other BI tools (Tableau, QlikView, etc.). Experience with cloud platforms (Azure, AWS, Google Cloud) and Power BI integrations. Power BI certification is a plus. Job Profile: 40% New features/reports, 40% Maintenance / Bug Fixing, 20% Meetings (developers / business) Hire process: Get to know meeting in Polish followed by a second one, technical oriented within 2 weeks period. Brief English conversation is possible to estimate the skills. Benefits: private medical insurance, 21 paid vacations a year in B2B, hardware/software, training and career-oriented development path, freedom in tools, friendly environment based on trust.","[{""min"": 10000, ""max"": 14000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,10000,14000,Net per month - B2B
Full-time,Mid,B2B,Remote,56,Data Scientist,in4ge sp. z o.o.,"Zakres obowiƒÖzk√≥w: Opracowywanie innowacyjnych rozwiƒÖza≈Ñ z wykorzystaniem zaawansowanych technologii uczenia maszynowego i/lub sztucznej inteligencji. Analiza i przetwarzanie du≈ºych zbior√≥w danych. Poszukiwanie nowych ≈∫r√≥de≈Ç danych oraz eksploracja zale≈ºno≈õci w danych. Wsp√≥≈Çpraca z zespo≈Çami programist√≥w i analityk√≥w biznesowych. Przekazywanie z≈Ço≈ºonych wynik√≥w analizy w spos√≥b zrozumia≈Çy dla odbiorc√≥w technicznych i nietechnicznych. ≈öledzenie nowych trend√≥w i technologii w obszarze sztucznej inteligencji, machine learning i analizy danych oraz proponowanie ich zastosowania w projektach. Automatyzacja proces√≥w przetwarzania i analizy danych przy u≈ºyciu nowoczesnych narzƒôdzi i bibliotek. Wdra≈ºanie oraz zarzƒÖdzanie modelami w ≈õrodowiskach produkcyjnych z uwzglƒôdnieniem praktyk MLOps. Wymagania: Minimum 3 lata do≈õwiadczenia zawodowego na podobnym stanowisku. Do≈õwiadczenie w pracy z danymi i tworzeniem modeli uczenia maszynowego. Znajomo≈õƒá jƒôzyka Python i bibliotek zwiƒÖzanych z analizƒÖ danych (np. Pandas, NumPy) oraz podstawowe umiejƒôtno≈õci w zakresie SQL. Znajomo≈õƒá narzƒôdzi chmurowych (AWS, Azure, GCP) bƒôdzie dodatkowym atutem. Umiejƒôtno≈õƒá pracy z danymi tekstowymi (NLP) lub danymi obrazowymi (CV) mile widziana. Znajomo≈õƒá podstaw MLOps i narzƒôdzi do wdra≈ºania modeli (Docker, MLflow, CI/CD). Wiedza z zakresu statystyki i podstawowych algorytm√≥w uczenia maszynowego. Umiejƒôtno≈õƒá analitycznego my≈õlenia i rozwiƒÖzywania problem√≥w. Znajomo≈õƒá jƒôzyka angielskiego na poziomie pozwalajƒÖcym na swobodnƒÖ komunikacjƒô. Mile widziane: Do≈õwiadczenie w pracy z du≈ºymi modelami jƒôzykowymi (LLM) i koncepcjami takimi jak Retrieval Augmented Generation, bazy wektorowe czy in≈ºynieria prompt√≥w. Znajomo≈õƒá framework√≥w LLM, takich jak Langchain, LLamaindex czy agentowych framework√≥w. Wiedza z zakresu przetwarzania jƒôzyka naturalnego (NLP). Znajomo≈õƒá dodatkowych jƒôzyk√≥w programowania (np. Java, C#, Go). Znajomo≈õƒá narzƒôdzi i bibliotek zwiƒÖzanych z agentami GenAI (np. Taskweave, Autogen). Co oferujemy? Rozw√≥j kariery w miƒôdzynarodowych projektach, z wykorzystaniem nowoczesnych narzƒôdzi i technologii. Elastyczny model pracy: mo≈ºliwo≈õƒá 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejƒôtno≈õci i do≈õwiadczenia. Wsp√≥≈Çpracƒô w zgranym zespole, kt√≥ry ceni wymianƒô wiedzy oraz otwartƒÖ komunikacjƒô. Realny wp≈Çyw na projekty oraz wdra≈ºane rozwiƒÖzania.","[{""min"": 13000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Science,13000,25000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,57,Data Scientist ‚Äì Investment Management,ITDS,"Data Scientist ‚Äì Investment Management Join a world of innovation and data-driven excellence! This is a Wroc≈Çaw -based hybrid opportunity ‚Äì 3 days in the office per week (Relocation package available, including move and acocomodation) As a Data Scientist, you will be working for our client ‚Äì a global leader in quantitative and systematic investment management. You will be responsible for leveraging data to drive investment strategies and enhance trading decisions. Your main responsibilities: Collaborate with Quantitative Researchers and Traders to design impactful datasets. Prototype and design code for data extraction, cleaning, and aggregation. Work with Engineers to automate and optimize data processes. Manage the onboarding of new datasets from start to finish. Solve data-related challenges to expedite production timelines. Innovate with novel data extraction methods to enhance capabilities. You're ideal for the role if you have: 3+ years of experience as a Data Scientist; buy-side quantitative finance experience is a plus. A postgraduate degree in Mathematics, Physics, or Engineering. Advanced Python programming skills, with proficiency in Pandas and NumPy. A keen interest in financial markets and data analysis. Experience with traditional and alternative financial datasets. Excellent communication skills for effective stakeholder collaboration. Ability to thrive in a high-performance, fast-paced environment. We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7131 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 25000, ""max"": 30000, ""type"": ""Gross per month - Permanent""}]",Data Science,25000,30000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,58,Data Scientist,Ework Group,"Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking forSenior Data Scientist ‚Äì Medical Image Analysis - remote worküîπ ‚úîÔ∏èThe Position This position focuses on exploring new insights and developing innovative technologies in the field of medical image analysis. Key responsibilities include the collection, curation, and processing of medical image data, as well as applying software engineering principles within the context of medical imaging and computer vision. You will also collaborate with team members, as well as clinical scientists from other disciplines to design, code, train, test, deploy, and iterate on machine learning systems. ‚úîÔ∏èMain responsibilities include: Research, develop and implement medical image processing pipelines utilizing state of the art computer-vision-based approaches in the medical image analysis space. Implement and maintain local and cloud-based data and computational environments and platforms to enable the work. Build data pipelines (data curation, preparation, cleaning, and consolidation) as an integral part of data science activity. Utilize AI/ML (Artificial Intelligence/Machine Learning) to develop complex methodologies and analyses, all around various medical imaging modalities. Perform informed semi-automated quality assessments of the acquired imaging data and of the derived measures. Align with clinical experts on the requirements, constraints and deliverables of a project. ‚úîÔ∏èQualifications: In this role, it is necessary to have either a PhD degree with 3+ years‚Äô relevant direct non-academic professional experience or PhD degree with a strong post-doc experience in the field of medical image analysis. Degree within computer science, mathematics, (medical-)engineering, physics, statistics, or a related quantitative discipline is preferred. ‚úîÔ∏èFurthermore, you must have: Hands-on experience with deep learning for computer vision using deep/machine learning frameworks, mainly PyTorch, Sklearn, and with conventional image processing techniques. Strong practical knowledge in medical image analysis in multiple medical imaging modalities (X-ray, US, MRI, CT, Digital Pathology) across different therapies (cardiology, obesity, diabetes etc.) with a proven high impact academic publication record Solid understanding of deep learning theory: CNNs, Transformers, RNN, GenAI, etc., and its various applications in semantic segmentation, classification, object detection and more. The ability to develop and validate algorithms against clinical data, ensuring appropriateness of fit for data science processes The ability to do quick prototyping/proofs of concept up to production ready models following best practices. The ability to perform in-depth data analysis and present results and conclusions to engineering and leadership teams. Appetite for research work Ideally, you have hands on knowledge in imaging biomarkers in pharmaceutical industry, healthcare industry, medical device development or in another regulated field. Experience with modern software development toolset (CI/CD, AWS, Azure Machine Learning, git, JIRA) which includes writing high quality code processing vision/imaging data using Python is highly preferred and so are excellent written and oral communication skills. ‚úîÔ∏èDepartment Overview The Imaging Analytics department part of AI and Analytics Department within Data Science division, where we apply state of the art algorithms and machine learning techniques to some of the hardest problems in the discovery and development of new healthcare solutions, focusing on medical imaging in the clinical trials space. By leveraging a blend of scientific, problem-solving, and quantitative skills, we provide superior data insights that empower us to further develop and deliver life-changing treatments. We work in multidisciplinary teams with strong collaboration across all areas of the organization and engage in external collaborations to ensure access to cutting edge research and technology. ‚úîÔ∏èJoin the team The organisation values flexibility in ways of working to support various life situations. Employees are recognised for their unique qualities and skills, and the environment fosters development and collaboration. The broader mission includes improving the lives of millions of patients globally through innovation and dedication to chronic disease care. There is a commitment to becoming not just the best company in the world, but the best company for the world. This vision can only be achieved through the contributions of talented employees with diverse backgrounds and perspectives. An inclusive culture is fostered that celebrates diversity across employees, patients, and communities. ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 162, ""max"": 179, ""type"": ""Net per hour - B2B""}]",Data Science,162,179,Net per hour - B2B
Full-time,Mid,B2B,Remote,59,ETL Developer,EndySoft,"Position Overview: We are seeking an experienced ETL Developer to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust ETL processes to ensure the efficient flow of data from various sources to data warehouses or data lakes. This role involves collaborating with business and technical teams to support data-driven decision-making and analytics initiatives. MD rate: 16600 - 20000PLN Roles and Responsibilities: Design, develop, and optimize ETL pipelines to extract, transform, and load data from multiple data sources. Collaborate with data architects and business analysts to gather and understand data requirements. Implement and maintain data integration workflows using ETL tools such as Informatica , Talend , SSIS , or Apache NiFi . Perform data validation and quality checks to ensure data integrity and accuracy. Troubleshoot and resolve issues related to ETL processes and data flows. Monitor and enhance the performance of ETL jobs to meet business SLA requirements. Maintain and document technical solutions, including data mappings, workflows, and procedures. Work closely with other data team members to support data warehouse and data lake initiatives. Required Skills and Experience: Proficiency in SQL for querying and transforming data. Hands-on experience with ETL tools such as Informatica , Talend , SSIS , or similar. Strong knowledge of data modeling techniques, including star schema and snowflake schema . Experience in data integration and data warehousing concepts. Familiarity with cloud platforms (e.g., AWS Glue , Azure Data Factory , Google Dataflow ) for ETL processes. Strong problem-solving skills and the ability to troubleshoot complex data issues. Experience with scripting languages such as Python , Shell , or Bash for automation. Excellent communication and collaboration skills to work effectively with cross-functional teams. Nice to Have: Experience with big data tools such as Spark , Kafka , or Hadoop . Knowledge of NoSQL databases like MongoDB or Cassandra . Familiarity with DataOps practices and CI/CD pipelines for ETL workflows. Exposure to data governance and metadata management tools. Understanding of data security and compliance requirements. Experience with version control systems like Git . Exposure to Agile/Scrum methodologies. Additional Information: This role provides an opportunity to work on complex data integration projects and contribute to the development of scalable data solutions. If you are passionate about transforming raw data into actionable insights and thrive in a fast-paced environment, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,16600,20000,Net per month - B2B
Full-time,Senior,B2B,Remote,62,Senior Cloud Data Engineer (Azure and Databricks),Future Processing,"Do naszej linii biznesowej Data Solutions poszukujemy osoby na stanowisko Senior Cloud Data Engineer ze znajomo≈õciƒÖ Azure. Szukamy Ciebie, je≈õli: masz min. 5 lat do≈õwiadczenia w IT, w tym min. 3,5 roku w pracy z danymi w chmurze Azure (potwierdzone projektami komercyjnymi wdro≈ºonymi na produkcje), masz komercyjne do≈õwiadczenie w przetwarzaniu sporych danych przy u≈ºyciu Databricks, korzystasz z SQL na poziomie zaawansowanym i wykorzystujesz go na rozwiƒÖzaniach technologicznych MS i nie tylko, znasz metodyki i stosujesz biegle Git oraz CI/CD , masz do≈õwiadczenie w pracy z platformƒÖ Microsoft Fabric , tworzysz i optymalizujesz rozwiƒÖzania przetwarzajƒÖce dane (ETL, ELT, itp.) poprzedzone projektem technicznym oraz alternatywami rozwiƒÖza≈Ñ, monitoring, diagnostyka oraz rozwiƒÖzywanie problem√≥w w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanowaƒá infrastrukturƒô oraz obliczyƒá jej koszt, koncepcje Delta Lake i Data Lakehouse sƒÖ Ci znane, znasz architekturƒô SMP oraz MPP wraz z przyk≈Çadami rozwiƒÖza≈Ñ opartych o te architektury, masz wiedzƒô na temat migracji rozwiƒÖza≈Ñ on-premise do chmury oraz znasz podstawowe typy migracji, masz wiedzƒô na temat stosowania mechanizm√≥w zwiƒÖzanych z bezpiecznym przechowywaniem i przetwarzaniem danych w chmurze, bardzo dobrze znasz us≈Çugi zwiƒÖzanie z przechowywaniem i przetwarzaniem danych, oferowanych przez dostawcƒô chmury Azure, masz do≈õwiadczenie w bezpo≈õredniej wsp√≥≈Çpracy z klientem, pos≈Çugujesz siƒô j. angielskim na poziomie ≈õredniozaawansowanym (min. B2). Praca na tym stanowisku w naszej firmie oznacza: odpowiedzialno≈õƒá za ca≈Ço≈õƒá rozwiƒÖza≈Ñ wsp√≥≈Çtworzonych wraz z zespo≈Çem, tworzenie lub modyfikowanie rozwiƒÖza≈Ñ do przetwarzania danych w chmurze, tworzenie i modyfikowanie dokumentacji, analizowanie i optymalizowanie rozwiƒÖza≈Ñ w zakresie dzia≈ÇajƒÖcego lub projektowanego systemu, analizowanie wymaga≈Ñ klienta pod kƒÖtem dostarczenia optymalnego rozwiƒÖzania jego potrzeby biznesowej, analizowanie potencjalnych zagro≈ºe≈Ñ, dostosowywanie rozwiƒÖza≈Ñ wzglƒôdem wymaga≈Ñ biznesowych.","[{""min"": 135, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,135,200,Net per hour - B2B
Full-time,Senior,B2B,Remote,63,Programista Baz Danych,TSS,"W TSS tworzymy najwy≈ºszej jako≈õci rozwiƒÖzania z zakresuSoftware Development, FinTech, AI Solution. Tworzymy systemy p≈Çatnicze, bramki p≈Çatnicze online oraz rozwiƒÖzania umo≈ºliwiajƒÖce innowacyjne procesowanie p≈Çatno≈õci. Nasze zespo≈Çy uczestniczƒÖ r√≥wnie≈º w projektach wykonywanych dla klient√≥w z wielu r√≥≈ºnych bran≈º i specjalizacji. Je≈õli chcesz do≈ÇƒÖczyƒá do zespo≈Çu entuzjast√≥w, dla kt√≥rych praca jest jednocze≈õnie pasjƒÖ, przygodƒÖ i mo≈ºliwo≈õciƒÖ rozwoju zawodowego do≈ÇƒÖcz do team‚Äôu TSS ju≈º teraz! Do≈õwiadczenie: 5 lat w pracy z relacyjnymi bazami danychnp. PostgreSQL, MySQL, MSSQL, ORACLE 4 lata do≈õwiadczenia w zakresie wykorzystania jednego z proceduralnych jƒôzyk√≥w programowania np.PL/SQL, PL/PqSQL Bardzo dobra znajomo≈õƒá SQL orazPostgreSQL Znajomo≈õƒá zasad zarzƒÖdzania, konfiguracji i optymalizacji bazy danych PostgresSQL Do≈õwiadczenie w migracji danych z system√≥wklasy enterprise Do≈õwiadczenia w analizie i transformacji danych Projektowanie i eksploatacja baz danych systemu Optymalizacja obecnie eksploatowanych baz danych systemu Bie≈ºƒÖca wsp√≥≈Çpraca z zespo≈Çem wytw√≥rczym Co oferujemy? Mo≈ºliwo≈õƒá pracy w pe≈Çni zdalnej lub w biurze w Warszawie; StabilnƒÖ wsp√≥≈Çpracƒô na podstawie B2B; Dofinansowanie do prywatnej opieki medycznej w PZU; Wsparcie w rozwoju zawodowym - wewnƒôtrzne szkolenia z zakresu cyberbezpiecze≈Ñstwa;","[{""min"": 14000, ""max"": 18000, ""type"": ""Net per month - B2B""}]",Database Administration,14000,18000,Net per month - B2B
Full-time,Mid,Permanent,Remote,64,Data Engineer,INFOPLUS TECHNOLOGIES,"Job Description Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources data sources using SQL and AWS ‚Äòbig data‚Äô technologies. Build analytics tools (Tableu) that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Work with system integration and middlewares (MuleSoft, Talend, Solace and etc) Perform proof of concept with customer in data integration and data load Managing structure and unstructured data set Managing and design data privacy, integrity and security solution Managing data with high confidentiality, integrity and privacy Skills Experience with working in agile methodologies (Scrum or SAFe) Good teamwork and communication skill (Higher Performance Team) Able to be self organized and focus in delivering values to the business Always work with integrity, passion and courage Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) and Nosql unstructured database Familiarity with a variety of databases (Microsoft SQL is mandatory). Familiarity reporting tools and dashboards like Tableu. Experience building and optimizing ‚Äòbig data‚Äô data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‚Äòbig data‚Äô data stores. Preferable middleware knowledge, example: MuleSoft, Solace. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.","[{""min"": 220000, ""max"": 250000, ""type"": ""Gross per year - Permanent""}]",Data Engineering,220000,250000,Gross per year - Permanent
Full-time,Senior,B2B,Remote,65,Analityk Systemowy,Detable,"Do≈ÇƒÖcz doDetable Sp. Z o.o.- prƒô≈ºnie rozwijajƒÖcej siƒô firmy, kt√≥ra stawia na d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô z do≈õwiadczonymi profesjonalistami. Od ponad 3 lat jeste≈õmy partnerem dla instytucji z sektora publicznego, wsp√≥≈Çpracujemym.in. zCentrum e-zdrowia, Aplikacjami Krytycznymi Ministerstwa Finans√≥w, G≈Ç√≥wnym Urzƒôdem Nadzoru Budowlanego, Urzƒôdem Do Spraw Cudzoziemc√≥w,Narodowym Funduszem Zdrowiai wieloma innymi. Nasi konsultanci pracujƒÖ nad rozwojem aplikacji i system√≥w, z kt√≥rych korzystajƒÖ miliony Polak√≥w! Posiadasz minimum 3 lata do≈õwiadczenia w pracy na stanowisku Analityka w projektach dla klienta masowego; Na co dzie≈Ñ pracujesz z SQL, Python, PostgresSQL, Oracle; Sk≈Çadnice danych, Mapowania, Raportowanie (BI) nie majƒÖ przed TobƒÖ tajemnic; Wykorzystujesz w codziennej procesy ETL/ELT; Mia≈Çe≈õ okazjƒô tworzyƒá modele logiczne i fizyczne z wykorzystniem narzƒôdzia Enterprise Architect; Mo≈ºesz pochwaliƒá siƒô znajomo≈õciƒÖ i do≈õwiadczeniem w obszarze ochrony zdrowia; Posiadasz do≈õwiadczenie w obszarze Hurtowni Danych; Dodatkowym atutem bƒôdzie je≈õli mo≈ºesz pochwaliƒá siƒô jednym z certyfikat√≥w: AgilePM, certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL/Python. Pozyskiwaniu wymaga≈Ñ systemowo- biznesowych dla projekt√≥w IT; Modelowaniem otoczenia i proces√≥w systemowo-biznesowych oraz wytwarzaniem modelu danych; Okre≈õlanie przypadk√≥w u≈ºycia system√≥w informatycznych; Modelowanie danych na poziomie logicznym; Projektowanie i dokumentowanie przep≈Çyw√≥w danych (ETL) oraz struktur raportowych; Okre≈õlanie architektury dla projektowanych system√≥w; Definiowanie interfejs√≥w oraz przep≈Çywu komunikacji miƒôdzy modu≈Çami; Modelowanie diagram√≥w aktywno≈õci, sekwencji oraz stan√≥w dla modu≈Ç√≥w projektowanych system√≥w; Proponowanie i konsultowanie rozwiƒÖza≈Ñ systemowych ze zleceniodawcami oraz realizatorami zdefiniowanych wymaga≈Ñ; Wsparcie analityczne na etapach projektowania, wytwarzania i testowania system√≥w informatycznych. Konkurencyjne wynagrodzenie w oparciu o kontrakt B2B ( do 140PLN netto/h); D≈ÇugofalowƒÖ wsp√≥≈Çpracƒô opartƒÖ o wzajemny szacunek i partnerstwo; Dedykowanego opiekuna kontraktu po stronie Detable; Mo≈ºliwo≈õƒá 100% pracy zdalnej lub z naszego biura w Bia≈Çymstoku; Mo≈ºliwo≈õƒá podnoszenia swoich kwalifikacji poprzez skorzystanie z bud≈ºetu szkoleniowego; Realny wp≈Çyw na rozw√≥j projektu; Atrakcyjny program polece≈Ñ pracowniczych; Zdalny proces rekrutacji. Rozmowa HR z naszƒÖ IT RekruterkƒÖ; Weryfikacja umiejƒôtno≈õci technicznych przez naszego Lidera technicznego; Spotkanie z klientem; Decyzja i rozpoczƒôcie wsp√≥≈Çpracy","[{""min"": 20160, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20160,23520,Net per month - B2B
Full-time,Mid,B2B,Hybrid,66,Data Engineer (Scala),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: rozwijanie platformy DataHub do przechowywania i przetwarzania danych. Wykorzystywany stos technologiczny: Spark, Scala, TDD, Hadoop, Hive, DataBricks, CI/CD, Git, GitHub, Jenkins, Sonar, Nexus, Jira, SQL, PostgreSQL, rozw√≥j, testowanie i wdra≈ºanie specyfikacji technicznych i funkcjonalnych przygotowanych przez Solution Designer√≥w, Architekt√≥w Biznesowych oraz Analityk√≥w Biznesowych, zapewnienie poprawnego dzia≈Çania opracowanych rozwiƒÖza≈Ñ, dbanie o zgodno≈õƒá z wewnƒôtrznymi standardami jako≈õci, praca w modelu hybrydowym: 1-2 razy w tygodniu praca z biura w Warszawie, stawka do 170 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz minimum 2 lata do≈õwiadczenia z Apache Spark i ScalƒÖ, znasz wzorce projektowe i pracujesz w podej≈õciu Test-Driven Development (TDD), masz do≈õwiadczenie z technologiami Big Data ‚Äì Spark, Hadoop, Hive (znajomo≈õƒá Azure Databricks bƒôdzie dodatkowym atutem), masz do≈õwiadczenie w pracy w metodyce SCRUM/Agile, masz do≈õwiadczenie w integracji i zarzƒÖdzaniu du≈ºymi wolumenami danych, dobrze znasz narzƒôdzia CI/CD i DevOps: Git, GitHub, Jenkins, Sonar, Nexus, Jira, posiadasz znajomo≈õƒá struktur baz danych (PostgreSQL, SQL, Hive), biegle komunikujesz siƒô w jƒôzyku angielskim (min. B2), mile widziane do≈õwiadczenie z Bash, Control-M, Docker, Kubernetes, OS3, Azure, AWS. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 25200, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,28560,Net per month - B2B
Full-time,Mid,B2B,Hybrid,67,Big Data Analyst,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: udzia≈Ç w projekcie gromadzƒÖcym dane dotyczƒÖce u≈ºytkowania urzƒÖdze≈Ñ domowych. Wykorzystywany stos technologiczny w projekcie: SQL, Python, R, Scala, Hadoop, Spark, Hive, Kafka, Power BI, DAX, Power Query, przetwarzanie, modelowanie i transformacja danych, tworzenie i wdra≈ºanie modeli danych reprezentujƒÖcych struktury i relacje danych, programowanie skrypt√≥w w Pythonie, R lub Scala na potrzeby analizy i wizualizacji danych, automatyzacja proces√≥w przetwarzania danych, wdra≈ºanie efektywnych metod przechowywania i pozyskiwania danych, integracja technologii Big Data z przep≈Çywami danych, projektowanie i tworzenie interaktywnych, profesjonalnych dashboard√≥w w Power BI, zapewnienie intuicyjno≈õci i dopasowania dashboard√≥w do potrzeb biznesowych, tworzenie trafnych wizualizacji skutecznie komunikujƒÖcych wnioski z danych, dostosowywanie wizualizacji do r√≥≈ºnych odbiorc√≥w biznesowych, optymalizacja przep≈Çyw√≥w pracy zwiƒÖzanych z przetwarzaniem i wizualizacjƒÖ danych, praca w modelu hybrydowym: min. 1x na miesiƒÖc w biurze w Warszawie, stawka do 120 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz min. 3 lata do≈õwiadczenia na stanowisku Data Analyst, masz bardzo dobrƒÖ znajomo≈õƒá SQL i technik zapyta≈Ñ do baz danych, masz praktyczne do≈õwiadczenie w przetwarzaniu, modelowaniu i transformacji danych, znasz przynajmniej jeden jƒôzyk programowania wykorzystywany w analizie danych (np. Python, R, Scala), masz do≈õwiadczenie w pracy z du≈ºymi, z≈Ço≈ºonymi zbiorami danych, potrafisz tworzyƒá profesjonalne, interaktywne dashboardy w Power BI, masz do≈õwiadczenie w pracy z DAX i Power Query przy modelowaniu i transformacji danych w Power BI, potrafisz przek≈Çadaƒá potrzeby biznesowe na rozwiƒÖzania techniczne oraz prezentowaƒá rekomendacje oparte na danych, znasz jƒôzyk angielski na poziomie min. C1, mile widziane do≈õwiadczenie z technologiami Big Data (np. Hadoop, Spark, Hive, Kafka). Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 16000, ""max"": 19200, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,16000,19200,Net per month - B2B
Full-time,Mid,B2B,Remote,68,Data Architect with AWS,Link Group,"About the Role: We are looking for a skilled and experiencedData Architectwith strong knowledge ofAWS cloud servicesto join our team. This role involves designing scalable, secure, and high-performance data solutions in a cloud-native environment. You'll work closely with data engineers, analysts, and business stakeholders to create modern data platforms that support analytics, reporting, and AI/ML initiatives. Design and implement cloud-native data architecture using AWS services Define data models, architecture standards, and best practices for data pipelines, storage, and security Collaborate with stakeholders to understand business and technical requirements Guide development of data lakes, data warehouses, and real-time data streaming systems Ensure compliance with data governance, security, and privacy standards Evaluate and recommend appropriate AWS services and tools Support the development and review of ETL/ELT processes Provide technical leadership to data engineering teams Proven experience as aData Architect,Cloud Architect, orSenior Data Engineerwith architectural responsibilities Expertise inAWS cloud services(e.g., S3, Redshift, Glue, Lambda, EMR, Athena, Kinesis, Lake Formation) Strong knowledge ofdata modeling,ETL/ELT, anddata warehousing concepts Experience designing and implementingdata lakesandmodern data platforms Familiarity withinfrastructure-as-code(e.g., Terraform, CloudFormation) Experience withSQLand at least one scripting/programming language (e.g., Python) Understanding ofdata governance,compliance, andsecurity standards Excellent communication skills in English Experience withSnowflake,Databricks, or similar platforms Knowledge ofmachine learning workflowsand MLOps Familiarity withdata meshorevent-driven architectures","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Architecture,150,180,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,70,Senior Data Engineer,N-iX,"We are seeking a Senior Data Engineer specializing in Databricks to join our global team. You will be instrumental in setting up and maintaining our Databricks platform, building robust data pipelines, and collaborating closely with our solution architects and data scientists. Your expertise will directly support our mission to leverage data and AI effectively within a cutting-edge automotive claims management environment. Key Responsibilities: Design, build, and maintain robust data pipelines within Databricks. Collaborate closely with international teams, including data scientists and architects, to develop scalable data solutions. Debug complex issues in data pipelines and proactively enhance system performance and reliability. Set up Databricks environments on cloud platforms (Azure/AWS). Automate processes using CI/CD practices and infrastructure tools such as Terraform. Create and maintain detailed documentation, including workflows and operational checklists. Develop integration and unit tests to ensure data quality and reliability. Migrate legacy data systems to Databricks, ensuring minimal disruption. Participate actively in defining data governance and management strategies. What We Expect from You (Requirements): 5+ years of proven experience as a Data Engineer. Advanced proficiency in Python for developing production-grade data pipelines. Extensive hands-on experience with Databricks platform. Strong knowledge of Apache Spark for big data processing. Familiarity with cloud environments, specifically Azure or AWS. Proficiency with SQL and experience managing relational databases (MS SQL preferred). Practical experience with Airflow or similar data orchestration tools. Strong understanding of CI/CD pipelines and experience with tools like GitLab. Solid skills in debugging complex data pipeline issues. Proficiency in structured documentation practices. B2 level or higher proficiency in English. Strong collaboration skills, ability to adapt, and eagerness to learn in an international team environment. Nice to have: Experience with Docker and Kubernetes. Familiarity with Elasticsearch or other vector databases. Understanding of DBT (data build tool). Ability to travel abroad twice a year for on-site workshops. Why Join Us Work on impactful projects with cross-functional teams. Opportunity to grow your BI and analytics career in a data-driven organization. Flexible working hours and remote work options. Competitive compensation and benefits. Opportunity to work on presales We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 29553, ""max"": 30661, ""type"": ""Net per month - B2B""}, {""min"": 24381, ""max"": 25489, ""type"": ""Gross per month - Permanent""}]",Data Engineering,29553,30661,Net per month - B2B
Full-time,Senior,B2B,Remote,71,"Senior BI Developer (Power BI, Power Automate)",Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä We are looking for an experienced Business Intelligence Developer (Power BI)to join our team at a leadingSwedish manufacturing company. In this role, you will take ownership of the frontend development of a proprietary Business Intelligence (BI) and analytics platform used across distribution and customer management services. You‚Äôll become part of across-functional team currently leading the migration of reporting systems from on-premise to the cloud, using modern Microsoft technologies. This is a great opportunity to contribute your expertise to a high-impact transformation initiative. The project environment is dynamic and international, and success in this role will requirestrong technical proficiency,proactive communication, anda solutions-driven mindset. While expectations are high, you‚Äôll be working in a company shaped byScandinavian work culture, wheretrust, autonomy, and transparencyare central values. Responsibilities: Lead frontend development in Power BI Premium (Cloud) for BI and analytics tools. Design, build, and maintain dashboards, reports, and data visualizations that serve business-critical needs. Develop and optimize solutions using Power BI and Power Automate, ensuring high usability and performance. Collaborate with backend engineers, frontend engineers, analysts, and business stakeholders to translate complex requirements into clear, actionable reports. Contribute to the continuous improvement of reporting frameworks and analytical tools across the organization. We offer a B2B Contract: 120-150 PLN net/hour + VAT (depending on experience) You might be the perfect match if you are/have: 4+ years of experience withPower BI(including Premium features) andPower Automate. Proficiency inSQLand a strong understanding of data modeling, cubes, and reporting tools. Experience working oncomplex BI projects(preferablyin the finance sector), involving multiple dashboards and data sources. Familiarity with large-scale BI projects or migration efforts. Strong problem-solving skills and a passion for building well-designed, user-friendly reporting solutions. Clear and proactive communication skills, especially in cross-functional, international teams. Aself-starter mindset, able to work independently, prioritize tasks, and drive projects to completion. Fluent Englishfor smooth collaboration. Located in Polandto support formalities and occasional team meetings. Moreover, we appreciate skills in these areas: Hands-on experience withPython and/or Snowflake. Experience in cross-functional development teams within global organizations. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Perks and benefits: Fully remotework or in our office in Wroc≈Çaw; Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories. If you apply for this position and match our expectations, then: 1) You will be invited to an HR Screening with our IT Recruiter.2) You will have an interview with client. Submit your application online in one easy step! Apply now!","[{""min"": 120, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,120,150,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,73,Data Warehouse Developer,ITDS,"Shape the Future of Banking Data ‚Äì Become a DWH Developer Warsaw/ Wroc≈Çaw based opportunity with hybrid work model (1 day in the office/week) As aData Warehouse Developer, you will be working for our client, a leading financial institution dedicated to digital transformation and data excellence. You will join the Data Warehouse team supporting both strategic initiatives and day-to-day development efforts. The project involves maintaining and enhancing a complex enterprise data platform used for business analysis, regulatory compliance, and operational reporting. You will play a key role in building efficient ETL processes and supporting scalable data solutions, contributing to long-term data architecture goals. Your main responsibilities: Develop and implement ETL processes based on business and technical requirements Optimize existing data pipelines to improve performance and reliability Test and validate technical solutions against data quality standards Collaborate with analysts and developers to understand data flow and dependencies Maintain technical documentation related to ETL jobs and data architecture Support Business-as-Usual tasks and participate in ongoing project work Ensure compliance with internal coding and security standards Monitor and troubleshoot ETL jobs and provide issue resolution Contribute to the continuous improvement of the Data Warehouse platform Participate in team planning and development lifecycle activities You're ideal for this role if you have: Proven experience in developing ETL processes using any technology Strong knowledge of SQL in a Data Warehouse context Hands-on experience working in a Data Warehouse or enterprise data environment Understanding of data pipeline design and data integration concepts Familiarity with Microsoft Azure and Databricks Ability to analyze and troubleshoot performance issues in ETL jobs Good communication skills and the ability to work in a cross-functional team Experience working in an Agile or iterative development environment Strong attention to detail and quality mindset Ability to document technical solutions clearly and accurately We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere. Ref. number 7467","[{""min"": 16800, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Engineering,16800,23100,Net per month - B2B
Full-time,Senior,B2B,Remote,74,Data Engineer with Snowflake & DBT,Holisticon Insight,"Holisticon Insightis a division ofhttp: //nexergroup.comfocused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! üëáhttps: //holisticon.pl/holisticon-insight/ üöÄ We are looking for aSenior Data Engineerwho is skilled inSnowflake and DBTto work on a project in a team of our client, a Swedish-based leading provider of transport solutions. In the role of Data Engineer, you willoversee and drive the backend development of our client's BI and analytics tool in the Finance Department. You might be the perfect match if you possess the following competencies: 5+ yearsof previous commercial experience in similar role - a strong backgroundin data engineering, data modeling and database management Proven experience withSnowflake and DBT Experience in settingCI/CD pipelinesfor seamless code integration Experience working with complex projects withFinance reports. Proactive communication skills and ability to communicate with different stakeholders Excellent English communication skills Location in Poland or EU üôå Nice to have: Experience Migrating from Onprem to Cloud. Experience working with AWS or other cloud By joining Holisticon Insight you will get: Life insurance Multisport card Fully remote job Private medical care Flexible working hours B2B or contract of employment Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories)","[{""min"": 26500, ""max"": 31900, ""type"": ""Net per month - B2B""}]",Data Engineering,26500,31900,Net per month - B2B
Full-time,Senior,B2B,Hybrid,75,DevOps Engineer (AI Team),Jit Team,"Salary: 1000 - 1200 PLN/day on B2B Work model: elastic hybrid from Gdynia / Gda≈Ñsk / Warszawa (at least 2 days per week from the office) Why choose this offer? You can expect a flexible work organization The international work environment will give you the opportunity to interact with the English language on a daily basis Scandinavian organizational culture will provide you with work-life balance, you will gain time for additional training (financed by Jit) The Jit community will bring you a nice time during regular integration meetings Project You will be involved in a financial project focused on building and deploying generative and predictive AI models to enhance operations in Financial Crime Prevention. We are currently seeking experienced DevOps Engineer to support the infrastructure, orchestration, and deployment of our AI-powered applications. You‚Äôll be part of a fast-moving team, experimenting with new methods and tools to deliver scalable, production-ready AI solutions. Responsibilities you'll have Design, implement, and manage cloud-based infrastructure for generative AI solutions Develop and maintain orchestration workflows using tools like AWS Step Functions, EventBridge, and Lambda Support data processing pipelines on Glue, EMR, and EKS Collaborate with Data Scientists and Engineers on data transformation using PySpark, Python, and Hadoop ecosystem tools Build and optimize CI/CD pipelines using Jenkins and Terraform Expected competences and knowledge Solid experience with orchestration services such as AWS Step Functions, EventBridge, Managed Workflows for Apache Airflow (MWAA), and AWS Lambda Good knowledge of data processing frameworks and platforms like AWS Glue, EMR, and EKS Hands-on experience working with AWS S3 for data storage and Athena for querying and analysis Proven experience developing Big Data ETL pipelines using PySpark with Python; experience with Spark and Scala is a plus Strong skills in data manipulation and transformation using Python and Pandas Experience working with the Hadoop ecosystem , including tools like Hive, Impala, Sqoop, HDFS, and Oozie Familiarity with CI/CD practices and tools, particularly Jenkins English min. B2 Nice to have: Experience with MLflow and AWS SageMaker for machine learning lifecycle management Familiarity with AWS Bedrock and other generative AI services AWS Certified Cloud Practitioner or higher certification Experience with MLflow and AWS SageMaker for machine learning lifecycle management Familiarity with AWS Bedrock and other generative AI services AWS Certified Cloud Practitioner or higher certification Technologies you'll work with AWS Python Hadoop Jenkins, Terraform Apache Airflow AWS Lambda AWS Glue, EMR, EKS S3, Athena PySpark, Pandas Client ‚Äì why choose this particular client from the Jit portfolio? Jit Team has had an over-decade-long relationship with the leading financial group in the Nordic countries, and we are privileged to be our client's premier partner in Poland. At present, over 200 Jit personnel are engaged in the completion of more than 60 projects for this Norwegian major provider of financial services with a global presence and a strong focus on modern technology. Our customer's work atmosphere is epitomized by the Scandinavian culture , which is conducive to people who place emphasis on work-life balance and feedback culture . Furthermore, all projects are executed in international teams, giving constant exposure to the English language. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 21000, ""max"": 25200, ""type"": ""Net per month - B2B""}]",Data Science,21000,25200,Net per month - B2B
Full-time,Mid,B2B,Remote,78,Programista PowerBI,Eyzee S.A.,"Poszukujemy Programisty BI z dog≈ÇƒôbnƒÖ znajomo≈õciƒÖ Power BI, kt√≥ry do≈ÇƒÖczy do naszego zespo≈Çu i bƒôdzie odpowiedzialny za rozwijanie, utrzymanie oraz optymalizacjƒô zaawansowanych rozwiƒÖza≈Ñ Business Intelligence. Je≈õli posiadasz do≈õwiadczenie w pracy z du≈ºymi zbiorami danych, tworzeniu raport√≥w i dashboard√≥w zarzƒÖdczych, a tak≈ºe cenisz sobie samodzielno≈õƒá i ciƒÖg≈Çy rozw√≥j, to ta oferta jest dla Ciebie! Tworzymy przyjazne miejsce pracy i rozwoju dla specjalist√≥w w bran≈ºy IT, zapewniamy ciekawe wyzwania, dbajƒÖc o dobrƒÖ komunikacjƒô i atmosferƒô w zespole. Z nami przyspieszysz rozw√≥j swojej kariery! Praca zdalna, pe≈Çen etat. Zadania dla Ciebie: rozw√≥j i wdra≈ºanie rozwiƒÖza≈Ñ w oparciu o Power BI projektowanie i budowanie proces√≥w ETL tworzenie zaawansowanych raport√≥w i dashboard√≥w zarzƒÖdczych konfiguracja, rozwiƒÖzywanie problem√≥w i wdra≈ºanie hurtowni danych z wykorzystaniem Power BI praca z relacyjnymi bazami danych i programowanie w SQL oraz Python zapewnienie bezpiecze≈Ñstwa danych i implementacja row-level-security optymalizacja wydajno≈õci istniejƒÖcych rozwiƒÖza≈Ñ Power BI Wymagania: min. 5 lat do≈õwiadczenia na stanowisku programisty system√≥w raportowych w obszarze wizualizacji danych do≈õwiadczenie w rozwijaniu rozwiƒÖza≈Ñ Power BI praktyczna znajomo≈õƒá architektury ≈õrodowiska Power BI umiejƒôtno≈õƒá budowania raport√≥w w Power BI oraz projektowania dashboard√≥w zarzƒÖdczych znajomo≈õƒá jƒôzyka DAX oraz technik optymalizacji w Power BI do≈õwiadczenie w budowaniu proces√≥w ETL umiejƒôtno≈õƒá pracy z relacyjnymi bazami danych znajomo≈õƒá zagadnie≈Ñ bezpiecze≈Ñstwa danych w Power BI, w tym umiejƒôtno≈õƒá budowania funkcjonalno≈õci row-level-security. znajomo≈õƒá technik agregacji i przetwarzania danych dobra znajomo≈õƒá jƒôzyka Python Mile widziane: do≈õwiadczenie w bran≈ºy medycznej z systemami przetwarzajƒÖcymi du≈ºe wolumeny danych certyfikat z obszaru znajomo≈õci Power BI Co oferujemy? stabilne zatrudnienie w oparciu o kontrakt B2B s≈Çu≈ºbowy laptop i monitor dofinansowanie prywatnej opieki medycznej sportowƒÖ kartƒô Multisport nauka jƒôzyka angielskiego omawianie postƒôp√≥w i rozwoju co p√≥≈Ç roku transparentna komunikacja z pracownikami mo≈ºliwo≈õƒá zaanga≈ºowania siƒô w rozw√≥j organizacji chƒôtnie dzielimy siƒô wiedzƒÖ - do≈ÇƒÖcz do Akademii Eyzee mocny kompetencyjnie zesp√≥≈Ç sk≈ÇadajƒÖcy siƒô w wiƒôkszo≈õci z senior√≥w praca z narzƒôdziami JIRA, Confluence, BitBucket dbamy o integracje i chƒôtnie wsp√≥lnie spƒôdzamy czas Kim jeste≈õmy? Jeste≈õmy polskƒÖ firmƒÖ specjalizujƒÖcƒÖ siƒô w realizacji z≈Ço≈ºonych projekt√≥w informatycznych oraz doradczych dla firm z sektora finansowego, telekomunikacyjnego i publicznego. Stanowimy zgrany zesp√≥≈Ç konsultant√≥w z wiedzƒÖ i wieloletnim do≈õwiadczeniem w tworzeniu i utrzymywaniu rozwiƒÖza≈Ñ. Wa≈ºne dla nas sƒÖ: doprecyzowanie wymaga≈Ñ przed napisaniem kodu, jako≈õƒá tworzonego kodu, testowanie oraz CI/CD. Nasze projekty to g≈Ç√≥wnie tworzenie nowych mikroserwis√≥w lub nowych funkcjonalno≈õci do istniejƒÖcych rozwiƒÖza≈Ñ. Dodatkowo rozwijamy w≈Çasne aplikacje i plugin‚Äôy, kt√≥re nie tylko usprawniajƒÖ pracƒô, ale te≈º pozwalajƒÖ rozwinƒÖƒá nasze do≈õwiadczenie. Jeden z nich mo≈ºesz pobraƒá tutaj (eZee Worklog). Jeste≈õmy partnerem Atlassian.","[{""min"": 15000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Unclassified,15000,22000,Net per month - B2B
Full-time,Senior,B2B,Remote,81,Analityk Danych,Detable,"Do≈ÇƒÖcz doDetable Sp. Z o.o.- prƒô≈ºnie rozwijajƒÖcej siƒô firmy, kt√≥ra stawia na d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô z do≈õwiadczonymi profesjonalistami.Od ponad 3 lat jeste≈õmy partnerem dla instytucji z sektora publicznego, wsp√≥≈Çpracujemym.in. zCentrum e-zdrowia, Aplikacjami Krytycznymi Ministerstwa Finans√≥w, G≈Ç√≥wnym Urzƒôdem Nadzoru Budowlanego, Urzƒôdem Do Spraw Cudzoziemc√≥w,Narodowym Funduszem Zdrowiai wieloma innymi. Nasi konsultanci pracujƒÖ nad rozwojem aplikacji i system√≥w, z kt√≥rych korzystajƒÖ miliony Polak√≥w! Posiadasz minimum 5 lata do≈õwiadczenia w pracy na stanowisku Analityka Danych / lub na stanowisku zwiƒÖzanym z analizƒÖ danych lub analizƒÖ biznesowƒÖ w projektach dla klienta masowego; Nie jest Ci obce przetwarzanie i analiza du≈ºych zbior√≥w danych (Big Data); Na co dzie≈Ñ pracujesz z SQL, Python, PySpark; Wykorzystujesz w codziennej procesy ETL/ELT; Mia≈Çe≈õ okazjƒô pracy w Data Quality; Biegle pos≈Çugujesz siƒô relacyjnymi bazami danych; Mo≈ºesz pochwaliƒá siƒô znajomo≈õciƒÖ i do≈õwiadczeniem w obszarze ochrony zdrowia; Posiadasz do≈õwiadczenie w obszarze Hurtowni Danych; Dodatkowym atutem bƒôdzie je≈õli mo≈ºesz pochwaliƒá siƒô jednym z certyfikat√≥w: certyfikatAgilePM, certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Airflow/tworzenie DAG√≥w Airflow/Apache Spark, certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL. Analiza danych i ≈∫r√≥de≈Ç danych dla projekt√≥w IT; Analiza danych pod kƒÖtem ich jako≈õci i sp√≥jno≈õci; Okre≈õlanie przypadk√≥w u≈ºycia system√≥w informatycznych; Zbieranie i specyfikacja wymaga≈Ñ analityczno-raportowych; Projektowanie i dokumentowanie przep≈Çyw√≥w danych (ETL) oraz struktur raportowych; Zbieranie i specyfikacja wymaga≈Ñ analityczno-raportowych; Utrzymywanie modelu proces√≥w biznesowych w zakresie us≈Çug analityczno-raportowych; Zapewnienie sp√≥jno≈õci proces√≥w analizy i projektowania; Utrzymanie i rozw√≥j kanonicznego modelu danych; Ocenianie i analiza zbior√≥w danych; Przygotowywanie analiz i dokumentacji projektowej; Wsp√≥≈Çpraca z zespo≈Çem projektowym, wytw√≥rczym i interesariuszami w celu zapewnienia zgodno≈õci dzia≈Ça≈Ñ z oczekiwaniami i wymaganiami projektowymi w zakresie praktyki ZarzƒÖdzania Danymi; Proponowanie i konsultowanie rozwiƒÖza≈Ñ systemowych ze zleceniodawcami oraz realizatorami zdefiniowanych wymaga≈Ñ; Wsparcie analityczne na etapach projektowania, wytwarzania i testowania system√≥w informatycznych. Konkurencyjne wynagrodzenie w oparciu o kontrakt B2B ( do 140PLN netto/h); D≈ÇugofalowƒÖ wsp√≥≈Çpracƒô opartƒÖ o wzajemny szacunek i partnerstwo; Dedykowanego opiekuna kontraktu po stronie Detable; Mo≈ºliwo≈õƒá 100% pracy zdalnej lub z naszego biura w Bia≈Çymstoku; Mo≈ºliwo≈õƒá podnoszenia swoich kwalifikacji poprzez skorzystanie z bud≈ºetu szkoleniowego; Realny wp≈Çyw na rozw√≥j projektu; Atrakcyjny program polece≈Ñ pracowniczych; Zdalny proces rekrutacji. Rozmowa HR z naszƒÖ IT RekruterkƒÖ; Weryfikacja umiejƒôtno≈õci technicznych przez naszego Lidera technicznego; Spotkanie z klientem; Decyzja i rozpoczƒôcie wsp√≥≈Çpracy","[{""min"": 20160, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20160,23520,Net per month - B2B
Full-time,Mid,B2B,Remote,82,Data Engineer,best HR and PM solutions,"For our client we are looking forData Engineer. Client is the emerging leader in the $100B+ cloud communications platform market. Customers like Airbnb, Viber, Whatsapp, Snapchat, and many others depend on client's APIs and SDKs to connect with their customers all over the world. As businesses continue to shift to a real-time, customer-centric communications model, we are experiencing a time of impressive growth. They're looking for a Data Analyst or Data Engineer to join the Engineering Productivity team and help them make smarter, data-driven decisions that improve developer productivity and engineering outcomes. You'll work closely with development, platform, and product teams to analyze, structure, and optimize data flows related to engineering metrics, DevOps performance, and platform usage. This role is ideal for someone who thrives in transforming ambiguous data into actionable insight and is excited about the potential of AI tools in developer platforms. Key Responsibilities: Analyze engineering, operational, and productivity data to uncover trends, risks, and opportunities Design and implement data models that improve accessibility, structure, and long-term maintainability of engineering metrics. Build or enhance ETL pipelines to collect, transform, and export data from various systems (e.g., GitHub, Jira, Security Scans, Costs tools). Partner with stakeholders to define meaningful KPIs across engineering domains (e.g., reliability, security, velocity). Explore and implement GenAI tooling to support automation, summarization, and pattern detection in engineering workflows. Maintain data hygiene and enforce best practices in data governance and lineage within the API Engineering environment. What You‚Äôll Gain: A unique opportunity to shape how engineering data is used across a large and evolving platform organization. The chance to make a difference using GenAI tools in a real-world engineering context. Collaboration with a team driving developer experience, reliability, and engineering consistency at scale. Required Skills and Experience: Proven experience as a Data Analyst or Data Engineer, preferably in a software engineering or DevOps context. Strong SQL skills and experience with Python or another scripting language for data transformation and analysis. Hands-on experience working with APIs and integrating data across SaaS tools (e.g., Jira, GitHub, Datadog). Familiarity with dashboarding/visualization platforms like Looker, Grafana or Tableau. Demonstrated experience structuring unorganized or siloed data into actionable reporting models. Desirable: Experience designing and building ETL pipelines and data lakes or warehouses (e.g. Snowflake). Exposure to GenAI tooling and experience applying AI to engineering or operational workflows. Knowledge of modern data orchestration tools (e.g., Airflow, dbt). Understanding of software development lifecycle and metrics used in engineering productivity and platform health. What we offer: Contract: B2B directly with US company Salary: up to 160 pln/h 100% remote Polish time zone Polish public holidays Long term cooperation Recruitment process: 1-2 technical calls","[{""min"": 120, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,85,Remote Data Engineer,Ework Group,"Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking forData Engineer with AWS experience - remote worküîπ At our Digital Data & IT, we‚Äôre on a mission to unlock the full potential of data towards helping more patients worldwide. It's an ambitious mission that requires many bright minds to succeed. So, we warmly welcome your data engineering skills to help us unfold smarter solutions that better support the business needs and, in the end, save more lives. Do you want to join a purpose-driven ride? Then read on. ‚úîÔ∏èWhat you will be doing: As a Data Engineer, you will be an integral member of the Data Harmonisation Layer (DHL) Agile team. DHL is a global platform designed to collect and harmonize of data from various Operational Technology (OT) data sources across company production facilities. DHL makes data easily accessible to develop several front-end Manufacturing Intelligence (MI) products, focusing on improving our production processes. Key technologies used in DHL are Kafka, Ignition, and AWS services. ‚úîÔ∏è Your main responsibilities include: Software development activities i.e., writing, reviewing, refactoring, testing, and documenting the code base Build production ready stable and scalable data pipelines Act as a trusted advisor to provide technical expertise on one or more projects for our business partners Work with the Agile teams to understand and handling enabler work and work towards technical agility Contribute in Agile events such as Program Increment (PI) planning, system demos, and Inspect and Adapt (I&A) We are looking forAn ambitious and proactive colleague who can contribute to the team both professionally and personally. ‚úîÔ∏è On a professional level, you have: A master‚Äôs degree in computer science, engineering, chemistry, biology, or other relevant fields Experienced with data engineering principles such as data warehousing, batch processing, data streaming, data lakes, databases, and data modelling Experience with building CI/CD pipelines Coding/scripting skills (Bash, Python, or similar) Hands-on experience in AWS services such as Lambda, Glue, IAM, Kinesis, MKS, Step Functions, DMS, RDS, Managed Grafana, SNS, S3, CloudWatch, etc Hands-on experience with IaC for managing the cloud Strong mathematical, statistical, and problem-solving skills ‚úîÔ∏è On a personal level, you are: Strategic mindset Innovative mindset Strong communicator Systematic ‚úîÔ∏èThe team waiting for you... You will be part of the Manufacturing Intelligence (MI) department. Our vision is to enable data using cutting-edge technologies and empower our colleagues in Product Supply to make data-driven decisions to continuously improve manufacturing processes, in short #EnableDataEmpowerDecisions. ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 162, ""max"": 179, ""type"": ""Net per hour - B2B""}]",Data Engineering,162,179,Net per hour - B2B
Full-time,Senior,Permanent,Hybrid,86,Senior Data Engineer,Sparta Global,"Senior Data Analytics Specialist ‚Äì Financial Services (Krak√≥w) Join one of our leading financial clients in Krak√≥w and play a key role in delivering accurate, timely, and insightful data and management information. You‚Äôll be supporting Risk, Compliance, and Finance functions within a major global financial institution. Role Overview As a Data Analytics Specialist, you will be responsible for ingesting data from internal and external sources, performing quality assessments, and creating visualisations. The role also includes peer benchmarking, platform security, capacity management, and active contribution to global risk data projects. Key Responsibilities Perform data analysis, ensuring data integrity and suitability for intended use. Collaborate with model development and monitoring teams to understand data requirements and governance standards. Contribute to planning the data and business process architecture roadmap. Prepare and present insights to senior stakeholders, fostering strong cross-functional relationships. Identify opportunities for process improvement and drive efficiency initiatives. About You ‚Äì Qualifications & Experience A bachelor‚Äôs degree in IT, Computer Science, or a numerate discipline. At least 5 years‚Äô experience in data analytics or a related role. Proficiency in programming and data tools such as SAS, Python, PySpark, and SQL. Solid understanding of business analysis, data architecture, and project management principles. Strong organisational and analytical skills with the ability to manage multiple tasks effectively. Familiarity with JIRA, Confluence, and an aptitude for working under pressure. Fluency in English (spoken and written).","[{""min"": 20000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,27000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,87,Senior MLOps Engineer (Azure/ AWS),Scalo,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania tom.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! budowa nowoczesnej Platformy Analitycznej opartej o Azure Databricks w domenie finansowej, implementacja narzƒôdzi monitorujƒÖcych dzia≈Çanie modeli ML w ≈õrodowisku produkcyjnym, bliska wsp√≥≈Çpraca z zespo≈Çami Data Science w zakresie MLOps i platformy zapewnienie wysokiej dostƒôpno≈õci, bezpiecze≈Ñstwa i skalowalno≈õci platformy ML, implementacja proces√≥w trenowania, wdra≈ºania, wersjonowania i monitorowania modeli w ≈õrodowisku chmurowym, integracja rozwiƒÖza≈Ñ ML z innymi systemami biznesowymi w organizacji, udzia≈Ç w d≈Çugoterminowej rozbudowie i doskonaleniu Platformy Analitycznej, praca 100% zdalna, a dla chƒôtnych mo≈ºliwo≈õƒá pracy z biura we Wroc≈Çawiu, stawka do 200 z≈Ç/h przy B2B, w zale≈ºno≈õci od do≈õwiadczenia. masz minimum 3-4 lat do≈õwiadczenia w MLOps oraz budowie platform ML w chmurze (Azure preferowane, AWS tak≈ºe akceptowalne), znasz bardzo dobrze Azure Databricks (konfiguracja, utrzymanie, optymalizacja koszt√≥w), posiadasz umiejƒôtno≈õci programistyczne w Python i SQL (R bƒôdzie dodatkowym atutem), pracowa≈Çe≈õ z MLFlow, Spark, Rest API oraz narzƒôdziami CI/CD i automatyzacjƒÖ pipeline‚Äô√≥w, masz do≈õwiadczenie z Docker, Kubernetes; Terraform bƒôdzie plusem, znasz cykl ≈ºycia modeli ML od trenowania po wdro≈ºenie i monitoring, pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie B1/B2 (dokumentacja, okazjonalne spotkania). d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 26880, ""max"": 33600, ""type"": ""Net per month - B2B""}]",Data Science,26880,33600,Net per month - B2B
Full-time,Mid,B2B,Remote,88,BI Consultant (Power BI),Onwelo Sp. z o.o.,"Poznaj Onwelo: Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Bƒôdziesz pe≈Çniƒá zar√≥wno rolƒô konsultanta, jak i developera, pomagajƒÖc organizacjom w optymalizacji proces√≥w raportowania i podejmowaniu ≈õwiadomych decyzji opartych na danych. Do≈ÇƒÖcz do zespo≈Çu Data & Analytics w Onwelo, kt√≥ry realizuje projekty dla r√≥≈ºnych klient√≥w ‚Äì zar√≥wno polskich, jak i zagranicznych. W zale≈ºno≈õci od Twoich kompetencji i dostƒôpno≈õci bƒôdziesz mieƒá mo≈ºliwo≈õƒá pracy nad wdro≈ºeniami Power BI do r√≥≈ºnych organizacji, migracjƒÖ narzƒôdzi BI (np.Tableau) do Power BI, wsparciem biznesu w definiowaniu potrzeb analitycznych oraz budowƒô nowoczesnych hurtowni danych. Tworzyƒá i rozwijaƒáraporty oraz dashboardy w Power BIzgodnie z wymaganiami biznesowymi. Modelowaƒá dane, tworzyƒázapytania DAXoraz optymalizowaƒá wydajno≈õƒá raport√≥w. Projektowaƒá i wdra≈ºaƒáprocesy ETLdo przekszta≈Çcania oraz ≈Çadowania danych. Integrowaƒá danez r√≥≈ºnych ≈∫r√≥de≈Ç ‚Äì baz danych SQL, plik√≥w Excel, us≈Çug chmurowych i system√≥w ERP/CRM stanie siƒô czƒô≈õciƒÖ Twojej codziennej pracy. Wsp√≥≈Çpracowaƒá z zespo≈Çami biznesowymi i technicznymi, aby zapewniƒáefektywne raportowanie i analizƒô danych. Utrzymywaƒá i optymalizowaƒáistniejƒÖce rozwiƒÖzania BI, dbajƒÖc o ich wydajno≈õƒá oraz skalowalno≈õƒá. Pracowaƒá nadmigracjƒÖ danych i system√≥w BI, np. przenoszeniem raport√≥w zTableau lub Qlik do Power BI. Braƒá udzia≈Ç wbudowie i rozwoju hurtowni danych np. Snowflake, modelujƒÖc i integrujƒÖc dane. Doradzaƒá w zakresienajlepszych praktyk BIoraz wspieraƒá klient√≥w w podejmowaniu≈õwiadomych decyzji opartych na danych. Maszmin. 3 lata do≈õwiadczeniaw pracy zPower BI‚Äì tworzysz raporty, dashboardy i modelujesz dane. ZnaszDAX oraz Power Queryi potrafisz je wykorzystaƒá do analizy oraz transformacji danych. Swobodniemodelujesz danei projektujeszoptymalne struktury raportowe. Posiadasz do≈õwiadczenie w pracy zbazami danych SQL‚Äì tworzysz zapytania, optymalizujesz je i integrujesz dane. Znasz procesyETLi masz praktyczne do≈õwiadczenie wprzekszta≈Çcaniu oraz ≈ÇƒÖczeniu danychz r√≥≈ºnych ≈∫r√≥de≈Ç. Rozumiesz potrzeby biznesowe i potrafisz przek≈Çadaƒá je nakonkretne rozwiƒÖzania analityczne. Pos≈Çugujesz siƒô jƒôzykiemangielskim na poziomie min. B2, co pozwala Ci pracowaƒá w miƒôdzynarodowym ≈õrodowisku. Dodatkowym atutem bƒôdzie, je≈õli: Masz do≈õwiadczenie wmigracji narzƒôdzi BI(np.Tableau, Qlik) do Power BI. ZnaszDynamic 365 Praca zeSnowflakenie jest Ci obca Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 800, ""max"": 950, ""type"": ""Net per day - B2B""}]",Data Analysis & BI,800,950,Net per day - B2B
Full-time,Senior,B2B,Hybrid,89,Data Engineer,Unit8,"Who We Are Founded in 2017, Unit8 is a fast-growing Swiss AI and data analytics consulting and services company dedicated to solving complex problems of traditional industries like automotive, chemical, financial services, manufacturing and pharma. We work with some of the biggest organisations in Europe to solve the challenges that directly affect their business - be it operations, finance, manufacturing or R&D. Since our foundation, we have successfully delivered more than 180 projects and have grown to 140+ talented individuals across 6 office locations. Unit8's mission is to drive the adoption of AI and Data Science in the non-digital industries and to help accelerate their digital transformation. Among its activities, Unit8 dedicates a part of its resources and time on projects that deeply matter to us - that includes collaboration on pro-bono and ""engineering for good"" causes. You can learn more about what we are passionate about at Unit8 Talks and on our website you can find more useful information about our business and recruiting process. These are some examples of projects that we have been working on Building a real-time production line monitoring system for the pharmaceutical industry in order to improve the throughput of the production processes for the Pharmaceutical Industry. Technologies: AWS, EKS, CloudFormation, Docker, Python Building system for calculating and delivering global climate scores i.e. estimating the impact of climate change on the risk of natural disasters (i.e. floods, wildfires & droughts) for the Insurance Industry. Technologies: PySpark, Python, Proprietary Data Processing Platform Building a modern data science platform including a data lake for Chemical Industry. Technologies: Azure, ML Studio, Data Factory, Databricks, Spark About You As a member of agile project teams, your mission will be to build solutions and infrastructure aiming at solving the business problems of our clients. You are a proficient software engineer who knows the fundamentals of computer science and you master at least one widely adopted programming language (Python, Java, C#, C++). You know how to write distributed services and work with high-volume heterogeneous data, preferably with distributed systems such as Spark. You are knowledgeable about data governance, data access, and data storage techniques. You have strong client-facing skills: comfortable interacting with clients (business & technical audience), delivering presentations, problem-solving mindset. You are willing to travel to meet with our clients and the team (mainly in Europe - up to 10% of your time). You are eligible to register as a sole trader (self-employment) in Poland. Don't worry if you don't know how to do the registration, we can help with that. What You'll Do Design, build, maintain, and troubleshoot data pipelines and processing systems that are relied on for both production and analytics applications, using a variety of open-source and closed-source technologies. Help drive optimization, testing, and tooling to improve data quality. Collaborate with other software engineers, ML experts, and stakeholders, taking learning and leadership opportunities that will arise every single day. Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives. What we offer Opportunity to shape the expansion of one of the leading Swiss data & AI consultancies Compensation package including base salary, yearly bonus based on the both individual + company performance Above the norm flexibility regarding when and from where you work as long as both client and internal commitments are met Work on cutting-edge data, AI & analytics topics (e.g., Generative AI) that have real impact across industries Dedicated time and budget for training and pro-bono projects 30 paid days off Private health care and Multisport Cross offices/company-wide frequent events (off-site or online) as well as quarterly budget to spend with the team on after-work activities","[{""min"": 20000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,20000,26000,Net per month - B2B
Full-time,Mid,B2B,Remote,90,Oracle SQL/APEX Developer,Pretius,"WPretiusposzukujemyOracle SQL/APEX Developeraw projekcie platformy w obszarze Healthcare na rynku UK. Lokalizacja: zdalnie lub Warszawa Wynagrodzenie: 90-150 pln netto/h O projekcie: Wsp√≥≈Çpraca z zespo≈Çem projektowym, szacowanie zmian, definiowanie API, wsparcie test√≥w Tworzenie formatek APEX, zapyta≈Ñ SQL i pakiet√≥w PL/SQL Wykorzystanie technologii frontendowych do budowy UI Udzia≈Ç w R&D i wprowadzaniu nowych rozwiƒÖza≈Ñ technicznych Stack: SQL, PL/SQL, Oracle APEX (wer. 18+), Oracle Cloud Oczekiwania: Dobra znajomo≈õƒá relacyjnych baz danych (struktury, SQL, PL/SQL - funkcje, procedury, pakiety) Do≈õwiadczenie w budowaniu aplikacji web komunikujƒÖcych siƒô z bazƒÖ danych Znajomo≈õƒá standard√≥w w obszarach test√≥w, CI/CD, jako≈õci kodu Podstawowa znajomo≈õƒá HTML/CSS/JS Jƒôzyk angielski - B2+ Co oferujemy w Pretius? Stawiamy na d≈Çugofalowe relacje oparte na uczciwych zasadach i rzetelno≈õci Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Mo≈ºliwo≈õƒá pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnƒôtrzne, konferencje, certyfikacje","[{""min"": 90, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,90,150,Net per hour - B2B
Full-time,Senior,B2B,Remote,91,Senior Administrator Baz Danych Oracle / Senior Oracle DBA,Simora,"Do≈ÇƒÖcz do naszego zespo≈Çu jako Senior Administrator Baz Danych Oracle! Poszukujemy osoby, kt√≥ra zajmie siƒô zarzƒÖdzaniem bazami danych naszych klient√≥w. Jeste≈õmy firmƒÖ, kt√≥ra rozwija nowoczesne rozwiƒÖzania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzƒôdzi wspierajƒÖcych naszych klient√≥w. Cenimy pozytywnƒÖ atmosferƒô, wzajemny szacunek i zaanga≈ºowanie ‚Äì to fundamenty naszego zespo≈Çu. Tw√≥j zakres obowiƒÖzk√≥w Tworzenie baz danych oraz ≈õrodowisk bazodanowych na ≈õrodowisku produkcyjnym i testowym Migrowanie baz danych na nowe ≈õrodowiska ZarzƒÖdzania i aktualizacja baz danych i ≈õrodowisk bazodanowych Konfigurowanie i optymalizacja ≈õrodowiska bazodanowego Automatyzacja zada≈Ñ za pomocƒÖ jƒôzyk√≥w skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jako≈õci i bezpiecze≈Ñstwa dla tworzonych rozwiƒÖza≈Ñ Wdra≈ºanie rozwiƒÖza≈Ñ opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Sta≈Çe doskonalenie sposobu Twojej pracy Nasze wymagania Min 5 letnie do≈õwiadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomo≈õƒá jƒôzyka SQL i PLSQL Umiejƒôtno≈õƒá dbania o szczeg√≥≈Çy i jako≈õƒá rozwiƒÖza≈Ñ Komunikatywno≈õƒá Znajomo≈õƒá jƒôzyka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykszta≈Çcenie wy≈ºsze (preferowany kierunek: informatyka) Znajomo≈õƒá jƒôzyk√≥w programowania: Python, Java itp. Znajomo≈õƒá system√≥w operacyjnych Linux / Windows Znajomo≈õƒá system√≥w wirtualizacyjnych np.: OLVM Oferujemy CiekawƒÖ pracƒô w firmie o wysokiej dynamice rozwoju Mo≈ºliwo≈õƒá podniesienia kwalifikacji w obszarach zwiƒÖzanych z bazami danych, bezpiecze≈Ñstwem danych oraz sztucznƒÖ inteligencjƒÖ Benefity Karta Multisport Prywatna opieka zdrowotna ‚Äì Medicover Praca zdalna Brak dress code‚Äôu Dofinansowanie szkole≈Ñ i kurs√≥w Elastyczny czas pracy","[{""min"": 10000, ""max"": 16000, ""type"": ""Net per month - B2B""}]",Database Administration,10000,16000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,92,Data Visualization Specialist (Tableau),ITDS,"Data Visualization Specialist (Tableau) Join us, and shape how data tells its story! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As a Data Visualization Specialist, you will be working for our client, a leading global financial institution focused on optimizing financial resource management through innovative IT solutions. You will be joining a high-performing team to design and deliver advanced data visualizations that turn complex datasets into meaningful insights. Your work will directly support strategic decision-making by developing and maintaining dashboards, ensuring data integrity, and promoting best practices in data sharing. This is an exciting opportunity to shape visualization standards and influence how data is consumed across the organization. Your main responsibilities: Developing advanced dashboards and reports using Tableau Managing data sources to ensure accuracy and consistency Collaborating with stakeholders to gather analytical requirements Maintaining and overseeing project sites on Tableau Server Documenting dashboards, data sources, and development processes Promoting best practices for data sharing and user access management Ensuring optimal performance of Tableau workbooks for large datasets Supporting infrastructure and architectural requirements for Tableau deployment Following internal control standards and compliance procedures Working with cross-functional teams to deliver effective visual solutions You're ideal for this role if you have: 3+ years of professional experience in data visualization or analysis Proven proficiency in Tableau development and dashboard optimization Ability to handle large data sets while ensuring performance efficiency Experience managing Tableau Server project sites Strong analytical mindset with a high level of mathematical competence Familiarity with data governance and compliance standards Ability to collaborate effectively with cross-functional stakeholders Excellent communication and documentation skills Strong attention to detail and problem-solving abilities Demonstrated ability to work in fast-paced, high-stakes environments It is a strong plus if you have: Working knowledge of Tableau administration and architecture Experience developing dashboards in QlikSense Understanding of infrastructure best practices for enterprise data tools Familiarity with regulatory environments in financial institutions Knowledge of offshoring practices and documentation workflows We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7098 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 20800, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,20800,28000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,93,Senior Data Engineer,N-iX,"#3204 Join our team to work on enhancing a robust data pipeline that powers our SaaS product, ensuring seamless contextualization, validation, and ingestion of customer data. Collabd engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Client was established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client‚Äôs platform rates with product teams to unlock new user experiences by leveraging data insights. Engage with domain experts to analyze real-world empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renowned Scandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities : Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements : Programming: Minimum of 3-4 years as a data engineer, or in a relevant field Python Proficiency: Advanced experience in Python, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights Cloud: Familiarity with cloud platforms (preferably Azure) Data Platforms: Experience with Databricks, Snowflake, or similar data platforms Database Skills: Knowledge of relational databases, with proficiency in SQL. Big Data: Experience using Apache Spark Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability Version Control: Experience with Gitlab or equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have : Experience with Docker and Kubernetes Experience with document and graph databases Ability to travel abroad twice a year for an on-site workshops","[{""min"": 18359, ""max"": 30993, ""type"": ""Net per month - B2B""}, {""min"": 14776, ""max"": 26228, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18359,30993,Net per month - B2B
Full-time,Senior,B2B,Remote,94,Senior Data Engineer,emagine Polska,"Project information: Industry: Insurance and IT services Rate: up to 175 z≈Ç/h netto + VAT Location: remote work with occasional meetings (Warsaw) Project language: English Summary: Join a newly forming team responsible for building the next Data Hub in a large-scale enterprise environment. You‚Äôll have a direct impact on the architecture and development of a modern data platform based on a proven internal framework (Databricks + Azure). The platform supports a variety of use cases, from analytics and reporting to operational systems and Generative AI. We‚Äôre looking for an experienced Senior Data Engineer to shape the foundation of this new initiative and bring deep expertise in scalable data engineering solutions. Your Responsibilities: Data Hub Development: Implement a scalable Data Hub platform based on a company-wide framework using Azure and Databricks. Data Engineering: Build and optimize both batch and streaming data pipelines using Python and PySpark. Architectural Collaboration: Work closely with the Data & Solution Architect to implement enterprise-grade architecture. Technical Standards & Mentorship: Help define best practices, participate in code reviews, and support knowledge sharing within the team. Automation & CI/CD: Automate deployment and data operations using tools like Azure DevOps, Terraform, Docker, and Kubernetes. Data Quality & Monitoring: Ensure data validation, anomaly detection, and pipeline monitoring. Cross-Team Collaboration: Collaborate with multidisciplinary teams to align technical solutions with business goals. Documentation: Produce high-quality technical documentation in line with corporate standards. Must-Have Qualifications At least6 years of experienceas a Data Engineer in enterprise-scale environments. Proficiency inPythonandPySpark. Solid hands-on experience withMicrosoft Azure. Familiarity withAzure DevOpsand automation workflows. Practical knowledge ofDatabricks. Comfortable working incross-functional teams(e.g., architects, DevOps, analysts) within complex organizational structures. Fluent inEnglish(minimumB2 level). Nice to Have Hands-on experience withdbt(Data Build Tool) andDLT pipelinesin Databricks. Understanding ofmedallion architecturein a production context. Background inlarge enterprisesorconsulting firms, ideally with exposure to complex data ecosystems. Experience working inAgile/Scrumdevelopment environments. Experience with infrastructure as code (Terraform), containerization (Docker), orchestration (Kubernetes).","[{""min"": 160, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,175,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,95,Database Administrator MS SQL,emagine Polska,"INFORMACJE O PROJEKCIE: Bran≈ºa : Bankowo≈õƒá Lokalizacja : hybryda (1 x w tygodniu w biurze Krak√≥w lub Warszawa) Stawka: do 120 PLN/h netto + VAT (B2B) Szukamy do≈õwiadczonego administratora baz danych MS SQL, kt√≥ry posiada 3-5 letnie do≈õwiadczenie w tej roli. Idealny kandydat powinien wykazaƒá siƒô solidnymi umiejƒôtno≈õciami w administracji baz danych oraz dostosowywaniu system√≥w operacyjnych i silnika baz danych do z≈Ço≈ºonych wymaga≈Ñ. Administrator baz danych MS SQL bƒôdzie odpowiedzialny za zapewnienie ciƒÖg≈Ço≈õci dzia≈Çania baz danych oraz wsparcie podczas incydent√≥w i awarii. OBOWIƒÑZKI: Wdra≈ºanie i utrzymywanie system√≥w baz danych MS SQL Server. Wykonywanie zada≈Ñ administracyjnych, takich jak instalacja, konfiguracja i uaktualnienia. Monitorowanie wydajno≈õci baz danych oraz identyfikowanie problem√≥w. Wdra≈ºanie i utrzymywanie bezpiecze≈Ñstwa baz danych. Wsp√≥≈Çpraca z zespo≈Çami rozwoju i operacji. Planowanie i wykonywanie procedur tworzenia kopii zapasowych. Opracowywanie plan√≥w odzyskiwania po awarii. Automatyzowanie zada≈Ñ przy u≈ºyciu jƒôzyk√≥w skryptowych. Tworzenie dokumentacji operacyjnej i technicznej. WYMAGANIA: 3+ lat do≈õwiadczenia jako administrator bazy danych. Solidne do≈õwiadczenie z bazami danych MS SQL Server. Udokumentowane do≈õwiadczenie w implementacji i utrzymywaniu baz danych . Do≈õwiadczenie w dostrajaniu wydajno≈õci baz danych. Znajomo≈õƒá najlepszych praktyk bezpiecze≈Ñstwa baz danych. Do≈õwiadczenie w procedurach tworzenia kopii zapasowych. Znajomo≈õƒá jƒôzyk√≥w skryptowych, takich jak PowerShell. Silne umiejƒôtno≈õci analityczne i rozwiƒÖzania problem√≥w. Doskona≈Çe umiejƒôtno≈õci komunikacyjne. Umiejƒôtno≈õƒá pracy w zespole. Do≈õwiadczenie z s ystemem operacyjnym Windows Server. Znajomo≈õƒá bankowo≈õci bƒôdzie atutem, ale nie jest obowiƒÖzkowa. Komunikatywna znajomo≈õƒá jƒôz. angielskiego. MILE WIDZIANE: Do≈õwiadczenie z us≈Çugami baz danych w chmurze. Do≈õwiadczenie z bazami danych PostgreSQL. Do≈õwiadczenie z systemem operacyjnym Linux. Do≈õwiadczenie z IBM InfoSphere Data Replication.","[{""min"": 100, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Database Administration,100,120,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,96,Senior Looker Specialist (LookML),Spyrosoft,"Requirements: Minimum 2 years of experience as a Data Engineer working with Google Cloud Platform (GCP) and cloud-based infrastructure. Hands-on experience with LookML and Looker modelling language. Proven experience in transforming and migrating data models from SSAS and Power BI Semantic Models to LookML. Ability to translate DAX expressions into LookML formulas. Deep understanding of GCP services and cloud computing architecture. Strong background in designing, building, and deploying cloud-based data pipelines, including ingestion from various data sources (e.g., relational databases). Proficiency in data modelling and database optimisation, including query tuning, indexing, and performance optimisation for efficient data processing and retrieval. Nice to Have: Experience with at least one orchestration and scheduling tool ‚Äì Airflow is strongly preferred. Familiarity with ETL/ELT processes and the ability to integrate data from multiple sources into a usable analytical format. Working knowledge of modern data transformation tools such as DBT and Dataform. Strong communication skills to collaborate effectively with cross-functional teams (data scientists, analysts, business stakeholders). Ability to translate technical concepts into business-friendly language and present findings. Experience leading or actively contributing to discussions with stakeholders to identify business needs and improvement opportunities. Relevant certifications in big data technologies and/or cloud platforms (GCP, Azure). Main responsibilities: Design and develop data models in LookML, adhering to best practices. Support business users in building Looker dashboards. Migrate existing data structures and models to LookML. Support and mentor team members in designing and managing LookML-based data models. Build efficient and optimised aggregations and calculations for analytics purposes. Optimise and continuously improve existing data models to enhance performance and usability.","[{""min"": 120, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Unclassified,120,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,99,PowerBI Developer,Spyrosoft,"Project description: Our Partner is a prestigious Saudi Arabian conglomerate that stands today as one of the Middle East‚Äôs most influential family businesses, blending commercial expansion, global partnerships, and impactful philanthropy. Over eight decades, it has diversified into seven major sectors, including automotive, energy and financial services, operating in over 30 countries. Together, we are looking for a skilled and proactivePower BI Data Engineersto join a cross-functional analytics team supporting a client from the automotive and mobility industry. This is a delivery-focused role requiring fast turnaround, strong attention to UX and quality, and close collaboration with both technical teams and non-technical stakeholders across various countries and cultures. Main responsibilities: Designing and delivering intuitive dashboards and reports that consolidate data from multiple business systems to provide critical insights for company leadership, Play a key part in helping decision-makers track performance across sales, revenue, margins, and regional brand performance. Must-Have Skills and Experience: AdvancedPower BIskills, including data modeling, DAX, and dashboard/report creation Proficient inSQLand experience working with relational databases Experience integrating data from APIs into reporting solutions Ability to design clean, user-friendly dashboards for both desktop and mobile formats Strong understanding of KPIs, metrics, and performance reporting in a business context Excellent communication skills ‚Äì able to present insights clearly to non-technical stakeholders Strong problem-solving skills and attention to detail Proactive, self-motivated, and able to manage work independently Comfortable working in fast-paced, delivery-driven environments with tight deadlines Culturally aware and respectful ‚Äì capable of working effectively in an international, multicultural team English proficiency at B2 level or higher (spoken and written) Familiarity with Microsoft Dynamics CRM and Business Central Experience working with external data sources like Google Analytics Background in the automotive industry or dealership operations Previous experience in Agile, cross-functional, or distributed teams Strong collaboration skills ‚Äì open to feedback and able to contribute constructively in a team setting Adaptable mindset ‚Äì comfortable with change and shifting priorities Interest in the automotive and mobility domain Our Partner is a prestigious Saudi Arabian conglomerate that stands today as one of the Middle East‚Äôs most influential family businesses, blending commercial expansion, global partnerships, and impactful philanthropy. Over eight decades, it has diversified into seven major sectors, including automotive, energy and financial services, operating in over 30 countries. Together, we are looking for a skilled and proactivePower BI Data Engineersto join a cross-functional analytics team supporting a client from the automotive and mobility industry. This is a delivery-focused role requiring fast turnaround, strong attention to UX and quality, and close collaboration with both technical teams and non-technical stakeholders across various countries and cultures.","[{""min"": 90, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,90,160,Net per hour - B2B
Part-time,Mid,B2B,Remote,100,Regular Data Scientist,P&P Solutions,"üöÄ We are looking for aData Scientistfor our client from the insurance sector to carry out apilot project in the area of predictive scoring. The project aims to assess the potential of predictive models within the client's environment. If the results of the pilot are positively evaluated, the client plans to extend the project to a production phase, which will includeoperationalizing machine learning models on the Microsoft Azure platformandintegrating them with Microsoft Dynamics. üïê Start no later than thebeginning of September, initially for25 man-days, with the option to extend for an additional 3 months. 2+ years of experiencein Data Science and/or Machine Learning Proficiency inPythonorR(knowledge of one is sufficient) Familiarity with theMicrosoft Azureplatform Strong analytical mindset and practical experience with predictive modeling Good communication skills and ability to work independently as well as in a team English language proficiency at B1/B2 level Key Technologies: Azure, Machine Learning, Python Performexploratory data analysis (EDA)on customer and insurance scoring data Build and evaluate 2‚Äì3 competitive machine learning models, including training and testing phases Prepare a summary of final results, including model performance metrics and key insights Validate the results with the clientto support a go/no-go decision for production deployment Rate: up to 135 PLN/hour net(B2B contract) Flexible payment method Short 14-day invoice payment term Opportunity to work oninteresting and growth-oriented projects","[{""min"": 100, ""max"": 135, ""type"": ""Net per hour - B2B""}]",Data Science,100,135,Net per hour - B2B
Full-time,Mid,B2B,Remote,101,Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Development and maintenance of a large platform for processing automotive data. A significant amount of data is processed in both streaming and batch modes. The technology stack includes Spark, Cloudera, Airflow, Iceberg, Python, and AWS. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Centralized reporting platform for a growing US telecommunications company. This project involves implementing BigQuery and Looker as the central platform for data reporting. It focuses on centralizing data, integrating various CRMs, and building executive reporting solutions to support decision-making and business growth. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. üöÄ Your main responsibilities: Develop and maintain a high-performance data processing platform for automotive data, ensuring scalability and reliability. Design and implement data pipelines that process large volumes of data in both streaming and batch modes. Optimize data workflows to ensure efficient data ingestion, processing, and storage using technologies such as Spark, Cloudera, and Airflow. Work with data lake technologies (e.g., Iceberg) to manage structured and unstructured data efficiently. Collaborate with cross-functional teams to understand data requirements and ensure seamless integration of data sources. Monitor and troubleshoot the platform, ensuring high availability, performance, and accuracy of data processing. Leverage cloud services (AWS) for infrastructure management and scaling of processing workloads. Write and maintain high-quality Python (or Java/Scala) code for data processing tasks and automation. üéØ What you'll need to succeed in this role: At least 3 years of commercial experience implementing, developing, or maintaining Big Data systems, data governance and data management processes. Strong programming skills in Python (or Java/Scala): writing a clean code, OOP design. Hands-on with Big Data technologies like Spark , Cloudera, Data Platform, Airflow, NiFi, Docker, Kubernetes, Iceberg, Hive, Trino or Hudi. Excellent understanding of dimensional data and data modeling techniques. Experience implementing and deploying solutions in cloud environments. Consulting experience with excellent communication and client management skills, including prior experience directly interacting with clients as a consultant. Ability to work independently and take ownership of project deliverables. Fluent in English (at least C1 level). Bachelor‚Äôs degree in technical or mathematical studies. ‚ûï Nice to have: Experience with an MLOps framework such as Kubeflow or MLFlow. Familiarity with Databricks, dbt or Kafka. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn ).","[{""min"": 15120, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Data Engineering,15120,21000,Net per month - B2B
Full-time,Senior,B2B,Remote,104,Senior Data Modeller,GS Services,Poszukujemy superbohater√≥w ‚Äì Senior Data Modeller‚Äô√≥w! ü¶∏‚Äç‚ôÄÔ∏èü¶∏‚Äç‚ôÇÔ∏è,"[{""min"": 180, ""max"": 210, ""type"": ""Net per hour - B2B""}]",Data Science,180,210,Net per hour - B2B
Full-time,Senior,B2B,Remote,105,Senior Data Engineer,Link Group,"Senior Data Engineer We are looking for a seasoned Senior Data Engineer to join a dynamic team that‚Äôs building a cutting-edge platform designed to enable automated decision-making and intelligent automation across the entire business lifecycle. This role offers a unique opportunity to help shape a transformative solution from the ground up. We're seeking individuals who are mission-driven, proactive, and passionate about data engineering and innovation. Key Responsibilities Partner with business stakeholders and product owners to gather data requirements and design scalable technical solutions. Build and maintain robust data models and schema structures to support analytics, business intelligence, and machine learning initiatives. Optimize data processing pipelines for speed, scalability, and cost-efficiency across cloud and on-premise environments. Ensure high data quality and consistency through validation frameworks, automated monitoring, and comprehensive error-handling processes. Collaborate closely with data analysts and data scientists to deliver reliable, well-structured, and easily accessible datasets. Stay informed of emerging trends, tools, and best practices in data engineering to help drive innovation and technical excellence. Maintain operational stability and system performance across data pipelines and platforms. Provide Level 3 production support when necessary, resolving critical data-related issues swiftly and effectively. Required Experience and Skills Minimum 8+ years of experience in data engineering, data architecture, or a similar technical role. Strong programming skills in SQL , Python , Java , or equivalent languages for data processing and pipeline development. Experience with both relational (e.g., PostgreSQL, SQL Server, Oracle) and NoSQL (e.g., MongoDB) databases, OLAP tools like Clickhouse , and vector databases (e.g., PGVector , FAISS , Chroma ). Expertise in distributed data processing frameworks such as Apache Spark , Flink , or Storm . Experience with cloud data solutions (e.g., Azure , AWS Redshift , BigQuery , Snowflake ) is highly desirable. Solid understanding of ETL/ELT pipelines , data transformation , and metadata management using tools such as Airflow , Kafka , NiFi , Airbyte , and Informatica . Proficiency in query performance tuning, profiling, and data pipeline optimization. Hands-on experience with data visualization platforms like Power BI , Tableau , Looker , or Apache Superset . Familiarity with DevOps principles, version control systems ( Git ), and CI/CD pipelines . Strong problem-solving skills, attention to detail, and the ability to work under pressure. Effective communicator with the ability to collaborate across multidisciplinary teams.","[{""min"": 200, ""max"": 250, ""type"": ""Net per hour - B2B""}]",Data Engineering,200,250,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,107,Senior Oracle Database Administrator,emagine Polska,"PROJECT DESCRIPTION: Work model: Hybrid (2 days/week form office Warsaw) Assignment type: B2B Start: ASAP Project length: long term Rate: 175 pln/h net + vat Project language: English Industry: Banking The Senior DBA will be responsible for managing Oracle-based applications, ensuring continuous service and integrity through implementation, testing, troubleshooting, and support for critical databases. RESPONSIBILITIES: Prepare the prePROD environment for enabling FSFO. Sync the DataGuard in case of issues or errors. Rebuild the Standby as needed. Create DB service and validate PDB-level services. Create a new database and migrate old PDB to a new container. Activate FSFO and troubleshoot related issues. Perform tests on prePROD, including switchover and failover. Apply DB-level patching. Execute changes related to OS (Unix processes, load management, daemon) as needed. Handle RAC issues with disaster recovery. Set up OGG Microservices and OEM. Implement changes in production. MUST HAVE: 10+ years of managing Oracle Database versions 12c to 19c (RAC, Data Guard). Operating system experience in Linux. Proficiency with Oracle management tools (Data Guard, RMAN, Data Pump). Understanding of architecture design principles. Strong problem-solving skills and ability to work independently and in a team. Experience with creating PDB services and handling Data Guard issues. Strong practical experience in a production RAC (ASM) environment (19c). Self-starter with attention to detail. Knowledge of Oracle GoldenGate. Understanding of storage systems. Knowledge on FSFO problem-solving, observers, and change/incident management processes. Experience with Oracle SR handling and OEM architecture.","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Database Administration,150,175,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,108,BI Backend Developer,emagine Polska,"PROJECT INFORMATION: Industry: Construction Assignment type: B2B Start: 1st April 2025 Work model: Hybrid model (3 days/week in office - Warsaw) Project length: 12 months + extensions Project language: English We are looking for a dedicated BI Developer to join our Danish client's BI scrum team of six members, who develop and maintain our BI back-end solutions. The team you will join works in close collaboration with our highly innovative departments and companies, where you identify and build the foundation for their new BI reports. Reports covering everything from working environment, diversity, IOT machine data to the more financial. Our team is at the forefront of digitization and automation, focused on streamlining the way we work by processing and presenting data through Business Intelligence tools. The role involves collaborating with various departments to develop insightful BI reports that enhance decision-making and operational efficiency. Main Responsibilities Develop and maintain BI back-end solutions. Collaborate with various teams to identify and implement new BI reporting frameworks. Analyze data trends and provide actionable insights to colleagues. Facilitate the transition to cloud environments and integrate new data sources. Contribute to the development and execution of our digital strategies. Identify and implement new data sources and improve existing data frameworks. Work closely with cross-functional teams to define requirements for BI initiatives. Analyze complex data sets and translate findings into actionable insights. Contribute to cloud integration and Data Lakehouse projects. Key Requirements +3 years of experience in BI area Experience in Business Intelligence and data analysis. Strong understanding of Python and SQL at an advanced level. Strong Experience with Databricks Basic experience with DevOps practices Knowledge of cloud-based solutions, particularly Azure. Capacity to perform dimensional data modeling. Experience with structured and unstructured data sources (API, SQL). Adept at communicating complex ideas effectively. Good problem-solving skills and a proactive attitude toward technological advancements. Nice to Have Familiarity Data Factory. Knowledge of MS Dynamics and process automation. Familiarity with Power BI and data transformation tools.","[{""min"": 170, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,180,Net per hour - B2B
Full-time,Manager / C-level,B2B,Remote,109,Lead AI Data Engineer,dotLinkers,"Position: Lead AI Data Engineer Employment: B2B Working model: Remote Our client is a rapidly growing, innovation-driven real estate company with a strong focus on customer experience and sustainable asset design. Operating across the UK, Spain, Portugal, and expanding into Germany and the Netherlands, the company is committed to leveraging technology and data to enhance business decisions, asset performance, and customer service. Their dedicated data and innovation unit is building a cloud-based, AI-enabled data platform to support smarter investment strategies and scalable data infrastructure. Role Overview We are currently seeking aLead AI Data Engineerto join the team on a B2B basis. This is a hands-on, full-stack data engineering role that combines leadership, architecture, and AI/ML deployment to scale a dynamic data platform. The successful candidate will lead a small, talented team and play a critical role in shaping the technological future of real estate investment. Responsibilities: Lead and mentor a growing team of data engineers and analysts. Design and implement scalable, cloud-native infrastructure (GCP) to support AI/ML-powered data solutions. Develop and optimize end-to-end data pipelines to extract real-time, actionable insights. Collaborate with domain experts to align data architecture with business needs. Maintain data SLAs and ensure accessibility and usability for investment analysts and decision-makers. Drive innovation through the deployment of GenAI and ML models to solve real business problems. Requirements: 5+ years of hands-on experience as a Senior or Lead Data Engineer with a focus on AI/ML solutions. Proven track record in designing, coding, and deploying scalable data architectures. Proficiency in Python (with experience in key libraries), JavaScript, and SQL. Strong understanding of CI/CD, cloud-native infrastructure, and back-end systems. Master‚Äôs degree in a STEM field, preferably Data Science, Data Engineering, or Computer Science. Demonstrated ability to coach and lead a team (3+ years of mentoring experience). GCP certification or experience with Google Cloud Platform and its AI/ML tools is a strong plus. Full-stack mindset with experience building robust, production-grade data pipelines. The offer: Full-time engagement on a B2B basis. Competitive compensation: ¬£6,500 ‚Äì ¬£7,500 per month. Work remotely with minimal travel (2 days/month in London). Work in a fast-paced, innovation-first environment with a collaborative team. Opportunity to shape a cutting-edge data platform at the intersection of AI and real estate. Direct collaboration with company founders and strategic stakeholders for high-impact outcomes.","[{""min"": 31843, ""max"": 36741, ""type"": ""Net per month - B2B""}]",Data Science,31843,36741,Net per month - B2B
Full-time,Mid,B2B,Hybrid,110,ML Platform / Data Engineer,Beesafe,"About Us: Join our trailblazing team as we expand our digital horizons. From our beginnings as visionary InsurTech to becoming a key player in the Polish digital insurance market, we are part of the esteemed Vienna Insurance Group. We're redefining the rules in the insurance industry with our innovative approach. Our hybrid working model supports both the collaborative energy of office work and the flexibility of remote work. About the Role: We're seeking passionate Data Engineer, both at mid and senior levels, to join our team. You'll be at the heart of developing and implementing high-quality application software, using state-of-the-art tools and technologies. This is your chance to make a significant impact on one of the most exciting and unique products in the Polish digital insurance market. You‚Äôll be leadingcutting-edge tech initiativesin the Azure Cloud, using state-of-the-art tools and methodologies. You‚Äôllown your work, influencing the project‚Äôsarchitecture, strategy, and direction. You‚Äôll play a key role inshaping MLOps/LLMOps pipelines, improving data availability, and ensuring seamless collaboration between analytics and IT teams. You‚Äôll be joining a team ofcustomer-focusedData Analysts, Data Engineers, and Machine Learning Engineers to drive one of the most exciting products on the Polish insurance market. Proven experience as a Data Engineeror a strong aspiration to take on aleadership rolein cloud-based data and AI projects. Commercial experience in building Data, Analytics, or MLOps/LLMOps Platforms in the Cloud(Azure preferred). Hands-on expertise inmachine learning model lifecycle management, deployment, and monitoring in a cloud environment. Practical knowledge ofopen-source Big Data tools(e.g., Kafka, Airflow, Presto, Spark). +3 years of programming experience(Python preferred, but Java/Scala experience with a willingness to learn Python is also welcome). Experience indatabase development, data model design, and distributed systems. Strongbusiness acumen and collaboration skills, with a proactive approach to addressing service needs and operational demands. Understanding ofAgile and Scrum principles(we work in Scrum, by the way). Hands-on experience inMLOps/LLMOps, including model versioning, CI/CD pipelines for ML, and monitoring of deployed AI models. Familiarity withDatabricks, BI Tools, and Azure Machine Learning Services. Experience withDevOps, Kubernetes, and microservices architectures. Strongmentoring skills, supporting other engineers in their professional growth. Passion forinformal discussions over coffee or tea‚Äîwe believe great ideas start in a relaxed atmosphere! Why Join Us? Be part of a dynamic team driving digital innovation in the insurance and eCommerce sectors Opportunity to work in a collaborative and forward-thinking environment Contract options: B2B cooperation Engage in meaningful work that directly impacts business success Join a company that values work-life balance and fosters a positive team culture Comprehensive onboarding, including a dedicated Buddy program Remote work flexibility with hybrid office visits Flexible working hours Access to the latest tools and cloud-native solutions A comprehensive benefits package, including health insurance and MultiSport card Employee discounts on insurance products Referral program and sports club memberships Join us and help shape the future ofAI-driven insurance solutions! üöÄ","[{""min"": 18000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Science,18000,23000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,111,Data Visualization Specialist,ITDS,"Data Visualization Specialist (Qlik) Join us, and bring data to life through smart visuals Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As a Data Visualization Specialist , you will be working for our client, a globally recognized financial institution that is transforming how finance teams leverage data for strategic resource management. You will be contributing to the development of best-in-class visualization solutions by building advanced dashboards and reports in Qlik. Working closely with analysts and engineers, you will translate complex data into intuitive visual tools that support high-impact decision-making. This is a great opportunity to work in a dynamic, fast-paced environment where innovation, precision, and collaboration are key. Your main responsibilities: Developing advanced dashboards and reports using Qlik and QlikSense Managing data sources to ensure accuracy and integrity Collaborating with IT analysts and engineers to define visualization requirements Maintaining and overseeing project sites on Qlik Server Documenting processes, dashboards, and data sources clearly and thoroughly Promoting data sharing best practices and managing user access controls Ensuring optimal performance of Qlik workbooks with large datasets Supporting infrastructure and deployment requirements for Qlik tools Following internal control standards and compliance procedures Engaging with stakeholders to gather feedback and implement improvements You're ideal for this role if you have: 3+ years of experience in data analysis or data visualization Proficiency in Qlik and hands-on experience with QlikSense Ability to work with and visualize large, complex datasets efficiently Experience managing Qlik Server environments and deployments Strong analytical skills and a high level of mathematical competence Excellent collaboration and communication skills Ability to document technical and process information effectively Understanding of compliance and internal control frameworks Experience working in fast-paced environments with tight deadlines Proactive attitude and ability to solve problems independently It is a strong plus if you have: Working knowledge of Qlik architecture and administration Experience with data governance and regulatory compliance in finance Familiarity with offshoring workflows and documentation standards Background in financial services or resource management analytics Ability to influence stakeholders and drive data visualization best practices We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7099 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 20800, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,20800,28000,Net per month - B2B
Full-time,Mid,B2B,Remote,112,Hadoop/Big Data Devops,Detable,"Posiadasz minimum 5 lat do≈õwiadczenia na stanowisku DevOps Engineer, w projektach dla klienta masowego, zwiƒÖzanych z przetwarzaniem du≈ºych wolument√≥w danych; Realizowa≈Çe≈õ projekty zwiƒÖzane z klastrami High Availability i HDP ( tu integracja z Karberos/ AD); Bra≈Çe≈õ udzia≈Ç w wdro≈ºeniu standardami CI/CD; Biegle pos≈Çugujesz siƒô oprogramowaniem ekosystemu Apache Hadoop Bigtop (hdfs, spark, yarn, mapreduce, hive, ranger) w zakresie instalacji i konfiguracji; Swobodnie poruszasz siƒô u≈ºywajƒÖc Ansible; Posiadasz praktyczne umiejƒôtno≈õci z Kubernetes, Docker, Grafana/Prometheus, PostrgreSQL (EDB), Airflow, JupyterLab, Zeppelin ‚Äì(instalacja, administracja i zarzƒÖdzanie); Masz udokumentowane do≈õwiadczenie w zakresie pisania skrypt√≥w shellowych; Mia≈Çe≈õ okazje pracowaƒá w obszarze Big Data, Hurtowni Danych i ZarzƒÖdzania Danymi minimum 3 lata; Nie jest Ci obca platforma programistyczna Hadoop Cloudera/Hortonworks; Z ≈Çatwo≈õciƒÖ odnajdujesz siƒô w programowaniu proces√≥w Apache Spark w Python(lub Scala); Pracowa≈Çe≈õ z r√≥≈ºnymi formatami danych (np. JSON, ORC, PARQUET); Biegle pos≈Çugujesz siƒô jƒôzykiem polskim. Praca w ≈õrodowisku bazodanowym Big Data, Spark, Apache Hadoop; Projektowanie, implementacja i optymalizacja proces√≥w przetwarzania danych w ≈õrodowiskach Big Data; Instalacja, konfiguracja, zarzƒÖdzanie klastrem Hadoop; Instalacja bibliotek w klastrze HDP a tak≈ºe monitorowanie wydajno≈õci i aktywne zarzƒÖdzanie oraz strojenie klastra HDP; Projektowanie i zarzƒÖdzanie hurtowniami danych oraz zapewnienie jako≈õci danych; ZarzƒÖdzanie klastrem Kubernetes; Integracja danych z wielu ≈∫r√≥de≈Ç o r√≥≈ºnych formatach (m.in. JSON, PARQUET, ORC, AVRO); Tworzenie zaawansowanych zapyta≈Ñ SQL oraz optymalizacja dostƒôpu do danych; Wykorzystywanie odpowiednich technologii bazodanowych w zale≈ºno≈õci od scenariusza u≈ºycia; Udzia≈Ç w projektowaniu architektury danych oraz wdra≈ºaniu najlepszych praktyk zarzƒÖdzania danymi; RozwiƒÖzywanie problem√≥w i awarii klastra HDP; Monitorowanie i zapewnianie wysokiej wydajno≈õci system√≥w Big Data. Konkurencyjne wynagrodzenie w oparciu o kontrakt B2B ( 140-170pln netto/h); D≈ÇugofalowƒÖ wsp√≥≈Çpracƒô opartƒÖ o wzajemny szacunek i partnerstwo; Dedykowanego opiekuna kontraktu po stronie Detable; Mo≈ºliwo≈õƒá 100% pracy zdalnej lub z naszego biura w Bia≈Çymstoku; Mo≈ºliwo≈õƒá podnoszenia swoich kwalifikacji poprzez skorzystanie z bud≈ºetu szkoleniowego; Realny wp≈Çyw na rozw√≥j projektu; Atrakcyjny program polece≈Ñ pracowniczych; Zdalny proces rekrutacji. Rozmowa HR z naszƒÖ IT RekruterkƒÖ; Weryfikacja umiejƒôtno≈õci technicznych przez naszego Lidera technicznego; Spotkanie z klientem; Decyzja i rozpoczƒôcie wsp√≥≈Çpracy","[{""min"": 23520, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Unclassified,23520,28560,Net per month - B2B
Full-time,Mid,B2B,Hybrid,113,FinOps Specialist (GCP),Ness Solution,"Poszukiwany FinOps Specialist (GCP)! üìÖStart: wrzesie≈Ñ üåçLokalizacja: Warszawa + praca zdalna (min. 1 dzie≈Ñ/tydz. w biurze) üìÉUmowa B2Bdo 180 PLN/h üîçProjekt Do≈ÇƒÖcz do zespo≈Çu wspierajƒÖcego wdro≈ºenie ≈õrodowiska chmurowego wGoogle Cloud Platform‚Äì obejmujƒÖcego obszaryzarzƒÖdzania kosztami, struktury projektowej (Landing Zone) i bud≈ºetowania. TwojƒÖ rolƒÖ bƒôdzie zapewnienie wsparcia FinOps, optymalizacja koszt√≥w oraz wsp√≥≈Çpraca z zespo≈Çami technicznymi i finansowymi. üõ†Zakres zada≈Ñ Wsparcie wdro≈ºenia ≈õrodowiska GCP z perspektywy FinOps Tworzenie i zarzƒÖdzanie bud≈ºetami chmurowymi (alerty, limity, tagowanie) Monitorowanie i analiza koszt√≥w (Google Cloud Billing, Budgets & Alerts, BigQuery) Wsp√≥≈Çpraca z zespo≈Çami IT i finansowymi Identyfikowanie mo≈ºliwo≈õci optymalizacji koszt√≥w Wdra≈ºanie dobrych praktyk FinOps i prowadzenie szkole≈Ñ ‚úÖWymagania Do≈õwiadczenie w FinOps w ≈õrodowisku GCP Znajomo≈õƒá narzƒôdzi: Google Cloud Billing, Budgets & Alerts, BigQuery, Cloud Monitoring Umiejƒôtno≈õƒá pracy z bud≈ºetami chmurowymi i analiza koszt√≥w Znajomo≈õƒá GCP Landing Zone Accelerator Znajomo≈õƒá Terraform i/lub Ansible Umiejƒôtno≈õƒá wsp√≥≈Çpracy z dzia≈Çami technicznymi i finansowymi Certyfikat FinOps Certified Practitioner(mile widziany r√≥wnie≈º CCFM) üéØMile widziane Do≈õwiadczenie w szkoleniu zespo≈Ç√≥w z zakresu FinOps Praktyczna znajomo≈õƒá optymalizacji koszt√≥w w du≈ºych ≈õrodowiskach chmurowych Co oferujemy: Elastyczny model pracy: hybrydowy ‚Äì 1 dzie≈Ñ w biurze, pozosta≈Çe dni zdalnie Benefity: karta Multisport, prywatna opieka medyczna Jasny i sprawny proces rekrutacyjny: Rozmowa z rekruterem Spotkanie techniczne z klientem Je≈õli oferta jest dla Ciebie interesujƒÖca, prze≈õlij swoje CV!","[{""min"": 23520, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Unclassified,23520,30240,Net per month - B2B
Full-time,Senior,Permanent or B2B,Hybrid,117,Data Scientist,Link Group,"Poszukujemy do≈õwiadczonego Data Scientista do zespo≈Çu in≈ºynieryjnego miƒôdzynarodowej firmy technologicznej specjalizujƒÖcej siƒô w rozwiƒÖzaniach automatyzujƒÖcych procesy finansowe i ksiƒôgowe. Firma od ponad 20 lat rozwija w≈ÇasnƒÖ platformƒô SaaS wykorzystywanƒÖ globalnie przez dzia≈Çy finansowe du≈ºych organizacji. Projekt skupia siƒô na budowaniu nowoczesnych, skalowalnych rozwiƒÖza≈Ñ AI/ML wspierajƒÖcych procesy ksiƒôgowe i finansowe, z wykorzystaniem Python, TensorFlow lub PyTorch, Google Cloud Platform oraz MLOps. Na co dzie≈Ñ Data Scientist w tym zespole odpowiada za projektowanie, trenowanie i wdra≈ºanie modeli uczenia maszynowego, budowanie pipelines danych, integracjƒô mikroserwis√≥w w ≈õrodowisku chmurowym oraz rozw√≥j us≈Çug AI w bliskiej wsp√≥≈Çpracy z product ownerami, innymi in≈ºynierami i zespo≈Çami cloudowymi. Wa≈ºny jest tu nie tylko mocny background techniczny, ale tak≈ºe umiejƒôtno≈õƒá szukania rozwiƒÖza≈Ñ w otwartych, nieoczywistych problemach i dzielenia siƒô wiedzƒÖ wewnƒÖtrz zespo≈Çu. Wymagania: Bardzo dobra znajomo≈õƒá Python, SQL, Spark Do≈õwiadczenie w budowie i wdra≈ºaniu modeli ML (klasyfikacja, klasteryzacja, prognozowanie) Znajomo≈õƒá TensorFlow i/lub PyTorch Praktyka w pracy z GCP lub innƒÖ du≈ºƒÖ chmurƒÖ (AWS, Azure) Do≈õwiadczenie z MLOps, pipelines, kontrolƒÖ wersji (Git) Wykszta≈Çcenie wy≈ºsze kierunkowe (Informatyka, Statystyka, Data Science) Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego ‚Äì komunikacja wewnƒôtrzna w firmie odbywa siƒô w tym jƒôzyku Mile widziane: Do≈õwiadczenie z GCP (BigQuery, Vertex AI) Znajomo≈õƒá specyfiki us≈Çug finansowych lub rozwiƒÖza≈Ñ dla ksiƒôgowo≈õci Wiedza z zakresu generative AI vs AI agents Informacje organizacyjne: üìç Lokalizacja: Krak√≥w ‚Äì model hybrydowy (2 dni w tygodniu z biura) üí¨ Wymagana bardzo dobra znajomo≈õƒá jƒôzyka angielskiego üìë Forma wsp√≥≈Çpracy: Umowa o pracƒô na czas nieokre≈õlony lub B2B (w zale≈ºno≈õci od preferencji) üóì Start: mo≈ºliwie jak najszybciej üíº Proces rekrutacyjny: rozmowy techniczne oraz spotkanie z przedstawicielem firmy","[{""min"": 21000, ""max"": 25000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Science,21000,25000,Net per month - B2B
Full-time,Senior,B2B,Remote,118,DataBricks Architect,CRODU,"üå¥ Forma pracy: d≈Çugoterminowo, fulltime, 100% zdalnieüëà ‚è∞ Start: ASAPüëà Cze≈õƒá! üëã Dla naszego klienta z USA poszukujemy DataBrick Architect√≥w. Prace dotyczƒÖ dzia≈Ça≈Ñ w obszarachm.in. migracji, zbierania danych i optymalizacji rozwiƒÖza≈Ñ opartych na DataBricks. Klient posiada sta≈Çe zapotrzebowanie na specjalist√≥w. Projekty kt√≥re prowadzƒÖ przewa≈ºnie sƒÖ kr√≥tkoterminowe (spore prawdopodobie≈Ñstwo na przed≈Çu≈ºenia projekt√≥w) i ze wzglƒôdu na sta≈Ço≈õƒá zapotrzebowania klient jest w stanie zaproponowaƒá nowy temat po zako≈Ñczeniu danego projektu. Obecnie poszukiwani sƒÖ specjali≈õci do projektu AI/ML z obszar√≥w healthcare. Projekt dotyczy analizy danych tekstowych i analizy obraz√≥w generowanych przez urzƒÖdzenia medyczne (rentgen, rezonans magnetyczny itp.). Zebrane dane bƒôdƒÖ migrowane do chmurowej bazy opartej na DataBricks. Platforma ma obs≈Çugiwaƒá ca≈Çy cykl ≈ºycia danych w zgodzie z wbudowanymi funkcjonalno≈õciami zapewniajƒÖcymi zgodno≈õƒá z przepisami, mo≈ºliwo≈õƒá przeprowadzania audyt√≥w, tworzenia kohort czy wt√≥rnego wykorzystania modeli. Celem jest rozwiƒÖzanie problem√≥w zwiƒÖzanych z istniejƒÖcymi systemami zarzƒÖdzania danych (rozproszone ≈∫r√≥d≈Ça, rƒôczne procesy, niewystarczajƒÖce bezpiecze≈Ñstwo). Poszukujemy os√≥b, kt√≥re biegle znajƒÖ Pythona. Dla klienta kluczowe jest obycie w ≈õrodowiskach chmurowych oraz znajomo≈õƒá DataBricks i Apache Spark. Projekty prowadzone przede wszystkim dla firm z USA - w wiƒôkszo≈õci przypadk√≥w wymagana jest praca jedynie z niewielkƒÖ zak≈ÇadkƒÖ godzinowƒÖ (np. od 10: 00 do 18: 00) natomiast jeste≈õmy w stanie dogadaƒá siƒô je≈õli chodzi o godziny pracy. Og√≥lny zakres obowiƒÖzk√≥w: üìç Stworzenie ≈õrodowiska i architektury platformy na DataBricks üìç Kontakt z biznesem pod kontem ustale≈Ñ projektowych üìç Zapewnienie bezpiecze≈Ñstwa przechowywanie danych üìç Przetwarzanie i indeksowanie danych DICOM üìç Walidacja danych, tworzenie pipeline'√≥w przetwarzania danych, tworzenie i udostƒôpnianie kohort üìç Zaplanowanie i przeprowadzenie migracji baz danych üìç ≈öcis≈Ça wsp√≥≈Çpraca z zespo≈Çem (m.in. data engineers, data scientists, informatycy kliniczni, zesp√≥≈Ç wsparcia) Wymagania: ‚ö°Ô∏è Solidne do≈õwiadczenie w pracy w roli data engineera lub pokrewnych rolach (8+ lat) ‚ö°Ô∏è Bardzo dobra znajomo≈õƒá platformy DataBricks oraz Apache Spark ‚ö°Ô∏è Bardzo dobra znajomo≈õƒá Python ‚ö°Ô∏è Do≈õwiadczenie w przeprowadzaniu migracji chmurowych ‚ö°Ô∏è Do≈õwiadczenie w pracy w ≈õrodowisku AWS (Amazon s3) ‚ö°Ô∏èDo≈õwiadczenie w prowadzeniu projekt√≥w zwiƒÖzanych z AI/ ML ‚ö°Ô∏è Umiejƒôtno≈õci interpersonalne i zespo≈Çowe ‚ö°Ô∏è Umiejƒôtno≈õƒá podejmowania inicjatywy i samodzielno≈õƒá ‚ö°Ô∏è Angielski na poziomie umo≈ºliwiajƒÖcym swobodnƒÖ komunikacjƒô w zespole Mile widziane: ‚ö°Ô∏è Do≈õwiadczenie w pracy w ≈õrodowisku innych ≈õrodowiskach chmurowych (np. Azure - Data Factory, Synapse, Logic Apps, Data Lake) ‚ö°Ô∏è Do≈õwiadczenie w projektowaniu i optymalizacji przep≈Çyw√≥w danych za pomocƒÖ, DBT, SSIS, TimeXtender lub podobnych rozwiƒÖza≈Ñ (ETL, ELT) ‚ö°Ô∏è Do≈õwiadczenie z dowolnymi platformami big data lub noSQL (Redshift, Hadoop, EMR, Google Data itp.) Jak dzia≈Çamy i co oferujemy? üéØ Stawiamy na otwartƒÖ komunikacjƒô zar√≥wno w procesie rekrutacji jak i po zatrudnieniu - zale≈ºy nam na klarowno≈õci informacji dotyczƒÖcych procesu i zatrudnienia üéØ Do rekrutacji podchodzimy po ludzku, dlatego upraszczamy nasze procesy rekrutacyjne, ≈ºeby by≈Çy mo≈ºliwie jak najprostsze i przyjazne kandydatowi üéØ Pracujemy w imiƒô zasady ""remote first"", wiƒôc praca zdalna to u nas norma, a wyjazdy s≈Çu≈ºbowe ograniczamy do minimum üéØ Oferujemy prywatnƒÖ opiekƒô medycznƒÖ (Medicover) oraz kartƒô Multisport dla kontraktor√≥w","[{""min"": 200, ""max"": 250, ""type"": ""Net per hour - B2B""}]",Data Architecture,200,250,Net per hour - B2B
Freelance,Senior,B2B,Hybrid,120,Data Architect ‚Äì (Azure & MS Fabric),INFOPLUS TECHNOLOGIES,"Dutch is a preferred but not necessary Integration of Internal and External Data Responsible for onboarding and integrating internal and external data onto the data platform. Ensureinteroperabilitywith System Integrations. Data Modeling & Preparation According to Standards Structure data into standardizeddata modelsanddatamartsto enable dashboard development. Ensure consistency, quality, and alignment with enterprise data standards. Access Management Standardization Define and implement atechnical standard for access management, aligned with the organization‚ÄôsIAM (Identity & Access Management)framework. Facilitate secure and controlled access to data assets. Reduction of Technical Impediments Proactively identify and resolve technical bottlenecks to enable TDA dashboard development. Aim to meet a95% service levelfor business operations dashboards. DP-700: Microsoft Fabric Data Engineer certification DP-600: Microsoft Fabric Analytics Engineer certification At least2 years of experienceintegrating diverse data sources for consolidated reporting/dashboarding At least2 years of hands-on experiencewithAzure and Microsoft Fabric Knowledge of and experience withAzure DevOps, includingCI/CD pipelines Familiarity withIAMandRBAC(Role-Based Access Control) Proven experience working inAgile teams","[{""min"": 1707, ""max"": 2348, ""type"": ""Net per day - B2B""}]",Data Architecture,1707,2348,Net per day - B2B
Full-time,Senior,B2B,Remote,121,Senior Azure Data Engineer with Databricks,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: üåç100% remote from Poland üìùlong-term üíªworking hours - 7: 00-15: 00 üï∞Ô∏èASAP project start Responsibilities: Being responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems Building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies Evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards Driving creation of re-usable artifacts Establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation Working closely with analysts/data scientists to understand impact to the downstream data models Writing efficient and well-organized software to ship products in an iterative, continual release environment Contributing and promoting good software engineering practices across the team Communicating clearly and effectively to technical and non-technical audiences Defining data retention policies Monitoring performance and advising any necessary infrastructure changes Requirements: 3+ years‚Äô experience with Azure Data Factory and Databricks 5+ years‚Äô experience with data engineering or backend/fullstack software development Strong SQL skills Python scripting proficiency Experience with data transformation tools - Databricks and Spark Experience in structuring and modelling data in both relational and non-relational forms Experience with CI/CD tooling Working knowledge of Git English level: B2-C2 Nice to have: Experience with Azure Event Hubs, CosmosDB, Spark Streaming, Airflow Experience with Airflow Experience in Aviation Industry and Copilot Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,170,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,122,Data Engineer - Delivery Experience,Allegro,"About the team The salary range for this position is (contract of employment): 14 200 - 19 690 PLN in gross terms A hybrid work model requires 1 day a week in the office In the area of Delivery Experience, we are building technology that makes Allegro's deliveries easy, cost-effective, fast and predictable. Our team takes care of critical services along the Allegro shopping journey, responsible for predicting delivery times using statistical algorithms and machine learning, selecting the best delivery methods tailored to customers, and integrating with carrier companies. Delivery Experience is also one of the fastest-growing areas where we undertake new, complex projects to enhance logistics and warehousing processes. We are looking for a Mid/Senior Data Engineer with a focus on the data processing and preparation, deployment and maintenance of our data projects. Join our team to enhance your skills related to deploying data-based processes, data ops approaches and share the skills within the team. Your main responsibilities: - You will be actively responsible for developing and maintaining processes for handling large volumes of data - You will be streamlining and developing the data architecture that powers analytical products and work along a team of experienced analysts - You will be monitoring and enhancing quality and integrity of the data - You will manage and optimize costs related to our data infrastructure and data processing on GCP This is the right job for you if: - You have at least 3 years of experience as Data Engineer and working with large datasets. - You have experience with cloud providers (GCP preferred). - You are highly proficient in SQL. - You have strong understanding of data modeling and cloud DWH architecture. - You have experience in designing and maintaining ETL/ELT processes. - You are capable of optimizing cost and efficiency of data processing. - You are proficient in Python for working with large data sets (using PySpark or Airflow). - You use good practices (clean code, code review, CI/CD). - You have a high degree of autonomy and take responsibility for developed solutions. - You have English proficiency on at least B2 level. - You like to share knowledge with other team members. Nice to have: - Experience with Azure and cross-cloud data transfers and multi-cloud architecture What we offer: - Big Data is not an empty slogan for us, but a reality - you will be working on really big datasets (petabytes of data). - You will have a real impact on the direction of product development and technology choices. We utilize the latest and best available technologies, as we select them according to our own needs. - Our tech stack includes: GCP, BigQuery, (Py)Spark, Airflow. - We are a close -knit team where we work well together. - You will have the opportunity to work within a team of experienced engineers and big data specialists who are eager to share their knowledge, including publicly through allegro.tech Apply to Allegro and see why it is #dobrzetubyƒá (#goodtobehere)","[{""min"": 14200, ""max"": 19690, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14200,19690,Gross per month - Permanent
Full-time,Senior,Permanent,Hybrid,123,"Senior Business Intelligence Analyst, Cost Controlling (m/f/x)",HelloFresh,"HelloFresh Group, the world‚Äôs leading integrated food solutions provider, is expanding with a new R&D Tech office in Poland. With brands offering meal kits, ready-to-eat meals, and specialty products such as meat, seafood, and pet food, we are seeking individuals who are ready to make an impact from day one. Joining us in Wroc≈Çaw means shaping the culture, working on meaningful R&D Tech projects, and contributing to a global company changing how people eat. At HelloFresh Group, we are driven by a high-performance culture that values speed, agility, and continuous learning. We believe in hands-on contribution and fostering a truly collaborative, egoless environment where every team member contributes to our mission ofchanging the way people eat, foreverand welcome team players who thrive in a dynamic environment, lead with ownership, and bring diverse perspectives to the table. Our teams thrive on in-person collaboration, with an expectation to work from the office four days a week. This approach creates a dynamic space where ideas flourish, decisions are made efficiently, and our collective impact is accelerated. It's how we stay closely connected to our shared goals and drive swift execution. As a Senior Cost Controlling BI Analyst, you will be an integral part of HelloFresh's mission to empower our business with financial insights, data-driven strategies, and processes that elevate the effectiveness of how we invest our resources. You will support the business in developing global solutions that harmonize our reporting across our geographies and analyze our spend in key areas. You will help produce the data assets that provide insights to help our leaders make key decisions. Develop and implement complex calculations globally that help support local and global teams report and manage costs. Collaborate with cross-functional teams, including FP&A, Accounting, Finance Tech and key stakeholders in each area to enhance and automate reporting in relation to costs to meet the needs of senior stakeholders. Work closely with local stakeholders to ensure that local requirements and nuances are included in any central calculations. Support the cost controlling function in tracking their initiatives and efforts to reduce spend in key areas. 5+ years of experience in data analysis, financial controlling, FP&A, or a similar role, ideally in a fast-paced, data-driven environment as well as a university degree. Strong analytical and problem-solving skills, with the ability to translate complex data into actionable insights. Proficiency in SQL, and advanced Excel skill. Python is a plus. Excellent communication skills, with the ability to effectively collaborate with cross-functional teams and present financial insights to senior stakeholders. Ability to work independently, prioritize effectively, and manage multiple projects in a fast-paced environment. Experience with BI tools such as Tableau, Power BI, or similar is a plus. Health- You‚Äôre covered from your first day with private health insurance Hybrid Working Schedule- We work in-office 4 days a week to align on goals, with flexible hours to support work-life balance and personal needs Holidays- You receive 26 days of paid vacation each year, providing you time to rest and recharge Learning and Development- An annual Learning & Development budget and a Mentoring Program to support your ongoing professional growth Employee Referral Program- Our team members can participate in our internal employee referral program and receive a bonus for recommending successful candidates to open roles Daily Comforts- Free coffee, drinks, and fresh fruit are available to keep you refreshed throughout the day If you are passionate about making a tangible impact and thrive in a fast-paced environment where your work directly contributes to a global purpose, we encourage you to apply ‚Äì even if your experience doesn't tick every single box, we believe there are many ways to develop skills and grow with us.","[{""min"": 178000, ""max"": 267000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,178000,267000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,124,Senior Data Engineer (Azure/Fabric),Onwelo,"üü† Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, a tak≈ºe otworzy≈Ça biura w siedmiu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. üöÄ O projekcie: Do naszego zespo≈Çu Data & Analytics poszukujemy do≈õwiadczonego Azure/Fabric Data Engineera, kt√≥ry bƒôdzie wspiera≈Ç naszych klient√≥w w planowaniu, budowie i wdra≈ºaniu nowoczesnych rozwiƒÖza≈Ñ danych w ≈õrodowisku Microsoft Azure i Fabric. Bƒôdziesz pracowaƒá w zespole z ekspertami od analizy danych, chmury i architektury, w ≈õrodowisku miƒôdzynarodowym i projektach o du≈ºej skali. . üéØ Z nami bƒôdziesz: Projektowaƒá i wdra≈ºaƒá rozwiƒÖzania oparte na Microsoft Fabric ‚Äì nowoczesnej platformie analitycznej ≈ÇƒÖczƒÖcej dane, raportowanie i orkiestracjƒô w jednym ≈õrodowisku Planowaƒá i przeprowadzaƒá orkiestracjƒô danych w ≈õrodowisku Microsoft Azure oraz Fabric Budowaƒá, rozwijaƒá i wdra≈ºaƒá nowoczesnƒÖ hurtowniƒô danych w oparciu o Databricks, Data Vault 2.0, Python i PySpark Tworzyƒá i optymalizowaƒá potoki danych oraz procesy ETL/ELT zasilajƒÖce hurtownie danych Projektowaƒá modele danych wspierajƒÖce analitykƒô biznesowƒÖ i raportowanie w Power BI Wdra≈ºaƒá rozwiƒÖzania z wykorzystaniem Microsoft Fabric, Azure Data Factory, Synapse, Data Lake, Azure SQL Przeprowadzaƒá analizƒô danych i projektowaƒá modele danych wspierajƒÖce cele biznesowe Wspieraƒá innych cz≈Çonk√≥w zespo≈Çu ‚Äì technicznie i merytorycznie Monitorowaƒá jako≈õƒá i efektywno≈õƒá przep≈Çyw√≥w danych oraz optymalizowaƒá je pod kƒÖtem koszt√≥w i wydajno≈õci üòé Czekamy na Ciebie, je≈õli: Masz minimum 5-letnie do≈õwiadczenie jako Data Engineer ‚Äì w projektach zwiƒÖzanych z integracjƒÖ danych, modelowaniem i budowƒÖ hurtowni Masz praktyczne do≈õwiadczenie z Microsoft Fabric lub chcesz rozwijaƒá siƒô w tym obszarze i szybko siƒô uczysz Pracujesz z us≈Çugami chmurowymi Azure, w tym: Azure Data Factory, Azure Databricks, Azure SQL, Data Lake Znasz SQL na poziomie eksperckim Biegle pos≈Çugujesz siƒô jƒôzykami Python i/lub PySpark Rozumiesz architekturƒô nowoczesnych hurtowni danych (np. Data Vault 2.0 ) Masz wy≈ºsze wykszta≈Çcenie techniczne (np. informatyka, matematyka, in≈ºynieria danych) Komunikujesz siƒô po angielsku na poziomie min. B2 (czƒô≈õƒá projekt√≥w i zespo≈Ç√≥w jest miƒôdzynarodowa) ü§ù Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,27000,Net per month - B2B
Full-time,Mid,B2B,Remote,126,Data Engineer (Azure & Databricks),in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Dla naszego Klienta poszukujemy do≈õwiadczonych Data Engineer√≥w, kt√≥rzy chcƒÖ pracowaƒá w ≈õrodowisku opartym o Azure i Databricks. Szukamy os√≥b, kt√≥re potrafiƒÖ projektowaƒá wydajne modele danych i majƒÖ do≈õwiadczenie w przetwarzaniu du≈ºych zbior√≥w danych. Zakres roli: Projektowanie, implementacja i utrzymanie skalowalnych proces√≥w przetwarzania i integracji danych (ETL/ELT) w ≈õrodowisku Azure. Praca z du≈ºymi zbiorami danych z wykorzystaniem Apache Spark i PySpark w Azure Databricks. Tworzenie i rozw√≥j warstw danych w oparciu o Delta Lake ‚Äì z uwzglƒôdnieniem dobrych praktyk modelowania danych i kontroli jako≈õci. Udzia≈Ç w projektowaniu struktur danych pod kƒÖtem wydajno≈õci, przejrzysto≈õci i u≈ºyteczno≈õci biznesowej. Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç z wykorzystaniem Azure Data Factory i Azure Functions. Optymalizacja kodu oraz monitorowanie i rozwiƒÖzywanie problem√≥w wydajno≈õciowych. Praca z zespo≈Çami analitycznymi, data science i biznesowymi w celu zapewnienia dostƒôpno≈õci danych w odpowiedniej formie i czasie. Oczekujemy: Minimum 3 lata do≈õwiadczenia w roli Data Engineera lub pokrewnej. Bardzo dobra znajomo≈õƒá Azure Databricks, Spark i PySpark. Do≈õwiadczenie z Delta Lake oraz modelowaniem danych i tuningiem wydajno≈õci. Znajomo≈õƒá ADF, Azure Functions. Umiejƒôtno≈õƒá pracy z formatami danych: Parquet, Avro, JSON. Praktyczna znajomo≈õƒá Python i SQL. Do≈õwiadczenie w pracy z Git. Umiejƒôtno≈õƒá pracy z du≈ºymi wolumenami danych i do≈õwiadczenie w tworzeniu wydajnych rozwiƒÖza≈Ñ w ≈õrodowiskach chmurowych. Zrozumienie architektury przetwarzania danych i zasad projektowania system√≥w odpornych i ≈Çatwo skalowalnych. Mile widziane: Do≈õwiadczenie w pracy w oparciu o metodyki Agile (np. Scrum). Znajomo≈õƒá narzƒôdzi do monitorowania i automatyzacji proces√≥w (CI/CD) Praktyka w pracy z danymi wra≈ºliwymi lub w ≈õrodowiskach regulowanych (np. sektor finansowy, medyczny) Proponujemy: Rozw√≥j kariery w miƒôdzynarodowych projektach, z wykorzystaniem nowoczesnych narzƒôdzi i technologii. Elastyczny model pracy: mo≈ºliwo≈õƒá 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejƒôtno≈õci i do≈õwiadczenia. Wsp√≥≈Çpracƒô w zgranym zespole, kt√≥ry ceni wymianƒô wiedzy oraz otwartƒÖ komunikacjƒô. Realny wp≈Çyw na projekty oraz wdra≈ºane rozwiƒÖzania. Wynagrodzenie na poziomie 20 000 - 30 000 PLN/miesiƒôcznie w zale≈ºno≈õci od do≈õwiadczenia. üí°Nie przegap dopasowanych ofert! Mamy wiele rekrutacji, a nowe projekty pojawiajƒÖ siƒô na bie≈ºƒÖco. Pamiƒôtaj, ≈ºe zaznaczajƒÖc zgodƒô na przetwarzanie danych w celu przysz≈Çych proces√≥w , bƒôdziemy mogli zaprosiƒá Ciƒô do udzia≈Çu w kolejnych procesach, dopasowanych do Twojego do≈õwiadczenia i oczekiwa≈Ñ! PS Zamierzamy kontaktowaƒá siƒô z TobƒÖ wy≈ÇƒÖcznie wtedy, kiedy bƒôdziemy dla Ciebie ciekawe projekty, bez tej zgody nie bƒôdzie to mo≈ºliwe. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 17000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,17000,28000,Net per month - B2B
Full-time,Senior,B2B,Remote,127,Senior Data & Analytics Engineer,N-iX,"About the project: Our customer is the European online car market with over 30 million monthly users, with a market presence in 18 countries. The company is now merging with a similar company in Canada and needs support in this way. As a Data& Analytics Engineer, you will play a pivotal role in shaping the future of online car markets and enhancing the user experience for millions of car buyers and sellers. Requirements: 5+ years of experience in Data Engineering or Analytics Engineering roles Strong experience building and maintaining pipelines in BigQuery, Glue, Athena, and Airflow Solid Python skills, especially for data processing and workflow orchestration Advanced SQL skills and experience designing dimensional models (star/snowflake) Familiarity with data quality tools like Great Expectations Understanding of data governance, privacy, and security principles Experience working with large datasets and optimizing performance Proactive problem solver who enjoys building scalable, reliable solutions English - Upper-Intermediate+ Responsibilities: Build and maintain robust data pipelines that deliver clean and timely data Organize and transform raw data into well-structured, scalable models Ensure data quality and consistency through validation frameworks like Great Expectations Work with cloud-based tools like Athena and Glue to manage datasets across different domains Collaborate with analysts, engineers, and stakeholders to understand data needs and deliver solutions Help set and enforce data governance, security, and privacy standards Continuously improve the performance and reliability of data workflows Support the integration of modern cloud tools into the broader data platform","[{""min"": 18470, ""max"": 21795, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,18470,21795,Net per month - B2B
Full-time,Mid,B2B,Remote,128,Data Engineer / Scientist,Kyotu Technology,"üîç Data Engineer / Scientist (Mid) Location: Wroc≈Çaw / Warszawa / remote from anywhere in PolandContract: B2B / Employment contractCapacity: Full-time You know that before a fancy dashboard or predictive model happens, someone needs to build a solid pipeline, clean up messy datasets from three continents, and deal with a file calledfinal_final_REALLY_final.csv? That‚Äôs exactly the kind of challenge we‚Äôre talking about. Kyotu Technology is a boutique software house based inWroc≈Çaw and Warsaw, working fully remotely or in hybrid mode from anywhere in Poland. We partner with companies fromGermany, Switzerland, Western Europe, the United States, and the Middle East, focusing on complex, production-grade systems ‚Äî the kind where code moves real money, not just pixels. Our teams are highly experienced and independent. We build things from scratch, with strong ownership and zero hand-holding. We‚Äôre looking for someone who‚Äôs comfortable in the world of data: capable of building and maintaining data pipelines, integrating different sources, preparing clean and structured datasets ‚Äî and occasionally diving into analysis or experimentation. You don‚Äôt need to know every buzzword. But you should be confident with: Building and maintainingETL/ELT pipelines UsingSQLandPythonto automate and process data Working with tools likedbt, Airflow, Pandas, Jupyter Understanding data warehouse/lake architectures (Snowflake, BigQuery, Redshift, etc.) Making sense of data and describing it clearly for others Experience withmachine learning(e.g., scikit-learn, feature engineering, prompt tuning) is a plus, but not required. Curious minds welcome. Build and optimize pipelines that pull data from APIs, SQL databases, files ‚Äî and, yes, occasionally weird Excel sheets Validate and clean incoming data (because no one wants to base decisions on fairytales) Collaborate with AI/ML teams to provide high-quality, structured inputs Occasionally explore, visualize, or summarize data to support product or business goals You won‚Äôt be working in a vacuum. You‚Äôll have a team, proper code reviews, QA, DevOps support, and PMs who understand tech. We work in agile-ish cycles. If something works ‚Äî we keep it. If it doesn‚Äôt ‚Äî we fix it. English matters ‚Äî most projects are international and require written and spoken communication with clients. Hourly rate: 120‚Äì170 PLN net/hour (B2B)‚Äî negotiable depending on experience Flexible working hours and full remote freedom ‚Äî or drop by our Wroc≈Çaw or Warsaw offices if you like Engaging, end-to-end projects ‚Äî not just ticket-pushing Autonomy and influence ‚Äî if something doesn‚Äôt make sense, you‚Äôll help change it","[{""min"": 20160, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Science,20160,28560,Net per month - B2B
Full-time,Senior,B2B,Remote,130,Senior Oracle Developer,P&P Solutions,"Poszukujemy dw√≥chdo≈õwiadczonych Programist√≥w Oracle (PL/SQL)do udzia≈Çu w rozbudowanym projekcie dlafirmy z sektora energetycznegozlokalizowanej we Wroc≈Çawiu. Projekt zak≈Çada wieloetapowe dzia≈Çania rozwojowe nad systemem informatycznym bazujƒÖcym na technologii Oracle ‚Äì z silnym naciskiem na jako≈õƒá kodu, optymalizacjƒô wydajno≈õci oraz wsparcie architektoniczne. üìÖStart projektu: wrzesie≈Ñ 2025 üìçTryb pracy: Zdalnie + sporadyczne wizyty w biurze we Wroc≈Çawiu Min.10 latkomercyjnego do≈õwiadczenia zPL/SQLiOracle DB Zaawansowana znajomo≈õƒá projektowania struktur, optymalizacji zapyta≈Ñ, administracji Znajomo≈õƒá transformacji XML, JSON na poziomie bazy danych Praca z GIT, testy jednostkowe (np. utPLSQL) Do≈õwiadczenie w ≈õrodowisku SCRUM, znajomo≈õƒá JIRA üéØMile widziane: Znajomo≈õƒá Pythona Do≈õwiadczenie w systemach czasu rzeczywistego Projekty w bran≈ºy energetycznej lub elektroenergetycznej Projektowanie i implementacja struktur bazodanowych w Oracle Tworzenie z≈Ço≈ºonych procedur i optymalizacja zapyta≈Ñ PL/SQL Wsparcie architekta rozwiƒÖzania i architekta biznesowego Udzia≈Ç w projektowaniu aplikacji i procesie testowania (w tym testy automatyczne) Nadz√≥r technologiczny nad rozwiƒÖzaniami bazodanowymi Udzia≈Ç w projektowaniu standard√≥w dokumentacyjnych i rozwoju kompetencji zespo≈Çu ‚úÖ StabilnƒÖ i d≈ÇugoterminowƒÖ wsp√≥≈Çpracƒô ‚Äì projekt przewidziany na 12 miesiƒôcy z mo≈ºliwo≈õciƒÖ kontynuacji ‚úÖ Elastyczny model pracy ‚úÖ Atrakcyjne wynagrodzenie do 150 z≈Ç/h ‚úÖ Mo≈ºliwo≈õƒá wp≈Çywu na kluczowe decyzje technologiczne ‚úÖ Wsp√≥≈Çpraca przy projekcie o znaczeniu strategicznym ‚úÖ WspierajƒÖce i profesjonalne ≈õrodowisko ‚úÖ Dostƒôp do nowoczesnych narzƒôdzi i technologii","[{""min"": 120, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,150,Net per hour - B2B
Full-time,Senior,B2B,Remote,132,Data Architect ETRM,INFOPLUS TECHNOLOGIES,"Exciting Opportunity: ETRM Data Architect (Remote, Poland) | 10+ Years Experience | 2000 PLN/day About the Role: As an ETRM Data Architect, you will be the driving force behind creating robust data transformation architectures, working closely with trading systems like Allegro, RightAngle, Endur, and more. Your expertise in Python, Streamlit, and Azure data technologies will enable us to deliver seamless, scalable solutions in the dynamic energy trading domain. What You‚Äôll Do: Lead the design and development of cutting-edge data transformation solutions using Python. Build and optimize interactive web APIs leveraging Streamlit to enhance user engagement. Collaborate with cross-functional teams to translate business needs into high-performance technical solutions. Work extensively with Azure Data Factory, Data Lake, Snowflake, and Databricks to manage complex data workflows. Ensure data quality, integrity, and security throughout all processes. Drive continuous improvements for performance, scalability, and efficiency. Provide technical mentorship to junior team members and promote best practices. Bring your expertise in ETRM systems, especially in Gas and Power markets, to the forefront. What We‚Äôre Looking For: 10+ years of hands-on experience in data engineering, transformation, and architecture Strong proficiency in Python, Kafka, and Streamlit Deep experience with Azure Data Factory, Data Lake, Snowflake, Databricks Well-versed in DevOps practices (CI/CD) Proven ability to design scalable, real-time data processing systems Strong analytical, problem-solving, and communication skills Familiarity with ETRM systems and energy trading concepts (Allegro, RightAngle, Endur) is a big plus Why Join Us? Remote work from anywhere in Poland/EU Lead innovative projects in the fast-evolving energy sector Collaborative environment with passionate professionals","[{""min"": 38000, ""max"": 40000, ""type"": ""Net per month - B2B""}]",Data Architecture,38000,40000,Net per month - B2B
Full-time,Senior,B2B,Remote,133,Senior Data Scientist,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Senior Data Scientist Dla naszego Klienta, miƒôdzynarodowej organizacji, poszukujemy do≈õwiadczonego Senior Data Scientist, kt√≥ry swobodnie porusza siƒô w ≈õwiecie ML/AI, zna dobre praktyki MLOps i potrafi projektowaƒá skalowalne rozwiƒÖzania. Je≈õli masz mocne zaplecze w Pythonie, ≈õwietnie radzisz sobie z du≈ºymi zbiorami danych i chcesz pracowaƒá nad realnymi wyzwaniami biznesowymi ‚Äì aplikuj. Czego oczekujemy: 5+ lat do≈õwiadczenia jako Data Scientist oraz udokumentowane sukcesy w stosowaniu technik ML do rozwiƒÖzywania rzeczywistych problem√≥w. Bieg≈Ço≈õci w Pythonie i popularnych frameworkach ML (scikit-learn, TensorFlow, PyTorch). Do≈õwiadczenie w projektowaniu i wdra≈ºaniu kompleksowych rozwiƒÖza≈Ñ ML, w tym wstƒôpnego przetwarzania danych, in≈ºynierii cech, budowy i oceny modeli. Znajomo≈õƒá platform chmurowych (AWS, Azure, GCP) oraz praktyk MLOps. Solidne podstawy statystyki i matematyki oraz ich zastosowania w modelowaniu i ocenie modeli. Umiejƒôtno≈õƒá zarzƒÖdzania du≈ºymi, z≈Ço≈ºonymi zbiorami danych oraz przeprowadzania zada≈Ñ takich jak przetwarzanie danych, in≈ºynieria cech i optymalizacja modeli. Silne umiejƒôtno≈õci rozwiƒÖzywania problem√≥w i analitycznego my≈õlenia, pozwalajƒÖce na proponowanie innowacyjnych rozwiƒÖza≈Ñ w zakresie analizy danych. Doskona≈Çe umiejƒôtno≈õci komunikacyjne, w tym zdolno≈õƒá do przedstawiania koncepcji technicznych i wynik√≥w osobom nietechnicznym. Bieg≈Ço≈õƒá w jƒôzyku angielskim (w mowie i pi≈õmie). Mile widziane: Do≈õwiadczenie w pracy z Generative AI i du≈ºymi modelami jƒôzykowymi (LLMs). Znajomo≈õƒá przetwarzania jƒôzyka naturalnego (NLP) lub technik wizji komputerowej. Dodatkowe umiejƒôtno≈õci programistyczne w jƒôzykach takich jak R, SQL, Java itp. Do≈õwiadczenie w pracy z narzƒôdziami Big Data (np. Hadoop, Spark, Kafka). Twoje obowiƒÖzki: Projektowanie, rozwijanie i wdra≈ºanie kompleksowych rozwiƒÖza≈Ñ ML end-to-end. Analiza i przetwarzanie du≈ºych zbior√≥w danych. Budowa, testowanie i wdra≈ºanie modeli ML z wykorzystaniem nowoczesnych algorytm√≥w i technik uczenia maszynowego. Praca z platformami chmurowymi (AWS, Azure, GCP) oraz implementacja rozwiƒÖza≈Ñ zgodnych z praktykami MLOps. Eksperymentowanie z nowymi podej≈õciami w AI, w tym Generative AI i LLMs. Optymalizacja istniejƒÖcych modeli ML pod kƒÖtem wydajno≈õci i dok≈Çadno≈õci. Wsp√≥≈Çpraca z zespo≈Çami biznesowymi i technicznymi, w celu identyfikowania potrzeb i wdra≈ºania rozwiƒÖza≈Ñ opartych na danych. Tworzenie dokumentacji technicznej oraz prezentowanie wynik√≥w analiz osobom nietechnicznym. Co oferujemy? Rozw√≥j kariery w miƒôdzynarodowych projektach, z wykorzystaniem nowoczesnych narzƒôdzi i technologii. Elastyczny model pracy: mo≈ºliwo≈õƒá 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejƒôtno≈õci i do≈õwiadczenia. Wsp√≥≈Çpracƒô w zgranym zespole, kt√≥ry ceni wymianƒô wiedzy oraz otwartƒÖ komunikacjƒô. Realny wp≈Çyw na projekty oraz wdra≈ºane rozwiƒÖzania. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 20000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Data Science,20000,29000,Net per month - B2B
Full-time,Senior,B2B,Remote,135,"Senior Data Engineer (DBT, Snowflake)",Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä We are looking foran experiencedSenior Data Engineerwith strong DBT and Snowflake skills to join our team at a leadingSwedish manufacturing company. This role requires someone who is proactive, solution-oriented and not afraid to speak up, push for quality and champion their ideas in a dynamic, international setting involving close collaboration with external partners. You will thrive here if you combine technical expertise with a collaborative mindset and excellent communication skills. Although the project environment is fast-paced and engaging, it is firmly rooted in the Scandinavian work culture of trust, autonomy and transparency, where your initiative will be met with genuine support. Responsibilites: Drive backend development for a proprietary BI and analytics tool in theFinance stream, ensuring high-performance, scalable solutions. Take part in thetransition from on-premises to the cloud, designing and implementing Snowflake data models and DBT transformations for the new environment. Architect and maintainCI/CD pipelinesinGitLabfor seamless code integration and deployment. Partner with stakeholders to translate reporting requirements into robust data solutions. Monitor and optimize thedata warehousein Snowflake for query performance, storage efficiency, and cost control during and after migration. Implement secure, cloud-based data workflows onAWSorAzure, ensuring smooth cutover from legacy systems. Troubleshoot complex dataissues and provide timely feedback to both technical and non-technical stakeholders throughout the migration. We offer a B2B Contract: 140 ‚Äì 190 PLN net/hour + VAT You might be the perfect match if you are/have: Bachelor‚Äôs or Master‚Äôs degreein Computer Science or a related field (or equivalent experience). 5+ yearsof professional experience indata engineering, with at least 2 yearsfocused onSnowflakeandDBT. Proven track record incomplex Finance reporting projects. Experience inmigration projects, specifically moving from on-prem to cloud environments. Experience drivingon-prem to cloud migrationinitiatives. Experience setting up and maintainingCI/CD pipelinesfor data code inGitLab. Stronganalytical thinkingand passion for building innovative data models. Excellentcommunication skills, able to engage with diverse stakeholders and provide clear feedback. Aself-starter mindset, able to work independently, prioritize tasks, and drive projects to completion. Fluent Englishfor smooth collaboration in an international environment. Location in Polandto facilitate collaboration and attendance at occasional team meetings. Moreover, we appreciate skills in these areas: Experience in working inlarge, diverse, multinational development teams. Hands-on experience withAWSorAzuredata services beyond basic usage. Demonstrated leadership abilities and teamwork in high-pressure projects. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Perks and benefits: Fully remotework or in our office in Wroc≈Çaw; Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories. If you apply for this position and match our expectations, then: 1) You will be invited to an HR Screening with our IT Recruiter. 2) You will have a technical meeting with a Team Leader. 3) You will have a meeting with your team. Submit your application online in one easy step! Apply now!","[{""min"": 140, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,140,190,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,136,Backend Engineer (Data & AI),Appliscale,"About the role Our client is an early-stage, venture-backed startup transforming the $800B franchising industry. Their innovative AI platform helps leading franchise brands automate operations, leverage data insights, and scale faster and smarter. We're hiring a product-focused Backend Software Engineer eager to build impactful solutions, work closely with AI technology, and thrive in a fast-paced startup environment. This is an opportunity to contribute directly to product development, seeing your code and ideas quickly in users' hands. Responsibilities Please note, availability to attend daily afternoon/evening meetings is a specific requirement for this role as most of the team is located in the US Develop scalable backend systems, data pipelines, and APIs using Python, TypeScript, and AWS infrastructure Collaborate cross-functionally with product, ML, and infrastructure teams to integrate and deploy AI features in a multi-tenant SaaS environment Required qualifications Minimum of 2 years full-time commercial backend software development experience, ideally with Python and TypeScript Bachelor's or higher degree in Computer Science, Software Engineering, or a related field Comfortable building and managing services in AWS environments (EC2, Lambda, ECS, Airflow) Experienced using AutoML frameworks, time-series DBs Product-minded engineer who enjoys collaborating closely with product and business teams Startup-oriented: thrives in ambiguity, eager to learn quickly, iterate fast, and build impactful solutions Excellent communication skills and high fluency in English, it‚Äôs our daily business language Nice to have AI-curious: experience deploying ML models into production is a strong plus but not required","[{""min"": 14000, ""max"": 20000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 16000, ""type"": ""Gross per month - Permanent""}]",Data Science,14000,20000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,138,Senior Data Science/AI Engineer,N-iX,"Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management.Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact.Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company‚Äôs R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 21700, ""max"": 29000, ""type"": ""Net per month - B2B""}, {""min"": 17500, ""max"": 23950, ""type"": ""Gross per month - Permanent""}]",Data Science,21700,29000,Net per month - B2B
Full-time,Senior,B2B,Remote,143,Data Engineer ETRM,INFOPLUS TECHNOLOGIES,"Join Our Team as an ETRM Data Engineer ‚Äì Poland (Remote) | B2B Contract ETRM Data Engineer to play a key role in transforming complex data into actionable insights. If you're eager to work on cutting-edge energy trading systems from the comfort of your home in Poland, this could be your next big opportunity! What You‚Äôll Do: Design and Build robust, scalable data pipelines and manage high-performance ETRM systems. Drive data integration projects within the dynamic Energy Trading and Risk Management (ETRM) landscape. Collaborate with cross-functional teams to seamlessly integrate data from leading ETRM trading platforms like Allegro, RightAngle, and Endur. Optimize data storage solutions using Data Lake and Snowflake for faster, more reliable access. Develop and maintain ETL workflows leveraging Azure Data Factory and Databricks. Write clean, efficient Python code for data processing, analysis, and automation. Uphold the highest standards of data quality, accuracy, and integrity across diverse sources and platforms. Partner with traders, analysts, and IT professionals to understand data needs and deliver innovative solutions. Continuously enhance data architecture for performance and scalability to support business growth. What We‚Äôre Looking For: Proven expertise with Azure Data Factory (ADF) Experience working with Data Lake and Snowflake/SQL databases Strong Python/PySpark programming skills Familiarity with FastAPI for API development Hands-on experience with Databricks platform Why Join Us? Remote flexibility from anywhere in Poland Engage with innovative energy trading solutions Be part of a forward-thinking team shaping the future of energy markets Competitive B2B contract terms","[{""min"": 28000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,28000,30000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,144,Senior Data Engineer with Databricks,Crestt,"Cze≈õƒá! Poszukujemy osoby na stanowisko Senior Data Engineer , kt√≥ra do≈ÇƒÖczy do zespo≈Çu naszego Klienta zajmujƒÖcego siƒô projektowaniem i rozwijaniem nowoczesnych rozwiƒÖza≈Ñ w obszarze Data Lakehouse, Business Intelligence oraz zaawansowanej analityki w chmurze. Wsp√≥≈ÇpracujƒÖc w miƒôdzynarodowym ≈õrodowisku, bƒôdziesz mia≈Ç(a) okazjƒô pracowaƒá z najnowszymi technologiami w tej dziedzinie. Lokalizacja : praca g≈Ç√≥wnie zdalna (1x w miesiƒÖcu spotkanie w biurze w Warszawie) Wide≈Çki: B2B 160-200 pln netto+vat/h UoP 26-30 tys. brutto/mies. Wymagania: Bieg≈Ço≈õƒá w SQL oraz Pythonie (min. 5 lat do≈õwiadczenia) Co najmniej 2-letnie do≈õwiadczenie w pracy z Databricks Do≈õwiadczenie w pracy w ≈õrodowisku chmurowym (preferowany Azure) Minimum 5-letnie do≈õwiadczenie w projektowaniu oraz implementacji rozwiƒÖza≈Ñ klasy BI, ETL/ELT, Data Warehouse, Data Lake, Big Data oraz OLAP Praktyczna znajomo≈õƒá zar√≥wno relacyjnych, jak i nierelacyjnych Do≈õwiadczenie z narzƒôdziami typu Apache Airflow, dbt, Apache Kafka, Flink, Azure Data Factory, Hadoop/CDP Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z zarzƒÖdzaniem danymi, jako≈õciƒÖ danych oraz przetwarzaniem wsadowym i strumieniowym Umiejƒôtno≈õƒá stosowania wzorc√≥w architektonicznych w obszarze danych (Data Mesh, Data Vault, Modelowanie wymiarowe, Medallion Architecture, Lambda/Kappa Architectures) Praktyczna znajomo≈õƒá system√≥w kontroli wersji (Bitbucket, GitHub, GitLab) Wysoko rozwiniƒôte umiejƒôtno≈õci komunikacyjne, otwarto≈õƒá na bezpo≈õredni kontakt z Klientem ko≈Ñcowym Certyfikaty z Databricks lub Azure bƒôdƒÖ dodatkowym atutem Zakres obowiƒÖzk√≥w: Projektowanie i wdra≈ºanie nowych rozwiƒÖza≈Ñ oraz wprowadzanie usprawnie≈Ñ w istniejƒÖcych platformach danych Udzia≈Ç w rozwoju platform danych i proces√≥w ETL/ELT, optymalizacja przetwarzania du≈ºych zbior√≥w danych zgodnie z najlepszymi praktykami in≈ºynierii danych Standaryzacja i usprawnianie proces√≥w technicznych ‚Äì implementacja standard√≥w kodowania, testowania i zarzƒÖdzania dokumentacjƒÖ Dbanie o jako≈õƒá kodu i zgodno≈õƒá z przyjƒôtymi standardami ‚Äì przeprowadzanie regularnych code review Aktywna wsp√≥≈Çpraca z innymi ekspertami technologicznymi, w celu doskonalenia proces√≥w oraz identyfikacji nowych wyzwa≈Ñ technologicznych Mentoring i wsparcie zespo≈Çu w zakresie projektowania rozwiƒÖza≈Ñ, optymalizacji proces√≥w i wdra≈ºania najlepszych praktyk Klient oferuje: Udzia≈Ç w miƒôdzynarodowych projektach opartych na najnowocze≈õniejszych technologiach chmurowych Pokrycie koszt√≥w certyfikacji (Microsoft, AWS, Databricks) 60 p≈Çatnych godzin rocznie na naukƒô i rozw√≥j Mo≈ºliwo≈õƒá wyboru miƒôdzy pracƒÖ zdalnƒÖ a spotkaniami w biurze Indywidualnie dopasowane benefity: prywatna opieka medyczna, dofinansowanie karty sportowej, kursy jƒôzykowe, premie roczne i medialne oraz bonus za polecenie nowego pracownika (do 15 000 PLN)","[{""min"": 25600, ""max"": 32000, ""type"": ""Net per month - B2B""}, {""min"": 26000, ""max"": 30000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,25600,32000,Net per month - B2B
Full-time,Senior,Permanent,Remote,145,Staff Engineer,dotLinkers,"The role: Staff Engineer Salary: up to 37 500 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Staff Software Engineer on the Compute team, you will lead the design and development of a next-generation compute platform that powers scalable workloads. Your focus will be on creating robust, event-driven, and batch-capable compute infrastructure using Kubernetes, KEDA, Temporal, Apache Spark, and advanced stream processing technologies. You will be part of an Infrastructure Services organization, structured into specialized teams focused on core platform areas such as compute, networking, and storage. The Compute team enables developers to deploy scalable workloads‚Äîfrom microservices and scheduled jobs to real-time data pipelines‚Äîleveraging cloud-native patterns that prioritize performance, elasticity, and developer autonomy. Key Responsibilities: Architect and implement cloud-native compute solutions to orchestrate background jobs, long-running workflows, and streaming data pipelines. Develop and maintain compute abstractions integrating Kubernetes, KEDA, and Temporal to support scalable job and service orchestration. Lead development of elastic data processing approaches using Apache Spark for batch workloads and streaming frameworks for real-time analytics. Define and promote best practices for running event-driven and parallel workloads in production environments. Collaborate with engineering teams to support various compute use cases, including microservices, cron jobs, ETL, workflow engines, and machine learning workloads. Ensure reliable autoscaling, failure handling, and resource optimization across compute workloads. Partner with platform security and observability teams to ensure compliance, transparency, and monitoring of workload execution. Mentor engineers on distributed system design and modern compute orchestration techniques. Minimum Qualifications: 8+ years of experience in backend, infrastructure, or data platform engineering roles. Extensive production experience with Kubernetes (preferably AKS). Hands-on experience with orchestration/eventing frameworks like KEDA, Temporal, or similar. Skilled in developing batch (e.g., Spark) and streaming (e.g., Kafka, Flink, Azure Event Hubs) processing systems. Strong programming skills in Go, Python, or C#, focusing on distributed or event-driven systems. Familiarity with secure compute design, autoscaling, and high-availability architectures. Preferred Qualifications: Experience implementing KEDA, Temporal, or Argo Workflows in production environments. Knowledge of Azure Synapse, Azure Data Explorer, Azure Data Factory, or Databricks. Experience building compute platforms for streaming analytics, ETL pipelines, or AI/ML workloads. Understanding of event-driven and pub-sub architectures like Kafka or Azure Event Grid. Contributions to cloud-native or CNCF open-source projects. Leadership Expectations: Define the strategy and technical roadmap for a unified compute platform. Promote cloud-native, event-driven, and scalable architecture practices across teams. Lead architecture reviews and encourage adoption of modern orchestration technologies. Mentor engineers and lead teams toward resilient, observable, and developer-friendly compute solutions. Drive long-term initiatives to improve workload performance, efficiency, and maintainability. Core Competencies: Cloud-Native Orchestration: Expert knowledge of Kubernetes-based job orchestration and scalable compute architectures. Data Processing: Ability to design for hybrid streaming and batch workloads in modern applications. Technical Leadership: Influences platform strategy and architectural direction organization-wide. Scalable System Design: Proven track record of scaling compute across diverse workloads and tenants. Developer Experience: Passionate about empowering engineering teams with powerful yet easy-to-use platforms. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 32500, ""max"": 37500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,32500,37500,Gross per month - Permanent
Full-time,Junior,Any,Hybrid,147,M≈Çodszy/a Programista/tka ERP,Unisoft,"Jeste≈õmy producentem oprogramowania wspomagajƒÖcego zarzƒÖdzaniem przedsiƒôbiorstwem. Od prawie 40 latnie tylko tworzymy, ale tak≈ºe doradzamy, wdra≈ºamy i szkolimy w zakresie zarzƒÖdzania wiƒôkszo≈õciƒÖ proces√≥w zachodzƒÖcych w przedsiƒôbiorstwie. Poszukujemy trzech os√≥b, kt√≥re do≈ÇƒÖczƒÖ do zespo≈Çu i w ramach przydzielonego obszaru merytorycznego bƒôdƒÖ odpowiedzialne za rozwijanie naszego autorskiego systemu. programowanie wed≈Çug przygotowanych za≈Ço≈ºe≈Ñ projektowych, dbanie o rozw√≥j aplikacji zgodnie z wewnƒôtrznymi standardami programowania i projektowania, tworzenie dokumentacji technicznej umo≈ºliwiajƒÖcej dalszy rozw√≥j i eksploatacjƒô oprogramowania. uko≈Ñczone (bƒÖd≈∫ w trakcie ostatniego roku) studia na kierunku zwiƒÖzanym z informatykƒÖ, matematykƒÖ, elektronikƒÖ lub telekomunikacjƒÖ, znajomo≈õƒá dowolnej relacyjnej bazy danych, znajomo≈õƒá jƒôzyka SQL, znajomo≈õƒá programowania obiektowego, znajomo≈õƒá jƒôzyka angielskiego na poziomie czytania dokumentacji technicznej, umiejƒôtno≈õƒá pracy w zespole, chƒôƒá rozwoju zawodowego. Mile widziane: znajomo≈õƒá kt√≥rego≈õ z jƒôzyk√≥w programowania: Delphi, C#/C++ itp., umiejƒôtno≈õƒá pos≈Çugiwania siƒô systemami kontroli wersji (np. Git), do≈õwiadczenie w realizacji projekt√≥w informatycznych. pracƒô w firmie o prawie 40-letniej tradycji w bran≈ºy IT, zatrudnienie w oparciu o wybranƒÖ formƒô wsp√≥≈Çpracy (UoP, B2B, zlecenie), atrakcyjne wynagrodzenie dopasowane do kompetencji, stabilizacjƒô i work-life balance (pracujemy w sta≈Çych godzinach i nie zabieramy pracy do domu) mo≈ºliwo≈õƒá rozwoju zawodowego pod opiekƒÖ do≈õwiadczonych programist√≥w, wsparcie merytoryczne od lider√≥w technologicznych, zgrany zesp√≥≈Ç chƒôtny do pomocy i dzielenia siƒô wiedzƒÖ, pracƒô w centrum Gdyni: 10 min. do Dworca PKP/SKM Gdynia G≈Ç√≥wna, 2 minuty pieszo z przystanku ZKM ‚ÄûPlac Kaszubski-≈öwiƒôtoja≈Ñska 01‚Äù, 5 min pieszo do Skweru Ko≈õciuszki, 10 min. na pla≈ºƒô miejskƒÖ, co najmniej 40 restauracji i kawiarni w promieniu 800m, parking rowerowy pod biurem i prysznice w biurze (≈õwietna lokalizacja ma te≈º swoje minusy w postaci trudno dostƒôpnych miejsc parkingowych).","[{""min"": 6500, ""max"": 9000, ""type"": ""Gross per month - Any""}]",Unclassified,6500,9000,Gross per month - Any
Full-time,Mid,B2B,Remote,148,Data Engineer with Palantir,Link Group,"About the Role We are looking for aData Engineer experienced with Palantir Foundryto join a cross-functional team working on large-scale data integration, modeling, and analytics platforms. The ideal candidate is hands-on, proactive, and capable of navigating complex data ecosystems in an enterprise environment. Design and build data pipelines and models usingPalantir Foundry Integrate multiple data sources (structured and unstructured) into usable, high-quality data assets Collaborate with data scientists, analysts, and business stakeholders to support advanced analytics initiatives Apply data governance, lineage, and cataloging principles within Foundry Develop and maintain Foundry ‚ÄúObjects‚Äù, Code Workbooks, and other tooling Ensure quality, performance, and scalability of the implemented data solutions Support and document platform usage and development best practices 3+ years of experience in Data Engineering Hands-on experience withPalantir Foundryin a commercial or enterprise setting Proficiency inSQL,Python, and data transformation techniques Good understanding ofdata modeling(dimensional, relational, and graph-based) Familiarity withdata governanceandmetadata management Experience working incloud-based environments(AWS, GCP, or Azure) Excellent communication skills and ability to work with cross-functional teams Previous experience in highly regulated industries (finance, pharma, defense, etc.) Experience integrating Foundry with external tools and systems via APIs Knowledge ofCI/CD,Git, and software engineering best practices Exposure to tools likeAirflow,dbt,Databricks,Snowflake, etc. Experience withdata privacy regulations(GDPR, HIPAA, etc.)","[{""min"": 90, ""max"": 105, ""type"": ""Net per hour - B2B""}]",Data Engineering,90,105,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,150,Data Engineer with Hadoop,Antal Sp. z o.o.,"Develop automation tools and integrate existing solutions within a complex platform ecosystem Provide technical support and design for Hadoop Big Data platforms (Cloudera preferred) Manage user access and security (Kerberos, Ranger, Knox, TLS, etc.) Implement and maintain CI/CD pipelines using Jenkins and Ansible Perform capacity planning, performance tuning, and system monitoring Collaborate with architects and developers to design scalable and resilient solutions Deliver operational support and improve engineering tooling for platform management Analyze existing processes and design improvements to reduce complexity and manual work Building scalable automation in a diverse ecosystem of tools and frameworks Enhancing service resilience and reducing operational toil Supporting the adoption of AI agents and real-time data capabilities Integrating with corporate identity, CI/CD, and service management tools Collaborating with cross-functional teams in a global environment Minimum 5 years of experience in engineering Big Data environments (on-prem or cloud) Strong understanding of Hadoop ecosystem: Hive, Spark, HDFS, Kafka, YARN, Zookeeper Hands-on experience with Cloudera distribution setup, upgrades, and performance tuning Proven experience with scripting (Shell, Linux utilities) and Hadoop system management Knowledge of security protocols: Apache Ranger, Kerberos, Knox, TLS, encryption Experience in large-scale data processing and optimizing Apache Spark jobs Familiarity with CI/CD tools like Jenkins and Ansible for infrastructure automation Experience working in Agile or hybrid development environments (Agile, Kanban) Ability to work independently and collaboratively in globally distributed teams To learn more about Antal, please visitwww.antal.pl","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,180,220,Net per hour - B2B
Full-time,Senior,B2B,Remote,152,Senior Data Scientist,Sii,"Minimum 5 years of experience in machine learning, data modeling, and statistical analysis Proficient in Python and ML frameworks such as scikit-learn, TensorFlow, or PyTorch Experience with cloud-based ML development (preferably AWS) Familiarity with Docker, Kubernetes, and CI/CD tools like Jenkins or GitLab CI Strong SQL skills and experience working with large datasets and data preprocessing Fluency in English (spoken and written) Fluent Polish required Residing in Poland required We are looking for an experienced and forward-thinking Senior Data Scientist to join our medical client‚Äôs team. In this role, you will lead the development and implementation of machine learning algorithms, ensuring their seamless integration into scalable, high-performance systems. You‚Äôll play a key role in driving data-driven decision-making and delivering robust AI solutions that align with strategic goals. Develop and apply machine learning algorithms Perform experiments, run tests, and assess results to enhance model precision and effectiveness Translate data into actionable insights through advanced analytics and modeling Collaborate with cross-functional teams to deploy and monitor ML solutions in production Provide mentorship to junior team members and contribute to data science best practices Support cloud engineering initiatives, particularly on AWS infrastructure Rekrutacja online Jƒôzyk rekrutacji: polski Start ASAP Praca w pe≈Çni zdalna Darmowe ≈õniadanie Bez wymaganego dress code'u Nowoczesne biuro Darmowa kawa Szkolenia wewnƒôtrzne Pakiet sportowy Bud≈ºet na szkolenia Ma≈Çe zespo≈Çy Prywatna opieka medyczna Miƒôdzynarodowe projekty Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title ‚Äì get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers ‚Äì Power People. Learn more atsii.pl.","[{""min"": 22000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Science,22000,28000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,154,Qlik Developer,ITDS,"Qlik Developer Join us, and turn data into powerful business decisions! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As aQlik Developer,you will be working for our client, a leading global financial institution undergoing a major digital transformation to enhance its data analytics capabilities. You will be part of a dynamic team focused on developing and maintaining scalable Qlik dashboards and reports, aimed at improving decision-making across various business units. The project involves managing complex data sets, implementing infrastructure best practices, and ensuring compliance within a fast-paced, highly regulated environment. Your role plays a key part in optimizing how data is shared, visualized, and utilized to deliver impactful business insights. Developing advanced dashboards and reports using QlikSense Ensuring data integrity and managing updates to data sources Managing project sites and content on Qlik Server Documenting data sources, processes, and dashboards clearly Analyzing data sharing policies and promoting compliance best practices Collaborating with cross-functional teams and stakeholders at all levels Supporting Qlik deployment by following infrastructure best practices Enhancing dashboard performance for large data sets Influencing stakeholders through thoughtful data presentations Following established internal control standards and audit requirements Proficiency in Qlik with strong dashboard development experience 3+ years of experience in data analysis roles Practical knowledge of QlikSense and Qlik architecture Ability to work with large data sets while maintaining performance High level of mathematical and analytical skills Proven experience in stakeholder management and communication Strong collaboration skills within cross-functional teams Experience documenting technical processes and data sources Ability to adapt quickly in fast-changing environments Understanding of regulatory requirements for data sharing Experience with Qlik administration Familiarity with financial services or highly regulated industries Knowledge of best practices for offshore project coordination Prior involvement in change or transformation projects Awareness of risk management and internal control standards We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7326 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere.","[{""min"": 25200, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,31500,Net per month - B2B
Full-time,Senior,B2B,Remote,156,Data & AI Copilot Senior Developer,Link Group,"üíº Senior Data & AI Copilot Developer üìç Remote or Hybrid | International enterprise client üí∞ B2B | Start: ASAP Join a project focused on building advanced Copilot solutions within the Microsoft ecosystem (D365, Power Platform, Azure). We‚Äôre looking for an experienced developer with a strong background in AI automation, data integration, and enterprise-grade solution design. üîß Key Responsibilities: Design and develop customized Microsoft Copilot solutions based on business needs Integrate Copilot with D365, Power Platform, and external systems Automate business processes using Power Automate, Logic Apps, and other tools Continuously improve performance based on user feedback and usage metrics üß† Requirements: Proven experience with Microsoft Copilot, AI, automation, and integration projects Strong knowledge of Power Platform, D365, Azure (especially Logic Apps, Power Automate) Hands-on experience with Python or similar tools for AI integration Excellent communication skills ‚Äì able to work cross-functionally and explain tech to business Nice to have: Microsoft AI certifications (e.g., Azure AI Engineer Associate)","[{""min"": 110, ""max"": 145, ""type"": ""Net per hour - B2B""}]",Data Science,110,145,Net per hour - B2B
Full-time,Senior,B2B,Remote,158,Remote Senior Quantitative Developer,Montrose Software,"Client description: Montrose Software has an ongoing relationship with one of the largest North American banks to enhance and maintain its production libraries and applications. The bank serves 17 million clients worldwide and provides personal and commercial banking, wealth management, insurance, investor services, and capital markets products and services globally. We work mainly with their capital markets teams, focusing on derivatives and fixed income systems. Project description: We work on maintaining and enhancing a Risk management system for a big bank. The system is a core internal instrument for the data analyst and economist to make risk calculations. It is split into 3 layers: Front end - Microsoft Excel with a fully custom set of function and menus; standalone Python app Business Logic Library - C++ and Python modules on the network Calculation grid - 50k computing grid running the simulations in house, AWS and Azure Qualifications: 5+ years of experience in software development, ideally in the quantitative finance area Good knowledge of C++ and basic knowledge of Python In-depth knowledge of various (vanilla and exotic) derivative products Familiarity with mathematical models for the dynamics of the financial market FpML knowledge is a plus Numeracy to be able to understand quantitative finance Commercial experience in financial or capital markets Ability to use a version control system, ideally Git Finding yourself in a large codebase Good communication skills Being open to code in other programming languages Fluent in spoken and written English Nice to have: Familiarity with AWS, Docker Experience in SQL Please note that the interview process is divided into four parts: Technical phone screening~30 minutes Technical interview~2 hours Non-technical call with hiring manager~45 minutes Optional: call with client for which you will be working We offer: Premier equipment to work Flexible working hours Remote work possibility Interesting, challenging and exciting work with international teams English lessons with native speaker Training Budget Multisport card Food : Lunches from Krak√≥w's restaurants that are delivered both to the office and homes( or a refund of the budget allocated for it) Kitchen full of food, drinks, fruit, and snacks Health Private medical insurance Air-conditioning Well-being: No dress code The chillout area incudes comfortable bean bags, therapy balls, PlayStation 4 or Nintendo Switch + games stretching area and pull-up bar Team events Shower Additional : Indoor parking place for bicycles","[{""min"": 22000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,30000,Net per month - B2B
Full-time,Senior,B2B,Remote,159,Senior Data Engineer (GCP + Python),emagine Polska,"Industry: Energy System, Startup Rate: 42-50 Euro/h on B2B Location: Poland (Remote) Contract length: 6-9 months with possible extension Introduction & Summary The primary objective of this role is to spearhead a data migration project focused on transferring data pipelines from a third-party orchestration tool to native Google Cloud Platform (GCP) services. The ideal candidate should possess 5+ years of experience as a Data Engineer with strong competencies in Google Cloud Platform, Python, and Terraform. Main Responsibilities Design and implement the data migration architecture. Establish infrastructure representation using Terraform. Select, configure, and manage an orchestrator. Implement reliable CI/CD practices for testing and deployments. Migrate data pipelines to the GCP environment and validate performance. Onboarding a new full-time employee. Key Requirements At least 5 years of experience as Data Engineer. Proven experience with Google Cloud Platform (GCP). Proficiency in Python and SQL for pipeline development. Hands-on expertise with Terraform as an infrastructure-as-code tool. Nice to Have Experience with DBT (Data Build Tool). Previous migration experience of data pipelines from tools like Twirl to Composer. Understanding of CI/CD best practices in data projects.","[{""min"": 28500, ""max"": 35280, ""type"": ""Net per month - B2B""}]",Data Engineering,28500,35280,Net per month - B2B
Full-time,Mid,B2B,Remote,161,Experienced Data Scientist,Future Mind,"Future Mind is a brilliant, inspiring team, one of the most awarded tech consulting companies in the region with a broad portfolio of clients, including ≈ªabka, Jeronimo Martins (Hebe, Biedronka), LPP (Reserved, Sinsay, Mohito), eObuwie, Modivo, and other well-recognized brands. We have received several industry awards for delivering some of the best eCommerce applications in Poland, listed among the most popular across app stores. Our expert engineers, designers, project managers, and analysts work on projects ranging from top mobile commerce apps used daily by millions of customers to IoT, and telematics platforms that produce vast amounts of data. In 2023, we joined forces with Solita, a Finnish tech powerhouse with a vibrant community of over 2000 specialists across Europe, that combines data, business, and technology skills to build and improve digital services for leading organizations in manufacturing, medical, shipping, and other major industries, including such clients as NATO, Nokian Tires, Pfizer and many others. Together we are dedicated to delivering cutting-edge, data-driven solutions. We value proactive professionals who take ownership, enjoy solving problems, sharing knowledge, and collaboration. Together, we create high-quality software solutions that fulfil our clients' business needs and impact their customers' lives every day. As part of the Solita Group, Future Mind provides digital advisory & delivery services with the support of our international partners and upholds equally high cultural standards. Role description: As a Data Scientist, your role involves taking part in data insights initiatives, designing and implementing machine learning solutions, while collaborating closely with the business stakeholders. You have a strong statistics background and have worked with cloud technologies for several years. This job is all about: Designing data science solutions based on large, complex data sets that meet both functional and non-functional business requirements; Building, testing, and deploying machine learning and statistics models on cloud environments; Working with cross-functional teams on delivering business value; Collaborating within our project teams to meet client needs and deliver high-quality solutions. Here‚Äôs what we‚Äôre looking for: Ability to analyze large datasets, understand data patterns, and implement AI solutions that drive business impact. Strong background in statistics, technical proficiency in Python, SQL, and leading modern ML frameworks. Familiarity with cloud platforms (AWS, Google Cloud, Azure) and deploying machine learning models in production environments. Previous experience in Ad Tech or analysing large behavioral datasets would be an advantage. Effective communication skills, proficient in Polish and English. You also must have the legal right to work in the EU to apply for this position. ‚Ä¶and here‚Äôs what we offer: A dynamic work environment where innovation and collaboration are valued. Access to cutting-edge projects and technologies in a variety of industries. A supportive community of experts to foster your professional growth and development. Competitive compensation, comprehensive benefits, and a focus on work-life balance. Opportunities for continuous learning and career advancement, including specialized training in big data technologies and Snowflake certifications. The ability to work fully remotely or check into one of our offices whenever you like, Fully paid private health insurance, subsidized sports membership, mental health support, and language courses.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Science,14000,22000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,162,Senior + / Staff Big Data Engineer,The Stepstone Group Polska,"At The Stepstone Group, we have a simple yet very important mission: The right job for everyone. Using our data, platform, and technology, we create opportunities for job seekers and companies around the world to find a perfect match, in fair and equitable way. With over 20 brands across 30+ countries, we strive for fair and unbiased hiring. At our Tech Hub, located near Wilanowska Metro, we are here as more than 300 ambitious specialists who work on the development of our IT products. We are proud to be part of The Stepstone Group, a global expert in job-tech platforms and e-recruiting. Join our team of 4,000+ employees and be part of reshaping the labour market and becoming the world‚Äôs leading job-tech platform. The job at a glance We‚Äôre looking for a Staff Big Data Engineer who is passionate about building meaningful, data-driven products that have real-world impact. In this hands-on and strategic role, you‚Äôll design and lead the development of secure, scalable data solutions that power intelligent search, matching, and recommendations across our global platform. Working within the Marketplace Enablement portfolio in the Data Products and Search & Match function, you‚Äôll collaborate with a diverse mix of engineers, data scientists, product managers, and stakeholders to drive innovation and build with purpose. This is a hybrid role based in Warsaw, with flexible remote working options. Your responsibilities Lead the architecture and development of big data solutions within a modern cloud environment (AWS) Design, build, and scale batch and real-time data pipelines that support inclusive, AI-driven features Partner with teams across the business to align data products with user needs and long-term goals Establish best practices in Big Data Engineering, fostering a culture of quality, learning, and collaboration Mentor engineers across teams, helping them grow while raising the overall standard of our work Actively contribute to our big data engineering community, sharing knowledge and supporting others Your skills and qualifications Solid experience designing and deploying scalable, secure big data systems Proficiency in Python, Apache Spark / PySpark, and AWS big data technologies Familiarity with tools like Apache Airflow, AWS Step Functions, or AWS Glue Workflows Understanding of batch and real-time processing; experience with Kafka is a plus Solid understanding of the software development lifecycle, CI/CD, containerization, observability, and security Strong collaboration, communication, and mentoring skills Comfortable working in a cross-functional environment and advocating for best practices A growth mindset, with a passion for inclusive technology and solving meaningful problems Fluency in English; experience in agile environments is a plus Benefits We‚Äôre a community here that cares as much about your life outside work as how you feel when you‚Äôre with us. Because your job shouldn‚Äôt take over your life, it should enrich it. Here are some of the benefits we offer: Medical and dental care Life insurance Benefit platform budget Employee Referral Programme Hackathons, Knowledge Sharing Hours In-house projects Events and integration parties Charity initiatives, 2 extra volunteer days English/German lessons Game room and chillout zone Our commitment Equal opportunities are important to us. We believe that diversity and inclusion at The Stepstone Group are critical to our success as a global company, so we want to recruit, develop, and keep the best talent. We encourage applications from everyone, regardless of background, gender identity, sexual orientation, disability status, ethnicity, belief, age, family or parental status, and any other characteristic.","[{""min"": 18000, ""max"": 30500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,30500,Gross per month - Permanent
Full-time,Senior,B2B,Remote,163,Data Engineer,TechTorch,"TechTorch is a pioneer among service companies in the way it conducts projects in the area of digital transformation. Through an implementation process supported by an AI-powered platform and a global team of world-class managers and technology experts, TechTorch has enabled dozens of private equity sector companies to accelerate the realization of business benefits. Job Description We are seeking a skilled and motivated Data Engineer to join our growing team. The ideal candidate will have a strong background in data engineering, with specific experience in working with Snowflake. You will be responsible for designing, building, and maintaining scalable data pipelines that can handle large volumes of data, ensuring the reliability, security, and performance of our data infrastructure. Key Responsibilities Design and Implement Data Pipelines: Develop, test, and maintain scalable data pipelines to support business analytics and reporting needs. Work with Snowflake: Design and optimize data architecture using Snowflake, including data modeling, data warehousing, and performance tuning. Data Integration: Integrate data from various sources, including APIs, databases, and third-party platforms, into our centralized data warehouse. Collaborate Across Teams: Work closely with data scientists, analysts, and other engineers to ensure data availability, quality, and reliability. Maintain Data Infrastructure: Monitor and maintain the performance and health of the data infrastructure, addressing any issues proactively. Documentation: Document data processes, pipeline architecture, and procedures for ongoing maintenance and future enhancements. Qualifications Experience with Snowflake: Minimum of 3 years of hands-on experience with Snowflake, including data warehousing, schema design, and query optimization. Programming Skills: Proficiency in programming languages such as Python, SQL. Data Pipeline Tools: Experience with ETL/ELT tools such as Apache Airflow, or similar. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or Google Cloud, particularly in data storage and processing. Database Management: Strong knowledge of relational and non-relational databases. Problem-Solving: Excellent problem-solving skills and ability to troubleshoot data issues quickly and efficiently. Communication: Strong communication skills, with the ability to collaborate effectively with cross-functional teams. What We Offer: Work in an international team Work with the largest capital groups in the world Development and rapid promotion opportunities Autonomy in action Participation in building a new global company Team-building activities","[{""min"": 21000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,30000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,165,üëâ Data Platform Architect,Xebia sp. z o.o.,"üü£You will be: üü£Your profile: extensive experience working with Azure cloud provider, üü£Recruitment Process: CVreview ‚ÄìHRcall ‚ÄìTechnical Interview‚ÄìClientInterview (with Live-coding) ‚ÄìHiring ManagerInterview ‚ÄìDecision üéÅBenefits üéÅ ‚úçDevelopment: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏èWe are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 29800, ""max"": 36800, ""type"": ""Net per month - B2B""}, {""min"": 23900, ""max"": 29700, ""type"": ""Gross per month - Permanent""}]",Data Architecture,29800,36800,Net per month - B2B
Full-time,Senior,B2B,Remote,166,MicroStrategy Developer,Calimala.ai,"Calimala.aiis seeking a highly skilledMicroStrategy Developerwith over 5 years of proven experience. In this role, you'll play a pivotal part in designing, developing, and optimizing advanced MicroStrategy reports, dashboards, and datasets. Your expertise will drive business insights for our clients, particularly within the Telecom industry, ensuring that our reporting solutions are both efficient and scalable. Responsibilities Develop and optimize a wide range of MicroStrategy reports, dashboards, and datasets. Collaborate with data engineers and analysts to translate business requirements into actionable insights. Design and implement robust schema structures to support high-performance reporting. Integrate ETL processes to ensure comprehensive and reliable data feeds. Manage performance tuning and remediation efforts to maintain system efficiency. Requirements Candidates must have a demonstrated background in business intelligence with specific expertise in MicroStrategy BI and related technologies. The ideal candidate will combine technical proficiency with strong problem-solving and communication skills. Experience in the Telecom industry is highly preferred, as is a commitment to continuous improvement and excellence in data-driven environments. Benefits Competitive salary package with performance-based incentives. The position is fully remote Opportunities for career advancement within a rapidly growing company. Continuous learning and professional development resources. A collaborative work culture that fosters innovation and teamwork. ÔªøWhy Apply? This is an excellent opportunity to contribute to a dynamic team in a company that values technical innovation and professional growth. If you thrive in a challenging environment and are excited to apply your skills to impactful projects, we invite you to apply today.","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,14000,25000,Net per month - B2B
Full-time,Junior,Permanent,Hybrid,171,Associate Data Analyst,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Associate Data Analyst Are you ready to kickstart your career in data analytics and make a meaningful impact in the world of business intelligence? We are seeking a motivated Associate Data Analyst to join our dynamic Data Assets, Analytics, and AI Platform at Bayer IT. In this entry-level role, you will support the analysis of critical data across market, finance, and product supply domains, contributing to our innovative initiatives. As part of our diverse, international team, you will help harness the power of data to drive informed decision-making and enhance product performance. You will work with advanced analytics and visualization tools, gaining valuable experience in a modern tech stack featuring Azure Databricks and Snowflake. If you‚Äôre passionate about data and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks and Responsibilities: Collaborate with product managers and technical teams to gather and clarify product objectives and data requirements. Assist in the exploration and analysis of diverse data sets to uncover insights that can drive product improvements. Support the development and maintenance of data structures, ensuring they are scalable and meet the needs of the team. Contribute to the monitoring of key performance indicators (KPIs) to identify trends and support data-driven recommendations. Help with the extraction, cleaning, and preparation of data from various sources to ensure accuracy and reliability in reporting. Document analytical processes, findings, and data models to facilitate knowledge sharing and team collaboration. Participate in team discussions and brainstorming sessions to propose ideas for enhancing data utilization and reporting. Qualifications and Competencies: Bachelor‚Äôs degree in Statistics, Computer Science, Data Science, or a related field. Some experience in data analysis, internships, or relevant coursework, preferably within the pharmaceutical or healthcare industry. Familiarity with data manipulation and analysis tools, such as Azure Databricks and Snowflake, is a plus. Knowledge of relational databases (PostgreSQL, MySQL) is desirable. Strong analytical and problem-solving skills with attention to detail. Good communication skills, with the ability to present data clearly and understandably. Ability to work collaboratively in a team-oriented environment. Fluent in English, both written and spoken. What do We offer: A flexible, hybrid work model. A great workplace in a new modern office in Warsaw. Career development, 360¬∞ feedback, and mentoring programs. Wide access to professional development tools, training, and conferences. Company bonus and reward structure. VIP medical care package (including dental and mental health). Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù). Life and travel insurance. Pension plan. Co-financed sport card - FitProfit. Meals subsidy in the office. Additional days off. Budget for home office setup and maintenance. Access to the company game room equipped with table tennis, soccer table, Sony PlayStation 5, Xbox Series X consoles, and massage chairs. Tailored support for relocation to Warsaw when needed. If you feel you do not meet all criteria we are looking for, that doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence; we value potential over perfection! WORK LOCATION: WARSAW, AL. JEROZOLIMSKIE 158","[{""min"": 10000, ""max"": 16000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,10000,16000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,173,Data Architect,TechTorch,"TechTorch is a pioneer among service companies in the way it conducts projects in the area of digital transformation. Through an implementation process supported by an AI-powered platform and a global team of world-class managers and technology experts, TechTorch has enabled dozens of private equity sector companies to accelerate the realization of business benefits. Position Overview: The Data Architect is responsible for designing, creating, deploying, and managing an organization‚Äôs data architecture. This role involves defining how the data will be stored, consumed, integrated, and managed by different data entities and IT systems, as well as any applications using or processing that data. The Data Architect works closely with business and IT stakeholders to ensure that the data architecture aligns with business objectives and supports the needs of the organization. Key Responsibilities: Data Architecture Design: Develop and maintain the overall data architecture, including data models, data flow diagrams, and data integration plans. Define and document the data architecture framework, standards, and principles. Design scalable and flexible data solutions that meet the business requirements and integrate with existing systems. Data Governance and Management: Establish and enforce data management and governance policies, procedures, and standards. Ensure data integrity, quality, and security across the organization. Define and manage data architecture principles and guidelines for data modeling, design, and implementation. Data Integration: Design and oversee data integration processes, ensuring seamless data flow across various systems and platforms. Implement data integration solutions using ETL (Extract, Transform, Load) tools and other data integration technologies. Collaborate with application architects and developers to design and implement data interfaces and APIs. Qualifications: Experience: Minimum of 7 years of experience in data architecture, data management, or related roles. Proven experience in designing and implementing data solutions in complex environments. Strong background in data modeling, database design, and data warehousing. Technical Skills: Proficiency in data modeling tools (e.g., ER/Studio, Erwin). Expertise in database technologies (e.g., SQL Server, Oracle, MySQL, NoSQL databases). Experience with data integration tools (e.g., Informatica, Snowflake, Talend, Apache Nifi). Knowledge of big data technologies (e.g., Hadoop, Spark, Kafka) and cloud data platforms (e.g., AWS, Azure, Google Cloud). Understanding of BI systems such as PowerBI and Tableaux Key Competencies: Strong analytical and problem-solving skills. Excellent communication and interpersonal skills. Ability to lead and influence cross-functional teams. Strategic thinking with a focus on delivering business value. Adaptability and continuous learning mindset What We Offer: Work in an international team Work with the largest capital groups in the world Development and rapid promotion opportunities Autonomy in action Participation in building a new global company Team-building activities","[{""min"": 30000, ""max"": 35000, ""type"": ""Net per month - B2B""}]",Data Architecture,30000,35000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,176,Senior Database Engineer,Navblue,"Job Summary: Aviation. It connects our world, brings people together, provides opportunities, accelerates economic growth, and is just so very cool! Come work for NAVBLUE, a leading services company owned by Airbus, dedicated to Flight Operations, Air Traffic Management solutions and services for airlines, airports, and Air Navigation Service Providers (ANSPs). We combine aircraft manufacturer expertise, flight operations know-how, and agile development to enhance operational efficiency, optimize resources, and increase productivity for a safe and sustainable aviation future. Our global teams deliver a reliable, optimum, and customized user experience to more than 500 customers worldwide. We are looking for a seasoned Software Engineer with extensive experience in designing, implementing, and optimizing database solutions in a microservices-based environment. As a member of our team, you will contribute to the full lifecycle of our data persistence layer, from schema design and performance tuning to ensuring robust replication, disaster recovery, and seamless integration within our cloud-native microservice ecosystem. Responsibilities: Design & Develop Database Solutions: Architect, design, and implement highly optimized relational (e.g., MySQL, PostgreSQL, AWS Aurora, SQL Server) and NoSQL (e.g., MongoDB, DynamoDB, Redis) database schemas, ensuring data integrity, performance, and scalability for microservices. Performance Optimization & Tuning: Proactively analyze and optimize complex queries, implement efficient indexing strategies, and manage partitioning/sharding to ensure peak database performance and handle high throughput. Reliability & Disaster Recovery: Design, implement, and maintain robust backup, disaster recovery, and high-availability solutions, including replication (master-slave/multi-master) and failover configurations, to ensure data durability and system uptime. Cloud Database Management: Deploy, configure, and manage database instances within cloud infrastructure (e.g., AWS RDS, Aurora), leveraging cloud-native features for scalability and operational efficiency. Focus on quality by promoting coding best practices, a test-first mindset and highest security standards. Contribute to building new and improving existing development processes. Work within a small agile teams delivering new features and fixing defects. Lead technical designs, taking a holistic view of the product, and collaborate with multiple stakeholders to define the best approach to address upcoming challenges and deliverables. Define and drive the team's technical direction, mentor junior engineers, and proactively identify, propose, and implement new processes or architectural improvements to enhance team efficiency, code quality, and timely delivery. Contribute to software architecture discussions, translate system-level designs and architectural blueprints into robust, maintainable, and high-quality code, applying the latest best practices in software engineering. Required Skills/Experience: 6+ years in roles directly responsible for the availability, performance, and security of critical databases. Expertise in Database Technologies: Strong command of MySQL, PostgreSQL, AWS Aurora, SQL Server and experience with MongoDB, DynamoDB, or Redis, including schema design and complex query writing. Database Performance & Scalability: Proven ability in query optimization (e.g., EXPLAIN plans), operating system optimization, advanced indexing strategies, and implementing partitioning/sharding and caching (Redis). Cloud Database Operations: Hands-on experience deploying, managing, and optimizing databases within AWS infrastructure (RDS, Aurora) and virtual machine infrastructure (SQL Server). Database Reliability & Security: Experience with high availability (replication, failover), backup automation, PITR, and data security (encryption, SQL injection prevention). Automation & DevOps: Proficiency in integrating database changes into CI/CD pipelines using schema migration tools (DbUp, EF migrations, Flyway, Liquibase) and Git for version control. Scripting & Troubleshooting: Strong scripting skills (Python/Bash) for automation and ability to analyze logs and monitor performance using tools like AWS Cloudwatch, Datadog, Prometheus, Grafana, or pgBadger. Solid understanding of DevOps practices, including CI/CD pipelines (e.g., GitLab CI, Cloudbees, Jenkins, GitHub Actions), containerization with Docker, and monitoring/logging tools. Demonstrated experience in leading software development teams, fostering a collaborative and high-performance culture, and effectively representing the team's technical vision and needs to stakeholders, including architects and senior management Strong capability in identifying technical challenges and bottlenecks, constructively proposing and implementing effective solutions (either individually or by guiding the team), while actively building team engagement, fostering a positive atmosphere, and championing team spirit Master of Science Degree in software engineering or a related field Proficiency in English spoken and written Nice-to-Haves: Experience with ETL/ELT pipeline design and tools (e.g., Apache Airflow). Familiarity with Change Data Capture (CDC) solutions. Knowledge of database services on other cloud platforms (e.g., Azure SQL Database, Google Cloud Spanner). Understanding of ORM frameworks (Entity Framework, Dapper, SQLAlchemy, Hibernate) from a database performance perspective. Experience with data governance or data lineage concepts and tools. Active contributions to open-source database projects or the broader database community. Proficiency in designing and implementing GraphQL backends or similar API patterns that interact heavily with databases. Understanding of airline operations, flight planning, or air navigation principles. Passion for the aviation industry. We offer: Stable employment based on a full-time job contract International working environment in a dynamic company Access to the latest knowledge and technologies enabling professional development Training and development possibilities Participating in international projects and international trips Competitive salary dependent on experience and qualifications Flexible working hours and work-from-home opportunities Private medical coverage for you and your family Sport card Life insurance for you and your family Co-funding for meals Employee stock ownership plan How to Apply: Candidates who are interested in joining the NAVBLUE team are invited to submit their resume and cover letter, highlighting their work experiences and skills via email totalent@navblue.aero. We thank all applicants for applying. Only selected applicants will be contacted. Navblue is committed to creating an environment and a culture where everyone feels like they belong no matter who they are or where they are from. We are committed to providing equal employment opportunities to all individuals based on job-related qualifications and ability to perform a job. We do not discriminate against any employee or applicant for employment because of race, colour, sex, age, national or ethnic origin, religion, sexual orientation, gender identity or expression, marital status, family status, genetic characteristics, record of offences, and basis of disability or any protected class. Accommodations will be available on request for candidates throughout the entire recruitment and selection process. NAVBLUE is operating within the Airbus Helicopters Polska Structure. About Us: NAVBLUE, an Airbus Company, is a leading global provider of flight operations solutions, including aeronautical charts, navigation data solutions, flight planning, aircraft performance software (take-off/landing, weight and balance), and crew planning solutions. You‚Äôll be able to shape the future of the digital aviation industry by working on several of the best in the industry flagship products enabling pilots, dispatchers, flight engineers and other aviation personnel on a daily basis to deliver safe, efficient, and reliable flight operations all over the world. You‚Äôll have the opportunity to support millions of flights each year and help NAVBLUE customers maximize efficiency, reduce costs, ensure compliance with complex national and international safety regulations, and effectively deliver their services. You‚Äôll join a team with a focus on digital and collaborative innovation that is passionate and customer-focused. Over the last few years, Airbus has been supportive of various initiatives such as Going Digital, Performance Based Navigation Services, Air Traffic Management Modernization Programs, FlySmart on iOS and other digital projects related to new aircraft technologies; the launch of NAVBLUE was therefore a natural step to further develop its Flight Operations and Air Traffic Management Portfolio. NAVBLUE is a fully owned subsidiary of Services by Airbus, fueled by the agility of Airbus ProSky and Navtech (acquired in 2016), and the pioneering spirit of Airbus, NAVBLUE was created in July 2016 with one mission: lead aviation into the digital age. Airbus and all subsidiaries, including NAVBLUE are proud to have been recognized as aGlobal Top Employer for 2025. Based on eight criteria: physical workplace, work atmosphere and social, health financial and family benefits, vacation and time off, employee communication, performance manager, training and skills development and community involvement. It was determined that we offer some of the most progressive and forward-thinking programs within the area. This achievement reflects our commitment to nurturing and empowering our people to reach their full potential. We‚Äôre grateful to our people, whose dedication and collaboration make this possible. NAVBLUE is based in Hersham (UK), Cardiff (UK), Toulouse (France), Waterloo, ON (Canada), Bangkok (Thailand) and Gda≈Ñsk (Poland) with other offices all around the world. The Future is Yours for the Taking: https: //youtu.be/vdY6gYuceYY","[{""min"": 15800, ""max"": 24000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15800,24000,Gross per month - Permanent
Full-time,Mid,B2B,Hybrid,177,Data & Reporting Senior Developer - banking üí•,ITDS,"Join us, and revolutionize how data powers financial decision-making! Krak√≥w-based opportunity with the possibility to work 60% remotely. As a Data & Reporting Senior Developer, you will be working for our client, a global financial institution focused on enhancing data architecture and analytics to improve business performance. You will play a key role in delivering high-quality data insights and repository solutions aligned with industry standards. The role involves developing performance KPIs, designing data models, integrating data from multiple sources, and ensuring data security. You will work closely with stakeholders to understand business needs and create efficient, scalable reporting solutions using cutting-edge technologies. Your main responsibilities: Deliver assigned projects end-to-end, including data discovery, cleaning, transformation, and validation Develop and implement data models based on business requirements Perform matching and linking across datasets to ensure data consistency Securely transfer data from sources to target systems Utilize ETL and data tools for seamless data integration across databases and platforms Provide complex reporting solutions within Google Cloud and Microsoft SQL Server environments Develop and optimize SQL queries and scripts for data extraction and transformation Collaborate with stakeholders to understand business needs and process logic Identify and implement internal process improvement opportunities Document processes by creating roadmaps, descriptions, and technical documentation You're ideal for this role if you have: At least 3 years of experience in a data-related role (SQL development, data analysis, data engineering, or data architecture) Expertise in SQL programming and relational database systems Hands-on experience with Google Cloud Platform, especially BigQuery and Data Studio Knowledge of ETL principles and data warehousing Experience in developing business reports and front-end dashboards Proficiency in Python for data processing and automation Fluent English Proactive problem-solving approach and goal-oriented mindset Ability to identify risks and process blockers and propose solutions Experience in working with stakeholders and business partners to drive project success It is a strong plus if you have: Experience with additional cloud platforms and data integration tools Familiarity with machine learning concepts and their application in data analysis Background in financial services or banking sector Knowledge of regulatory requirements related to data governance Hands-on experience with visualization tools such as Tableau or Power BI We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #6476 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 1100, ""max"": 1250, ""type"": ""Net per day - B2B""}]",Data Analysis & BI,1100,1250,Net per day - B2B
Full-time,Manager / C-level,Permanent or B2B,Remote,179,Data Engineering Team Leader,Profitroom,"We are currently looking for an experiencedData Engineering Team Leaderto join our Data Team and help us make our company even more data-driven. The results of your work will directly impact product development, the way we support our customers, and influence our high-level business strategy. If you are ready to take initiative and believe in data-driven decision-making ‚Äì this role is perfect for you! Serve as a technical and team leader in the Data Engineering team: Lead team development and foster soft people management Support career paths and mentor team members Act as a Delivery Manager for data-related projects: Define, prioritize, and ensure timely delivery of engineering tasks Take ownership of major technical decisions and engineering excellence: Establish and enforce standards for testing, code review, CI/CD, documentation, and monitoring Oversee the architecture of our data platform: Maintain and evolve our Lakehouse infrastructure (preferably Databricks-based) Introduce new tools and technologies aligned with business and technical goals Supervise the logical structure and modeling of data: Ensure semantic and architectural consistency of datasets Leverage approaches such as the Kimball model or Medallion architecture Contribute to coding: Develop and optimize data pipelines using Dagster, Python, and PySpark Collaborate cross-functionally with data analysts, PMs, and other business stakeholders Drive and mature data governance practices, including cataloguing and lineage Minimum of 3+ years of experience as a Data Engineer or in a similar role related to data Experience in leading technical teams or managing data projects = at least 1 year (a big advantage) or willingness to grow into it Strong knowledge of Python, PySpark (nice to have), orchestration tools like Dagster or Airflow and SQL Experience working with cloud data platforms (Databricks experience is a plus) and Lake/Lakehouse architectures Ability to define and uphold high engineering standards and processes Proficiency in data modeling (Kimball, Medallion) Excellent communication skills (English at B2+ level) Strong ownership and self-organization Nice to have: Experience with Databricks and Delta Lake Familiarity with tools such as DataHub, Terraform, Docker Background in implementing data governance, lineage, and quality frameworks Python, PySpark, SQL, Databricks, GCP (BigQuery), Dagster, Airflow, Delta Lake, Docker, Terraform, DataHub Enjoy Work-Life Balance: Embrace a fully remote and flexible work environment. Explore the World: Avail annual 'Work with Us, Travel with Us' vouchers. Grow Your Skills: Access to English language classes along with a dedicated team development fund. Stay Healthy: Benefit from co-financed life and medical insurance, access sports facilities and receive professional mental health support whenever needed. Take Time Off: Get 26 days off with a Contract of Employment and 24 days off break with B2B contracts. Share hospitality: Take 2 extra days off (annually) for CSR activities. Join Celebrations: Participate in company retreats, events, and wedding & baby packs, benefit from our employee referral program. Transparent Culture: Experience a flat hierarchy and open communication channels for transparency. Contract Enhancements: earn between 25 500 PLN to 30 000 PLN on a B2B contract or between 21 000 to 25 000 PLN gross for Contract of Employment. About Us: We're a global leader in hospitality software, founded in Pozna≈Ñ, Poland in 2008. We‚Äôve grown to serve over 3,500 customers across five continents, helping hotels and resorts maximize their revenue and guest satisfaction.","[{""min"": 25500, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,25500,30000,Net per month - B2B
Full-time,Mid,Permanent,Remote,181,Power BI Developer,EndySoft,"Position Overview: We are seeking a talented Power BI Developer to join our team. The ideal candidate will have expertise in developing and implementing Power BI dashboards, reports, and data visualizations to support data-driven decision-making. This role involves collaborating with various stakeholders to gather requirements and deliver interactive, high-quality BI solutions. MD rate: 16600-20000 PLN Roles and Responsibilities: Design, develop, and deploy Power BI dashboards and reports based on business requirements. Connect to various data sources, including SQL databases , Excel , and cloud services, to create comprehensive BI solutions. Develop and maintain Power BI datasets , dataflows , and queries to ensure efficient and accurate data models. Implement DAX calculations and measures to enable advanced data analysis and insights. Optimize Power BI reports for performance and usability, ensuring quick load times and responsive user experiences. Collaborate with stakeholders to gather requirements and translate them into actionable dashboards and reports. Perform data analysis and create meaningful visualizations to support business strategies and operational decisions. Conduct user training and provide ongoing support for Power BI tools and solutions. Stay updated with the latest Power BI features and best practices to continuously enhance BI solutions. Required Skills and Experience: Proficiency in Power BI development, including report building, data modeling, and visualization. Strong knowledge of DAX (Data Analysis Expressions) for creating complex calculations and measures. Experience with SQL for querying and transforming data. Familiarity with data integration tools and methods for connecting Power BI to various data sources. Strong understanding of data modeling concepts , including star schema and snowflake schema . Experience with Power BI Service for publishing and managing dashboards. Knowledge of data governance and security in Power BI, including row-level security (RLS). Strong analytical and problem-solving skills. Excellent communication and collaboration abilities. Nice to Have: Experience with Power Query for data transformation. Familiarity with Azure Data Services such as Azure Synapse , Azure Data Factory , or Azure SQL Database . Knowledge of Power BI Paginated Reports . Experience with Python or R for advanced analytics within Power BI. Understanding of big data technologies such as Databricks or Spark . Exposure to Agile/Scrum methodologies. Experience with version control tools like Git for Power BI development. Additional Information: This role offers an exciting opportunity to work on impactful BI projects and transform data into actionable insights. If you are passionate about leveraging Power BI to drive business success, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,16600,20000,Gross per month - Permanent
Full-time,Senior,B2B,Hybrid,182,Big Data Engineer,Link Group,"üöÄ Big Data Engineer üìçRemote or Hybrid (EMEA preferred)| Full-time | AdTech Platform | Python/Java/Scala We‚Äôre looking for an experiencedBig Data Engineerto join our high-impact team building the backbone of a globaladvertising platformdelivering personalized content to millions of media-enabled devices. Your work will directly influencedata-driven decision making, real-time targeting, and analytics pipelinesfor one of the most advanced AdTech ecosystems in the industry. Build and maintainrobust, scalable data pipelinesto support attribution, targeting, and analytics Collaborate closely withData Scientists, Engineers, and Product Managers Design and implement efficient storage and retrieval layers for massive datasets Optimize data infrastructure andstreaming processing systems(e.g. Flink, Apache Ignite) Drive quality throughunit tests, integration tests, and code reviews Develop and maintainAirflow DAGsand other orchestration pipelines Translate business needs into robust, technical data solutions Lead or supportA/B testing and data-driven model validation Contribute toR&D initiativesaround cloud services and architecture 5+ years of experience in backend/data engineering usingPython,Java, orScala Strong experience withBig Data frameworks(Hadoop, Spark, MapReduce) Solid knowledge ofSQL/NoSQLtechnologies (e.g. Snowflake, PostgreSQL, DynamoDB) Hands-on withKubernetes,Airflow, andAWS(or similar cloud platforms) Stream processing experience: Flink, Ignite Experience withlarge-scale dataset processing and performance optimization Familiarity with modern software practices: Git, CI/CD, clean code, Design Patterns Fluent in English (B2+) Degree in Computer Science, Telecommunications, or related technical field Experience withGoLangorGraphQL Hands-on withmicroservicesorserverlesssolutions Experience incontainer technologies(Docker, Kubernetes) Previous work inAdTech, streaming media, or real-time data systems","[{""min"": 100, ""max"": 130, ""type"": ""Net per hour - B2B""}]",Data Engineering,100,130,Net per hour - B2B
Full-time,Mid,B2B,Remote,183,Business Intelligence Developer,EndySoft,"Position Overview: We are seeking a skilled Business Intelligence Developer to join our team. The ideal candidate will have a strong background in designing, developing, and maintaining BI solutions that help organizations make data-driven decisions. This role involves working with large datasets, building data pipelines, and creating insightful reports and dashboards to support business operations and strategy. MD rate: 16000 - 2 0000 PLN Roles and Responsibilities: Design, develop, and maintain BI solutions , including dashboards, reports, and data visualizations. Collaborate with stakeholders to gather requirements and translate them into technical BI solutions. Build and optimize ETL pipelines to ensure efficient data flow from various sources to data warehouses. Develop and maintain data models to support reporting and analytics needs. Create interactive dashboards and reports using tools like Power BI , Tableau , or QlikView . Perform data analysis to provide actionable insights and support decision-making processes. Ensure data accuracy and consistency through data validation and quality checks. Monitor and improve the performance of BI systems and applications. Stay updated with the latest BI tools, technologies, and best practices. Required Skills and Experience: Proficiency in SQL for querying and managing data in relational databases. Experience with ETL tools such as Informatica , Talend , or SSIS . Hands-on experience with BI tools like Power BI , Tableau , QlikView , or similar. Strong understanding of data modeling concepts, including star schema and snowflake schema . Familiarity with data warehousing technologies such as Snowflake , Redshift , or Azure Synapse . Experience with scripting languages like Python or R for data analysis and automation. Strong problem-solving skills and attention to detail. Good communication and collaboration abilities. Nice to Have: Experience with cloud platforms like AWS , Azure , or Google Cloud for BI solutions. Familiarity with big data technologies such as Spark , Hadoop , or Databricks . Knowledge of machine learning and predictive analytics . Experience with version control systems like Git . Exposure to Agile/Scrum development methodologies. Knowledge of performance tuning for BI applications and databases. Additional Information: This is an exciting opportunity to work on innovative BI solutions and contribute to data-driven decision-making. If you are passionate about transforming data into actionable insights and thrive in a fast-paced environment, we encourage you to apply.","[{""min"": 16000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,16000,20000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,184,"Senior Data Engineer, Finance Tech, SCM Tech Fulfilment (m/f/x)",HelloFresh,"HelloFresh Group, the world‚Äôs leading integrated food solutions provider, is expanding with a new R&D Tech office in Poland. With brands offering meal kits, ready-to-eat meals, and specialty products such as meat, seafood, and pet food, we are seeking individuals who are ready to make an impact from day one. Joining us in Wroc≈Çaw means shaping the culture, working on meaningful R&D Tech projects, and contributing to a global company changing how people eat. At HelloFresh Group, we are driven by a high-performance culture that values speed, agility, and continuous learning. We believe in hands-on contribution and fostering a truly collaborative, egoless environment where every team member contributes to our mission ofchanging the way people eat, foreverand welcome team players who thrive in a dynamic environment, lead with ownership, and bring diverse perspectives to the table. Our teams thrive on in-person collaboration, with an expectation to work from the office four days a week. This approach creates a dynamic space where ideas flourish, decisions are made efficiently, and our collective impact is accelerated. It's how we stay closely connected to our shared goals and drive swift execution. HelloFresh's Supply Chain Management (SCM) is fundamental to our operations, overseeing supplier contracts, order placement, goods receipt, inventory control, and production processes. Our mission is to enhance efficiency and minimize waste across the supply chain, aspiring to build the world's leading, scalable, and fully-integrated food supply chain management platform. We are seeking an experienced and highly motivated Senior Data Engineer to join our Finance Tech Squad within the Supply Chain Management organization. This role is instrumental in the design, development, and maintenance of robust data services built upon our critical finance core systems, including Oracle Fusion Cloud ERP, Workday, and various internal applications. If you are not familiar with SCM, you can find more informationhere. Assume comprehensive ownership of the end-to-end data pipeline lifecycle, including architecture, design, development, deployment, and operational support. This will involve the application of DevOps principles, pair programming, and other cutting-edge methodologies. Act as a proactive, solution-oriented member within autonomous, cross-functional agile teams, collaborating extensively with Business Stakeholders in our global and local finance teams, Product Owners, Back-end Engineers and Business Intelligence teams. Cultivate and demonstrate an in-depth understanding of HelloFresh‚Äôs core product offerings and architectural landscape. Serve as an ambassador for software solutions, providing expert support and mentorship to colleagues. Leverage and contribute to state-of-the-art data technologies, including AWS services (EMR, Glue, S3), Kafka, PySpark, Kubernetes, Airflow, Prefect, Tecton, Databricks, Snowflake, and our proprietary in-house Data Pipeline tools. Software Engineering Proficiency: Demonstrated strong software engineering experience with the capability to design, implement, and deliver maintainable, high-quality code primarily in Python. Cloud & Data Processing Experience: Hands-on experience with prominent cloud computing platforms and distributed data processing technologies, including PySpark, Kubernetes. Kafka is a plus. Data Modeling Expertise: Proven expertise in data modeling principles, relational databases (e.g. PostgreSQL), and object storage solutions (e.g. AWS S3). Data Pipeline Development: Extensive experience in building, optimizing, and maintaining efficient and robust data pipelines. Data Quality Advocacy: A strong commitment to ensuring data quality assurance and implementing comprehensive data monitoring strategies. End-to-End Development Lifecycle: Proficiency across the entire software development lifecycle, including unit, integration, and functional testing, application tuning and profiling, and continuous integration. Agile Methodologies: Extensive experience working within agile methodologies, with a strong emphasis on delivery, lean principles, and rapid iteration. Continuous Learning: A proactive approach to continuous professional development and a willingness to learn from and contribute to a peer-learning environment. Collaboration & Mentorship: Excellent collaborative skills with the ability to effectively mentor team members and share practical knowledge and industry trends. Preferred: Familiarity with e-commerce and/or subscription-based business models is a significant advantage. Desirable: Experience with Snowflake. This role requires strong communication skills due to frequent interactions with diverse global teams, including back-end developers, front-end developers, product analysts, product managers, and business stakeholders. We are seeking highly capable problem-solvers who can apply their engineering expertise across a wide range of platforms and environments, while also serving as a coach and mentor to team members and stakeholders. Health- You‚Äôre covered from your first day with private health insurance Hybrid Working Schedule- We work in-office 4 days a week to align on goals, with flexible hours to support work-life balance and personal needs Holidays- You receive 26 days of paid vacation each year, providing you time to rest and recharge Learning and Development- An annual Learning & Development budget and a Mentoring Program to support your ongoing professional growth Employee Referral Program- Our team members can participate in our internal employee referral program and receive a bonus for recommending successful candidates to open roles Daily Comforts- Free coffee, drinks, and fresh fruit are available to keep you refreshed throughout the day If you are passionate about making a tangible impact and thrive in a fast-paced environment where your work directly contributes to a global purpose, we encourage you to apply ‚Äì even if your experience doesn't tick every single box, we believe there are many ways to develop skills and grow with us.#Engineering","[{""min"": 14700, ""max"": 22100, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14700,22100,Gross per month - Permanent
Full-time,Senior,B2B,Remote,187,Big Data Developer,Altimetrik Poland,"2-3 day per week you need to be available until 9: 00 PM for meetings with the US team. Altimetrik Polandis a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are seeking a skilledSenior Data EngineerforAirbnb-our customer, an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. Together with them, we are building a world-class payments platform that moves billions of dollars, in 191 countries, with 75 currencies, through a complex ecosystem of payments partners. They are also reinventing how to serve users to improve performance, scalability and extensibility. Responsibilities: Proactively and effectively manages communications, ensuring all stakeholders are kept informed about project progress, changes, and updates. Debugs complex issues without assistance. Navigates ambiguous product requirements to build eloquent solutions. Manages multiple projects at once. Collaborates effectively with cross-functional teams such as Data Science, Product Engineering, Advanced Analytics, and other stakeholders to design and implement data solutions. And if you possess.. Solid understanding of Spark and ability to write, debug and optimize Spark code with Python. Strong knowledge of Python, and expertise with data processing technologies and query authoring (SQL). Nice to have: Expertise in data modeling, warehousing, and working with columnar databases (e.g., Redshift, BigQuery). Extensive experience designing, building, and operating robust distributed data platforms (e.g., Spark, Kafka, Flink) and handling data at the petabyte scale. ‚Ä¶ then we are looking for you! We work 100% remotely or from our hub inKrak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 25000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Engineering,25000,33000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,188,Senior Data Engineer,emagine Polska,"Project information: Industry: Insurance and IT services Rate: up to 180 z≈Ç/h netto + VAT Location: Warsaw (first 2-3 months of office visits once a week, then occasionally) Project language: Polish, English Summary: As aSenior Data Engineer, you will play a pivotal role in developing scalableData Hubsto support various applications, includinganalytical, reporting, operational,andGenerative AI. This position emphasizes key skills such asPythonandSQLprogramming, expertise incloud environmentslikeAzure, and proficiency inDatabricks and Spark. Your responsibilities will include defining architecture standards, mentoring team members, and ensuring the performance and compliance of data solutions. Responsibilities: Data Hub Architecture: Design scalable Data Hub systems for integrating both structured and unstructured data. Technical Leadership: Define practices and patterns for data engineering implementations. Mentoring & Knowledge Sharing: Provide guidance and support to fellow engineers. Data Pipeline Optimization: Create and enhance batch and real-time data workflows. Quality & Monitoring: Implement measures for data validation and anomaly detection. Automation & CI/CD: Streamline deployment processes through automation and DevOps practices. Collaboration & Strategy: Work with diverse teams to align data solutions with business goals. Security & Compliance: Ensure data governance and adherence to regulations. Documentation & Knowledge Management: Produce quality technical documentation for reference. Key Requirements: Programming Skills: Proficient inPython and SQL. Cloud Experience: Familiarity withAzure Data Factory, ADLS, Azure SQL, Synapse. Databricks & Spark: Extensive knowledge ofDatabricks and Apache Spark. Data Architecture: Expertise in designing enterprise-scale platforms. Streaming Data Processing: Experience with real-time data analysis tools. Automation & DevOps: Skills inCI/CD, Terraform, Kubernetes, Docker. Data Governance: Knowledge of data compliance practices. Leadership: Capability to guide teams effectively. Communication: Proficient in technical documentation and fluent in English (minimum B2). Nice to Have: Stream Analytics Skills: Experience with Azure Stream Analytics. Event Streaming Tools: Familiarity with Azure Event Hubs. Agile Methodologies: Experience working in Agile/Scrum environments.","[{""min"": 160, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,180,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,189,Data Engineer,ITDS,"Data Engineer Join us, and create cutting-edge pipelines for seamless data transformation! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As a Data Engineer, you will be working for our client, a global financial institution that is driving DevOps transformation through data analytics and engineering. You will be part of a team that provides key metrics and analytical products to enhance software engineering practices across the organization. Your role will focus on developing data transformation pipelines, ensuring data quality, and supporting a cloud data platform to improve the overall DevOps experience. You will collaborate with diverse global teams to deliver enriched datasets, dashboards, and insights that enable strategic decision-making. Your main responsibilities: Designing, developing, testing, and deploying data ingest, quality, refinement, and presentation pipelines Operating and iterating on a cloud data platform to support internal goals Building and maintaining ETL processes and data transformation pipelines Ensuring data quality and implementing automated data validation solutions Developing data marts and optimizing schema designs for performance and usability Collaborating with business stakeholders to understand data needs and deliver actionable insights Working with cloud-based big data technologies, particularly Google Cloud Platform (GCP) and BigQuery Utilizing orchestration and scheduling tools such as Airflow and Cloud Composer Supporting continuous integration and continuous delivery (CI/CD) processes Following Agile methodologies and working within a product-oriented culture You're ideal for this role if you have: At least 7 years of professional experience in SQL development Strong experience in data engineering and ETL processes Expertise in GCP, BigQuery, and data build tools (DBT) Hands-on experience with Apache Airflow and Cloud Composer Proficiency in data modeling and designing optimized data schemas Experience with data streaming technologies such as Kafka Familiarity with BI tools, especially Looker Studio Understanding of DevOps principles and working in a DevOps environment Experience with Continuous Integration and Continuous Delivery (CI/CD) practices Strong communication skills and ability to work with global teams It is a strong plus if you have: Experience in building and operating a cloud data platform Knowledge of data architecture and data marts Proficiency in Git, Shell scripting, and Python Ability to quickly learn and adapt to new technologies Experience collaborating with technical staff and project managers for efficient delivery Proactive approach to identifying improvement opportunities and solving issues Comfort in working in fast-paced, changing, and ambiguous environments","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,26000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,192,Senior Database Developer,1dea,"Do warszawskiego oddzia≈Çu firmy z UK - FinTech, praca nad wewnƒôtrznym produktem - szukamy Senior Database Developer'a. Zakres obowiƒÖzk√≥w: Tworzenie i rozwijanie nowych funkcjonalno≈õci oraz modyfikacja istniejƒÖcych system√≥w bazodanowych. Optymalizacja i utrzymanie aplikacji wykorzystywanych przez klient√≥w firmy. Wprowadzanie ulepsze≈Ñ technicznych oraz administracja bazami danych (instalacja poprawek, aktualizacje). Wsparcie zespo≈Ç√≥w biznesowych i wsp√≥≈Çpraca z Project Managerami. RozwiƒÖzywanie incydent√≥w oraz problem√≥w zwiƒÖzanych z bazami danych. Wymagania: Minimum 5 lat do≈õwiadczenia w pracy jako Database Developer, w tym min. 2 lata z Oracle PL/SQL. Wymagane do≈õwiadczeniem w migracji z Oracle do PostgreSQL Bardzo dobra znajomo≈õƒá SQL i PL/SQL oraz architektury baz danych Oracle. Do≈õwiadczenie w pracy z systemami kontroli wersji (np. SVN, Git). Znajomo≈õƒá metod debugowania i testowania jednostkowego kodu PL/SQL. Umiejƒôtno≈õƒá pracy w zespole i dobra organizacja pracy. Dobra znajomo≈õƒá jƒôzyka angielskiego B2/C1 (praca w miƒôdzynarodowym ≈õrodowisku). Lokalizacja biura: Warszawa ≈ör√≥dmie≈õcie, 1 dzie≈Ñ/ tydzie≈Ñ w biurze Umowa: B2B, d≈Çugoterminowe, bezpo≈õrednie zatrudnienie Rekrutacja: 2 etapy online","[{""min"": 24000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Engineering,24000,33000,Net per month - B2B
Full-time,Mid,B2B,Remote,193,Data Engineer,Link Group,"About the Role We're looking for aData Engineerto join a growing team working on modern data platforms. You will play a key role in designing, developing, and maintaining scalable data pipelines and infrastructure to support data analytics and reporting initiatives. This is a great opportunity to work with cutting-edge cloud and big data technologies. Design, build, and maintain scalableETL/ELT pipelines Work with structured and unstructured data from diverse sources Optimize data workflows for performance, reliability, and cost Implement data quality checks and monitoring Collaborate with analysts, architects, and other engineers to support data needs Build data integrations with internal and third-party APIs Support cloud data infrastructure and automation 3+ years of experience as a Data Engineer or similar role Strong knowledge ofSQLand data modeling principles Experience withPythonorScalafor data processing Hands-on experience with cloud platforms (ideallyAWS, but Azure/GCP also valuable) Familiarity with tools likeApache Spark,Airflow,Kafka, or similar Experience withdata lakes,data warehouses, orlakehouse architectures Git and CI/CD workflows Strong problem-solving skills and ability to work independently Experience withSnowflake,Redshift, orBigQuery Familiarity withdbt,Terraform, or other IAC tools Background indata governanceorsecurity Experience with real-time data processing (e.g., Flink, Kinesis) Exposure toML pipelinesorMLOps","[{""min"": 90, ""max"": 105, ""type"": ""Net per hour - B2B""}]",Data Engineering,90,105,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,194,üëâ Senior AWS Data Engineer (Future Opening),Xebia sp. z o.o.,"üü£ You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. üü£ Your profile: 3+ years‚Äô experience with AWS (Glue, Lambda, Redshift, RDS, S3), 5+ years‚Äô experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools ‚Äì Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, working knowledge of Git, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ üü£ Nice to have: experience with Amazon EMR and Apache Hadoop, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification, üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,22300,33700,Net per month - B2B
Full-time,Senior,B2B,Remote,196,Senior Administrator Baz Danych Oracle / Senior Oracle DBA,Simora,"Do≈ÇƒÖcz do naszego zespo≈Çu jako Senior Administrator Baz Danych Oracle! Poszukujemy osoby, kt√≥ra zajmie siƒô zarzƒÖdzaniem bazami danych naszych klient√≥w. Jeste≈õmy firmƒÖ, kt√≥ra rozwija nowoczesne rozwiƒÖzania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzƒôdzi wspierajƒÖcych naszych klient√≥w. Cenimy pozytywnƒÖ atmosferƒô, wzajemny szacunek i zaanga≈ºowanie ‚Äì to fundamenty naszego zespo≈Çu. Tw√≥j zakres obowiƒÖzk√≥w Tworzenie baz danych oraz ≈õrodowisk bazodanowych na ≈õrodowisku produkcyjnym i testowym Migrowanie baz danych na nowe ≈õrodowiska ZarzƒÖdzania i aktualizacja baz danych i ≈õrodowisk bazodanowych Konfigurowanie i optymalizacja ≈õrodowiska bazodanowego Automatyzacja zada≈Ñ za pomocƒÖ jƒôzyk√≥w skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jako≈õci i bezpiecze≈Ñstwa dla tworzonych rozwiƒÖza≈Ñ Wdra≈ºanie rozwiƒÖza≈Ñ opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Sta≈Çe doskonalenie sposobu Twojej pracy Nasze wymagania Min 5 letnie do≈õwiadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomo≈õƒá jƒôzyka SQL i PLSQL Umiejƒôtno≈õƒá dbania o szczeg√≥≈Çy i jako≈õƒá rozwiƒÖza≈Ñ Komunikatywno≈õƒá Znajomo≈õƒá jƒôzyka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykszta≈Çcenie wy≈ºsze (preferowany kierunek: informatyka) Znajomo≈õƒá jƒôzyk√≥w programowania: Python, Java itp. Znajomo≈õƒá system√≥w operacyjnych Linux / Windows Znajomo≈õƒá system√≥w wirtualizacyjnych np.: OLVM Oferujemy CiekawƒÖ pracƒô w firmie o wysokiej dynamice rozwoju Mo≈ºliwo≈õƒá podniesienia kwalifikacji w obszarach zwiƒÖzanych z bazami danych, bezpiecze≈Ñstwem danych oraz sztucznƒÖ inteligencjƒÖ Benefity Karta Multisport Prywatna opieka zdrowotna ‚Äì Medicover Praca zdalna Brak dress code‚Äôu Dofinansowanie szkole≈Ñ i kurs√≥w Elastyczny czas pracy","[{""min"": 10000, ""max"": 16000, ""type"": ""Net per month - B2B""}]",Database Administration,10000,16000,Net per month - B2B
Full-time,Mid,B2B,Remote,198,Data Engineer,Ework Group,"üíª Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking forData Engineerüîπ‚úîÔ∏èIdeally, candidates should be available to work from Wroc≈Çaw office (hybrid) ; however, remote work is also possible. ‚úîÔ∏èYour responsibilities include: Develop and enhance application features that support internal users and business processes. Diagnose and resolve issues related to data quality and system maintenance. Ensure efficient and reliable operation of the organization‚Äôs data processing systems. Design and implement solutions using cloud platforms ‚Äì preferably Microsoft Azure. Work hands-on with Databricks (primarily PySpark) for big data processing and analysis. Manage and optimize databases such as SQL Server or Netezza. Implement and integrate solutions using services such as: Azure Data Factory, Databricks, PySpark, Python. Work in a diverse and rapidly changing landscape of data sources. Collaborate within an Agile/Scrum team environment, following modern software development practices. Take initiative and ownership of tasks while maintaining accuracy and accountability for results. Communicate and collaborate effectively in an international environment using English. Contribute to a knowledge-sharing and team-oriented culture. ‚úîÔ∏èYour experience and background: You are graduated with a Master's degree in Computer Science, Data Engineering or any related field Data & Engineering has been your world for at least 3-5 years Cloud service platform expertise; preferably Azure Hands-on experience working with Databricks (PySpark) Databases (like SQL Server or Netezza) don‚Äôt have any secret for you Knowledge of at least three of these services: Azure Data Factory, Azure Synapse, Databricks, Azure SQL Database, Power BI, Spark, Python, Mongo DB, Azure Functions You are comfortable working in a diverse, complex and fast-changing landscape of data sources You care about agile software processes, data-driven development, reliability, and responsible experimentation. Capacity to work in an international environment using English Team player - having a sharing and collaborative mindset Autonomous, Rigorous, takes ownership at work Agile - flexibility to execute available tasks in a rapidly changing environment ‚úîÔ∏èSoft Skills: Excellent communication skills Ability to understand business stakeholders and the business landscape Ability to challenge decisions made by the platform or other architects Strong analytical, organizational, problem-solving, and time-management skills Comfortable working in a diverse, complex, and fast-changing landscape of data sources Proactive problem solver with innovative thinking and a strong team player Fluent in English (C1) ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events Contact person: karolina.rosikiewicz@eworkgroup.com(","[{""min"": 115, ""max"": 128, ""type"": ""Net per hour - B2B""}]",Data Engineering,115,128,Net per hour - B2B
Full-time,Mid,B2B,Remote,200,BI Developer,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: projektowanie, budowa i utrzymanie kompleksowych przep≈Çyw√≥w danych oraz proces√≥w wzbogacania danych, wsp√≥≈Çpraca z Product Managerami, analitykami biznesowymi, Data Scientistami oraz interesariuszami na r√≥≈ºnych poziomach organizacji w celu przekszta≈Çcania wymaga≈Ñ biznesowych w konkretne rozwiƒÖzania BI, projektowanie i rozw√≥j hurtowni danych oraz struktur analitycznych (data marts) wspierajƒÖcych raportowanie i analizƒô danych, tworzenie i utrzymywanie dashboard√≥w w Power BI oraz rozw√≥j modeli semantycznych, zarzƒÖdzanie transformacjami danych w ramach proces√≥w ELT przy u≈ºyciu DBT, utrzymanie wysokiej wydajno≈õci i skalowalno≈õci rozwiƒÖza≈Ñ BI, wsparcie i szkolenie u≈ºytkownik√≥w ko≈Ñcowych w zakresie korzystania z system√≥w BI, praca zdalna w zespole miƒôdzynarodowym (+1h r√≥≈ºnicy wzglƒôdem polskiej strefy czasowej), stawka do 130 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Aktualnie posiadamy dwa osobne wakaty, na kt√≥re aktualnie rekrutujemy!1) Ta oferta jest dla Ciebie, je≈õli: posiadasz minimum 3 lata do≈õwiadczenia na podobnym stanowisku, masz bardzo dobrƒÖ znajomo≈õƒá SQL posiadasz praktyczne do≈õwiadczenie z Lookerem, w tym rozw√≥j LookML, tworzenie dashboard√≥w oraz administracja Lookerem i zarzƒÖdzanie uprawnieniami, posiadasz do≈õwiadczenie z DBT w kontek≈õcie transformacji danych w pipeline'ach ELT, znajomo≈õƒá angielskiego na poziomie min. B2, posiadasz znajomo≈õƒá Snowflake, lub innych nowoczesnych baz danych analitycznych, 2) Ta oferta jest dla Ciebie, je≈õli: posiadasz minimum 4 lata do≈õwiadczenia na podobnym stanowisku, posiadasz bardzo dobrƒÖ znajomo≈õƒá SQL oraz modelowania danych, posiadasz do≈õwiadczenie w pracy z Power BI (dashboardy, modele danych, DAX), posiadasz praktycznƒÖ znajomo≈õƒá DBT i architektury ELT, posiadasz znajomo≈õƒá hurtowni danych - szczeg√≥lnie Snowflake i/lub BigQuery, posiadasz do≈õwiadczenie w pracy z systemami. znajomo≈õƒá angielskiego na poziomie min. B2, mile widziane do≈õwiadczenie z Lookerem oraz znajomo≈õƒá platform chmurowych (Azure, AWS, GCP). Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 18480, ""max"": 21840, ""type"": ""Net per month - B2B""}]",Data Engineering,18480,21840,Net per month - B2B
Full-time,Manager / C-level,Permanent,Hybrid,206,"Engineering Manager, Data Infrastructure",Asana,"We are looking for an Engineering Manager who can provide technical leadership for our data platform and effectively support engineers with large-scope responsibilities. You will be responsible for the delivery and technical quality of our programs, taking on complex multi-year projects, and leading the work of several ICs. You‚Äôll also partner with cross-functional stakeholders in Infrastructure, Data Science, and Business teams to drive initiatives forward, and mentor team members in technical design, project leadership, and running team processes. The Data Infrastructure team builds the infrastructure responsible for moving product and business data in a secure, efficient and timely manner to Data consumers. We operate several systems ranging from data ingestion pipelines, Spark processors, data/ML tooling, experimentation platform, and logging pipelines for Asana‚Äôs Data Lake. This role is based in our Warsaw office with an office-centric hybrid schedule - in-office days are Monday, Tuesday, and Thursday. We offer a Contract of Employment (UoP) for our employees in Poland. What you‚Äôll achieve: Support and coach engineers in their technical and professional development. Partner with cross-functional stakeholders to guide the team‚Äôs roadmap and prioritization. Collaborate with data engineering and with other infrastructure teams on problems related to data infrastructure service models, operations, deployment and security. Evangelize good code and solid engineering and operability practices. Co-create and follow best practices of running data applications in the cloud (AWS) and all the state-of-the-art tooling around it. Bootstrap our Data Infra presence in Warsaw! Some upcoming technical challenges you may work on: Build cost tracking/observability systems for data use cases; Help improve the Databricks experience for Asana users; Improve deployment and release automation for data pipelines; and Implement best practices for security and privacy of our data infrastructure. About you: Prior experience building and operating cloud infrastructure (preferably involving analytical/real-time data or storage) and distributed systems at scale. Familiarity with distributed data processing systems (e.g. Spark, Databricks, Snowflake or similar systems). Expertise in AWS services (e.g. IAM, S3) and with infrastructure-as-code tooling (e.g. Terraform). 2-4 years building/leading engineering teams as a manager. Planning/execution as well as career and impact growth for your reports. Experience collaborating with other engineering teams, PMs, and cross-functional partners on alignment and project execution, and with teams in other timezones. Focus on maximizing impact, for yourself and your team. Ability to provide valuable input to any technical or product discussion. Oriented around the multi-year consequences of your decisions. What we offer: Generous, transparent and fair compensation system (base salary and generous Restricted Stock Unit for Asana Inc.). Contract of Employment (with 50% tax deductible costs for author‚Äôs rights usage for Engineers). Health insurance with dental and travel coverage (Lux Med). Lunch catering on the days that you work from the office. Career growth budget. Home office setup budget. Gym/Fitness card. Fertility healthcare and family-forming support with Carrot . Mental health support in Modern Health. Group life insurance. MacBooks with all necessary accessories. For this role, the estimated base salary range is between 39 500 - 50 400 PLN gross monthly on the contract of employment (UoP). The actual base salary will vary based on various factors and individual qualifications objectively assessed during the interview process. The listed range above is a guideline, and the base compensation range for this role may be modified. Our total compensation consists of base salary and equity (RSUs). About us Asana helps teams orchestrate their work, from small projects to strategic initiatives. Millions of teams around the world rely on Asana to achieve their most important goals, faster. Asana has been named a Top 10 Best Workplace for 5 years in a row, is Fortune's #1 Best Workplace in the Bay Area, and one of Glassdoor‚Äôs and Inc.‚Äôs Best Places to Work. After spending more than a year physically distanced, Team Asana is safely and mindfully returning to in-person collaboration, incorporating flexibility that adds hybrid elements to our office-centric culture . With 11+ offices all over the world, we are always looking for individuals who care about building technology that drives positive change in the world and a culture where everyone feels that they belong. We believe in supporting people to do their best work and thrive, and building a diverse, equitable, and inclusive company is core to our mission. Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We provide equal employment opportunities to all applicants without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by law. We also comply with the San Francisco Fair Chance Ordinance and similar laws in other locations. Our comprehensive compensation package plays a big part in how we recognize you for the impact you have on our path to achieving our mission. We believe that compensation should be reflective of the value you create relative to the market value of your role. To ensure pay is fair and not impacted by biases, we're committed to looking at market value which is why we check ourselves and conduct a yearly pay equity audit.","[{""min"": 35000, ""max"": 45000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,35000,45000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,209,Data Engineer with Data Vault,emagine Polska,"PROJECT INFORMATION: Industry: FinTech Client: company from Sweden Remote work: 100% remote Consultant‚Äôs location: Poland Project language: English Business trips: None Project length: 6 months contracts + prolongations (we‚Äôre looking for long-term) Start: ASAP / Flexible Assignment type: B2B Stack in the order of importance on the project: Data Vault, SQL, Python, dbt. Nice to have: AWS, Snowflake Summary: The Data Engineer role is central to enhancing data infrastructure as part of a team committed to innovative financial solutions. The position aims to drive data innovation and improve data management practices to support various business initiatives. Responsibilities: Develop and maintain scalable Data pipelines using tools like Matillion and SQL. Collaborate with data infrastructure and Data teams to design optimal solutions for data ingestion and reporting. Seamlessly ingest data into Snowflake to enable advanced analytics and reporting. Serve as a technical expert, supporting business users in leveraging data effectively. Ensure data integrity, security, and governance across systems and workflows. Document and communicate technical solutions to ensure clarity across the organization. Must Haves: Strong SQL skills and a solid understanding of cloud-based data platforms. Experience with dbt (ideally with Data Vault and Snowfake). Hands-on experience with Data Vault modeling. Knowledge of best practices in data warehousing and data modeling. Excellent communication and collaboration skills to work cross-functionally. Ability to work independently or as part of a team to deliver high-quality results. Experience with Python. Previous work experience in finance or fintech industries - payment cards, loans. Nice to have: Experience with Matillion. Experience with Snowflake (or with Databricks). Experience with AWS. We offer: Long-term cooperation. Transparently built relations based on trust and fair play. Co-financed benefits: Medicover card, Multisport card.","[{""min"": 213, ""max"": 256, ""type"": ""Net per day - B2B""}]",Data Architecture,213,256,Net per day - B2B
Full-time,Mid,B2B,Remote,210,Big Data Developer,B2Bnetwork,"KOMPETENCJE OBLIGATORYJNE: Do≈õwiadczenie zawodowe na stanowisku zwiƒÖzanym z przetwarzaniem du≈ºych zbior√≥w danych jako programista, minimum 2 lata Do≈õwiadczenie projektowe w przetwarzaniu du≈ºych zbior√≥w danych, minimum 1 projekt Do≈õwiadczenie projektowe w programowaniu w jƒôzyku Python, minimum 1 projekt Do≈õwiadczenie projektowe w ≈õrodowisku obliczeniowym on-premise, minimum 1 projekt Do≈õwiadczenie w programowaniu w ≈õrodowisku Apache Spark Do≈õwiadczenie w programowaniu w Python Do≈õwiadczenie w programowaniu w Apache Airflow Do≈õwiadczenie w programowaniu w SQL Znajomo≈õƒá zagadnie≈Ñ Hadoop Programowanie proces√≥w ELT/ETL Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z procesami CI/CD Umiejƒôtno≈õƒá korzystania z systemu kontroli wersji (Git) KOMPETENCJE DODATKOWE: Wykszta≈Çcenie wy≈ºsze Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Airflow (np. Airflow Fundamentals lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá tworzenia DAG√≥w Airflow (np. Dag Authoring lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL (np. W3Schools SQL Certificate lub r√≥wnowa≈ºny) WYMAGANIA TECHNICZNE: Apache Spark Python Apache Airflow SQL Hadoop Git","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,110,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,211,Senior Data Engineer ‚Äì Graph & Analytics Systems,Holisticon Insight,"Holisticon Insightis a division ofhttp: //nexergroup.comfocused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! üëáhttps: //holisticon.pl/holisticon-insight/ We offer a B2B Contract: 165 ‚Äì 195 PLN net/hour + VAT We're seeking an experiencedSenior Data Engineerto lead work on our client's analytics platform. You'll assist in evaluating modern database technologies and designing migration strategies that enable advanced relationship analysis and improve client's ability to track metrics across complex interconnected systems. Responsibilities Design data models to represent events, entity relationships, and patterns Develop ETL pipelines to transform data structures Optimize query performance for complex relationship traversals Integrate new database solutions with existing analytics workflows Create proof-of-concept demonstrations for key use cases Help assessing current architecture and designing database migration strategy Evaluate and recommend appropriate database technologies, with focus on graph databases (Neo4j, Amazon Neptune, ArangoDB, etc.) Key Qualifications 5+ years hands-on experiencein similar role Strongunderstanding of data modeling and query optimization Experience migrating between different database paradigms (document, relational, graph) Proficiency with distributed data processing frameworks (Apache Spark preferred) Experience working with document databases (MongoDB) and JSON data structures Experience with large-scale analytics platforms Nice to have: Experience with graph databases and graph query languages(Cypher, Gremlin) Knowledge ofevent-driven data architecturesand batch processing pipelines Familiarity with visualization platforms(Grafana or similar) Experience with both batch and stream processing patterns Ability to evaluate trade-offs between different database technologies Strong communication skills to explain complex concepts to stakeholders Experience with hybrid architectures (combining multiple database types) Understanding of metrics-driven analysis and KPI tracking By joining Holisticon Insight you will get: Life insurance Multisport card Fully remote job Private medical care Flexible working hours B2B or contract of employment Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories)","[{""min"": 27720, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,27720,32000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,212,Technical Lead Erlang Engineer,Spyrosoft,"Join our team in Warsaw , where we‚Äôre collaborating on a cutting-edge fintech venture with a global industry leader. Together with our Partner ‚Äì Klarna , we‚Äôre building an IT hub designed to drive innovation in digital payment solutions . We‚Äôre on the lookout for top-tier engineers who thrive in dynamic, forward-thinking environments. Spyrosoft is leading the recruitment process, facilitating a seamless experience for candidates who are ready to shape the future of online shopping and payments. This opportunity is ideal for engineers who value independence, proactiveness, and flexibility. Our engagement begins with a B2B contract through Spyrosoft , transitioning to a direct contract with our Partner . We offer a hybrid work model in Warsaw‚Äôs vibrant Wola district. English fluency and eligibility to work in Poland are essential, as is the successful completion of a background check to meet the rigorous standards of the financial domain. Our process: CV selection Initial recruitment screening Technical interview Online logic test Cultural fit interview Project Description: You will be part of a project focused on building a robust, scalable system with high availability, designed to support complex data flows and transaction processing. The system leverages Erlang/OTP for fault-tolerant, concurrent applications and operates within a modern AWS-based cloud environment. Key components include PostgreSQL for data storage and Kafka for message streaming. The system also integrates payment functionalities and aims for high performance and reliability in financial transaction processing. Tech Stack: Erlang + OTP AWS PostgreSQL Apache Kafka Elixir (nice to have) Python (nice to have) Requirements: Minimum 1 year of relevant experience. Hands-on experience with Erlang/OTP for system development. Strong understanding of concurrent programming and building fault-tolerant systems. Proven track record in working with AWS services, architecture, deployment, management, and scaling of cloud applications. Solid experience in managing Postgres databases and leveraging Kafka for real-time data processing and integration. Familiarity with Elixir is a plus, especially with a willingness to deepen Erlang expertise. Experience with the design and development of scalable, reliable payment and collection systems is a significant advantage. Deep understanding of cloud infrastructure, especially AWS tooling. Basic knowledge of Python is beneficial to broaden the tech stack and support versatile development tasks. Strong analytical and problem-solving skills. Ability to challenge the status quo and drive continuous improvement. Excellent verbal and written communication skills, enabling effective collaboration across teams and stakeholders. Ability to mentor and support peers while contributing to team success. A degree in Computer Science, Information Technology, or a related field. Fluency in English, both written and spoken. Main Responsibilities: Design, develop, and maintain backend systems using Erlang/OTP. Implement and optimize cloud infrastructure using AWS services. Integrate and manage PostgreSQL and Kafka in a production environment. Collaborate with cross-functional teams to understand requirements and deliver scalable solutions. Participate in architectural decisions and propose improvements to enhance system performance and reliability. Support the development of payment and collection modules with a focus on reliability and scalability. Mentor junior developers and support knowledge sharing within the team. Ensure best practices in code quality, testing, and documentation. Communicate effectively with internal teams and external stakeholders to align technical goals.","[{""min"": 220, ""max"": 255, ""type"": ""Net per hour - B2B""}]",Data Engineering,220,255,Net per hour - B2B
Full-time,Senior,B2B,Remote,214,Data Engineer ETRM domain,INFOPLUS TECHNOLOGIES,"Job Description We are seeking a skilled ETRM Data Engineer to join our data engineering team and support critical initiatives in the Energy Trading and Risk Management (ETRM) domain. This role involves building robust data pipelines, integrating trading systems, and ensuring data quality across platforms such as Azure Data Factory , Databricks , and Snowflake . You will collaborate closely with traders , analysts , and IT teams to design and implement scalable, high-performance data solutions that power decision-making in fast-paced trading environments. Key Responsibilities Design, develop, and maintain scalable data pipelines and ETRM systems Lead data integration projects within the energy trading ecosystem Integrate data from ETRM platforms such as Allegro , RightAngle , and Endur Build and optimize data storage solutions using Data Lake and Snowflake Develop and orchestrate ETL/ELT workflows using Azure Data Factory and Databricks Write efficient, production-grade Python / PySpark code for data processing and analytics Build and expose APIs using FastAPI for data services Ensure data quality, consistency, and reliability across complex systems Work closely with stakeholders to translate business requirements into technical data solutions Optimize and enhance data architecture for scalability and performance Mandatory Skills Strong experience with Azure Data Factory (ADF) Proficient in Data Lake architecture and best practices Hands-on expertise with Snowflake and SQL Solid experience in Python and PySpark Knowledge of FastAPI for building scalable APIs Proven work with Databricks in production environments Nice to Have Domain experience in ETRM / energy trading systems Familiarity with Streamlit for internal dashboards Experience integrating with Allegro , RightAngle , or Endur trading platforms","[{""min"": 32000, ""max"": 36800, ""type"": ""Net per month - B2B""}]",Data Engineering,32000,36800,Net per month - B2B
Full-time,Mid,B2B,Remote,215,Big Data Developer,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 2-letnie do≈õwiadczenie na stanowisku zwiƒÖzanym z przetwarzaniem du≈ºych zbior√≥w danych jako programista Do≈õwiadczenie projektowe w przetwarzaniu du≈ºych zbior√≥w danych Do≈õwiadczenie projektowe w programowaniu w jƒôzyku Python Do≈õwiadczenie projektowe w ≈õrodowisku obliczeniowym on-premise Do≈õwiadczenie w programowaniu w ≈õrodowisku Apache Spark Do≈õwiadczenie w programowaniu w Apache Airflow Do≈õwiadczenie w programowaniu w SQL Znajomo≈õƒá zagadnie≈Ñ Hadoop Do≈õwiadczenie w programowaniu proces√≥w ELT/ETL Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z procesami CI/CD Umiejƒôtno≈õƒá korzystania z systemu kontroli wersji (Git) Dobra organizacja pracy w≈Çasnej, orientacja na realizacje cel√≥w Umiejƒôtno≈õci interpersonalne i organizacyjne, planowanie Komunikatywno≈õƒá, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista i dociekliwo≈õƒá Zdolno≈õƒá adaptacji i elastyczno≈õƒá, otwarto≈õƒá na sta≈Çy rozw√≥j i gotowo≈õƒá uczenia siƒô Mile widziane Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Do≈õwiadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Airflow (np. Airflow Fundamentals lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá tworzenia DAG√≥w Airflow (np. Dag Authoring lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL (np.. W3Schools SQL Certificate lub r√≥wnowa≈ºny) Kluczowe zadania Projektowanie, implementacja i utrzymanie rozwiƒÖza≈Ñ do przetwarzania du≈ºych zbior√≥w danych z wykorzystaniem jƒôzyka Python oraz SQL Realizacja projekt√≥w w ≈õrodowiskach obliczeniowych on-premise z wykorzystaniem Apache Spark i Apache Airflow Budowa i rozw√≥j proces√≥w integracji danych w modelu ETL/ELT Przetwarzanie danych w ≈õrodowiskach opartych o technologiƒô Hadoop Tworzenie i utrzymywanie wydajnych pipeline‚Äô√≥w danych oraz automatyzacja zada≈Ñ przetwarzania danych Wdra≈ºanie rozwiƒÖza≈Ñ zgodnych z praktykami CI/CD oraz praca z systemem kontroli wersji Git Wsp√≥≈Çpraca z zespo≈Çami projektowymi w celu realizacji cel√≥w biznesowych zwiƒÖzanych z analizƒÖ i przetwarzaniem danych Planowanie i organizacja w≈Çasnej pracy w spos√≥b umo≈ºliwiajƒÖcy realizacjƒô zada≈Ñ zgodnie z harmonogramem Aktywne rozwiƒÖzywanie problem√≥w, analiza danych oraz usprawnianie istniejƒÖcych proces√≥w CiƒÖg≈Çe poszerzanie wiedzy technicznej i gotowo≈õƒá do nauki nowych technologii oraz narzƒôdzi","[{""min"": 90, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,90,140,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,216,IPC Developer,ITDS,"Drive Innovation in Data Warehousing: IPC Developer wanted! Warsaw based opportunity with remote work model (2 days in the office/month). As anIPC Developer, you will be working for our client, a leading player in the online banking sector, on the development and optimization of data warehouse solutions. This includes designing and implementing ETL processes, enhancing data architecture, and supporting the business intelligence environment to ensure effective data reporting and analysis. The project involves using cutting-edge technologies like Informatica Power Center and Oracle-based systems to support complex financial systems. You‚Äôll be part of a dynamic IT team that ensures data integrity, performance, and scalability of large-scale data systems. Your main responsibilities: Design and implement ETL processes using Informatica Power Center Develop and maintain data warehouses and reporting data marts Participate in the implementation and integration of Informatica tools Design, implement and develop BI-class analytical environments Create and maintain Oracle databases and applications Define and document technical specifications and administrative documentation Test and validate software solutions created by others Prepare software installation packages Support the software release and handover to production teams You're ideal for this role if you have: Strong knowledge of Informatica Power Center for ETL process development Solid understanding of RDBMS Oracle 9i/10g and database design Excellent command of SQL and good knowledge of PL/SQL Understanding of information systems engineering and development methodologies At least 6 months of experience designing and implementing ETL solutions At least 1 year of experience with Oracle-based systems in information environments Practical experience designing data warehouses for large-scale institutions Ability to read and write technical documentation in English Strong analytical thinking and problem-solving skills Team-oriented mindset with attention to quality and detail Nice to have: Theoretical knowledge and practical experience with Big Data, Python, and Spark Familiarity with Business Intelligence (BI) concepts Ability to work using Agile methodologies (Scrum, Kanban) Knowledge of tools such as SQL Developer, SVN, GitHub, JIRA, and Confluence We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere. Ref. number 7273","[{""min"": 16800, ""max"": 21500, ""type"": ""Net per month - B2B""}]",Data Engineering,16800,21500,Net per month - B2B
Full-time,Mid,B2B,Office,217,Data Analyst - Monopoly World,Reality Games,"Monopoly World is going global, and we‚Äôre looking for a talented Data Analyst to help us turn data into actionable insights that will shape the future of gaming. This is your chance to be part of an exciting, high-impact project that will reach players worldwide. This role will report to the Head o f Analytics. This is a full-time, in-office position based in the iconic railway station building in Krakow. If you have a knack for solving complex problems, love working with data, and want to be part of a team that‚Äôs pushing the boundaries of mobile gaming, this is the opportunity for you. Join us in creating something truly innovative and make your mark on one of the most iconic games of all time. Come build something incredible with us! Check out our trailer and see what‚Äôs in store! üöÄüé• Key Responsibilities: Analyze and model data to provide actionable insights for game design and monetization strategies Develop and maintain data pipelines, and work with large datasets to derive meaningful patterns Collaborate with game designers and engineers to create data-driven solutions Perform web scraping and interact with REST APIs to collect and consume data Work with BigQuery SQL to query and manipulate game data for analysis Design and run simulations to support the development of Free-to-Play mobile games Continuously monitor key performance indicators and suggest improvements based on data analysis Job requirements Proficiency in at least one programming language, such as Python, JavaScript, Java, R, or Scala Experience with web scraping or consuming data from REST APIs Experience with BigQuery SQL Strong analytical thinking skills and the ability to interpret complex data At least 2 years of experience in data modeling and simulations for Free-to-Play mobile games Very good command of English Why Join Us? Leader‚Äôs support ‚Äì Get mentorship, feedback, and career growth guidance Knowledge sharing ‚Äì We invest in employee development Fast-paced career ‚Äì Clear growth opportunities with performance reviews Top-notch equipment ‚Äì Get the right tools for your job Beautiful office ‚Äì Spacious, creative workspace in the city center Flexible hours ‚Äì Whether you prefer early mornings or late evenings, we accommodate your schedule Open kitchen ‚Äì Enjoy coffee, juices, fruits, a stocked fridge, and more Team gatherings ‚Äì Connect with colleagues over pizza, games, and great company If you are passionate about data analytics and eager to apply your expertise to a groundbreaking project, we want to hear from you. Join us and contribute to an exciting and fast-growing industry. Apply now!","[{""min"": 90, ""max"": 110, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,90,110,Net per hour - B2B
Full-time,Mid,B2B,Remote,218,Data Platform Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients‚Äô systems, departments and sites. We provide an open technology platform that‚Äôs shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience.Want to be part of it? Job Title: Data Platform Engineer Job Summary: We are seeking a highly skilled and experienced Data Engineer to join our team. The successful candidate will be responsible for developing and maintaining our Data Lake and existing Data Pipelines, as well as continually exploring, analyzing, and proposing improvements to existing processes and tooling. They will also be responsible for ensuring best practices are being adopted and staying up to date with the latest research and trends in Data Engineering. Skills Required: Strong background in Data Engineering, with experience in developing and maintaining Data Pipelines and Data Lakes Proven record of accomplishment of staying up to date with the latest research and best practices in Data Engineering Excellent technical skills in Data Engineering tools and technologies Advanced proficiency in Python and SQL Understanding of AWS cloud technologies, including infrastructure as code (CDK preferred) Effective communication and interpersonal skills, with the ability to work effectively with stakeholders at all levels Strong understanding of information security and data protection principles Experience in driving technical and career development, creating appropriate goals and seeking learning opportunities within the company and the wider software community Good understanding and prior experience of the Agile process (Scrum or Kanban) Fluency with software design patterns Experience working with automotive retail technology would be a distinct advantage Key Responsibilities: Maintain and develop the Data Lake and existing Data Pipelines to support the product and data teams‚Äô requirements Continuously explore, analyze, and propose improvements to existing processes and tooling Stay up to date with the latest research, trends and best practices in Data Engineering Support the Business Intelligence team and wider Company in querying centralized data stores, including the Data Lake Work within department to maintain an ongoing understanding of the company‚Äôs data strategy and roadmap Proactively report on issues and problems Work independently, manage day-to-day workload and priorities, and take accountability for direction and output Drive your own technical and career development, create appropriate goals, and seek learning opportunities within the company and the wider software community Support colleagues on calls or in meetings with clients, partners, and suppliers as required Maintain systems under the team‚Äôs control, including user and access management Support colleagues and HR with onboarding as well as offboarding processes Ensure information security, data protection and support the business in complying with any legal obligations imposed upon it through positive actions Technologies: Python SQL: Trino, Spark-SQL, Hive, TSQL AWS Cloud services (including: s3, step functions, glue, CDK) Terraform Linux Windows Why join us? We‚Äôre on a journey to become market leaders in our space ‚Äì and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We‚Äôre committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles ‚Äì not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn‚Äôt require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply.","[{""min"": 18000, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Engineering,18000,23100,Net per month - B2B
Full-time,Senior,B2B,Remote,221,Data Modeller,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie z bran≈ºy finansowej/windykacyjnej, kt√≥rego celem jest rozbudowa platformy danych i transformacja proces√≥w danych w organizacji. Bƒôdziesz mieƒá realny wp≈Çyw na rozw√≥j nowoczesnej platformy danych , wspierajƒÖcej analitykƒô i zarzƒÖdzanie danymi w ca≈Çej organizacji, wykorzystywany stos technologiczny w projekcie: Azure (ADF, Databricks), SQL, dbdiagram.io, Azure Purview , modelowanie danych w podej≈õciu Domain-Driven Design (DDD) ‚Äì od warstwy logicznej po fizycznƒÖ, tworzenie i rozw√≥j modeli danych hurtowni (DWH) w chmurze, opracowanie i dokumentowanie kontrakt√≥w danych ‚Äì definiowanie wymaga≈Ñ danych wobec system√≥w ≈∫r√≥d≈Çowych, praca z danymi z r√≥≈ºnych ≈∫r√≥de≈Ç: bazy danych (querying), API, event streaming, projektowanie i implementacja proces√≥w ELT w ≈õrodowisku Azure / Databricks (Bronze/Silver/Gold), wsp√≥≈Çpraca z zespo≈Çami domenowymi, w≈Ça≈õcicielami proces√≥w, architektami i ≈∫r√≥d≈Çowymi zespo≈Çami IT, udzia≈Ç w budowie sp√≥jnego glosariusza danych oraz integracji z Data Governance i narzƒôdziami klasy Purview, praca 100% remote, stawka do 220z≈Ç/h NET + VAT Ta oferta jest dla Ciebie, je≈õli: posiadasz min. 4‚Äì5 lat do≈õwiadczenia w obszarze modelowania danych / DWH / Data Engineeringu, masz praktycznƒÖ znajomo≈õƒá podej≈õcia DDD oraz umiejƒôtno≈õƒá tworzenia logicznych i fizycznych modeli danych, znasz rozwiƒÖzania chmurowe ‚Äì Azure (ADF, Databricks) lub pokrewne (GCP, AWS), bardzo dobrze znasz SQL i masz do≈õwiadczenie w procesach ELT/ETL, potrafisz tworzyƒá dokumentacjƒô: glosariusze danych, specyfikacje techniczne, wymagania kontraktowe, masz do≈õwiadczenie w pracy z danymi z system√≥w ≈∫r√≥d≈Çowych i ich mapowaniu do struktur docelowych, znasz zasady Data Governance, Data Lineage i Data Quality, sprawnie komunikujesz siƒô w j. angielskim (min. B2), dodatkowo mile widziane: znajomo≈õƒá Azure Purview, dbdiagram.io, Power BI, SSIS, Python/PySpark. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 33600, ""max"": 36960, ""type"": ""Net per month - B2B""}]",Data Science,33600,36960,Net per month - B2B
Full-time,Junior,B2B,Remote,224,Junior Oracle SQL/APEX Developer,Pretius,"W Pretius poszukujemyJuniorOracle SQL/APEX Developerado projektunowegosystemu dla klienta z bran≈ºy us≈Çug doradczych i in≈ºynierskich. Lokalizacja: zdalnie lub Warszawa Wynagrodzenie: 40-60 pln netto/h O projekcie: System zarzƒÖdzania aktywami (asset management) w przestrzeni publicznej oparty o rozwiƒÖzania goespatial Modu≈Çy planowania, bud≈ºetowania inspekcji i inwentaryzacji Wykorzystywany w ponad 120 miastach Rozw√≥j i utrzymanie aktualnego systemu w celu ekspansji nowego rozwiƒÖzania Prace nad nowymi funkcjonalno≈õciami Stack: Oracle APEX, Oracle Cloud, Azure Oczekiwania: Podstawowa znajomo≈õƒá Oracle APEX Jƒôzyk angielski na poziomie B2/C1 Znajomo≈õƒá baz danych i SQL Co oferujemy w Pretius? Stawiamy na d≈Çugofalowe relacje oparte na uczciwych zasadach i rzetelno≈õci Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Mo≈ºliwo≈õƒá pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnƒôtrzne, konferencje, certyfikacje","[{""min"": 40, ""max"": 60, ""type"": ""Net per hour - B2B""}]",Database Administration,40,60,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,226,Lead Azure Data Engineer - Cybersecurity ‚òÅÔ∏èüí•,ITDS,"Join us, and enhance data analytics capabilities for a secure digital future! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office) As a Lead Data Engineer , you will be working for our client, a global leader in cybersecurity and data analytics. The Client‚Äôs team focuses on advancing cybersecurity capabilities through innovative data engineering and analytics solutions. You will be contributing to the development and enhancement of their data lake and analytics platform, supporting cutting-edge technologies and cloud infrastructures. Your role will involve collaborating with data engineers and cybersecurity professionals to build scalable data pipelines and ensure the continuous growth of security analytics systems. Your main responsibilities: Designing and implementing data pipelines for cybersecurity use cases Ingesting and provisioning raw datasets and curated data assets Driving improvements in the reliability and frequency of data ingestion Supporting and enhancing data ingestion infrastructure and pipelines Automating and optimizing data engineering workflows Identifying and onboarding new data sources for cybersecurity use Conducting exploratory data analysis for new schemas Collaborating with platform engineers for cloud infrastructure and platform engineering Ensuring the operational efficiency of production data pipelines Monitoring and enhancing cloud-based data transport and data cleaning processes You're ideal for this role if you have: 5+ years of experience in data engineering or cloud infrastructure engineering Proficiency in programming languages like Python, Java, or C# Experience with cloud technologies, especially Azure (Azure Data Factory, Azure Databricks, etc.) Strong understanding of data engineering principles and data pipeline design Expertise in building and maintaining ETL workflows across disparate datasets Knowledge of data acquisition, cloud-based data pipelines, and data transport Experience with SQL, Kusto query language, or similar query languages Familiarity with cloud cost optimization and data asset curation Ability to work in a fast-paced, collaborative environment Knowledge of cybersecurity principles (preferred but not required) It is a strong plus if you have: Experience with Infrastructure-as-Code tools such as Terraform or Ansible Familiarity with big data technologies like Kafka, Spark, and streaming services Previous exposure to Security Information & Event Management (SIEM) systems Experience in real-time analytics deployment for large-scale datasets Understanding of cybersecurity frameworks such as NIST, ISO27001, or OWASP Knowledge of cloud security practices and network protocols Experience with cloud-based security orchestration, automation, and response (SOAR) technologies We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #6917 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 1040, ""max"": 1450, ""type"": ""Net per day - B2B""}]",Data Engineering,1040,1450,Net per day - B2B
Full-time,Senior,B2B,Remote,229,Senior Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Senior Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Development and maintenance of a large platform for processing automotive data. A significant amount of data is processed in both streaming and batch modes. The technology stack includes Spark, Cloudera, Airflow, Iceberg, Python, and AWS. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Centralized reporting platform for a growing US telecommunications company. This project involves implementing BigQuery and Looker as the central platform for data reporting. It focuses on centralizing data, integrating various CRMs, and building executive reporting solutions to support decision-making and business growth. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. üöÄ Your main responsibilities: Develop and maintain a high-performance data processing platform for automotive data, ensuring scalability and reliability. Design and implement data pipelines that process large volumes of data in both streaming and batch modes. Optimize data workflows to ensure efficient data ingestion, processing, and storage using technologies such as Spark, Cloudera, and Airflow. Work with data lake technologies (e.g., Iceberg) to manage structured and unstructured data efficiently. Collaborate with cross-functional teams to understand data requirements and ensure seamless integration of data sources. Monitor and troubleshoot the platform, ensuring high availability, performance, and accuracy of data processing. Leverage cloud services (AWS) for infrastructure management and scaling of processing workloads. Write and maintain high-quality Python (or Java/Scala) code for data processing tasks and automation. üéØ What you'll need to succeed in this role: At least 5 years of commercial experience implementing, developing, or maintaining Big Data systems, data governance and data management processes. Strong programming skills in Python (or Java/Scala): writing a clean code, OOP design. Hands-on with Big Data technologies like Spark , Cloudera, Data Platform, Airflow, NiFi, Docker, Kubernetes, Iceberg, Hive, Trino or Hudi. Excellent understanding of dimensional data and data modeling techniques. Experience implementing and deploying solutions in cloud environments. Consulting experience with excellent communication and client management skills, including prior experience directly interacting with clients as a consultant. Ability to work independently and take ownership of project deliverables. Fluent in English (at least C1 level). Bachelor‚Äôs degree in technical or mathematical studies. ‚ûï Nice to have: Experience with an MLOps framework such as Kubeflow or MLFlow. Familiarity with Databricks, dbt or Kafka. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn ).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,31920,Net per month - B2B
Full-time,Senior,B2B,Remote,230,Senior Data Engineer with Snowflake,N-iX,"#3668 Join an exciting journey to create a greenfield, cutting-edge Consumer Data Lake for a leading global organization based in Europe. This platform will unify, process, and leverage consumer data from various systems, unlocking advanced analytics, insights, and personalization opportunities. As a Senior Data Engineer, you will play a pivotal role in shaping and implementing the platform's architecture, focusing on hands-on technical execution and collaboration with cross-functional teams. Your work will transform consumer data into actionable insights and personalization on a global scale. Using advanced tools to tackle complex challenges, you‚Äôll innovate within a collaborative environment alongside skilled architects, engineers, and leaders. Key Responsibilities: Hands-On Development: Build, maintain, and optimize data pipelines for ingestion, transformation, and activation. Create and implement scalable solutions to handle diverse data sources and high volumes of information. Data Modeling & Warehousing: Design and maintain efficient data models and schemas for a cloud-based data platform. Develop pipelines to ensure data accuracy, integrity, and accessibility for downstream analytics. Collaboration: Partner with Solution Architects to translate high-level designs into detailed implementation plans. Work closely with Technical Product Owners to align data solutions with business needs. Collaborate with global teams to integrate data from diverse platforms, ensuring scalability, security, and accuracy. Platform Development: Enable data readiness for advanced analytics, reporting, and segmentation. Implement robust frameworks to monitor data quality, accuracy, and performance. Testing & Quality Assurance: Implement robust security measures to protect sensitive consumer data at every stage of the pipeline Ensure compliance with data privacy regulations (e.g., GDPR, CCPA ..) and internal policies. Monitor and address potential vulnerabilities, ensuring the platform adheres to security best practices. Requirements: Over 4+years of experienceshowcasing technical expertise and critical thinking in data engineering. Hands-on experience withDBTand strongPythonprogramming skills. Proficiency inSnowflakeand expertise in data modeling are essential. Demonstrated experience in building consumer data lakes and developing consumer analytics capabilities is required. In-depth understanding of privacy and security engineering withinSnowflake, including concepts like RBAC, dynamic/tag-based data masking, row-level security/access policies, and secure views. Ability to design, implement, and promote advanced solution patterns and standards for solving complex challenges. Familiarity with multiple cloud platforms (Azureor GCP preferred, with a focus on Azure). Practical experience with Big Data batch and streaming tools. Competence in SQL, NoSQL, relational database design (SAP HANA experience is a bonus), and efficient methods for data retrieval and preparation at scale. Proven ability to collect and process raw data at scale, including scripting, web scraping, API integration, and SQL querying. Experience working in global environments and collaborating with virtual teams. A Bachelor‚Äôs or Master‚Äôs degree in Data Science, Computer Science, Economics, or a related discipline. We offer*: Flexible working format - remote, office-based, or flexible. A competitive salary and a good compensation package. Personalized career growth. Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more). Active tech communities with regular knowledge sharing. Education reimbursement. Memorable anniversary presents. Corporate events and team building. Other location-specific benefits. *not applicable for freelancers","[{""min"": 22164, ""max"": 30993, ""type"": ""Net per month - B2B""}]",Data Engineering,22164,30993,Net per month - B2B
Full-time,Senior,Permanent,Remote,235,Service Delivery Manager,INFOPLUS TECHNOLOGIES,"Summary ‚Äì Service Delivery Manager with a technical background that is comfortable managing projects, and interacting with the business owners. Technical skills that are required is a rounded understanding of web frontend development, Enterprise reporting tools, data warehousing, and databases. Overall a firm understanding of how to build a solution well, and set it up to be a sustainable Enterprise service. Looking for about 10 years of experience. Pharma experience is an asset. Job Description: DSM - Global Portfolio Planning and Business Excellence Service Delivery Manager with a technical background that is comfortable managing projects, and interacting with the business owners. Technical skills that are required is a rounded understanding of web frontend development, Enterprise reporting tools, data warehousing, and databases. Overall a firm understanding of how to build a solution well, and set it up to be a sustainable Enterprise service. Looking for about 10 years of experience. Pharma experience is an asset. Knowledge/Competencies: Ability to influence and ensure efficient and effective delivery of operational plans Knowledge on customer service standards and ITSM processes Knowledge and experience defining and controlling SLAs, KPIs and contract management Significant experience working with virtual teams in a cross-functional organization. Demonstrates customer, quality, cost, and delivery focus Advanced analytical, problem-solving and decision making skills Interpersonal skills to interact and communicate effectively including Conflict Management Technical Skills: Experience in an audited, or validated sytems environment. Core ServiceNow knowledge (Incident, Service Requests, Problem and Change) Strong understanding of frontend and backend development. Data Warehousing, ETL, and Reporting background (Snowflake, TalenD, and Tableau preferred)","[{""min"": 240000, ""max"": 265000, ""type"": ""Gross per year - Permanent""}]",Unclassified,240000,265000,Gross per year - Permanent
Full-time,Senior,B2B,Remote,236,Data Engineer,Mirumee Software,"Hi! üëã We are Mirumee , a Python and TypeScript software house specializing in developing unique products. Since 2009 , we‚Äôve been helping businesses transform by delivering sustainable, API-first, efficient, and modern solutions. Our expertise empowers brands across Healthcare, Commerce, and Self-Publishing , from high-volume online retailers to disruptive innovators featured on the Forbes Next Billion Dollar Startups list. At Mirumee , culture matters. We thrive in open-source communities and believe in technical excellence, open communication, and mutual respect . We actively contribute to open-source projects - check out our work here: GitHub.com/mirumee . But first of all, we care about technical excellence, open communication, and treating each other with respect as equal human beings . We are seeking an experienced Senior Data Engineer to join an e-commerce project in the cannabis branch in the US, who combines strong technical execution with curiosity, ownership, and a collaborative mindset. You‚Äôll help shape and scale our data platform while proactively engaging with cross-functional teams to drive clarity, resilience, and value in our data pipelines and tooling. The ideal candidate brings experience with modern data stacks (e.g., Airflow, DBT, cloud-native tooling), strong fundamentals in SQL and Python, and a willingness to engage beyond narrowly defined tasks. If you enjoy working through ambiguity, taking initiative, and helping set new standards of quality, we‚Äôd love to meet you! Your profile Strong SQL skills with experience writing performant, maintainable queries on large datasets Fluency in Python , especially with libraries like Pandas, and a strong testing discipline Experience with Airflow , DBT , or similar orchestration and transformation tools Familiarity with BI tools such as Sigma or Tableau Experience with AWS and IaaC best practices (especially serverless data services) Ability to break down complex projects into iterative steps and communicate progress effectively A growth-oriented mindset ‚Äîyou‚Äôre resourceful, self-directed, and open to feedback Someone curious and engaged , regularly asking thoughtful questions and offering insights beyond your immediate responsibilities A positive, collaborative teammate who brings energy and focus to the work and team Good command of English (both written and spoken) Responsibilities Design, build, and maintain scalable data pipelines using Airflow, DBT, and Python. Partner with stakeholders to clarify requirements, uncover edge cases, and ensure quality delivery - even when the problem space isn‚Äôt fully defined. Own and improve our data models and warehouse practices, helping ensure reliability, performance, and clarity Proactively monitor and respond to data issues (e.g., Sentry alerts, data quality checks), driving resolution and follow-through. Collaborate with others through well-scoped pull requests, clear documentation, and knowledge sharing Champion testing and observability, ensuring our pipelines Benefits 26 days of paid service disruption per is a standard for us üå¥ Personal development in the fast-growing industry based on your and our needs: boredom is something foreign to us Work wherever the way you want: we don't care if you start at 6 or 10 am and give you the choice to choose between a beautiful office, home office spot or combine both! We give you 5 extra days for health recovery (b2b) and the basics to take care of prophylactically - private healthcare that works (Signal Iduna) & sports card (Multisport Plus) for free Work-life balance (and we mean it!) High-end tools, physical (MacBook), and software ones And much more that we want to share during interviews! We can't wait to meet YOU . üôåüèº","[{""min"": 17000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Engineering,17000,23000,Net per month - B2B
Full-time,Mid,Permanent,Hybrid,238,Data Engineer - Allegro Pay,Allegro,"About the team The salary range for this position is (contract of employment): 14 200 - 19 690 PLN in gross terms A hybrid work model requires 1 day a week in the office. Allegro Pay is the largest fintech in Central Europe ‚Äì we are growing fast and need engineers who want to learn and develop, while at the same time solving problems related to serving thousands of RPSs. If, like us, you like flexing your mental muscles to solve complex problems and you would be happy to co-create the infrastructure which underpins our solutions, make sure you apply! In this role, you will be a contributor, helping us expand our modern cloud-based analytical solutions. We embrace challenging and interesting projects and take quality very seriously. Depending on your preference, your position may be more business-oriented or platform-oriented. Your main responsibilities: - You will be actively responsible for developing and maintaining processes for handling large volumes of data. - You will be streamlining and developing the data architecture that powers analytical products and work along a team of experienced analysts. - You will be monitoring and enhancing quality and integrity of the data. - You will manage and optimize costs related to our data infrastructure and data processing on GCP. This is the right job for you if: - You have at least 3 years of experience as Data Engineer and working with large datasets. - You have experience with cloud providers (GCP preferred). - You are highly proficient in SQL. - You have strong understanding of data modeling and cloud DWH architecture. - You have experience in designing and maintaining ETL/ELT processes. - You are capable of optimizing cost and efficiency of data processing. - You are proficient in Python for working with large data sets (using PySpark or Airflow). - You use good practices (clean code, code review, CI/CD). - You have a high degree of autonomy and take responsibility for developed solutions. - You have English proficiency on at least B2 level. - You like to share knowledge with other team members. What we offer: - Big Data is not an empty slogan for us, but a reality - you will be working on really big datasets (petabytes of data). - You will have a real impact on the direction of product development and technology choices. We utilize the latest and best available technologies, as we select them according to our own needs. - Our tech stack includes: GCP, BigQuery, (Py)Spark, Airflow. - We are a close -knit team where we work well together. - You will have the opportunity to work within a team of experienced engineers and big data specialists who are eager to share their knowledge, including publicly through allegro.tech Apply to Allegro and see why it is #dobrzetubyƒá (#goodtobehere)","[{""min"": 14200, ""max"": 19690, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14200,19690,Gross per month - Permanent
Full-time,Senior,Any,Remote,242,Data Product Engineer,The Ozone Project,"Do≈ÇƒÖcz do naszego zespo≈Çu pasjonat√≥w technologii reklamy cyfrowej, kt√≥rzy aktywnie wspierajƒÖ bardziej ekologicznƒÖ i ≈õwietlanƒÖ przysz≈Ço≈õƒá zar√≥wno dziennikarstwa, jak i reklamy online. Bƒôdziesz w centrum strategii opartych na danych, pomagajƒÖc markom nawiƒÖzywaƒá kontakt ze spo≈Çeczno≈õciami online w bezpiecznych i anga≈ºujƒÖcych ≈õrodowiskach. Wsp√≥≈ÇpracujƒÖc z utalentowanymi, zaanga≈ºowanymi osobami, bƒôdziesz mieƒá realny wp≈Çyw na kszta≈Çtowanie pozytywnego krajobrazu cyfrowego üíª. üåüZakres obowiƒÖzk√≥w: üîÆ Co oferujemy: ","[{""min"": 24494, ""max"": 29393, ""type"": ""Net per month - Any""}]",Data Engineering,24494,29393,Net per month - Any
Full-time,Senior,Permanent,Hybrid,243,Senior Data Engineer,KUBO,"We are seeking an experienced Senior Data Engineer to join our Data & Analytics team and play a critical role in designing, building, and maintaining high-quality data products. As a member of a cross-functional and agile team, you will collaborate with data architects, data scientists, analytics leads, front-end developers, and business stakeholders to support data-driven decision-making across the organization. This role is ideal for someone passionate about data engineering excellence, digital transformation, and delivering impactful analytical solutions. Key responsibilities: Design and implement scalable data pipelines and globally harmonized data models by integrating data from various domains Ensure data products adhere to architectural standards, data protection regulations, and quality guidelines Provide technical leadership and guidance to other data engineers Enhance and maintain implementation frameworks to support analytical use cases and evolving product needs Ensure accurate estimations of time and cost, high-quality delivery, and smooth handover of solutions to operations Ideal candidate profile: 5+ years of experience in Data & Analytics Min. 3 years of experience and good knowledge of the Azure data ecosystem: Azure Data Lake, Azure Synapse, Databricks Knowledge of data cataloguing, data quality management, and modern data management practices Proficiency with CI/CD tools and processes Fluency in English Knowledge of Snowflake is a plus Conditions: Work model: Hybrid ‚Äì 1 day per week in the Warsaw office Salary: 20 000 - 25 000 PLN gross/month Employment type: Full-time employment contract (UoP) directly with the client Business trips to Germany 1-2 times a year Benefits: VIP Medical Care Package, Life & Travel Insurance, Company Bonus, Holiday allowance, Co-financed sport card *Relocation package and full support with relocation to Warsaw Recruitment steps: Phone call with a Recruiter (20 - 30 min.) First interview with a Manager (1h) Second interview with a technical team (1h) Feedback and decision","[{""min"": 20000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,25000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,245,Tech Lead (Azure),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. projekt budowy nowoczesnej Data Platform w chmurze Azure, obejmujƒÖcej warstwƒô Lakehouse (Bronze / Silver / Gold) oraz Analytical Platform (MLOps), modelowanie struktur bazodanowych w podej≈õciu DDD (Data Domain Driven Design), opracowywanie modeli danych na podstawie dokumentacji z obszaru Data Governance (glosariusz danych, modele konceptualne/logiczne), projektowanie przep≈Çywu danych ze ≈∫r√≥de≈Ç do warstw platformy danych (Ingest: direct query, API, event streaming), tworzenie i dokumentowanie Data Contracts, wsp√≥≈Çpraca z w≈Ça≈õcicielami system√≥w ≈∫r√≥d≈Çowych i docelowych w zakresie integracji danych i SLA, implementacja struktur danych w architekturze medallion (Bronze / Silver / Gold) przy u≈ºyciu ETL na Azure Data Platform, doradztwo w zakresie doboru narzƒôdzi, architektury integracyjnej i praktyk CI/CD, mo≈ºliwo≈õƒá realnego wp≈Çywu na standardy technologiczne i projektowe, praca 100% zdalna, a dla chƒôtnych mo≈ºliwo≈õƒá pracy z biura we Wroc≈Çawiu, stawka do 200 z≈Ç/h przy B2B, w zale≈ºno≈õci od do≈õwiadczenia. masz do≈õwiadczenie w modelowaniu danych (ERD), przygotowywaniu Data Contracts i implementacji struktur domenowych w ≈õrodowisku Data Warehouse znasz procesy Data Ingestion i architekturƒô nowoczesnych DWH w Azure (Azure Synapse, Data Lake, Databricks), masz do≈õwiadczenie z platformami MLOps / Analytical Platform, mile widziane do≈õwiadczenie w: tworzeniu dokumentacji mapowania danych ≈∫r√≥d≈Çowych, zarzƒÖdzaniu metadanymi i jako≈õciƒÖ danych (np. Azure Purview), komunikujesz siƒô p≈Çynnie w jƒôzyku angielskim (B2/C1). d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed.","[{""min"": 26880, ""max"": 33600, ""type"": ""Net per month - B2B""}]",Data Engineering,26880,33600,Net per month - B2B
Full-time,Senior,B2B,Remote,246,Cloud Data Architect (AWS/Azure),Future Processing,"posiadasz min. 5 lat do≈õwiadczenia w IT, w tym min.3,5 roku w pracy z danymi w chmurze AWS lub Azure(potwierdzone projektami komercyjnymi wdro≈ºonymi na produkcjƒô) oraz min.rok pracy w roli Lead Developera/Architekta lub podobnej, masz wiedzƒô w zakresie us≈ÇugData wAWS lub Azure bardzo dobrze znaszDatabricks, ze szczeg√≥lnym naciskiem na najnowsze funkcjonalno≈õci tej platformy (np. Delta Live Tables i Unity Catalog), znasz narzƒôdzia, mechanizmy i procesy, tj.CI/CD, IaaC/Terraform, Git, umiesz tworzyƒá i modyfikowaƒá modele danych oraz dobieraƒá odpowiedni model do rozwiƒÖzywanego problemu (DWH, Data Marts, Data Lake, Delta Lake, Data Lakehouse), architektury Lambda i Kappanie kryjƒÖ przed TobƒÖ tajemnic, wiesz, jak budowaƒá rozwiƒÖzania do analizy danych w trybie ‚Äûnear to real time‚Äù, monitoring, diagnostyka oraz rozwiƒÖzywanie problem√≥w w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanowaƒá infrastrukturƒô oraz obliczyƒá jej koszt, posiadasz wiedzƒô na temat migracji rozwiƒÖza≈Ñ on-premise do chmury oraz znasz podstawowe typy migracji, wiesz, jak stosowaƒá mechanizmy zwiƒÖzane z bezpiecznym przechowywaniem i przetwarzaniem danych w chmurze, bra≈Çe≈õ/a≈õ udzia≈Ç w spotkaniach przedsprzeda≈ºowych, cechuje Ciƒô my≈õlenie strategiczne i analityczne oraz dobra samoorganizacja pracy, samodzielno≈õƒá, adaptacyjno≈õƒá, komunikatywno≈õƒá oraz proaktywno≈õƒá, posiadasz umiejƒôtno≈õƒá mentorowania oraz szkolenia os√≥b o mniejszym do≈õwiadczeniu oraz prowadzenia dzia≈Ça≈Ñ majƒÖcych na celu ich wsparcie rozwojowe, prowadzenie prezentacji i prelekcji podczas wydarze≈Ñ bran≈ºowych nie spƒôdza Ci snu z powiek, jeste≈õ otwarty na wyjazdy s≈Çu≈ºbowe, pos≈Çugujesz siƒôjƒôzykiem angielskim na poziomie zaawansowanym, min. C1. konsultowanie rozwiƒÖza≈Ñ z klientami - zar√≥wno obecnymi, jak i nowymi. Doradzanie w wyborze rozwiƒÖzania technicznego adekwatnego do problemu biznesowego klienta. DƒÖ≈ºenie do wyboru rozwiƒÖzania, kt√≥re jest optymalne kosztowo i odpowiada na potrzebƒô rozwiƒÖzujƒÖcƒÖ problem klienta. udzia≈Ç w spotkaniach z klientem na wczesnym etapie - pitch naszego do≈õwiadczenia, procesu, podej≈õcia do technologii, dopytywanie, zbieranie/doszczeg√≥≈Çawianie wymaga≈Ñ; tworzenie wk≈Çadu merytorycznego do ofert - schemat proponowanej architektury, wyliczenia Total Cost of Ownership/Return of Investment/koszt√≥w chmury za pomocƒÖ kalkulator√≥w dostawcy chmury; estymowanie projektu/wyceny zaanga≈ºowania przedstawicieli DS w projekcie; budowanie argument√≥w przekonujƒÖcych klienta do naszego rozwiƒÖzania, pokazywanie przewag w stosunku do innego podej≈õcia, udzia≈Ç w spotkaniach prezentujƒÖcych ofertƒô, odpowiadanie na pytania klienta, prezentowanie oferty, za kt√≥rƒÖ stoi nasz fragment rozwiƒÖzania, prace badawczo-rozwojowe w zakresie analizy funkcjonalno≈õci i przydatno≈õci nowych technologii i narzƒôdzi w rozwiƒÖzaniach biznesowych klienta, regularny kontakt z osobami decyzyjnymi w obszarze Data po stronie klienta (VP, IT Director), tworzenie PoC w obszarze Data w celu zaprezentowania wynik√≥w prowadzonych prac R&D, projektowanie i tworzenie ca≈Ço≈õci platformy przetwarzania danych uwzglƒôdniajƒÖc wszystkie jej czƒô≈õci oraz powiƒÖzania z pozosta≈Çymi rozwiƒÖzaniami (przyk≈Çadowo BI i ML) z uwzglƒôdnieniem ekosystemu chmurowego, optymalizowanie ca≈Ço≈õciowych rozwiƒÖza≈Ñ/system√≥w przechowywania i analizy danych, utrzymywanie relacji z zespo≈Çem technicznym po stronie klienta, koordynowanie pracy in≈ºynier√≥w zaanga≈ºowanych w tworzenie rozwiƒÖzania, nadzorowanie przebiegu ca≈Çego projektu, od poczƒÖtku do ko≈Ñca, zaanga≈ºowanie w rozw√≥j linii biznesowej Data Solutions (pozyskiwanie pracownik√≥w, klient√≥w, szkolenia, udzia≈Ç w konferencjach, mentoring itp.).","[{""min"": 165, ""max"": 245, ""type"": ""Net per hour - B2B""}]",Data Architecture,165,245,Net per hour - B2B
Full-time,Mid,B2B,Remote,249,Experienced Azure Data Engineer,Future Mind,"Future Mind is a brilliant, inspiring team, one of the most awarded tech consulting companies in the region with a broad portfolio of clients, including ≈ªabka, Jeronimo Martins (Hebe, Biedronka), LPP (Reserved, Sinsay, Mohito), eObuwie, Modivo, and other well-recognized brands. We have received several industry awards for delivering some of the best eCommerce applications in Poland, listed among the most popular across app stores. Our expert engineers, designers, project managers, and analysts work on projects ranging from top mobile commerce apps used daily by millions of customers to IoT, and telematics platforms that produce vast amounts of data. In 2023, we joined forces with Solita, a Finnish tech powerhouse with a vibrant community of over 2000 specialists across Europe, that combines data, business, and technology skills to build and improve digital services for leading organizations in manufacturing, medical, shipping, and other major industries, including such clients as NATO, Nokian Tires, Pfizer and many others. Together we are dedicated to delivering cutting-edge, data-driven solutions. We value proactive professionals who take ownership, enjoy solving problems, sharing knowledge, and collaboration. Together, we create high-quality software solutions that fulfil our clients' business needs and impact their customers' lives every day. As part of the Solita Group, Future Mind provides digital advisory & delivery services with the support of our international partners and upholds equally high cultural standards. Role description: As an Azure Data Engineer, your role involves designing and implementing ingestion and transformation pipelines, and data models while collaborating closely with the business stakeholders. You have worked with the Azure data stack and cloud for several years and understand the value created for customers. This job is all about: Building end-to-end ELT data pipelines on top of an Azure Data Services ; Designing data models based on large, complex data sets that meet both functional and non-functional business requirements; Holistic platform view (source systems, networking, orchestration, etc.) with the ability to deliver business value; Collaborating within our project teams to meet client needs and deliver high-quality solutions. Here‚Äôs what we‚Äôre looking for: Vast experience with Azure Data Services; Deep understanding of dimensional data modeling techniques (data vault would be a plus); Proficiency with combining SQL and Python with a modern ELT toolset (Fabric, Synapse, Spark, or equivalent); Experience with ADF, Airflow, or other orchestration tools; Effective communication skills, proficient in Polish and English. You also must have the legal right to work in the EU to apply for this position. ‚Ä¶and here‚Äôs what we offer: A dynamic work environment where innovation and collaboration are valued. Access to cutting-edge projects and technologies in a variety of industries. A supportive community of experts to foster your professional growth and development. Competitive compensation, comprehensive benefits, and a focus on work-life balance. Opportunities for continuous learning and career advancement, including specialized training in big data technologies and Snowflake certifications. The ability to work fully remotely or check into one of our offices whenever you like, Fully paid private health insurance, subsidized sports membership, mental health support, and language courses.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Engineering,14000,22000,Net per month - B2B
Full-time,Senior,B2B,Remote,250,Data Scientist,INFOPLUS TECHNOLOGIES,"üöÄ Hiring Now: ETRM Data Scientist (Remote | B2B Contract) Are you a Data Scientist with deep expertise in time-series forecasting , machine learning , and Azure ML ecosystems ? Do you thrive in complex energy trading environments and love building scalable, production-ready ML systems ? We have the perfect opportunity for you! üîç Role : ETRM Data Scientist üìç Location : Remote üìÑ Type : B2B Contract üéØ What You‚Äôll Do Design and implement cutting-edge forecasting models using ARIMA, LSTM, Prophet, and more. Develop scalable and reusable machine learning pipelines using Azure ML, PySpark , and Python-based MLOps frameworks . Deploy, monitor, and fine-tune models in operational environments to ensure long-term accuracy and performance. Work on large-scale datasets and high-performance ML workflows using Azure Databricks and Data Lake . Apply ensemble methods (stacking, boosting, bagging) for robust and accurate predictions. Collaborate in a dynamic energy trading environment to optimize decision-making using advanced data science techniques. ‚úÖ What We‚Äôre Looking For Education üéì Master‚Äôs in Mathematics, Statistics, Data Science, or related fields (Ph.D. preferred but not mandatory) Mandatory Skills Advanced Time-Series Forecasting and Predictive Modelling Deep understanding of ML frameworks: scikit-learn, XGBoost, TensorFlow, PyTorch, Darts Strong coding skills in Python and PySpark Solid hands-on with Azure Machine Learning SDK , Azure Databricks , and Azure Data Lake Expertise in MLOps and model lifecycle automation Proficiency in data handling libraries : Pandas, NumPy Preferred Skills K-Means Clustering , Bottom-Up Forecasting Experience with Azure Data Factory and pipeline orchestration Knowledge of Power/Energy Trading concepts Exposure to Generative AI (GenAI) and large language models like GPT üåü Why Join Us? Work with cutting-edge technology in the ETRM and energy analytics domain 100% remote ‚Äì collaborate with global experts from the comfort of your space Competitive B2B contract with flexible terms High-impact role with real-world business value üì© Interested? Apply now and let‚Äôs transform energy trading with data-driven intelligence! üîó [Apply Now] or send your CV to siva.s@infoplusltd.co.uk #ETRM #DataScience #MachineLearning #AzureML #MLOps #EnergyTrading #TimeSeriesForecasting #RemoteJobs #PythonJobs #DeepLearning #AzureDatabricks #ContractJobs #PySpark #GenAI #PowerTrading #B2BJobs #Contract #Hiring #Remotejobs","[{""min"": 31500, ""max"": 37800, ""type"": ""Net per month - B2B""}]",Data Science,31500,37800,Net per month - B2B
Full-time,Senior,B2B,Remote,251,Data Scientist,Altimetrik Poland,"Altimetrik Polandis a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. For our clientfrom FinTech UKwe are looking for an experiencedData Scientistwith a specialization intime series modelingand the application ofmachine learning and deep learning techniquesto temporal data. The candidate will focus on building predictive models and extracting insights from time-dependent datasets using both statistical and modern ML/DL approaches. Responsibilities: Design and train models for time series analysis. Develop ML and DL models specifically for time-based data. Analyze time series data for forecasting, classification, and pattern recognition. Collaborate with the wider team to support the deployment of models into production environments. If you possess... Hands-on experience with time series modeling (e.g., ARIMA, Prophet, LSTM). Proven use of ML and DL methods applied to temporal datasets. Strong skills in working with time-stamped data from preprocessing through to model evaluation. We work 100% remotely or from our hub inKrak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 25000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Data Science,25000,29000,Net per month - B2B
Full-time,Senior,B2B,Remote,252,Data Engineer / Big Data Analyst,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 5-letnie do≈õwiadczenie na stanowisku zwiƒÖzanym z analizƒÖ danych lub analizƒÖ biznesowƒÖ Min. 2-letnie do≈õwiadczenie na stanowisku wymagajƒÖcym przetwarzania i analizy du≈ºych zbior√≥w danych (Big Data) Do≈õwiadczenie projektowe w tworzeniu i optymalizacji zaawansowanych zapyta≈Ñ SQL Do≈õwiadczenie projektowe w programowaniu w jƒôzyku Python Do≈õwiadczenie projektowe w przetwarzaniu i analizie du≈ºych zbior√≥w danych Do≈õwiadczenie projektowe w Data Quality Do≈õwiadczenie w programowaniu w SQL Do≈õwiadczenie w programowaniu w Python Do≈õwiadczenie w programowaniu w PySpark Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z procesami ETL Znajomo≈õƒá relacyjnych baz danych Dobra organizacja pracy w≈Çasnej, orientacja na realizacje cel√≥w Umiejƒôtno≈õci interpersonalne i organizacyjne, planowanie Komunikatywno≈õƒá, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista i dociekliwo≈õƒá Zdolno≈õƒá adaptacji i elastyczno≈õƒá, otwarto≈õƒá na sta≈Çy rozw√≥j i gotowo≈õƒá uczenia siƒô Mile widziane Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Do≈õwiadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Airflow (np. Airflow Fundamentals lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá tworzenia DAG√≥w Airflow (np. Dag Authoring lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá SQL (np.. W3Schools SQL Certificate lub r√≥wnowa≈ºny) Kluczowe zadania Analiza danych biznesowych i technicznych w celu wspierania podejmowania decyzji oraz optymalizacji proces√≥w Przetwarzanie, analiza i interpretacja du≈ºych zbior√≥w danych (Big Data) z wykorzystaniem Python, SQL oraz PySpark Projektowanie, tworzenie i optymalizacja zaawansowanych zapyta≈Ñ SQL w ≈õrodowiskach baz danych Wsp√≥≈Çpraca z zespo≈Çami technicznymi i biznesowymi w celu definiowania potrzeb analitycznych oraz tworzenia rozwiƒÖza≈Ñ opartych na danych Tworzenie i utrzymywanie proces√≥w zwiƒÖzanych z jako≈õciƒÖ danych (Data Quality), w tym ich weryfikacja, czyszczenie i walidacja Programowanie rozwiƒÖza≈Ñ analitycznych i integracyjnych w jƒôzyku Python (w tym PySpark) w ≈õrodowiskach przetwarzania danych Praca z relacyjnymi bazami danych oraz narzƒôdziami ETL w celu ekstrakcji, transformacji i za≈Çadunku danych Wspieranie inicjatyw zwiƒÖzanych z automatyzacjƒÖ raport√≥w i analiz, w oparciu o du≈ºe zbiory danych i zapytania Utrzymywanie wysokiej jako≈õci dokumentacji technicznej oraz przekazywanie wniosk√≥w i rekomendacji interesariuszom Monitorowanie integralno≈õci danych oraz proponowanie i wdra≈ºanie usprawnie≈Ñ w procesach analitycznych","[{""min"": 150, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,150,185,Net per hour - B2B
Full-time,Senior,B2B,Remote,253,Staff Frontend Engineer,Adverity,"In this role, you‚Äôll play a key part in shaping and developing our frontend applications. You‚Äôll work closely with our Product Developers and Staff Engineers to drive technical decisions and ensure high-quality delivery. You‚Äôll inspire confidence through your decisions, earn trust through your execution, and set a standard others are eager to follow. This is a full-time position, where you can work remotely in Europe. About Adverity: Our SaaS company was founded in Vienna in 2015, since then, Adverity has been driving marketing intelligence of some of the world‚Äôs leading brands and agencies including IKEA, Red Bull, GroupM, Unilever, Omnicom, Barilla, JD Sports and Forbes. Our mission is to empower Marketing and E-Commerce business users with our Marketing Analytics platform: we leverage all their data and turn it into powerful insights, decisions and actions. How do we do it? At Adverity, we bring together international and diverse teams in an inclusive and safe environment. We believe in an entrepreneurial mindset, we help each other succeed, we strive to provide an awesome customer and employee experience, we prioritise work-life harmony and we dare to make mistakes to learn from them. If you share our values, Adverity is a great place for you! Some of the things you‚Äôll work on: Design and implement a clean frontend architecture with both strong performance requirements and complexity challenges. Work with useful programming tools such as extensive code reviews , pair programming , multiple layers of testing, continuous integration and deployments. Help build our design system to eliminate design pain points and streamline collaboration between design, UX, and engineering. Create documentation , usage guidelines, and proof of concepts. Advocate for quality and practicality with frontend development teams. Serve as a subject matter expert to designers, engineers, and product managers regarding the frontend domain. Understand the design needs of a large-scale software project. We're excited if you have: 5+ years of professional frontend engineering experience, especially with React , React Query, and React Hook Form. Solid skills in frontend testing using Cypress, Vitest, and/or Playwright. A strong grasp of complex application state management , side effects, and action orchestration. Experience mentoring engineers and working closely with stakeholders through clear, proactive communication . Expertise with modern styling approaches (Tailwind, styled-components, emotion) and design systems (design tokens, theming). Hands-on experience with monorepo management tools (NX, Turborepo, Lerna) and improving developer tooling and workflows. A passion for evaluating and implementing new technologies , and optimizing CI/CD pipelines for frontend deployments. Some of our benefits: Flexible working hours and remote work Regular team events (also remote) Sustainable merch for all Modern technology International & diverse teams Apply now if you are ready to revolutionise the way businesses work with marketing data. We look forward to meeting you!","[{""min"": 24763, ""max"": 29887, ""type"": ""Net per month - B2B""}]",Data Engineering,24763,29887,Net per month - B2B
Full-time,Mid,B2B,Remote,254,BI Consultant,Primaris,"Forma wsp√≥≈Çpracy: kontrakt B2B Tryb: praca zdalna Aktualnie do jednego z projekt√≥w/zespo≈Ç√≥w poszukujemy osoby na stanowiskoBI Consultant, kt√≥ra posiada min.3lat komercyjnego do≈õwiadczenia. Dlatego je≈õli szukasz pracy z: zaawansowanym SQL, tworzeniem raport√≥w w Power BI, pracƒÖ z Microsoft SQL Server i SSRS, a tak≈ºe chcesz rozwijaƒá swoje umiejƒôtno≈õci w obszarze Business Intelligence ‚Äìchƒôtnie z TobƒÖ porozmawiamy! WiodƒÖcy stack technologiczny: SQL, T-SQL, Power BI, Microsoft SQL Server, SSRS, Report Builder, Salesforce. Projekt kt√≥ry bƒôdziemy gotowi Ci zaproponowaƒá dotyczy: Rozwoju rozwiƒÖza≈Ñ raportowych dla miƒôdzynarodowego systemu do zarzƒÖdzania portfelem projekt√≥w, wykorzystywanego przez globalne firmy z obszaru HR, logistyki i produkcji. Praca obejmuje tworzenie zaawansowanych raport√≥w na bazie produkcyjnej z wykorzystaniem SQL i Power BI, w oparciu o zg≈Çoszenia w systemie Salesforce. Zakres raportowania obejmuje dane projektowe, finansowe i zasoby projektowe. Wymagania: Min. 3 roku komercyjnego do≈õwiadczenia na podobnym stanowisku Bardzo dobra znajomo≈õƒá SQL + tworzenie zapyta≈Ñ do bazy Bieg≈Ça znajomo≈õƒá jƒôzyka angielskiego (C1) Zaawansowana znajomo≈õƒá PowerBI (g≈Ç√≥wnie Desktop) Do≈õwiadczenie w komunikacji i wsp√≥≈Çpracy bezpo≈õrednio z klientem Mile widziane bƒôdzie do≈õwiadczenie pracy z narzƒôdziami raportowymi: Microsoft Report Bulider W naszej firmie bƒôdziesz m√≥g≈Ç/mog≈Ça liczyƒá na: Pracƒô w organizacji z ugruntowanƒÖ pozycjƒÖ rynkowƒÖ Projekty, w kt√≥rych bƒôdziesz mia≈Ç/mia≈Ça wp≈Çyw na ich rozw√≥j Wsp√≥≈Çpracƒô z ciekawymi klientami biznesowymi z r√≥≈ºnych bran≈º (m.in.: finanse, bankowo≈õƒá, ubezpieczenia, healthcare, robotyzacja, energetyka, media), Permanentny mentoring zar√≥wno techniczny jak i biznesowo-mened≈ºerski, np. podczas naszych cyklicznych szkole≈Ñ (m.in. Git, Gitflow, Angular, Docker), czy wew. program√≥w rozwojowych (Primaris x TechTalks, Primaris Leadership Academy) oraz zewnƒôtrznych kurs√≥w.Ju≈º na etapie on-boardingu zapewniamy dostƒôp do naszych wewnƒôtrznych szkole≈Ñ, cyklicznych spotka≈Ñ, kt√≥re serializujemy na Confluence oraz platformy e-learning ≈öwietnƒÖ atmosferƒô pracy, w≈õr√≥d zaanga≈ºowanych ludzi z pasjƒÖ w p≈Çaskiej strukturze z prostymi procesami Kompleksowy pakiet benefit√≥w skrojonych na miarƒô - prywatna opieka medyczna dla Ciebie oraz dla Twojej rodziny, Multisport dla Ciebie i os. towarzyszƒÖcej - Ty decydujesz, co wybierasz! Ca≈Çy proces rekrutacyjny oraz onboarding prowadzony jest zdalnie. Proces rekrutacyjny sk≈Çada siƒô z: rozmowy telefonicznej z osobƒÖ z dzia≈Çu Rekrutacji & HR (do 30 min) zdalnej video rozmowy - weryfikacji techniczno-biznesowej z naszym specjalistƒÖ/specjalistkƒÖ (60-90 min) zdalnego spotkania z liderem po stronie klienta projektu (30-60 min) finalnej decyzji dotyczƒÖcej oferty Primaris Services to ponad 250 ekspert√≥w na pok≈Çadzie i 15 lat do≈õwiadczenia w bran≈ºy IT na rynku polskim oraz zagranicznym.Realizujemy ambitne projekty o wysokiej z≈Ço≈ºono≈õci z r√≥≈ºnych obszar√≥w -m.in. bankowo≈õci, ubezpiecze≈Ñ, funduszy inwestycyjnych czy bran≈ºy logistycznej (mamy ponad 40 aktywnych klient√≥w!). Ro≈õniemy w si≈Çƒô oraz ciƒÖgle poszerzamy portfolio zar√≥wno naszych us≈Çug jak i klient√≥w. Zakres naszej dzia≈Çalno≈õci obejmuje budowƒô system√≥w od zera, ich rozw√≥j oraz utrzymanie, wdro≈ºenia produktowe, alokacje ca≈Çych Zespo≈Ç√≥w, a tak≈ºe pojedynczych Ekspert√≥w w strukturach Klienta. Ponadto od kilku lat dzia≈Çamy bardzo intensywnie jako z≈Çoty Partner firmy UiPath (obszar Robotic Process Automation) budujƒÖc roboty i sprzedajƒÖc licencje u naszych Klient√≥w. Co miesiƒÖc do≈ÇƒÖcza do nas 7 nowych os√≥b! Wierzymy, ≈ºe zgrany zesp√≥≈Ç i ludzie z pasjƒÖ to klucz do naszego wsp√≥lnego sukcesu! W≈Ça≈õnie dlatego ciƒÖgle poszukujemy nowych, zdolnych os√≥b, kt√≥re zasilƒÖ nasze szeregi.","[{""min"": 80, ""max"": 100, ""type"": ""Net per hour - B2B""}]",Unclassified,80,100,Net per hour - B2B
Full-time,Senior,B2B,Remote,255,Data Modeler / Data Engineer,emagine Polska,"Informacje o projekcie: Bran≈ºa: finanse/po≈ºyczki Lokalizacja: zdalnie Umowa: B2B Stawka: 200 pln/h netto + VAT D≈Çugo≈õƒá projektu: d≈Çugoterminowy Poszukujemy do≈õwiadczonego Data Engineera do zespo≈Çu Data Platform, kt√≥ry bƒôdzie modelowaƒá dane oraz implementowaƒá procesy ELT w chmurze. ObowiƒÖzki: Modelowanie struktur bazodanowych w podej≈õciu DDD oraz tworzenie logicznych i fizycznych modeli danych. Data Mapping. Przygotowywanie warstwy Data Contracts (wymaga≈Ñ HD do system√≥w ≈∫r√≥d≈Çowych pod merytorycznƒÖ p≈Çaszczyznƒô kontraktu na dane) na podstawie zamodelowanych uprzednio struktur dla poszczeg√≥lnych domen danych. Wsp√≥≈Çpraca w procesie ingerencji danych z system√≥w ≈∫r√≥d≈Çowych. Implementacja modeli danych dla poszczeg√≥lnych domen w Data Platform (warstwa Bronze, Silver i Gold) w podej≈õciu ELT w ≈õrodowisku Azure Databricks. Wymagania: Do≈õwiadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejƒôtno≈õƒá implementacji proces√≥w ELT. Umiejƒôtno≈õƒá tworzenia dokumentacji technicznej. Do≈õwiadczenie w mapowaniu danych ze ≈∫r√≥d≈Çowych do docelowych struktur w DWH. Umiejƒôtno≈õƒá interpretacji fizycznego/logicznego modelu danych (ERD, modele relacyjne) i synchronizacji struktur tych≈ºe modeli z wymaganiami ≈õwiata analityki. Do≈õwiadczenie w podej≈õciu do projektowania s≈Çownik√≥w i zarzƒÖdzania nimi (masterdata). Znajomo≈õƒá narzƒôdziadbdiagram.io. Wiedza na temat zagadnie≈Ñ Data Quality, Data Lineage i zasad zarzƒÖdzania danymi.","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,180,200,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,256,Senior GCP Data Engineer (Java + Market Data),ITDS,"GCP Data Engineer Join us, and code the backbone of financial intelligence! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As aGCP Data Engineer, you will be working for our client, a global financial institution developing a cloud-based risk management platform used to generate and deliver risk factor definitions, historical market data, and scenarios for advanced financial modeling. The project involves building and optimizing scalable data pipelines, microservices, and integration layers that process high volumes of real-time and historical market data. You will be joining an international team of engineers focused on innovation, automation, and delivering measurable business value in a highly regulated environment. Your main responsibilities: Translating business requirements into secure, scalable, and performant data solutions Integrating internal systems with an emphasis on fast data processing and cost optimization Developing and documenting data ingestion blueprints for market data pipelines Reviewing data solutions created by other team members Assessing and modernizing existing data pipelines and microservices Collaborating with engineers, analysts, and stakeholders to align technical solutions with business needs Implementing consistent logging, monitoring, error handling, and automated recovery Promoting automated unit and regression testing through test-centric development Designing and implementing performant REST APIs Applying industry-standard integration frameworks and patterns You're ideal for this role if you have: Strong knowledge ofJava Solid understanding of software design principles such as KISS, SOLID, and DRY Proficiency with Spring Boot and its ecosystem Experience building performant data processing pipelines Familiarity withApache Beamor similar technologies Experience working with relational andNoSQLdatabases, such as PostgreSQL and Bigtable Basic understanding of DevOps practices and CI/CD tools likeJenkinsandGroovy Ability to design and implement RESTful APIs Excellent problem-solving and analytical skills Strong communication and team collaboration abilities Experience withGCPservices like GKE, Cloud SQL, DataFlow, and BigTable It is a strong plus if you have: Knowledge of monitoring tools such as Open Telemetry, Prometheus, and Grafana Familiarity with Kubernetes and Docker Exposure to Terraform for infrastructure-as-code Experience with messaging and streaming platforms like Kafka We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #6923 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere.","[{""min"": 28000, ""max"": 31080, ""type"": ""Net per month - B2B""}]",Data Engineering,28000,31080,Net per month - B2B
Full-time,Mid,B2B,Remote,257,Data Engineer_L4 with Databricks,Experis Manpower Group,"Tasks: Design and implement data processing solutions using Databricks for large-scale and diverse datasets Design, build, and enhance data pipelines with Python and cloud- native tools Work closely with solution architects to define and uphold best practices in data engineering Ensure data consistency, security, and scalability within cloud-based environments Requirements: Experience in data engineering, with hands-onDatabricksexpertise Strong experience inPythonfor automation and data transformation Experience working with at least one majorcloud platform(AWS, Azure, or GCP) Strong communication skills and strong English language skills Nice to have: Solid understanding of SQL with experience in query optimization and data modeling Familiarity with DevOps methodologies, CI/CD pipelines, and Infrastructure as Code (Terraform, Bicep) Experience with real-time data streaming technologies such as Kafka or Spark Streaming Knowledge of cloud storage solutions like Data Lake, Snowflake, or Synapse Hands-on experience with PySpark for distributed data processing Relevant certifications such as Databricks Certified Data Engineer Associate or cloud-based data certifications Our offer: Employment based onB2B contractvia Experis for a period of 12 months Compensation: 150-175 PLN per hour 100% remote work Multisport card Private healthcare system Life insurance","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,175,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,259,Senior Data Engineer (Microsoft),Onwelo,"üü† Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, a tak≈ºe otworzy≈Ça biura w siedmiu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. üöÄ O projekcie: Szukamy do≈õwiadczonego Microsoft Data Engineera , kt√≥ry poprowadzi techniczne aspekty projekt√≥w i bƒôdzie kluczowym ≈ÇƒÖcznikiem miƒôdzy IT a biznesem. PracujƒÖc z nami, bƒôdziesz mieƒá realny wp≈Çyw na spos√≥b, w jaki przetwarzane sƒÖ dane kluczowe dla biznesu i klient√≥w. Je≈õli masz do≈õwiadczenie w hurtowniach danych i lubisz wyzwania zwiƒÖzane z presales oraz definiowaniem wymaga≈Ñ , do≈ÇƒÖcz do nas i wsp√≥≈Çtw√≥rz rozwiƒÖzania. üéØ Z nami bƒôdziesz: Projektowaƒá, rozwijaƒá i optymalizowaƒá hurtownie danych opartych na technologii Microsoft SQL Server Implementowaƒá i utrzymywaƒá procesy ETL przy u≈ºyciu SSIS Tworzyƒá raporty i analizy z wykorzystaniem SSRS oraz modele wielowymiarowe i tabelaryczne w SSAS Integrowaƒá dane z r√≥≈ºnych ≈∫r√≥de≈Ç, w tym Oracle Optymalizowaƒá wydajno≈õci zapyta≈Ñ SQL oraz proces√≥w przetwarzania danych Aktywnie wsp√≥≈Çpracowaƒá z klientami w celu zbierania i definiowania wymaga≈Ñ biznesowych oraz ich przek≈Çadania na rozwiƒÖzania techniczne Prowadziƒá dzia≈Çania presales ‚Äì przygotowywanie ofert, udzia≈Ç w spotkaniach z klientami, doradztwo w zakresie architektury danych Koordynowaƒá i nadzorowaƒá pracƒô zespo≈Çu developerskiego, pe≈Çniƒá rolƒô lidera technicznego üòé Czekamy na Ciebie, je≈õli: Masz m in. 5 lat do≈õwiadczenia w pracy z hurtowniami danych oraz rozwiƒÖzaniami Microsoft BI Bardzo dobra znasz SQL Server , w tym mechanizm√≥w przechowywania i przetwarzania danych Posiadasz Do≈õwiadczenie w pracy z SSIS, SSRS, SSAS oraz umiejƒôtno≈õƒá efektywnego wykorzystywania tych narzƒôdzi. Pracujesz r√≥wnie≈º z innymi ≈∫r√≥d≈Çami danych, w szczeg√≥lno≈õci Azure , Oracle Potrafisz prowadziƒá projekty i wsp√≥≈Çpracowaƒá z biznesem ‚Äì umiejƒôtno≈õƒá definiowania wymaga≈Ñ, rekomendowania rozwiƒÖza≈Ñ oraz prezentowania wynik√≥w. Masz do≈õwiadczenie w dzia≈Çaniach presales , tworzeniu ofert i doradztwie technologicznym. Mo≈ºesz pochwaliƒá siƒô umiejƒôtno≈õciƒÖ zarzƒÖdzania zespo≈Çem oraz mentoringu m≈Çodszych cz≈Çonk√≥w zespo≈Çu. Znasz jƒôzyka angielskiego na poziomie min. B2 ü§ù Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,27000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,261,Data Engineer (Spark),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. bran≈ºy finansowej. Wykorzystywany stos technologiczny: Scala, Apache Spark, SQL/HQL, Airflow, Control-M, Jenkins, GitHub, Hive, Databricks, Azure, AWS S3, tworzenie kompleksowych proces√≥w ETL z wykorzystaniem Spark/Scala ‚Äì obejmujƒÖcych transfer danych do/z Data Lake, walidacje techniczne oraz implementacjƒô logiki biznesowej, dokumentowanie tworzonych rozwiƒÖza≈Ñ w narzƒôdziach takich jak: JIRA, Confluence czy ALM, weryfikacja jako≈õci dostarczanego rozwiƒÖzania ‚Äì projektowanie i przeprowadzanie test√≥w integracyjnych, zapewniajƒÖcych jego poprawne dzia≈Çanie i zgodno≈õƒá z innymi komponentami, praca w modelu hybrydowym: 1-2 razy w tygodniu praca z biura w Warszawie, stawka do 180z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz co najmniej 7 lat do≈õwiadczenia w programowaniu, przez minimum 3 lata pracujesz z technologiami Big Data, najlepiej Spark i Scala, znasz podej≈õcie Agile/Scrum, korzystasz z narzƒôdzi Continuous Integration ‚Äì Git, GitHub, Jenkins, swobodnie pos≈Çugujesz siƒô SQL, komunikujesz siƒô w jƒôzyku angielskim na poziomie minimum B2, dodatkowym atutem bƒôdzie: do≈õwiadczenie z technologiami frontendowymi: React, TypeScript, Next.js, Qlik, umiejƒôtno≈õƒá pisania skrypt√≥w w Bash, znajomo≈õƒá narzƒôdzia Control-M, znajomo≈õƒá Docker, Kubernetes, AWS S3, Azure, AWS. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 25200, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,30240,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,262,Tableau/BI Developer (in the EU),Andersen,"Andersen is seeking aTableau/BI Developerin the EUfor a major logistics project with a global UK-based transport leader. The role involves supporting data-related tasks for a mobile app focused on trip planning, ticket booking, and real-time transport information.","[{""min"": 10300, ""max"": 19400, ""type"": ""Gross per month - Permanent""}, {""min"": 10300, ""max"": 19400, ""type"": ""Net per month - B2B""}]",Data Engineering,10300,19400,Gross per month - Permanent
Full-time,Mid,B2B,Remote,264,Data Scientist,HR Contact,"Data Scientist ‚Äã You'll takeend-to-end ownership of machine learning solutionsfrom concept to production. Join a team where data science fuels smarter e-commerce and marketing automation. Company: a European MarTech ecosystem of technology companies with a product portfolio includingAI-powered personalization engines, e-commerce platform and data management solutions. üìç Poland, Remote. üóÉÔ∏è‚Äã B2B Contract. ü§ñ What will you be doing? Owning full model lifecycle: from research, experimentation and prototyping to deployment and monitoring. Building and optimizing machine learning models using Python, e.g., customer lifetime value models, AI modules that support the recommendation engines and the use of a customer data platform. Designing data pipelines and integrating ML solutions into production (search personalization, clickstream analysis, email triggers). Improving and maintaining existing models to ensure performance, scalability, and robustness in production. Collaborating closely with engineering, product and business teams to turn complex data into actionable product features. Upholding high development standards by writing clean, maintainable code, conducting code reviews, and embracing DevOps practices like testing and CI/CD. Contributing to the continuous improvement of the team by mentoring junior colleagues and bringing new perspectives to modeling and methodology discussions. üîß What will help you get the job done? Experience inend-to-end data science projects (preferably in eCommerce, SaaS, MarTech space),witharound 5 years of experience. Strong background inPython. Experiencedeploying models in production environments. Experience with cloud infrastructure(Azure or AWS)would be an advantage Solid understanding of statistics, modeling, and software engineering practices. Analytical mindset with a passion forpractical impact, not just research. Excellent Englishand communication skills. Strongcommunication skills,with the ability to explain data-driven insights to diverse audiences. ‚ú® Nice to have: Experience with: Azure, DevOps, Docker, Kubernetes, Databricks, Cassandra, SQL. Experience with Delta Lake and Spark for handling large-scale data. Hands-on experience with deep learning (PyTorch, TensorFlow, MXNet). ‚ú® Why is it worth it? An ability to take charge of your own projects. Possibility to develop further in the MLOps space. Flexible B2B Contract Model. Private medical care (Luxmed or Medicover) & MultiSport package fully covered. This is a unique opportunity to be at the heart of product innovation, turning behavioral data into real business impact. Supportive company culture ‚Äì flat structure, strong teamwork and openness to new ideas. Remote-first setup. Do you prefer slippers instead of shoes or maybe you are expecting Amazon delivery, no worries, we‚Äôve got you covered. üëã Meet Your Guide: Hello! I‚Äôm Olgaand I am responsible for this recruitment process. I will be happy to meet you! üó∫Ô∏è Recruitment stages: Online interview with Olga - we will talk in more details about the company, the role and your last projects : ) Online interview with a company - you will meet the team. Technical assignment - it is time for you to shine! Online interview - final assessment of your task and decision.","[{""min"": 19000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Science,19000,23000,Net per month - B2B
Full-time,Senior,B2B,Remote,268,Data Architect / Data Officer_L4,Experis Manpower Group,"Responsibilities: Building solutions that combine data from multiple sources to support our client's processes using the latest Azure technology stack. Supporting team members with technical issues, platform administration, and access rights. Understanding business targets, challenging choices and roadmaps, prioritizing needs based on technical capabilities, and designing target architectures. Acting as a data and cloud expert, sharing your knowledge with other architects, data engineers/data team members, business stakeholders, and platform experts. Developing data architecture with a solid understanding of data domain, data modeling, quality, and sensitive data protection topics. Documenting technical and architectural specifications and providing architectural advice in collaboration with business analysts. Act as an architect in data projects. Assist in creating Business Use Cases for several reporting projects. Define architecture of data management and advanced analytics solutions in the Azure cloud. Ensure solution compliance with client‚Äôs frameworks and standards. Develop documentation of system components following obligatory standards. Support IT team in technical implementation, testing, application deployment, and knowledge transfer to the IT support. Collaborate closely with business stakeholders to ensure the high quality of the final product. Code review and quality assurance Maintenance of Azure Data Relevant resources Requirements: Master‚Äôs degree in Computer Science or related field 5+ years of experience in Data Architecture & Engineering Experience with business intelligence, data analysis, and reporting in enterprise environments Experience in design, development, and implementation of solutions architecture on the Microsoft Azure platform Good understanding of data security topics, from networking layer to the end user interface Working experience with Azure Databricks (administration and data engineering) and Azure Data Factory Good understanding of Azure services (SQL database, administration, IAM, Data Lake) Experience with agile project delivery methodologies (Scrum, Kanban) SQL/Python programming skills Soft Skills: Fluent in English (C1) Hands-on mentality Self-managed Take end-to-end task ownership Excellent communication skills Ability to understand business stakeholders and the business landscape Ability to challenge decisions made by the platform or other architects Strong analytical, organizational, problem-solving, and time-management skills Comfortable working in a diverse, complex, and fast-changing landscape of data sources Proactive problem solver with innovative thinking and a strong team player Our offer: 100% remote work model Multisport card Private healthcare system Life insurance","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Architecture,180,220,Net per hour - B2B
Full-time,Senior,B2B,Remote,270,Senior and Mid BI Consultants,emagine Polska,"PROJECT INFORMATION: Industry: Marine infrastructure Client: from Denmark Remote work: yes Project language: English Business trips: no Project length: 12 months contract + option for prolongations Start: Flexible / at latest beginning of October 2025 Assignment type: B2B Remuneration: 160-200 PLN/h for a senior (depending on experience) Headcounts: 4 FTE: 100% Summary: The primary goal of this position is to support digital transformation efforts by migrating from a legacy data platform to a new cloud-based BI platform (from Databricks to Fabric), enabling enhanced data management and analytics capabilities. Responsibilities: Assist in the migration of data from a legacy platform to a new cloud-based BI platform. Implement large-scale enterprise data solutions and centralized cross-domain data models. Apply layered/medallion data architectures and dimensional modeling techniques. Utilize semantic modeling principles and create tabular models for data analysis. Ensure scalability, maintainability, and documentation of data models. Analyze business needs to align technical designs with requirements. Employ DevOps best practices in version control, CI/CD, and monitoring. Manage and execute tasks independently to meet project deadlines. Adopt agile methodologies to break down and track project work. Key Requirements: 5+ years of experience in data warehousing, BI, or data engineering. Experience in implementing large-scale enterprise data solutions. Strong understanding of layered and dimensional modeling techniques. Hands-on experience with semantic and tabular models. Proficiency in DevOps principles, including version control and CI/CD. Experience with Fabric, Power BI, (Py)spark, Azure DevOps, Git. Nice to Have: Familiarity with data normalization techniques. Experience with agile delivery methods. Understanding of best practices in data modeling and documentation. We offer: Long-term cooperation. Transparently built relations based on trust and fair play. Co-financed benefits: Medicover card, Multisport card.","[{""min"": 160, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Unclassified,160,200,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,271,Data Modeler üß©,ITDS,"Join us, and craft the foundation for intelligent analytics! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As a Data Modeller, you will be working for our client, a leading global financial institution driving large-scale digital transformation and data strategy initiatives. You will be supporting a major data architecture project aimed at improving data quality, accessibility, and consistency across the organization. You will collaborate with cross-functional teams and lead the development of enterprise-level data models that support both operational efficiency and analytical insights. This role is central to implementing scalable, secure, and compliant data solutions that empower better decision-making and foster innovation across the business. Your main responsibilities: Leading the design and implementation of conceptual, logical, and physical data models Collaborating with stakeholders to gather and translate data requirements into scalable models Establishing and maintaining best practices and governance standards for data modelling Mentoring and training junior data modellers on tools, methodologies, and techniques Coordinating with data architects, engineers, and analysts to align data strategies Participating in architecture and design discussions to ensure consistency across platforms Developing and enforcing data quality and integrity standards across modelling initiatives Supporting the integration of data modelling solutions into enterprise systems Researching and recommending new modelling tools and technologies Ensuring compliance with data governance and protection regulations You're ideal for this role if you have: Demonstrated leadership experience in managing data modelling teams Deep expertise in data modelling for both operational and analytical systems Proven ability to translate complex business requirements into data models Excellent verbal and written communication skills for diverse audiences Strong consulting skills with the ability to advise cross-functional teams Experience in stakeholder management and navigating complex organizations Advanced problem-solving capabilities for data-related challenges Solid understanding of metadata and data lineage concepts Knowledge of data governance frameworks and data quality standards Proficiency with data modelling tools such as ER/Studio, ERwin, or similar It is a strong plus if you have: Hands-on experience with cloud data platforms such as AWS, Azure, or Google Cloud Familiarity with big data technologies like Hadoop or Spark Experience working with industry-standard models like BIAN, FSDM, or BDW Understanding of data architecture patterns in financial services Exposure to master data management (MDM) concepts and platforms We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #6913 üìå You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here.","[{""min"": 23100, ""max"": 27720, ""type"": ""Net per month - B2B""}]",Data Science,23100,27720,Net per month - B2B
Full-time,Senior,B2B,Remote,274,DWH Developer (public/health),Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 5-letnie do≈õwiadczenie zawodowe na stanowisku Projektanta IT lub Starszego Programisty SQL Znajomo≈õƒá zasad i metodologii projektowania hurtowni danych i data mart√≥w analitycznych Do≈õwiadczenie w pracy z systemami relacyjnych baz danych PostgreSQL lub EDB Do≈õwiadczenie w projektowaniu i programowaniu przy wykorzystaniu jƒôzyka PL/SQL Do≈õwiadczenie w tworzeniu modeli logicznych i fizycznych z wykorzystaniem narzƒôdzia Enterprise Architect Do≈õwiadczenie w projektowaniu i programowaniu proces√≥w ETL Znajomo≈õƒá standard√≥w wymiany danych HL7 Znajomo≈õƒá narzƒôdzi s≈Çu≈ºƒÖcych do orkiestracji proces√≥w ETL Znajomo≈õƒá Airflow Znajomo≈õƒá Jira Do≈õwiadczenie w programowaniu w jƒôzyku Python Dobra organizacja pracy w≈Çasnej, orientacja na realizacje cel√≥w Umiejƒôtno≈õci interpersonalne, w szczeg√≥lno≈õci umiejƒôtno≈õƒá planowania, definiowania, realizacji, oraz monitorowania i rozliczania cel√≥w Efektywna komunikacja, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista i proaktywno≈õƒá Zdolno≈õƒá adaptacji i elastyczno≈õƒá, otwarto≈õƒá na sta≈Çy rozw√≥j i gotowo≈õƒá uczenia siƒô Mile widziane Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Do≈õwiadczenie projektowe w obszarze Hurtownia Danych Znajomo≈õƒá s≈Çownik√≥w i rejestr√≥w z obszaru zdrowia np.: ICD9, ICD10, OID, PESEL Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá programowania w jƒôzyku pl/sql (np. PostgreSQL Certification lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy wiedzƒô z zakresu administrowania EDB Certyfikat z obszaru administrowania ≈õrodowiskiem Hadoop Kluczowe zadania Projektowanie hurtowni danych i data mart√≥w analitycznych Tworzenie modeli danych (logicznych i fizycznych) z u≈ºyciem Enterprise Architect Implementacja i optymalizacja proces√≥w ETL z wykorzystaniem narzƒôdzi orkiestracyjnych (np. Airflow) Programowanie w SQL, PL/SQL oraz Pythonie, g≈Ç√≥wnie w ≈õrodowisku PostgreSQL/EDB Praca ze standardami wymiany danych HL7 oraz narzƒôdziami takimi jak Jira i Enterprise Architect Wsp√≥≈Çpraca z zespo≈Çami projektowymi w zakresie analizy wymaga≈Ñ i dostarczania rozwiƒÖza≈Ñ danych","[{""min"": 110, ""max"": 139, ""type"": ""Net per hour - B2B""}]",Data Engineering,110,139,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,275,Data Scientist/Analyst üåüüíª,ITDS,"Join Our Dynamic Team and Shape the Future of Digital Banking! Warsaw-based opportunity with hybrid work model (2 days in the office/month). As a Data Scientist/Analyst you will be working for our client, one of the leaders in online banking, on cutting-edge data-driven projects focused on areas such as process mining, churn analysis, fraud prevention, and NLP. You will play a key role in translating business problems into data models, uncovering insights, and developing impactful solutions. This role demands not only strong statistical and machine learning skills, but also the ability to understand business needs, communicate product vision, and collaborate with various stakeholders to drive data initiatives forward. Your main responsibilities: Build and validate machine learning models tailored to business needs Analyze and interpret complex data to extract actionable insights Collaborate with business stakeholders to define product vision and data strategy Identify, collect, and prepare internal and external data sources Engage in cross-functional teams to integrate data solutions into products Visualize and present findings to both technical and non-technical audiences Design scalable data workflows and pipelines using appropriate technologies Leverage statistical methods to support decision-making processes Participate in the continuous improvement of data infrastructure and tools Contribute to a culture of data-driven product development You're ideal for this role if you have: Strong foundation in machine learning concepts and algorithms Proficiency in SQL for data querying and manipulation Experience with Python and libraries such as NumPy, Pandas, SciPy, Matplotlib Familiarity with big data tools like Hadoop and Apache Spark Ability to translate business problems into analytical solutions Understanding of statistical analysis and data visualization techniques Strong communication and stakeholder management skills A product-oriented mindset with an engineering approach to problem solving Nice to have: Knowledge of DevOps principles and basic Linux administration skills Experience with Java or Scala for machine learning applications We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here . Ref. number 7005","[{""min"": 16800, ""max"": 19950, ""type"": ""Net per month - B2B""}]",Data Science,16800,19950,Net per month - B2B
Full-time,Mid,B2B,Remote,276,Data Modeler,emagine Polska,"Informacje o projekcie: Bran≈ºa: finanse/po≈ºyczki Lokalizacja: zdalnie Umowa: B2B Stawka: ~200 pln/h netto + VAT D≈Çugo≈õƒá projektu: d≈Çugoterminowy Poszukujemy do≈õwiadczonego Data Engineera do zespo≈Çu Data Platform, kt√≥ry bƒôdzie modelowaƒá dane oraz implementowaƒá procesy ELT w chmurze. ObowiƒÖzki: Modelowanie struktur bazodanowych w podej≈õciu DDD oraz tworzenie logicznych i fizycznych modeli danych. Data Mapping. Przygotowywanie warstwy Data Contracts (wymaga≈Ñ HD do system√≥w ≈∫r√≥d≈Çowych pod merytorycznƒÖ p≈Çaszczyznƒô kontraktu na dane) na podstawie zamodelowanych uprzednio struktur dla poszczeg√≥lnych domen danych. Wsp√≥≈Çpraca w procesie ingerencji danych z system√≥w ≈∫r√≥d≈Çowych. Implementacja modeli danych dla poszczeg√≥lnych domen w Data Platform (warstwa Bronze, Silver i Gold) w podej≈õciu ELT w ≈õrodowisku Azure Databricks. Wymagania: Do≈õwiadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejƒôtno≈õƒá implementacji proces√≥w ELT. Mile widziane: Umiejƒôtno≈õƒá tworzenia dokumentacji technicznej. Do≈õwiadczenie w mapowaniu danych ze ≈∫r√≥d≈Çowych do docelowych struktur w DWH. Umiejƒôtno≈õƒá interpretacji fizycznego/logicznego modelu danych (ERD, modele relacyjne) i synchronizacji struktur tych≈ºe modeli z wymaganiami ≈õwiata analityki. Do≈õwiadczenie w podej≈õciu do projektowania s≈Çownik√≥w i zarzƒÖdzania nimi (masterdata). Znajomo≈õƒá narzƒôdzia dbdiagram.io. Wiedza na temat zagadnie≈Ñ Data Quality, Data Lineage i zasad zarzƒÖdzania danymi Znajomo≈õƒá narzƒôdzi do zarzƒÖdzania metadanymi (np. Azure Purview).","[{""min"": 160, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,160,200,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,277,Data Engineer,emagine Polska,"Project information: Industry: Insurance and IT services Rate: up to 160 z≈Ç/h netto + VAT Location: Warsaw (first 2-3 months of office visits once a week, then occasionally) Project language: Polish, English Summary The Data Engineer will be responsible for designing, building, and maintaining Data Hubs that integrate multiple data sources for efficient analytics and operational purposes, with a focus on real-time data processing. Main Responsibilities Data Hub Development ‚Äì Design and implement scalable Data Hubs to support enterprise-wide data needs. Data Pipeline Engineering ‚Äì Build and optimize ETL/ELT pipelines for efficient data ingestion, transformation, and storage. Logical Data Modeling ‚Äì Structure Data Hubs to ensure efficient access patterns and support diverse use cases. Real-Time Analytics ‚Äì Enable real-time data ingestion and updating models. Data Quality & Monitoring ‚Äì Develop monitoring features to ensure high data reliability. Performance Optimization ‚Äì Optimize data processing for large-scale datasets. Automation & CI/CD ‚Äì Implement CI/CD pipelines for automating data workflows. Collaboration ‚Äì Align data solutions with enterprise needs through teamwork. Monitoring & Maintenance ‚Äì Continuously improve data infrastructure reliability. Agile Practices ‚Äì Participate in Scrum/Agile methodologies. Documentation ‚Äì Create and maintain clear documentation for data models and pipelines. Key Requirements Strong Python skills (or other relevant language) Experience with Azure Data Factory, ADLS, and Azure SQL Hands-on experience in building ETL/ELT pipelines Experience with real-time data processing Understanding of data preparation for AI/ML applications Experience in building data validation and monitoring features Proficiency in SQL for data transformation Familiarity with CI/CD and infrastructure-as-code principles Understanding data security and compliance best practices Proficient in English (B2 level minimum) Nice to Have Data Governance knowledge Experience with containerization technologies (Docker, Kubernetes/AKS) Agile collaboration experience Ability to produce high-quality technical documentation","[{""min"": 140, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,140,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,278,Senior Data Analyst,ERLI sp√≥≈Çka akcyjna,"ERLI to jedna z czo≈Çowych platform marketplace w Polsce, kt√≥ra co miesiƒÖc przyciƒÖga ponad 9 milion√≥w aktywnych kupujƒÖcych. Wsp√≥≈Çpracujemy z tysiƒÖcami sprzedawc√≥w, wspierajƒÖc ich w maksymalizacji sprzeda≈ºy i rozwoju ich biznes√≥w. Naszym celem jest dostarczanie szerokiej gamy produkt√≥w w konkurencyjnych cenach, zapewniajƒÖc klientom najwy≈ºszy standard obs≈Çugi i satysfakcjƒô z zakup√≥w. Do≈ÇƒÖcz do ERLI jako Senior Data Analyst! Co bƒôdziesz robiƒá? Jako Senior Data Analyst bƒôdziesz kluczowƒÖ postaciƒÖ w naszym zespole, odpowiedzialnƒÖ za dog≈Çƒôbne zrozumienie zachowa≈Ñ u≈ºytkownik√≥w i optymalizacjƒô naszych produkt√≥w z focusem na udoskonalenia wyszukiwarki (search) i konwersji w ≈õcie≈ºkach zakupowych u≈ºytkownik√≥w (CVR). Twoje codzienne zadania bƒôdƒÖ obejmowaƒá: Prowadzenie wszechstronnych analiz ilo≈õciowych i jako≈õciowych , koncentrujƒÖc siƒô na wyszukiwarce, ≈õcie≈ºkach u≈ºytkownik√≥w i innych kluczowych obszarach platformy. Wnioskowanie i formu≈Çowanie konkretnych rekomendacji na podstawie przeprowadzonych analiz, a nastƒôpnie ich wdra≈ºanie wraz z zespo≈Çem w celu poprawy funkcjonalno≈õci platformy i osiƒÖgania za≈Ço≈ºonych metryk. Projektowanie i analizowanie eksperyment√≥w A/B , w tym precyzyjne definiowanie kohort, KPI oraz metryk stra≈ºniczych, a tak≈ºe interpretowanie ich wynik√≥w. Aktywne wspieranie procesu projektowania i realizacji bada≈Ñ ilo≈õciowych . Tworzenie i optymalizowanie metryk jako≈õciowych i ilo≈õciowych dla listingu oraz wynik√≥w wyszukiwania, aby zapewniƒá ich najwy≈ºszƒÖ efektywno≈õƒá. Wspieranie w definiowaniu i monitorowaniu kluczowych wska≈∫nik√≥w efektywno≈õci (KPI) , kt√≥re przek≈ÇadajƒÖ siƒô na wyniki wyszukiwarki i konwersjƒô na ≈õcie≈ºce u≈ºytkownika. Gromadzenie, monitorowanie i interpretowanie danych z r√≥≈ºnorodnych ≈∫r√≥de≈Ç i obszar√≥w, takich jak performance marketing czy ≈õcie≈ºki konwersji . Bycie kluczowym wsparciem w przek≈Çadaniu danych ilo≈õciowych na identyfikacjƒô problem√≥w u≈ºytkownik√≥w , co w efekcie bƒôdzie stanowiƒá podstawƒô do aktywnego wsparcia w kreowaniu skutecznych rozwiƒÖza≈Ñ. Jeste≈õ idealnym kandydatem, je≈õli: Jeste≈õ w pe≈Çni samodzielnym analitykiem z silnym mindsetem produktowym , kt√≥ry doskonale odnajduje siƒô w dynamicznym i zmiennym ≈õrodowisku startupowym. Posiadasz minimum 5-letnie do≈õwiadczenie w pracy z du≈ºymi zbiorami danych, wykorzystujƒÖc narzƒôdzia takie jak BigQuery, Tableau czy Snowflake . Znakomicie pos≈Çugujesz siƒô narzƒôdziami takimi jak: Looker, Grafana, GA4, Firebase, BigQuery, Excel . Posiadasz bardzo dobrƒÖ znajomo≈õƒá metodyki eksperyment√≥w A/B zar√≥wno na poziomie teoretycznym, jak i praktycznym, ze szczeg√≥lnym uwzglƒôdnieniem bada≈Ñ wyszukiwarek, a tak≈ºe potrafisz trafnie interpretowaƒá ich wyniki. Potrafisz samodzielnie wyciƒÖgaƒá konkretne wnioski biznesowe z zachowa≈Ñ u≈ºytkownik√≥w i danych ilo≈õciowych, wspierajƒÖc siƒô w razie potrzeby danymi jako≈õciowymi. Potrafisz odnale≈∫ƒá siƒô w gƒÖszczu danych, narzƒôdzi i proces√≥w, wyciƒÖgajƒÖc z nich nie tylko suche informacje, ale przede wszystkim warto≈õciowe wnioski , kt√≥re pomagajƒÖ osiƒÖgaƒá cele biznesowe. Nie boisz siƒô dynamicznego i zmiennego ≈õrodowiska , gdzie otwarta g≈Çowa i proaktywno≈õƒá w rozwiƒÖzywaniu problem√≥w sƒÖ fundamentem sukcesu. Dobrze znasz bran≈ºƒô e-commerce lub pasjonujesz siƒô rozwojem wyszukiwarek oraz poprawƒÖ efektywno≈õci ≈õcie≈ºek u≈ºytkownik√≥w. Co oferujemy? Mo≈ºliwo≈õƒá indywidualnego, nieograniczonego rozwoju biznesowego w dynamicznie rosnƒÖcej organizacji w bran≈ºy e-commerce; Realny wp≈Çyw na rozw√≥j naszej organizacji - Twoje pomys≈Çy siƒô dla nas liczƒÖ! Dofinansowanie pakietu benefit√≥w (karta sportowa, pakiet medyczny, ubezpieczenie grupowe); R√≥≈ºnorodne zadania i cele ‚Äì wiesz, co masz robiƒá, ale pozostawiamy Ci du≈ºƒÖ swobodƒô dzia≈Çania i jeste≈õmy otwarci na Twoje pomys≈Çy; Zachƒôcamy do aplikowania!","[{""min"": 18000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,18000,25000,Net per month - B2B
Full-time,Senior,Permanent,Remote,279,Senior Consultant SAP CDC,INFOPLUS TECHNOLOGIES,"Key Responsibilities: ‚Ä¢ Architect and lead the implementation of secure, scalable CIAM solutions using SAP CDC, aligned with Coustomers digital strategy ‚Ä¢ Serve as a consultative partner to business and technology teams, translating business needs into secure, user-friendly identity experiences. ‚Ä¢ Drive the strategic integration of CIAM with digital platforms and applications, ensuring interoperability and performance across ecosystems. ‚Ä¢ Champion compliance and data privacy by designing solutions that meet global regulatory standards (GDPR, CCPA, etc.) and internal governance requirements. ‚Ä¢ Provide technical leadership and mentoring to developers and platform engineers, fostering a high-performing and knowledge-sharing team culture. ‚Ä¢ Oversee CIAM operations and continuous improvement, ensuring system reliability, security posture, and proactive incident resolution. ‚Ä¢ Engage in cross-functional program planning, contributing to timelines, resource strategies, and stakeholder alignment in an agile delivery environment. ‚Ä¢ Stay ahead of emerging technologies and trends in CIAM, identity protocols, and cybersecurity‚Äîproactively proposing enhancements to future-proof the platform. Your Profile: ‚Ä¢ 6+ years of experience in identity and access management, with a strong track record in delivering customer-centric solutions. ‚Ä¢ Deep hands-on expertise with SAP Customer Data Cloud (Gigya) and associated CIAM capabilities. ‚Ä¢ Strong understanding of identity standards and protocols such as SAML, OAuth 2.0, OpenID Connect, and their secure implementation. ‚Ä¢ Experience designing and managing privacy-first architectures, with knowledge of GDPR, CCPA, and industry data protection frameworks. ‚Ä¢ Backend experience with platforms such as Java, .NET, PHP, and front-end skills using JavaScript, HTML/CSS, REST APIs. ‚Ä¢ Proven ability to lead technical initiatives, influence architectural decisions, and collaborate effectively with both technical and business teams. ‚Ä¢ Professional certifications (SAP CDC, CIAM, CISSP, CISM, ITIL, etc.) are advantageous. ‚Ä¢ Excellent communication and stakeholder engagement skills‚Äîable to convey complex ideas in a clear and actionable way. ‚Ä¢ Agile working experience (Scrum, SAFe, etc.) and a mindset of continuous learning and adaptability.","[{""min"": 300000, ""max"": 325000, ""type"": ""Gross per year - Permanent""}]",Unclassified,300000,325000,Gross per year - Permanent
Full-time,Mid,B2B,Remote,281,DWH Developer,Connectis,"Wsp√≥lnie z naszym Partnerem,najwiƒôkszƒÖ grupƒÖ energetycznƒÖ specjalizujƒÖcƒÖ siƒô w produkcji, dystrybucji i sprzeda≈ºy energii elektrycznej,poszukujemy osoby na stanowiskoProgramista Hurtowni Danych. Nasz Partner specjalizuje siƒô w dostarczaniu i utrzymaniu nowoczesnych rozwiƒÖza≈Ñ informatycznych, kt√≥re wspierajƒÖ kluczowe procesy operacyjne w sektorze energetycznym. Odpowiada on za projektowanie i rozw√≥j system√≥w bilingowych, system√≥w klasy ERP, platform raportowych Business Intelligence oraz rozwiƒÖza≈Ñ z zakresu cyberbezpiecze≈Ñstwa. üë®‚Äçüíª TWOJA ROLA: Wsp√≥≈Çpraca z zespo≈Çem deweloper√≥w hurtowni danych oraz analityk√≥w i kierownik√≥w projekt√≥w w implementacji. Udzia≈Ç w innowacyjnych projektachdotyczƒÖcych rozwoju us≈Çugi oraz tworzenia. Opracowanie dokumentacji projektowej. Rozw√≥j i utrzymanie hurtowni danych. Testowanie i wdro≈ºenie rozwiƒÖza≈Ñ. Wsparcie istniejƒÖcych rozwiƒÖza≈Ñ. üîçCZEGO OCZEKUJEMY OD CIEBIE? Bardzo dobra znajomo≈õƒáMS SQL- procedury, sql-e, indexy. Mile widziane podstawowe umiejƒôtno≈õci optymalizacji zapyta≈Ñ/indeks√≥w. Znajomo≈õƒá bran≈ºy energetycznej lub rozwiƒÖza≈Ñ billingowych potwierdzona co najmniej 2 letnim do≈õwiadczeniem. Podstawy optymalizacji proces√≥wETL (SSIS). T-SQL- rozbudowany o funkcje analityczne. Dobra znajomo≈õƒáSSIS. Znajomo≈õƒáMDM. ‚ú® OFERUJEMY: Uczestnictwo w spotkaniach integracyjnych oraz meetupach technologicznych, umo≈ºliwiajƒÖcych dzielenie siƒô wiedzƒÖ i do≈õwiadczeniem. Szansƒô na rozw√≥j zawodowy i osobisty w renomowanej firmie, kt√≥ra bierze aktywny udzia≈Ç w rewolucji bran≈ºy energetycznej. Wsparcie dedykowanego opiekuna Connectis, kt√≥ry zawsze jest dostƒôpny, by pom√≥c Ci w sprawach zwiƒÖzanych z projektem. 5000 PLN za polecenie znajomego do naszych projekt√≥w. Szybki, zdalny proces. Pracƒô 100% zdalnƒÖ. Dziƒôkujemy za wszystkie zg≈Çoszenia. Pragniemy poinformowaƒá ≈ºe skontaktujemy siƒô z wybranymi osobami. 12162/PZ","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,110,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,284,Senior Data Architect,N-iX,"N-iX is a software development service company that helps businesses across the globe develop successful software products. During 21 years on the market and by leveraging the capabilities of Eastern Europe talents the company has grown to 2000+ professionals with a broad portfolio of customers in the area of Fortune 500 companies as well as technological start-ups. N-iX has come a long way and increased its presence in nine countries - Poland, Ukraine, Romania, Bulgaria, Sweden, Malta, the UK, the US, and Colombia. The Data and Analytics practice, part of the Technology Office, is a team of high-end experts in data strategy, data governance, and data platforms, and contribute to shaping the future of data platforms for our customers. AsSenior Data Architect, you will play a crucial role in designing and overseeing the implementation of our strategic Databricks-based data and AI platforms. You will collaborate with data engineers and data scientists, define architecture standards, and ensure alignment across multiple business units. Your role will be pivotal in shaping the future state of our data infrastructure and driving innovative solutions within the automotive claims management domain. Key Responsibilities: Design scalable and robust data architectures using Databricks and cloud technologies (Azure/AWS) Oversee and guide the implementation of Databricks platforms across diverse business units Collaborate closely with data engineers, data scientists, and stakeholders to define architecture standards and practices Develop and enforce governance strategies, ensuring data quality, consistency, and security across platforms Lead strategic decisions on data ingestion, processing, storage, and analytics frameworks Evaluate and integrate new tools and technologies to enhance data processing capabilities Provide mentorship and guidance to engineering teams, ensuring architectural compliance and effective knowledge transfer Develop and maintain detailed architectural documentation. Requirements: 5+ years of experience as a Solution/Data Architect in complex enterprise environments Extensive expertise in designing and implementing Databricks platforms Strong experience in cloud architecture, preferably Azure or AWS Proficient in Apache Spark and big data technologies Advanced understanding of data modeling, data integration patterns, and data governance Solid background in relational databases (MS SQL preferred) and SQL proficiency Practical knowledge of data orchestration and CI/CD practices (Terraform, GitLab) Ability to articulate complex technical strategies to diverse stakeholders Strong leadership and mentorship capabilities Fluent English (B2 level or higher) Exceptional interpersonal and communication skills in an international team setting. Nice to have: Experience with Elasticsearch or vector databases Knowledge of containerization technologies (Docker, Kubernetes) Familiarity with dbt (data build tool) Willingness and ability to travel internationally twice a year for workshops and team alignment.","[{""min"": 20317, ""max"": 29553, ""type"": ""Net per month - B2B""}]",Data Architecture,20317,29553,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,285,üëâ Senior GenAI Engineer (Future Opening),Xebia sp. z o.o.,"üü£About project: This role focuses on developing and deploying AI applications using tools such as Azure AI Studio, vector databases, and Retrieval-Augmented Generation (RAG) frameworks. You will collaborate closely with senior team members to deliver robust, high-quality solutions that drive innovation. üü£You will be: designing, developing, and implementing AI solutions using Python, with a focus on LLMs, vision models, and generative AI technologies, building, testing, and optimizing RAG applications, leveraging effective prompt engineering techniques to improve model performance, integrating AI models with Azure AI Studio, SharePoint, and Azure Blob Storage for efficient deployment and data handling, utilizing vector databases and Agentic frameworks (e.g., LlamaIndex) to enhance the functionality and intelligence of AI systems, implementing event-driven architectures using tools like Event Hub and Kafka for real-time data processing and scalability, collaborating with the AI Lead and team members to troubleshoot issues, test models, and ensure successful deployment, writing clean, efficient, and well-documented code adhering to best practices and version control standards, staying current with emerging AI tools, frameworks, and technologies to continuously improve development processes and outcomes. üü£Your profile: Bachelor‚Äôs degree in computer science, Data Science, Engineering, or a related field, 4+ years of hands-on experience in AI/ML development, with an emphasis on generative AI and related technologies, strong experience with Python, REST APIs, Git proven expertise in developing and deploying LLMs, vision models, vector databases, and RAG applications, strong proficiency in Azure AI Studio, Azure Blob Storage, Event Hub, Kafka, familiarity with Agentic frameworks and tools like LlamaIndex for advanced AI development, ability to thrive in a collaborative team environment while managing multiple tasks effectively, good verbal and written communication skills in English (min. B2). üü£Nice to have: exposure to cloud fundamentals (Azure preferred) and containerization tools like Docker, experience with CI/CD pipelines for AI model deployment, understanding RESTful services and API integration. üü£Recruitment Process: CVreview ‚ÄìHRcall ‚ÄìInterview(with Live-coding) ‚ÄìClientInterview (with Live-coding) ‚ÄìHiring ManagerInterview ‚ÄìDecision üéÅBenefits üéÅ ‚úçDevelopment: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏èWe are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 23500, ""max"": 33500, ""type"": ""Net per month - B2B""}, {""min"": 18000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,23500,33500,Net per month - B2B
Full-time,Mid,B2B,Remote,287,Cloud Data Engineer with snowflake,Experis Manpower Group,"Requirements: at least 3 years of experience in Big Data or Cloud projects in the areas of processing and visualization of large and/or unstructured datasets (including at least 1 year of hands-on Snowflake experience) understanding of Snowflake's pricing model and cost optimization strategies for managing resources efficiently experience in designing and implementing data transformation pipelines natively with Snowflake or Service Partners familiarity with Snowflake‚Äôs security model practical knowledge of at least one Public Cloud platform in Storage, Compute (+Serverless), Networking and DevOps area supported by commercial project work experience at least basic knowledge of SQL and one of programming languages: Python/Scala/Java/bash very good command of English Tasks: design, develop, and maintain Snowflake data pipelines to support various business functions collaborate with cross-functional teams to understand data requirements and implement scalable solutions optimize data models and schemas for performance and efficiency ensure data integrity, quality, and security throughout the data lifecycle implement monitoring and alerting systems to proactively identify and address issues plan and execute migration from on-prem data warehouses to Snowflake develop AI, ML and Generative AI solution stay updated on Snowflake best practices and emerging technologies to drive continuous improvement Our offer: Remote work Multisport card Private healthcare system Life insurance","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,175,Net per hour - B2B
Full-time,Senior,B2B,Remote,288,Software Engineer,Pretius,"WPretiusposzukujemySoftware Engineerado projektu dotyczƒÖcego rozwoju popularnej platformy eCommerce. Lokalizacja: zdalnie Wynagrodzenie: 140-170 pln netto/h O projekcie: Generowanie nazw produkt√≥w (r√≥wnie≈º z u≈ºyciem AI), automatyczna ocena ich jako≈õci i wyboru najlepszej nazwy dla produktu Weryfikacja poprawno≈õci numer√≥w GTIN (EAN) oraz ich precyzyjne przyporzƒÖdkowywanie do konkretnych produkt√≥w Automatyczne t≈Çumaczenie nazw i parametr√≥w produkt√≥w na wszystkie jƒôzyki obs≈Çugiwane na platformie Allegro Oczekiwania: 5+ lat do≈õwiadczenia na podobnym stanowisku Do≈õwiadczenie w pracy w Kotlinie, wykorzystywaniu AI do projektowania, budowania i rozwijania rozwiƒÖza≈Ñ Umiejƒôtno≈õƒá efektywnej wsp√≥≈Çpracy w zespole, proaktywno≈õƒá i samodzielno≈õƒá w dzia≈Çaniu Dba≈Ço≈õƒá o wysokƒÖ jako≈õƒá dostarczanych rozwiƒÖza≈Ñ, z uwzglƒôdnieniem testowania i optymalizacji Stack: Kotlin/Spring, MongoDB, Apache Spark/Apache Beam, BigQuery Mile widziane: Prompt Engineering AI Co oferujemy w Pretius? Stawiamy na d≈Çugofalowe relacje oparte na uczciwych zasadach i rzetelno≈õci Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Mo≈ºliwo≈õƒá pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnƒôtrzne, konferencje, certyfikacje","[{""min"": 140, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,140,170,Net per hour - B2B
Full-time,Mid,B2B,Remote,289,SQL Developer,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 3-letnie do≈õwiadczenie zawodowe na stanowisku Programisty SQL Do≈õwiadczenie projektowe zwiƒÖzane z tworzeniem z≈Ço≈ºonych zapyta≈Ñ SQL, procedur oraz funkcji bazy danych Do≈õwiadczenie projektowe zwiƒÖzane ze strojeniem i optymalizacjƒÖ zapyta≈Ñ SQL Do≈õwiadczenie projektowe zwiƒÖzane z hurtowniƒÖ danych, procesami ETL Znajomo≈õƒá praktyk CI/CD Umiejƒôtno≈õƒá pracy w systemie kontroli wersji GIT Do≈õwiadczenie w programowaniu w PL/pgSQL, PL/SQL Znajomo≈õƒá bazy danych PostgreSQL/EDB Znajomo≈õƒá JSON Znajomo≈õƒá zagadnie≈Ñ FDW/po≈ÇƒÖczenia miƒôdzy r√≥≈ºnymi bazami Do≈õwiadczenie w programowaniu w jƒôzykach skryptowych w ≈õrodowisku LINUX Do≈õwiadczenie w programowaniu w jƒôzyku Python Dobra organizacja pracy i odpowiedzialno≈õƒá za powierzone zadania Komunikatywno≈õƒá, kreatywno≈õƒá, samodzielno≈õƒá, kultura osobista Dociekliwo≈õƒá w diagnozowaniu i rozwiƒÖzywaniu bie≈ºƒÖcych problem√≥w Mile widziane Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Do≈õwiadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy umiejƒôtno≈õƒá programowania w jƒôzyku PL/SQL (np. PostgreSQL Certification lub r√≥wnowa≈ºny) Certyfikat potwierdzajƒÖcy znajomo≈õƒá Apache Spark (np. Spark Developer Associate lub r√≥wnowa≈ºny) Kluczowe zadania Tworzenie i rozwijanie z≈Ço≈ºonych zapyta≈Ñ SQL, procedur sk≈Çadowanych oraz funkcji bazy danych Optymalizacja i strojenie zapyta≈Ñ SQL w celu poprawy wydajno≈õci system√≥w Projektowanie i wdra≈ºanie rozwiƒÖza≈Ñ zwiƒÖzanych z hurtowniƒÖ danych oraz procesami ETL Praca z bazƒÖ danych PostgreSQL/EDB oraz implementacja rozwiƒÖza≈Ñ z wykorzystaniem PL/pgSQL i PL/SQL Wykorzystywanie i zarzƒÖdzanie danymi w formacie JSON oraz obs≈Çuga zagadnie≈Ñ zwiƒÖzanych z FDW i po≈ÇƒÖczeniami miƒôdzy r√≥≈ºnymi bazami danych Programowanie w jƒôzykach skryptowych w ≈õrodowisku Linux oraz rozwijanie aplikacji i narzƒôdzi w Pythonie Praca w systemie kontroli wersji GIT oraz stosowanie praktyk Continuous Integration/Continuous Deployment (CI/CD)","[{""min"": 80, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Database Administration,80,120,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,290,Data Engineer,Bayer Sp. z o.o.,"At Bayer we‚Äôre visionaries, driven to solve the world‚Äôs toughest challenges and striving for a world where ,Health for all, Hunger for none‚Äô is no longer a dream, but a real possibility. We‚Äôre doing it with energy, curiosity and sheer dedication, always learning from unique perspectives of those around us, expanding our thinking, growing our capabilities and redefining ‚Äòimpossible‚Äô. There are so many reasons to join us. If you‚Äôre hungry to build a varied and meaningful career in a community of brilliant and diverse minds to make a real difference, there‚Äôs only one choice. Data Analysis and Synthesis Undertake data profiling and source system analysis Present clear insights to colleagues to support the end use of the data Data Development Process Design, build and test data products that are complex or large scale Build teams to complete data integration services Data Innovation Understand the impact on the organization of emerging trends in data tools, analysis techniques and data usage Data Integration Design Select and implement the appropriate technologies to deliver resilient, scalable and future-proofed data solutions and integration pipelines Data Modeling Produce relevant data models across multiple subject areas Explain which models to use for which purpose Understand industry-recognized data modelling patterns and standards, and when to apply them Compare and align different data models Metadata Management Design an appropriate metadata repository and present changes to existing metadata repositories Understand a range of tools for storing and working with metadata Provide oversight and advice to more inexperienced members of the team Problem Resolution Respond to problems in databases, data processes, data products and services as they occur Initiate actions, monitor services and identify trends to resolve problems Determine the appropriate remedy and assist with its implementation, and with preventative measures Programming and Build Use agreed standards and tools to design, code, test, correct and document moderate-to-complex programs and scripts from agreed specifications and subsequent iterations Collaborate with others to review specifications where appropriate Technical Understanding Understand the core technical concepts related to the role, and apply them with guidance Testing Review requirements and specifications, and define test conditions Identify issues and risks associated with work Analyze and report test activities and results WHO YOU ARE: Required: Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Data Science, or a related field. 5+ years of experience in data engineering or related roles, with a strong understanding of data integration and modeling. Proficiency in programming languages such as Python, SQL, or Java, and experience with data processing frameworks (e.g., Apache Spark, Hadoop). Familiarity with data warehousing solutions and ETL (Extract, Transform, Load) processes. Experience with cloud data platforms (e.g., AWS, Azure, Google Cloud) and data storage solutions (e.g., relational databases, NoSQL databases). Strong analytical skills and the ability to present complex data insights clearly to stakeholders. Excellent problem-solving abilities and a proactive approach to identifying and resolving issues. Preferred: Experience with data visualization tools (e.g., Tableau, Power BI) and business intelligence practices. Knowledge of data governance frameworks and best practices. Familiarity with Agile methodologies and project management tools. You feel you do not meet all criteria we are looking for? That doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence, we value potential over perfection! WHAT DO WE OFFER: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure Increased tax-deductible costs for authors of copyrighted works VIP Medical Care Package (including Dental & Mental health) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English WORK LOCATION: WARSAW AL. JEROZOLIMSKIE 158 YOUR APPLICATION: Bayer welcomes applications from all individuals, regardless of race, national origin, gender, age, physical characteristics, social origin, disability, union membership, religion, family status, pregnancy, sexual orientation, gender identity, gender expression or any unlawful criterion under applicable law. We are committed to treating all applicants fairly and avoiding discrimination. Bayer is committed to providing access and reasonable accommodations in its application process for individuals with disabilities and encourages applicants with disabilities to request any needed accommodation(s) using the contact information below. Bayer offers the possibility of working in a hybrid model. We know how important work-life balance is, so our employees can work from home, from the office or combine both work environments. The possibilities of using the hybrid model are each time discussed with the manager.Bayer respects and applies the Whistleblower Act in Poland.","[{""min"": 15000, ""max"": 21000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15000,21000,Gross per month - Permanent
Full-time,Senior,Permanent,Remote,292,Staff Software Engineer,dotLinkers,"Salary: up to 37 500 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Staff Software Engineer focused on Structured Data Storage, you will lead the design and implementation of scalable, resilient cloud-native storage solutions using technologies like CosmosDB, Azure Parquet, ADLS, CockroachDB, TiDB, and Snowflake. Your work will enhance platform scalability, performance, and reliability, helping teams deliver greater value with less operational overhead.You‚Äôll join the Infrastructure Services group, supporting core platform areas like compute, networking, and storage. The Structured Storage team provides reliable and cost-effective storage building blocks for product teams. Acting as a technical leader and mentor, you‚Äôll drive architectural decisions and ensure collaboration across engineering teams to modernize and simplify data infrastructure. Job Responsibilities Lead the full software lifecycle and strategic implementation of modern cloud-native structured data storage technologies (such as CosmosDB, ADLS, Parquet, Snowflake, CockroachDB), aiming to enhance scalability, performance, and resilience across the data platform. Take hands-on responsibility for architecting and developing core structured storage services, ensuring high standards of reliability, security, and operational efficiency. Promote modern data design principles, storage strategies, and governance practices via design reviews, workshops, documentation, and knowledge-sharing initiatives. Serve as a trusted advisor to senior technical and product leadership (Directors, VPs, CTO), providing guidance on architectural decisions, technology trade-offs, and long-term platform direction. Act as technical owner and mentor for structured storage architecture, supporting multiple engineering teams and fostering technical excellence and collaboration. Evaluate new data storage technologies and drive their adoption where they align with business objectives and architectural evolution. Maintain and prioritize a roadmap for improvements and innovations in storage infrastructure, ensuring timely delivery through leadership and coordination. Detect and address scalability challenges, reliability risks, and performance limitations, driving cross-team initiatives to resolve them. Stay informed on trends in distributed databases, cloud storage architectures, and serverless data patterns to guide ongoing strategy development. Minimum Qualifications: 7+ years of experience in backend engineering, distributed systems, or data platform development. At least 4 years of practical experience designing and implementing cloud-native storage solutions on Azure, AWS, or GCP. Proven expertise in at least one large-scale data storage technology (e.g., CosmosDB, Snowflake, ADLS, Parquet). Strong understanding of data modeling, consistency models, data partitioning, and query optimization within distributed environments. Proficiency in C#, Java, Python, or similar programming languages. Experience developing production-grade APIs and integrating data solutions into enterprise systems. Preferred Qualifications: Familiarity with various structured storage models (relational, columnar, key-value, time-series) and associated technologies (e.g., Cassandra, MongoDB, Redis). Experience with infrastructure-as-code, container orchestration (Kubernetes), and serverless architectures for data management. Background in leading architectural and strategic platform initiatives at scale. Experience working within regulated industries (e.g., healthcare, finance, legal tech). Advanced degree in Computer Science or related discipline. Leadership Expectations: Define and drive technical strategy for structured data storage systems, ensuring adoption across engineering teams. Mentor senior and mid-level engineers, fostering high technical standards and continuous learning. Lead cross-functional initiatives requiring coordination between architecture, security, platform, and product teams. Represent structured storage topics in technical planning sessions, design reviews, and executive discussions. Cultivate a culture of ownership, innovation, and technical curiosity within the broader engineering community. Competencies and Skills: Technical Mastery: Expertise in cloud-native databases, storage design, and distributed systems scalability. System-Level Thinking: Ability to design holistic, optimized systems that balance performance, cost, and maintainability. Leadership & Influence: Comfortable guiding diverse engineering teams and aligning them to technical vision. Communication: Strong skills in articulating architectural strategies and technical recommendations to both engineers and leaders. Problem Solving: Skilled at analyzing and resolving complex scalability and performance challenges in cloud environments. Collaboration: Track record of building partnerships across platform, infrastructure, and application teams. Adaptability: Comfortable working in dynamic environments with shifting priorities and emerging technologies. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 32500, ""max"": 37500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,32500,37500,Gross per month - Permanent
Full-time,Senior,B2B,Remote,294,Senior Database Administrator,Link Group,"Job Summary: We are seeking a skilled and experiencedSenior Database Administrator (DBA)to join our IT team. In this role, you will be responsible for the installation, configuration, maintenance, and optimization of our database systems. You will collaborate closely with development teams to design and fine-tune database structures, support users, and ensure data security and integrity. Install, configure, and maintain database management systems (DBMS) to ensure high availability and optimal performance. Monitor database performance and proactively implement tuning and optimization measures. Ensure data integrity and security through robust access controls and best practices. Perform regular backups and manage recovery strategies to protect business-critical data. Collaborate with software developers to design, implement, and optimize database schemas and queries. Diagnose and resolve database issues, providing expert-level technical support to end-users and internal teams. Maintain accurate and up-to-date documentation for database architecture, configurations, and procedures. Stay current with new database technologies, tools, and industry trends to recommend and implement improvements. Bachelor‚Äôs degree in Computer Science, Information Technology, or a related field. Proven experience as a Database Administrator or similar role, ideally at a senior level. Strong proficiency with major database systems such as Oracle, SQL Server, or MySQL. Solid understanding of database design principles, data modeling, and performance tuning. Knowledge of backup, restore, and disaster recovery procedures. Excellent problem-solving skills and keen attention to detail. Strong communication and collaboration skills to work effectively within cross-functional teams. Experience with cloud-based database platforms (e.g., AWS, Azure). Familiarity with data warehousing concepts and ETL processes. Understanding of database security best practices and compliance requirements.","[{""min"": 100, ""max"": 135, ""type"": ""Net per hour - B2B""}]",Database Administration,100,135,Net per hour - B2B
Full-time,Senior,B2B,Remote,295,Senior Data Scientist (pharma),7N,"Senior Data Scientist (pharma) About the project For our client operating in thepharmaceutical industry,we are looking for aSenior Data Scientistto work on advanceddata scienceprojects related tomedical image analysis. Work Mode: 100% remote Research, develop and implementmedical image processingpipelines utilizing state of the artcomputer-vision-basedapproaches in the medical image analysis space. Implement and maintain local andcloud-based dataand computational environments and platforms to enable the work. Builddata pipelines(data curation, preparation, cleaning, and consolidation) as an integral part of data science activity. UtilizeAI/ML(Artificial Intelligence/Machine Learning) to develop complex methodologies and analyses, all around various medical imaging modalities. Perform informedsemi-automated quality assessmentsof the acquired imaging data and of the derived measures. Align with clinical expertson the requirements, constraints and deliverables of a project. PhD degreewith 3+ years‚Äô relevant direct non-academic professional experience or PhD degree with strong post-doc experience inmedical image analysis Hands-on experience with deep learning for computer vision using frameworks likePyTorchandSklearn Strong practical knowledge in medical image analysis in multiple modalities -X-ray,US,MRI,CT,Digital Pathology Solid understanding of deep learning theory: CNNs,Transformers,RNN,GenAI Ability todevelop and validate algorithmsagainst clinical data Ability to do quickprototyping/proofs of conceptup to production ready models Ability toperform in-depth data analysisand present results to engineering and leadership teams Ongoing support from adedicatedagent, taking care of your project continuity, client contact, necessary formalities, work comfort and development, Consultant Development Program‚Äì advice on growth planning based on the latest trends and market needs in IT, including consultations withagents and growth mentors, Access to7N Learning & Development‚Äì a development and educational platform with webinars, a library of articles and industry reports, and regular invitations to one-time and recurring development events ‚Äì technical, business, and lifestyle, Spectacular integration events, both for you (e.g.,annual Kick-Off trip, Christmas parties, or Summer Olympics sports events) and for your loved ones (e.g., family picnics, movie premieres), Professional developmentnot only during the project ‚Äì you can get involved in knowledge transfer to others within the7N Servicesoffering directed at 7N clients, Relationships and access to the knowledge ofthe most experienced IT expertsin the market ‚Äì the average professional tenure of our consultants in Poland is over 10 years, A complete benefits package, including funding for medical care, life insurance, sports cards for you and your loved ones, as well as discounts in stores in Poland and abroad. Constantly searching for projects, difficult rate negotiations, lack of development support ‚Äì sounds familiar? At 7N, you gain not only stability of contracts but also the personal involvement of a dedicated agent who ensures your professional comfort and continuous access to development initiatives. Our mission is to provide stable and rewarding collaborations that drive your success as an IT expert and the success of our clients. We build long-lasting relationships based on Scandinavian values and 30 years of experience creating IT solutions for over 200 organizations.","[{""min"": 25200, ""max"": 31080, ""type"": ""Net per month - B2B""}]",Data Science,25200,31080,Net per month - B2B
Full-time,Mid,B2B,Remote,296,Mid Data Engineer (Python/Spark/Databricks),7N,"Nasz klient -jedna z najwiƒôkszych firm na ≈õwiecie zajmujƒÖcych siƒô analizƒÖ danych finansowych‚Äì poszukuje do≈õwiadczonych specjalist√≥w do pracy nad nowoczesnym projektem transformacji danych. W ramach projektu bƒôdziesz odpowiadaƒá zaprzekszta≈Çcenie istniejƒÖcych makr Excelowychw nowoczesne rozwiƒÖzania dzia≈ÇajƒÖce w ≈õrodowiskuDatabricksprzy wykorzystaniuPythona i Apache Spark. Lokalizacja: 100% zdalnie Start: wrzesie≈Ñ/pa≈∫dziernik Min. 4-letnie do≈õwiadczenie jako Data / Python Engineer Bardzo dobra znajomo≈õƒáPythonaiApache Spark Do≈õwiadczenie w pracy zDatabricks Umiejƒôtno≈õƒá analizy i przekszta≈Çcania makr Excelowych Dobra orientacja w zagadnieniach in≈ºynierii danych P≈Çynny j. angielski i do≈õwiadczenie we wsp√≥≈Çpracy w miƒôdzynarodowych ≈õrodowiskach (np. USA) Mile widziane: Znajomo≈õƒáSQL Do≈õwiadczenie w pracy z chmurƒÖ (AWS, Azure) Znajomo≈õƒáGitlub innych system√≥w kontroli wersji Analiza i zrozumienie logiki dzia≈Çania istniejƒÖcych makr Excelowych Konwersja makr na kod w Pythonie (Databricks/Spark) Wsp√≥≈Çpraca z in≈ºynierami danych przy integracji kodu z pipeline‚Äôami Optymalizacja kodu Spark i zapewnienie jego wydajno≈õci Dokumentowanie rozwiƒÖza≈Ñ oraz udzia≈Ç w przeglƒÖdach kodu Sta≈Çe wsparcie osobistegoagenta, dbajƒÖcego o TwojƒÖ ciƒÖg≈Ço≈õƒá projektowƒÖ, kontakt z klientem, niezbƒôdne formalno≈õci, komfort pracy oraz rozw√≥j Consultant Development Program‚Äì doradztwo w planowaniu kariery w oparciu o najnowsze trendy i potrzeby rynku IT, obejmujƒÖcem.in.konsultacje z agentami i mentorami kariery Dostƒôp do7N Learning & Development‚Äì platformy rozwojowo-edukacyjnej z webinarami, bibliotekƒÖ artyku≈Ç√≥w i raport√≥w bran≈ºowych oraz regularnymi zaproszeniami na jednorazowe i cykliczne wydarzenia rozwojowe ‚Äì techniczne, biznesowe oraz life-stylowe Spektakularne eventy integracyjne, zar√≥wno dla Ciebie (np. corocznywyjazd Kick-Off, imprezy ≈õwiƒÖteczne czy sportowe Letnie Igrzyska), jak i dla Twoich bliskich (np. pikniki rodzinne, premiery filmowe) Rozw√≥j zawodowynie tylko podczas projektu ‚Äì mo≈ºesz zaanga≈ºowaƒá siƒô w przekazywanie wiedzy innym w ramachoferty 7N Serviceskierowanej do klient√≥w 7N Relacje i dostƒôp do wiedzynajbardziej do≈õwiadczonych ekspert√≥w IT na rynku‚Äì ≈õredni sta≈º zawodowy naszego Konsultanta w Polsce to ponad 10 lat Pakiet benefit√≥w zaplanowany od A do Z, czyli dofinansowanie do opieki medycznej, ubezpieczenia na ≈ºycie, karty sportowej dla Ciebie i Twoich bliskich, a tak≈ºe zni≈ºki do sklep√≥w w Polsce i za granicƒÖ. CiƒÖg≈Çe poszukiwanie projekt√≥w, trudne negocjacje stawek, brak wsparcia w rozwoju ‚Äì brzmi znajomo? W 7N zyskujesz nie tylko stabilno≈õƒá kontrakt√≥w, ale te≈º zaanga≈ºowanie osobistego opiekuna dbajƒÖcego o Tw√≥j komfort zawodowy i sta≈Çy dostƒôp do inicjatyw rozwojowych. Naszym celem jest zapewnienie Ci stabilnej i komfortowej wsp√≥≈Çpracy, kt√≥ra przyczyni siƒô do sukcesu Twojego jako eksperta IT oraz naszych klient√≥w. Budujemy trwa≈Çe relacje, bazujƒÖc na skandynawskich warto≈õciach i 30-letnim do≈õwiadczeniu w tworzeniu rozwiƒÖza≈Ñ IT dla ponad 200 organizacji.","[{""min"": 21840, ""max"": 25200, ""type"": ""Net per month - B2B""}]",Data Engineering,21840,25200,Net per month - B2B
Full-time,Senior,B2B,Remote,297,Snowflake Engineer/ Architect,CRODU,"üå¥ Forma pracy: d≈Çugoterminowo, fulltime, 100% zdalnie üëà ‚è∞ Start: ASAPüëà Cze≈õƒá! üëã Dla naszego klienta z USA poszukujemy Snowflake Engineera. Klient zajmuje siƒô wsparciem firm w transformacjach chmurowych. Przekr√≥j projekt√≥w i bran≈º jest szeroki. Prace dotyczƒÖ dzia≈Ça≈Ñ w obszarachm.in. migracji, zbierania danych i optymalizacji rozwiƒÖza≈Ñ. Klientowi zale≈ºy na d≈Çugoterminowej wsp√≥≈Çpracy. Prowadzone projekty sƒÖ r√≥≈ºnej d≈Çugo≈õci, ale dziƒôki sta≈Çemu zapotrzebowaniu specjali≈õci zatrudnieni u klienta swobodnie przechodzƒÖ z zako≈Ñczonego projektu do nowych temat√≥w. Obecnie poszukiwani sƒÖ kolejni specjali≈õci do projektu z wykorzystaniem Snowflake. Projekt dotyczy bran≈ºy prawniczej. Klient ma legacy systemy i chcƒÖ zmigrowaƒá sie do Snowflake. Dodatkowo klient boryka siƒô z jako≈õciƒÖ danych i proces√≥w w pipelines, dlatego chcieliby zajƒÖƒá siƒô r√≥wnie≈º automatyzacjƒÖ proces√≥w (np. czyszczenia danych) i dopracowaniem struktury danych. Klient posiada lokalnie ma≈Çy zesp√≥≈Ç, natomist widzi potrzebƒô rozszerzenia go o dodatkowych ludzi -> docelowo w planach jest powiƒôkszenie zespo≈Çu o 10 dodatkowych os√≥b. Projekt prowadzony dla firmy z USA, ale wymagana jest praca jedynie z niewielkƒÖ zak≈ÇadkƒÖ godzinowƒÖ (od 10: 00 do 18: 00). Codzienne zadania: üìç Projektowanie, rozwijanie i utrzymywanie zaawansowanej, skalowalnej i odpornej architektury danych oraz pipelines z wykorzystaniem Snowflake üìç Budowanie i rozwijanie z≈Ço≈ºonych transformacji danych z u≈ºyciem dbt lub podobnych narzƒôdzi üìç Zapewnienie wysokiej jako≈õci danych w pipeline'ach (kontrole i walidacje w celu wykrywania i korygowania nieprawid≈Çowo≈õci) üìçZbieranie danych do cel√≥w historycznych i audytowych (z naciskiem na sp√≥jno≈õƒá i dok≈Çadno≈õƒá zbieranych danych) üìç Przestrzeganie data governance - ze szczeg√≥lnym naciskiem na security i compliance üìç Monitorowanie i optymalizacja dzia≈Çania pipeline‚Äô√≥w (przepustowo≈õƒá, op√≥≈∫nienia) üìç Identyfikowanie wƒÖskich garde≈Ç i nieefektywno≈õci w istniejƒÖcych procesach oraz proponowanie rozwiƒÖza≈Ñ usprawniajƒÖcych przep≈Çyw danych i czas przetwarzania üìç ≈öcis≈Ça wsp√≥≈Çpraca z zespo≈Çami data warehouse i analytics üìç Dostarczanie dokumentacji technicznej i szkole≈Ñ dla innych cz≈Çonk√≥w zespo≈Çu w celu zapewnienia sprawnej obs≈Çugi i utrzymania system√≥w Wymagania: ‚ö°Ô∏è 7 lat do≈õwiadczenia w obszarach Data Engineering ‚ö°Ô∏è 4 lata do≈õwiadczenia w pracy z Snowflake ‚ö°Ô∏è Bardzo dobra znajomo≈õƒá SQL-a i umiejƒôtno≈õƒá optymalizacji zapyta≈Ñ ‚ö°Ô∏è Bardzo dobra znajomo≈õƒá Python ‚ö°Ô∏è Dobra znajomo≈õƒá proces√≥w ETL/ELT oraz narzƒôdzi takich jak dbt ‚ö°Ô∏è Solidne zrozumienie zasad hurtowni danych i modelowania danych ‚ö°Ô∏è Do≈õwiadczenie z platformami chmurowymi takimi jak AWS lub Azure ‚ö°Ô∏è Angielski na poziomie umo≈ºliwiajƒÖcym swobodnƒÖ komunikacjƒô w zespole Mile widziane: ‚ö°Ô∏è Certyfikacja SnowPro lub SnowPro Advanced ‚ö°Ô∏è Certyfikacja DBT ‚ö°Ô∏è Znajomo≈õƒá Astronomer/Airflow Jak dzia≈Çamy i co oferujemy? üéØ Stawiamy na otwartƒÖ komunikacjƒô zar√≥wno w procesie rekrutacji jak i po zatrudnieniu - zale≈ºy nam na klarowno≈õci informacji dotyczƒÖcych procesu i zatrudnienia üéØ Do rekrutacji podchodzimy po ludzku, dlatego upraszczamy nasze procesy rekrutacyjne, ≈ºeby by≈Çy mo≈ºliwie jak najprostsze i przyjazne kandydatowi üéØ Pracujemy w imiƒô zasady ""remote first"", wiƒôc praca zdalna to u nas norma, a wyjazdy s≈Çu≈ºbowe ograniczamy do minimum üéØ Oferujemy prywatnƒÖ opiekƒô medycznƒÖ (Medicover) oraz kartƒô Multisport dla kontraktor√≥w","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Architecture,150,180,Net per hour - B2B
Full-time,Senior,B2B,Remote,298,Data Architect (public/health),Britenet,"Projekt dla instytucji odpowiedzialnej za rozw√≥j i utrzymanie system√≥w informatycznych wspierajƒÖcych funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania: Minimum 5 lat do≈õwiadczenia na stanowisku Architekta IT, poparte projektowaniem architektur system√≥w zorientowanych na us≈Çugi (SOA, mikroserwisy), wielowarstwowych oraz o wysokiej wydajno≈õci i niezawodno≈õci. Praktyczne do≈õwiadczenie w roli Architekta w projektach o bud≈ºecie min. 1 mln z≈Ç brutto oraz w szacowaniu pracoch≈Çonno≈õci (min. 1000 roboczogodzin) i z≈Ço≈ºono≈õci rozwiƒÖza≈Ñ (ilo≈õƒá/wielko≈õƒá komponent√≥w). Umiejƒôtno≈õƒá skalowania aplikacji (horyzontalne i wertykalne) oraz badania i oceny bezpiecze≈Ñstwa system√≥w teleinformatycznych. Do≈õwiadczenie w architekturze rozwiƒÖza≈Ñ data warehouse i narzƒôdzi analitycznych, w tym znajomo≈õƒá projektowania hurtowni danych i data mart√≥w. G≈Çƒôboka znajomo≈õƒá architektury, projektowania i integracji system√≥w IT, w tym wzorc√≥w projektowych i architektonicznych, serwer√≥w aplikacyjnych oraz zagadnie≈Ñ DevOps i konteneryzacji. Bieg≈Ço≈õƒá w programowaniu SQL i Python, a tak≈ºe w projektowaniu i programowaniu proces√≥w ETL. Praktyczna znajomo≈õƒá baz danych: PostgreSQL/EDB, MySQL, MongoDB, Oracle. Swobodna obs≈Çuga narzƒôdzi: Enterprise Architect, MS Project, Jira. Doskona≈Ça organizacja pracy, zorientowanie na cel, umiejƒôtno≈õci interpersonalne (planowanie, definiowanie, monitorowanie cel√≥w) oraz efektywna komunikacja. Kreatywno≈õƒá, samodzielno≈õƒá, wysoka kultura osobista, odporno≈õƒá na stres, proaktywno≈õƒá oraz otwarto≈õƒá na rozw√≥j i nowe technologie. Mile widziane: Do≈õwiadczenie projektowe z obszaru Hurtowni Danych Do≈õwiadczenie projektowe w obszarze ochrony zdrowia Certyfikaty z obszaru zarzƒÖdzania projektem metodƒÖ zwinnƒÖ (np.. Agile PM lub r√≥wnowa≈ºny); potwierdzajƒÖcy umiejƒôtno≈õci z obszaru projektowania architektur rozwiƒÖza≈Ñ IT (np.. TOGAF¬Æ EA Foundation lub r√≥wnowa≈ºny); potwierdzajƒÖcy wiedzƒô z zakresu administrowania EDB (np. EDB Certification - PostgreSQL Essentials 15 lub r√≥wnowa≈ºny); z obszaru administrowania ≈õrodowiskiem Hadoop (np. ClouderaCertified Administrator for Hadoop (CCAH), Hortonworks Certified Apache Hadoop Administrator (HCAHA) lub r√≥wnowa≈ºny) Kluczowe zadania: Projektowanie architektur system√≥w zorientowanych na us≈Çugi (SOA) i mikroserwis√≥w Tworzenie wielowarstwowych system√≥w o wysokiej wydajno≈õci i niezawodno≈õci Skalowanie aplikacji zar√≥wno horyzontalnie, jak i wertykalnie Zapewnienie sp√≥jno≈õci i integralno≈õci architektury system√≥w Aktywny udzia≈Ç w projektach IT, w tym w projektach o znacznym bud≈ºecie (minimum 1 mln z≈Ç brutto) Szacowanie pracoch≈Çonno≈õci zada≈Ñ programistycznych i architektonicznych (na poziomie minimum 1000 roboczogodzin) Ocena z≈Ço≈ºono≈õci aplikacji i rozwiƒÖza≈Ñ (liczba i wielko≈õƒá komponent√≥w) Badanie i ocena bezpiecze≈Ñstwa informacji w systemach teleinformatycznych Zapewnienie zgodno≈õci system√≥w z zasadami bezpiecze≈Ñstwa IT Zastosowanie wzorc√≥w projektowych i architektonicznych w celu zapewnienia jako≈õci i niezawodno≈õci Realizacja architektury rozwiƒÖza≈Ñ zawierajƒÖcych hurtownie danych i narzƒôdzia analityczne Projektowanie i programowanie proces√≥w ETL Znajomo≈õƒá i wykorzystanie relacyjnych baz danych (PostgreSQL/EDB, MySQL, Oracle) oraz nierelacyjnych (MongoDB) Projektowanie i implementacja integracji system√≥w IT","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Architecture,150,180,Net per hour - B2B
Full-time,Mid,B2B,Remote,304,BI Developer with Snowflake,Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä Design, build, and develop data warehouses based on Snowflake; Create and optimize ETL/ELT processes using Matillion; Integrate data from various sources (databases, APIs, files); Develop reports and dashboards in Power BI for internal clients; Maintain, monitor, and further develop existing BI solutions; Collaborate with project teams and business stakeholders to gather and analyze requirements; Participate in data migration from legacy systems (e.g., Oracle, SSIS) to Snowflake; Ensure high-quality technical and business documentation; Implement best practices for data management, security, and version control; Actively participate in Agile team meetings (e.g., daily stand-ups, sprint planning, retrospectives); 2 years' experiencein a BI Developer role; At least 1 year of proven experience as a Snowflake developer,responsible for building ETL processes and creating tables and views within the Snowflake environment; Solid skills inPower BI; Eager to work as afull-stack BI Developer(both backend and frontend); Strong English skills (min. C1 level, daily communication with international clients); Proactive and creative- skills to drive improvements and engage with both technical and non-technical stakeholders; Strong documentation skills and a knack for business analysis. Experience withMatillion; Experience withOracle(we‚Äôre migrating to Snowflake, but legacy knowledge is a plus); Experience withSSAS/SSIS/SSRS; Familiarity withVisual Studio; Understanding ofversion control concepts(branching, merging, pushing; Git integrated with Matillion). Background in procurement, supply chain, or business data analysis related to orders and internal corporate stakeholders; Background inManaged Servicesdelivery models - you know how to take end-to-end ownership of BI solutions, ensuring their reliability, scalability, and alignment with client needs throughout the entire lifecycle; Experience working inAgileteams. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Fully remotework or in our office in Wroc≈Çaw; B2B Contract: 120 ‚Äì 140 PLN net/hour + VAT Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories.","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,305,AI Data Engineer,ITDS,"AI Data Engineer Join us, and build the future of AI-powered data systems! Krak√≥w - based opportunity with hybrid work model (2 days/week in the office). As anAI Data Developeryou will be working for our client, a global financial institution currently investing in advanced AI-driven solutions to modernize its internal data ecosystems. You will play a key role in a pioneering project that focuses on creating agents to collect and synchronize data from multiple systems, powering Retrieval-Augmented Generation (RAG) pipelines for GenAI models. This is a technically challenging role that involves building scalable data pipelines, designing microservices, and leading a collaborative team of developers within a high-impact cloud-based infrastructure. Designing and implementing scalable data pipelines to support GenAI systems Creating and managing microservices that integrate into a broader AI solution architecture Leading a team of developers and AI specialists across various project phases Developing and maintaining RESTful APIs using FastAPI Managing data sourcing processes for RAG using Python and time-series/analytics databases Overseeing deployment workflows using tools like Ansible and Jenkins Ensuring system reliability and maintainability within a Unix/Linux environment Coordinating with stakeholders to ensure requirements are captured and implemented Writing shell scripts for task automation and process monitoring Documenting technical specifications and system architecture Proven experience developing in Python within Unix/Linux environments Hands-on experience with time-series or analytics databases such as Elasticsearch Proficiency in building RESTful APIs using FastAPI Experience with Azure Cloud infrastructure and services Background in building microservices architectures Strong knowledge of data pipelines for RAG or similar GenAI applications Familiarity with Git/GitHub and automated deployment tools (Ansible, Jenkins) Basic shell-scripting and process automation skills Solid understanding of software development lifecycle (SDLC) practices Ability to work independently with a proactive, team-oriented mindset Experience with Generative AI APIs and frameworks Understanding of big data modeling using both relational and non-relational techniques Familiarity with cloud-native design patterns Excellent communication and documentation skills Willingness to quickly adapt to evolving project requirements We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7227 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere.","[{""min"": 21000, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Science,21000,31500,Net per month - B2B
Full-time,Senior,B2B,Hybrid,308,Data & Visualisation Specialist (Tableau),Antal Sp. z o.o.,"Data & Visualisation Specialist (Tableau) Location: hybrid, Cracow Contract Type: B2B Workload: Full-time Are you looking to make a real impact in a global financial project? We are working Data & Visualisation Specialist (Tableau) to join our Clients team. Company Description Our Client's team is dedicated to fostering a data-driven culture across the organization. Their strategy focuses on protecting the data through robust management policies, engaging colleagues with enhanced training opportunities, and building sustainable capabilities to unlock value for customers. Principal Accountabilities Developing data visualizations in the form of advanced dashboards and reports Understanding infrastructure requirements and best practices to support a Tableau deployment Data Source Management ‚Äì ensuring data integrity, updating data as needed Managing Project site on Tableau Server Documenting processes, data sources and dashboards Analyze & drive data sharing best practices around user access Strong stakeholder management and influencing skills Essential Criteria Proficiency in Tableau 3+ years‚Äô work experience in data analysis Ability to query and display large data sets while maximizing the performance of Tableau workbooks Working knowledge of Tableau administrator/architecture Ability to work collaboratively with cross-functional teams and stakeholders at all levels A high degree of mathematical competence Analytically minded Developing dashboards in QlikSense experience (nice to have) Major Challenges Manage self in a positive, engaging, thoughtful and constructive manner, from a role which often has a material influence on outcomes. Operate in a fast changing operating environment requiring quality decision making with often incomplete information. Maintaining well-regarded, thoughtful communications with principal stakeholders. Ensure data sharing compliance in a complex regulatory landscape What We Offer b2b contract and support of the Contractor Care Team Private Medical Care Cafeteria system Life insurance To learn more about Antal, please visit www.antal.pl","[{""min"": 30000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Unclassified,30000,33000,Net per month - B2B
Full-time,Mid,Permanent,Hybrid,310,Salesforce Consultant,Vaillant Group Business Services,"What we achieve together Are you ready to make a meaningful impact by joining our Service2Retention team? You will play a crucial role in enhancing service solutions for the Vaillant Group using Salesforce. In this position, you will have the opportunity to design and implement innovative solutions in collaboration with IT teams, customers, and product owners. Your work will span across Salesforce Service Cloud, Field Service (both Classic and LEX), and Marketing Cloud. Your responsibilities will also include writing and updating platform documentation, as well as conducting user trainings and demos to ensure everyone is equipped with the knowledge they need. You will manage stakeholder requirements and expectations through effective project management, ensuring that all parties are aligned and satisfied with the progress. Finally, you will contribute to a self-organizing and autonomous team, where your input and initiative are valued and encouraged. What makes us successful together Experience : You bring a minimum of 3 years of experience in designing salesforce or service processes. Ideally you designed and implemented such solutions in Salesforce Service Cloud or Field Service Know-how and skills: You have solid understanding of different types of service processes, such as orders, contracts, call center, invoicing, etc. Nice to have : You are familiar with Salesforce Marketing Cloud, including Journey Builder, Email Studio, and Automation Studio, experience in integrating Salesforce products and third-party applications, especially ERP systems. Personality : With your positive attitude and trustworthy personality, you can build strong stakeholder relationships. You are very well organized, have high standards for the quality of your work and communicate proactively and clearly. Language skills: You speak English fluently; Polish or German would be a plus. What makes us special Environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee Package of additional benefits: private medical care, multi-sport card. A fast growing, agile and very dynamic team that challenges established routines and helps transforming the Vaillant Group to a data informed business Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings","[{""min"": 17000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Unclassified,17000,20000,Gross per month - Permanent
Full-time,Mid,Mandate,Remote,314,IT Support & Systems Administrator (Salesforce CRM),BookingHost Sp. z o.o.,"Do zespo≈Çu IT poszukujemy osoby skrupulatnej, bystrej, komunikatywnej i samodzielnej, kt√≥ra wesprze nas w codziennej obs≈Çudze i rozwoju system√≥w informatycznych, w tym rozwiƒÖzywaniu zg≈Çosze≈Ñ naszych pracownik√≥w (ticket√≥w). Administracji system√≥w informatycznych - Salesforce CRM i szeregiem innych aplikacji - zarzƒÖdzaniem u≈ºytkownikami, konfiguracjƒÖ kont, zakresem ich uprawnie≈Ñ itp.); Udziale w planowaniu proces√≥w biznesowych w firmie, przygotowaniem pod nie wymaga≈Ñ technicznych oraz ich realizacji; Wdra≈ºaniu nowych funkcjonalno≈õci m.in. automatyzacji z u≈ºyciem Salesforce Flows, Make; Integracji nowych narzƒôdzi; Sprawowaniu pieczy nad porzƒÖdkiem w modelu danych; Generowaniu raport√≥w i analizie danych; Bie≈ºƒÖcym wsparciu u≈ºytkownik√≥w. Dobra znajomo≈õƒá administracji CRM Salesforce (ewentualnie innego systemu CRM / ERP); Do≈õwiadczenie w administracji innych system√≥w i/lub wsparciu technicznym; Sprawne pos≈Çugiwanie siƒô arkuszami Google Sheets / Excel; Samodzielno≈õƒá i chƒôƒá uczenia siƒô, tak≈ºe z wykorzystaniem dostƒôpnych w sieci materia≈Ç√≥w; Umiejƒôtno≈õƒá pracy w dynamicznym ≈õrodowisku, wielozadaniowo≈õƒá, szybko≈õƒá adaptacji do zmian; Dobra organizacja pracy, umiejƒôtno≈õƒá trafnej oceny priorytet√≥w; Swoboda w komunikacji, wysoka kultura osobista; Dobra znajomo≈õƒá jƒôzyka angielskiego (czytanie dokumentacji, aktywne uczestnictwo w spotkaniach). architekta rozwiƒÖza≈Ñ Salesforce i/lub w programowaniu w APEX lub Java; z narzƒôdziami takimi jak Front, Calendly, Make, Zapier, Google Workspace, Slack, wirtualna centralka, systemy ticketowe (np. Jira, ServiceNow); w nadzorowaniu projekt√≥w prowadzonych z zewnƒôtrznymi podmiotami jak agencje deweloperskie; W integracji Salesforce z innymi systemami / aplikacjami. W pe≈Çni zdalnƒÖ pracƒô, swobodnƒÖ atmosfera w zespole, elastyczne godziny; P≈Çatny urlop przy umowie B2B; Wsparcie w rozwoju umiejƒôtno≈õci administrowania system√≥w; Realny wp≈Çyw na codziennƒÖ pracƒô firmy i rozw√≥j struktury IT; Pracƒô w szybko rosnƒÖcej i dynamicznej firmie, aspirujƒÖcej do pozycji lidera na rynku wynajmu mieszka≈Ñ; Dostƒôpne opcje Karty MultiSport i MultiLife. üìùProces rekrutacji Proces rekrutacji sk≈Çada siƒô z dw√≥ch etap√≥w ‚Äì oba odbywajƒÖ siƒô zdalnie: Rozmowa telefoniczna(ok. 15‚Äì30 minut) ‚Äì kr√≥tkie poznanie siƒô, om√≥wienie do≈õwiadczenia i oczekiwa≈Ñ. Spotkanie online( ok. 60 minut) ‚Äì rozmowa techniczna z cz≈Çonkiem zespo≈Çu IT, podczas kt√≥rej sprawdzimy wiedzƒô praktycznƒÖ i dopasowanie do roli.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 9000, ""type"": ""Gross per month - Mandate""}]",Database Administration,10000,13000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,315,IT Data Warehouse Analyst - Leader (banking),Primaris,"Forma wsp√≥≈Çpracy: kontrakt B2B Tryb: gotowo≈õƒá do pracy w trybie hybrydowym z Wroc≈Çawia Aktualnie do jednego z projekt√≥w poszukujemy osoby na stanowiskoIT Data Warehouse Analyst - Leader, kt√≥ra posiada min.5lat komercyjnego do≈õwiadczenia. Dlatego je≈õli szukasz pracy z: budowƒÖ hurtowni danych w bankowo≈õci, SQL, Power BI/ Cognos, projektowaniem baz danych oraz analizƒÖ wymaga≈Ñchƒôtnie z TobƒÖ porozmawiamy! Projekt kt√≥ry bƒôdziemy gotowi Ci zaproponowaƒá dotyczy: budowy hurtowni danych i systemu raportowego w bankowo≈õci. Do≈õwiadczenie w budowie i rozwoju hurtowni danych w sektorze bankowym ‚Äì minimum 5 lat praktyki, ze zrozumieniem specyfiki danych i proces√≥w w bankowo≈õci Bardzo dobra znajomo≈õƒá SQL ‚Äì umiejƒôtno≈õƒá efektywnego tworzenia zapyta≈Ñ oraz optymalizacji Znajomo≈õƒá narzƒôdzia analityczno-raportowego: praktyczne do≈õwiadczenie z Power BI lub Cognos, w tym definicja struktur raportowych oraz tworzenie dashboard√≥w i wizualizacji Do≈õwiadczenie w zbieraniu i analizie wymaga≈Ñ biznesowych na hurtowniƒô danych i raportowanie ‚Äì umiejƒôtno≈õƒá wsp√≥≈Çpracy z u≈ºytkownikami biznesowymi i t≈Çumaczenia potrzeb na rozwiƒÖzania techniczne Do≈õwiadczenie w projektowaniu i tworzeniu struktur bazodanowych pod hurtowniƒô danych ‚Äì znajomo≈õƒá modelowania danych oraz pracy z du≈ºymi zbiorami danych Znajomo≈õƒá jƒôzyka angielskiego na poziomie B2 Umiejƒôtno≈õƒá skutecznej wsp√≥≈Çpracy i prezentacji wynik√≥w pracy Do≈õwiadczenie z platformƒÖ Teradata oraz narzƒôdziami do integracji i zarzƒÖdzania hurtowniami danych Znajomo≈õƒá narzƒôdzi do przetwarzania danych, np. Ab Initio Praktyka w pracy w ≈õrodowisku chmurowym (np. Azure, AWS, Google Cloud) i wdra≈ºaniu rozwiƒÖza≈Ñ data warehouse w chmurze Zakres obowiƒÖzk√≥w: Budowa i rozw√≥j hurtowni danych w obszarze bankowo≈õci, z uwzglƒôdnieniem specyfiki i wymaga≈Ñ bran≈ºy finansowej Projektowanie i tworzenie struktur bazodanowych dedykowanych hurtowniom danych, w tym modelowanie tabel fakt√≥w i wymiar√≥w oraz relacji miƒôdzy nimi Zbieranie i analiza wymaga≈Ñ biznesowych dotyczƒÖcych hurtowni danych i raportowania w ramach wsp√≥≈Çpracy z r√≥≈ºnymi dzia≈Çami firmy Implementacja oraz optymalizacja proces√≥w ETL do zasilania hurtowni danych, dbanie o jako≈õƒá i sp√≥jno≈õƒá danych Tworzenie i utrzymanie struktur raportowych oraz dashboard√≥w w narzƒôdziach analityczno-raportowych, przede wszystkim w Power BI lub IBM Cognos, zgodnie z wymaganiami u≈ºytkownik√≥w biznesowych Pisanie zaawansowanych zapyta≈Ñ SQL do ekstrakcji, transformacji i analizy danych Wsp√≥≈Çpraca i komunikacja z zespo≈Çami projektowymi i biznesowymi, w tym zbieranie potrzeb i szkolenie u≈ºytkownik√≥w ko≈Ñcowych Utrzymywanie i rozw√≥j ≈õrodowisk hurtowni danych z opcjonalnym wykorzystaniem platform takich jak TeraData czy narzƒôdzi Ab Initio(mile widziane) Dbanie o dokumentacjƒô technicznƒÖ i biznesowƒÖ rozwiƒÖza≈Ñ W naszej firmie bƒôdziesz m√≥g≈Ç/mog≈Ça liczyƒá na: Pracƒô w organizacji z ugruntowanƒÖ pozycjƒÖ rynkowƒÖ Projekty, w kt√≥rych bƒôdziesz mia≈Ç/mia≈Ça wp≈Çyw na ich rozw√≥j Wsp√≥≈Çpracƒô z ciekawymi klientami biznesowymi z r√≥≈ºnych bran≈º (m.in.: finanse, bankowo≈õƒá, ubezpieczenia, healthcare, robotyzacja, energetyka, media), Permanentny mentoring zar√≥wno techniczny jak i biznesowo-mened≈ºerski, np. podczas naszych cyklicznych szkole≈Ñ (m.in. Git, Gitflow, Angular, Docker), czy wew. program√≥w rozwojowych (Primaris x TechTalks, Primaris Leadership Academy) oraz zewnƒôtrznych kurs√≥w.Ju≈º na etapie on-boardingu zapewniamy dostƒôp do naszych wewnƒôtrznych szkole≈Ñ, cyklicznych spotka≈Ñ, kt√≥re serializujemy na Confluence oraz platformy e-learning ≈öwietnƒÖ atmosferƒô pracy, w≈õr√≥d zaanga≈ºowanych ludzi z pasjƒÖ w p≈Çaskiej strukturze z prostymi procesami Kompleksowy pakiet benefit√≥w skrojonych na miarƒô - prywatna opieka medyczna dla Ciebie oraz dla Twojej rodziny, Multisport dla Ciebie i os. towarzyszƒÖcej - Ty decydujesz, co wybierasz! Ca≈Çy proces rekrutacyjny oraz onboarding prowadzony jest zdalnie. Proces rekrutacyjny sk≈Çada siƒô z: rozmowy telefonicznej z osobƒÖ z dzia≈Çu Rekrutacji & HR (do 30 min) zdalnej video rozmowy - weryfikacji techniczno-biznesowej z naszym specjalistƒÖ/specjalistkƒÖ (60-90 min) zdalnego spotkania z liderem po stronie klienta projektu (30-60 min) finalnej decyzji dotyczƒÖcej oferty Primaris Services to ponad 250 ekspert√≥w na pok≈Çadzie i 15 lat do≈õwiadczenia w bran≈ºy IT na rynku polskim oraz zagranicznym.Realizujemy ambitne projekty o wysokiej z≈Ço≈ºono≈õci z r√≥≈ºnych obszar√≥w -m.in. bankowo≈õci, ubezpiecze≈Ñ, funduszy inwestycyjnych czy bran≈ºy logistycznej (mamy ponad 40 aktywnych klient√≥w!). Ro≈õniemy w si≈Çƒô oraz ciƒÖgle poszerzamy portfolio zar√≥wno naszych us≈Çug jak i klient√≥w. Zakres naszej dzia≈Çalno≈õci obejmuje budowƒô system√≥w od zera, ich rozw√≥j oraz utrzymanie, wdro≈ºenia produktowe, alokacje ca≈Çych Zespo≈Ç√≥w, a tak≈ºe pojedynczych Ekspert√≥w w strukturach Klienta. Ponadto od kilku lat dzia≈Çamy bardzo intensywnie jako z≈Çoty Partner firmy UiPath (obszar Robotic Process Automation) budujƒÖc roboty i sprzedajƒÖc licencje u naszych Klient√≥w. Co miesiƒÖc do≈ÇƒÖcza do nas 7 nowych os√≥b! Wierzymy, ≈ºe zgrany zesp√≥≈Ç i ludzie z pasjƒÖ to klucz do naszego wsp√≥lnego sukcesu! W≈Ça≈õnie dlatego ciƒÖgle poszukujemy nowych, zdolnych os√≥b, kt√≥re zasilƒÖ nasze szeregi.","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,150,170,Net per hour - B2B
Full-time,Senior,B2B,Remote,316,Cloud Database Engineer,Altimetrik Poland,"Altimetrik Poland is a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. For our Fintech client from the UK, we are seeking a Cloud Database Engineer to join our growing team. The ideal candidate for this role will designs, manages, and optimises cloud-based database systems, ensuring scalability, performance, and data security while supporting business-critical applications. Responsibilities: Customer / client centricity: Ensures that cloud database services meet client expectations regarding reliability, performance, and security Actively works to troubleshoot and resolve client issues related to cloud database environments, ensuring minimal downtime and optimal performance Collaborates with clients to ensure that cloud database solutions align with their needs and support their operational goals Data & reporting: Gathers and analyses data on database performance metrics, resource usage, and incidents in cloud environments Produces regular reports that highlight database performance, cost management, and optimization opportunities within the cloud infrastructure Ensures data accuracy in reporting to support strategic decisions regarding cloud database management Industry knowledge: Stays up to date with cloud database technologies, trends, and best practices, including platforms like AWS RDS, Azure SQL Database, or Google Cloud SQL Applies knowledge of cloud database advancements to drive improvements in data management and retrieval efficiency Ensures the organization‚Äôs cloud database strategies are aligned with modern industry practices and innovations Planning & prioritisation: Manages the prioritisation of cloud database tasks, such as updates, patches, and incident responses Plans cloud database resource allocation effectively, ensuring high-priority issues are addressed promptly and efficiently Works with other teams to coordinate database deployments and updates, ensuring seamless transitions and minimal disruption to service Product Knowledge: Requires a detailed understanding of cloud-based data products and database technologies that support the company‚Äôs offerings Familiarity with database architecture, cloud solutions, and how they support the scalability and reliability of products. Strategic impact: Ensures that cloud database strategies and solutions align with the organization‚Äôs broader business objectives Recognizes how cloud database performance impacts organizational scalability and agility, contributing to the company's success Identifies ways to optimize cloud database resources to improve cost-efficiency and performance, ensuring alignment with strategic goals If you possess... Previously used SQL monitoring and performance tools. Demonstrable experience installing, scaling and maintaining MSSQL in HA architecture Securing and safeguarding sensitive data and maintaining referential integrity Architecting backup and disaster recovery strategies Performance tuning T-SQL for high throughput environments Experience with Installation, Performance monitoring, Security, and high availability and backups. Experience working with development life cycle models including CI/CD pipelines. ‚Ä¶ then we are looking for you! We work 100% remotely or from our hub in Krak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,26000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,317,Data Analyst,Link Group,"Data Analyst Opis stanowiska: Poszukujemy analityka danych ze znajomo≈õciƒÖ ETL oraz zasad zarzƒÖdzania jako≈õciƒÖ danych. Kluczowe bƒôdzie do≈õwiadczenie w budowie przep≈Çyw√≥w danych (pipelines), zapewnianiu sp√≥jno≈õci danych oraz pracy z narzƒôdziem Dataiku. Zakres obowiƒÖzk√≥w: Projektowanie, rozw√≥j i utrzymanie proces√≥w ETL zgodnych z potrzebami raportowymi i biznesowymi Analiza danych kredytowych i identyfikacja problem√≥w jako≈õciowych Monitorowanie jako≈õci danych i wdra≈ºanie zasad governance Pisanie specyfikacji technicznych, przeprowadzanie test√≥w jednostkowych Wdra≈ºanie usprawnie≈Ñ w istniejƒÖcych przep≈Çywach danych Udzia≈Ç w spotkaniach projektowych i wsp√≥≈Çpraca z zespo≈Çami IT/biznesowymi Wymagania: Do≈õwiadczenie z narzƒôdziami ETL (Dataiku, Alteryx, Talend ‚Äì preferowane Dataiku) Bardzo dobra znajomo≈õƒá SQL i pracy z du≈ºymi zbiorami danych Umiejƒôtno≈õƒá pracy w ≈õrodowisku Agile Do≈õwiadczenie w tworzeniu specyfikacji technicznych i test√≥w jednostkowych Podstawowa znajomo≈õƒá Pythona Znajomo≈õƒá zasad zarzƒÖdzania jako≈õciƒÖ danych i kontroli wersji w Dataiku (GIT) Bieg≈Ço≈õƒá w jƒôzyku angielskim (w mowie i pi≈õmie) Otwarto≈õƒá na pracƒô w miƒôdzynarodowym ≈õrodowisku Mile widziane: Znajomo≈õƒá proces√≥w zwiƒÖzanych z kredytem korporacyjnym Do≈õwiadczenie w pracy nad projektami miƒôdzyregionalnymi Umiejƒôtno≈õƒá logicznego my≈õlenia i modelowania danych Technologie: Dataiku 13.2.0 SQL Developer 24.3 360 Suite Jira","[{""min"": 100, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,100,140,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,318,Middle Data Analyst,N-iX,"#3683 We are seeking afor aMiddle Data Analystin Polandwith strong expertise in Tableau and a passion for turning data into actionable insights. In this role, you will lead the development of advanced visual analytics, collaborate with stakeholders to understand business needs, and help shape data-driven decision-making across the organization. Responsibilities: Develop and maintain dashboards and reports using Tableau to support key business functions Translate complex business questions into analytical solutions Work closely with stakeholders to gather requirements, understand KPIs, and deliver meaningful insights Use SQL to extract, transform, and analyze data from various sources Present findings in a clear, concise, and impactful way to both technical and non-technical audiences Continuously improve reporting performance and usability through iteration and feedback Requirements: 4+ years of experience in analytics, business intelligence, or data visualization roles Python skills for data processing (at least 1 year of experience) Advanced Tableau skills (dashboard development, calculated fields, LOD expressions, performance optimization) Strong SQL proficiency for data querying and preparation Proven ability to derive insights from data and explain them effectively Solid understanding of data modeling, joins, and ETL principles Strong analytical thinking and attention to detail Experience working with cross-functional teams and translating business requirements into data deliverables Must-Have: Expert-level Tableau development experience (at least 2+ years of experience) Hands-on experience creating scalable, interactive dashboards for enterprise use Advanced SQL skills applied to analytical/reporting contexts Ability to work independently and proactively in a data-driven environment Nice-to-Have: Experience with additional BI tools (Power BI, Looker, etc.) Knowledge of modern data warehouses (Snowflake, BigQuery) Familiarity with dbt or Python for data manipulation Exposure to A/B testing, experimentation frameworks, or product analytics Understanding of data visualization best practices and UX principles We offer: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits","[{""min"": 12560, ""max"": 16254, ""type"": ""Net per month - B2B""}, {""min"": 9974, ""max"": 12929, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,12560,16254,Net per month - B2B
Full-time,Mid,Permanent or B2B,Hybrid,322,Data Analyst,Remodevs,"Please note - hybrid from Warsaw (3 days from the office, 2 remotely). About us: We help businesses use AI and digital tools to work better and grow faster, especially in private capital markets. Our Core Platform improves workflows and gives useful insights with AI. Olympus Software is a fast, smart cloud system that grows with your needs. The Pantheon Suite offers flexible tools to manage and improve business performance. With over 10 years of experience, we know how to turn technology into real business value. About the Role We are seeking a Data Analyst to join our PaaS (Platform-as-a-Service) Customer Delivery team in Warsaw. In this role, you will be responsible for transforming data into actionable insights to support decision-making across product, operations, and strategy for customer-facing platform implementations. You will work closely with stakeholders across implementation projects to develop dashboards, conduct ad-hoc analyses, and ensure the integrity of platform usage and performance metrics. This role is ideal for someone who is curious, business-minded, and eager to make an impact through data. Key Responsibilities Partner with cross-functional teams to define key metrics and build dashboards and reports that provide visibility into business performance. Conduct deep-dive analyses to answer business questions, uncover trends, and identify opportunities for growth and optimization. Design and maintain scalable data models and SQL queries to support reporting and analytical needs. Collaborate with data engineers to ensure data availability, quality, and consistency across systems. Communicate findings and recommendations clearly to technical and non-technical audiences. Develop documentation and contribute to data literacy across the organization. Qualifications Required 2‚Äì4 years of experience in a data analyst or business intelligence role. Strong SQL skills and experience working with large datasets in a cloud data warehouse environment. Proficiency with BI tools such as Looker, Tableau, Power BI, or similar. Strong analytical thinking and attention to detail. Excellent communication and data storytelling skills. Preferred Experience working with dbt or similar modeling tools. Familiarity with A/B testing design and analysis. Some experience with Python, R, or another scripting language for data analysis. Exposure to product analytics platforms (e.g., Mixpanel, Amplitude).","[{""min"": 16623, ""max"": 18470, ""type"": ""Net per month - B2B""}, {""min"": 16623, ""max"": 18470, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,16623,18470,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,324,Data Engineer,Aristocrat Interactive,"Join Aristocrat in Bringing Joy to Life Through the Power of Play. Be part of our growing global team where people come first because they fuel our success. Here, it‚Äôs All About the Player and we create a world of its own for everyone, everywhere with premium casino and world-class digital and mobile products. Our value of Good Business, Good Citizen ensures that corporate growth and responsible gameplay go hand in hand to help our industry remain sustainable. Aristocrat offers a highly diverse, inclusive, and equitable culture as well as the professional tools and resources to ensure your Talent is Unleashed. We achieve success through Collective Brilliance. Individually, we are great, but together, we are unstoppable. Aristocrat enhances the player experience‚Äîand careers‚Äîwith opportunities featuring meaningful challenges, strong advancement potential, and global exposure. Explore a Career with Our Team: Aristocrat Interactive We're looking for aData Engineerto help us build batch data pipelines, insightful dashboards, and data products that serve new gaming customers and internal teams. What You will Do Design and build scalable, maintainable data pipelines using Airflow and Snowflake Create data models and workflows that power dashboards, reports, and analytical products Collaborate with analysts, product teams, and game developers to deliver valuable insights Ensure data quality, documentation, and reliability across our data products Help shape the future of how data supports player engagement and product development What We're Looking For Hands-on experience with Airflow, Snowflake, and cloud data platforms (AWS/GCP) Strong SQL and Python skills for building and managing data workflows A knack for turning raw data into usable, impactful insights and visualizations Passion for games, analytics, and building cool stuff with data Very Independent and self-reliant What We offer High-level compensation on an employment contract and regular performance based salary and career development reviews; Medical insurance (health), employee assistance program; Multisport Card; English classes with native speakers, trainings, conferences participation; Referral program; Team buildings, corporate events. Why Aristocrat? Aristocrat is a world leader in gaming content and technology, and a top-tier publisher of free-to-play mobile games. Aristocrat has three operating business units, spanning regulated land-based gaming (Aristocrat Gaming), social casino (Product Madness), and regulated online real-money gaming (Aristocrat Interactive). Our team of over 7,300 employees worldwide is united by our company‚Äôs mission to bring joy to life through the power of play. We deliver great performance for our B2B customers and bring joy to the lives of the millions of people who love to play our casino and mobile games. And while we focus on fun, we never forget our responsibilities. We strive to lead the way in responsible gameplay, and to lift the bar in company governance, employee wellbeing and sustainability. We‚Äôre a diverse business united by shared values and an inspiring mission to bring joy to life through the power of play. Aristocrat is proud to be an equal opportunity employer. We celebrate diversity and do not discriminate based on gender, race, religion, color, national origin, sex, sexual orientation, age, veteran status, disability status, or any other applicable characteristics protected by law. Diversity and Inclusion are integral to our values of Talent Unleashed, Collective Brilliance, Good Business, Good Citizen, and It‚Äôs All About the Player.","[{""min"": 29553, ""max"": 33247, ""type"": ""Gross per month - Permanent""}]",Data Engineering,29553,33247,Gross per month - Permanent
Full-time,Mid,B2B,Hybrid,326,Database Developer,Connectis,"Wsp√≥lnie z naszym Partnerem Biznesowym zbran≈ºy ubezpieczeniowej,poszukujemy osoby na stanowiskoDatabase Developer, kt√≥ra do≈ÇƒÖczy do Zespo≈Çu Rozwoju Narzƒôdzi HD/BI i bƒôdzie wspieraƒá rozw√≥j technologiczny jednej z najwiƒôkszych Hurtowni Danych w Polsce. üí° TWOJA ROLA: Budowa API integrujƒÖcego r√≥≈ºne technologie wykorzystywane w HD (SAS, Oracle, Office365, REST API itd.). Tworzenie narzƒôdzi wspierajƒÖcych pracƒô developer√≥w Hurtowni Danych (SAS / SAS Viya / Oracle / inne). Prowadzenie Proof of Concept (POC) dla nowych narzƒôdzi (np. SAS Viya & Exadata w chmurze Azure). Integracja i wymiana danych pomiƒôdzy systemami PZU z u≈ºyciemm.in. Kafki. Tworzenie narzƒôdzi monitorujƒÖcych aktywno≈õƒá u≈ºytkownik√≥w Hurtowni Danych. Udzia≈Ç w dzia≈Çaniach optymalizacyjnych dla przetwarza≈Ñ HD ‚Äì analiza i tuning. Budowa technicznych data mart√≥w i raport√≥w BI (np. w SAS Viya, Power BI). Wsparcie projekt√≥w strategicznych w zakresie architektury i integracji danych. üîç CZEGO OCZEKUJEMY OD CIEBIE? Do≈õwiadczenie w projektowaniu rozwiƒÖza≈ÑHurtowni Danych i Business Intelligence. Praktyczna znajomo≈õƒá narzƒôdziSASiSAS Viya (DI Studio, EG, SAS Studio). Do≈õwiadczenie z bazamiOracle oraz PL/SQL(mile widziane: Oracle Exadata). Znajomo≈õƒá narzƒôdzi i proces√≥wCI/CD (Git, Bitbucket, Bamboo, JIRA). Gotowo≈õƒá dopracy hybrydowej‚Äì czƒô≈õƒá pracy w biurze w Warszawie. ‚ú® OFERUJEMY: Mo≈ºliwo≈õƒá uczestnictwa w spotkaniach integracyjnych oraz meetupach technologicznych, gdzie bƒôdziesz m√≥g≈Ç/mog≈Ça dzieliƒá siƒô wiedzƒÖ i do≈õwiadczeniem. Wsparcie dedykowanego opiekuna Connectis, kt√≥ry zawsze jest dostƒôpny, by pom√≥c Ci w sprawach zwiƒÖzanych z projektem. Pracƒô w modelu hybrydowym z biura w Warszawie (4 dni zdalnie, 1 dzie≈Ñ z biura / tyg.). Biuro w Warszawie przy wej≈õciu do stacji metra‚Äì ≈õwietna lokalizacja i ≈Çatwy dojazd. Szybki, zdalny proces (rozmowa HR + rozmowa techniczna = decyzja). 5000 PLN za polecenie znajomego do naszych projekt√≥w. Sprzƒôt do pracy. Dziƒôkujemy za wszystkie zg≈Çoszenia. Pragniemy poinformowaƒá, ≈ºe skontaktujemy siƒô z wybranymi osobami. 12143/OM","[{""min"": 134, ""max"": 158, ""type"": ""Net per hour - B2B""}]",Data Engineering,134,158,Net per hour - B2B
Full-time,Senior,B2B,Remote,327,Greenplum Database Administrator,Calimala.ai,"Calimala.aiis seeking a seasonedGreenplum Database Administratorto join our dynamic team. With a focus on ensuring optimal performance, security, and resilience of our Greenplum clusters, you will take charge of database maintenance, backup & recovery, monitoring, and performance tuning. This role is perfect for a professional who is passionate about leveraging modern technologies in a fast-paced, innovative environment. Responsibilities: In this role, you will: Manage and perform daily administration tasks for Greenplum clusters while ensuring optimal system performance and data integrity. Conduct routine health checks, cleanup, vacuuming, and reindexing operations to maintain a robust database environment. Oversee backup and restore processes along with disaster recovery setups, ensuring data safety and business continuity. Monitor system performance and tune queries to achieve high responsiveness and efficiency. Collaborate with development teams to support schema changes, manage access controls, and enhance overall system functionality. Requirements: Applicants must have a minimum of 5 years of hands-on experience in managing and maintaining enterprise database systems, including proficiency in Greenplum DB and PostgreSQL. A deep understanding of Linux environments, SQL scripting, and performance tuning is essential, along with a proven track record in backup & recovery and disaster recovery implementations. Benefits: We offer a competitive compensation package, flexible work arrangements, and continuous learning opportunities in a collaborative, innovative work culture. Join us atCalimala.aiand contribute to cutting-edge projects in a company that values your expertise and drive for excellence. WhyCalimala.ai? Be part of a team that pushes the boundaries of technology and innovation. AtCalimala.ai, your expertise will be valued, and your professional growth nurtured. We are committed to supporting our staff and providing a stimulating work environment where you can thrive.","[{""min"": 15000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Database Administration,15000,25000,Net per month - B2B
Full-time,Mid,B2B,Remote,328,Data Integrations Engineer,dotLinkers,"Position: Data Integrations Engineer Working model: Fully remote Employment type: B2B Salary: 6 000 ‚Äì 9 000 USD B2B/month We‚Äôre working with a fast-growing US tech company revolutionizing how organizations manage commissions, integrations, and operational data. As part of their Professional Services team, you‚Äôll help bridge their product with third-party systems like Workday, Salesforce, and BigQuery‚Äîthrough both hands-on scripting and strategic integrations work. This is a backend-leaning role with some customer interaction, best suited for someone who‚Äôs solid with SQL, APIs, and light Python/JavaScript. If you‚Äôre interested in delivering real business impact ‚Äì this one‚Äôs for you. Responsibilities: Build and maintain managed integrations between third-party platforms and the client‚Äôs system (e.g. Workday, Sage Intacct, MS Dynamics) Use tools likeWorkatoandSnowflaketo ingest and process customer data Handle support tickets related to integrations and act as the SME for integration architecture Work directly with enterprise clients and help design scalable data connection flows Support implementations and unblock client-facing solutions when product limitations arise Requirements: Strong command ofSQLandREST APIs(including JSON) Working knowledge ofPythonandJavaScript(for light scripting) Familiarity with CRM/HRIS systems likeSalesforce,Workday,Rippling, orMS Dynamics Comfort speaking with customers‚Äîideally 1‚Äì2 years of client-facing or cross-functional experience Nice to have: experience withSSO/SCIM,ETL tools, orintegration platforms(e.g. Workato) What‚Äôs in it for you: Fully remote work Work directly with a senior Director in a flat, agile team structure Be part of a well-funded scale-up with IPO ambitions and an internal culture of excellence Working hours: Must have overlap withUS Eastern Time (EST)for at least4‚Äì5 days/week Full overlap expected for first 3 months (onboarding period) Flexibility increases post-ramp-up No formal on-call, but some flexibility is appreciated during launches","[{""min"": 22164, ""max"": 33247, ""type"": ""Net per month - B2B""}]",Data Engineering,22164,33247,Net per month - B2B
Full-time,Senior,B2B,Office,332,Oracle Database Administrator,Onwelo Sp. z o.o.,"Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, powiƒôkszy≈Ça zesp√≥≈Ç do 700 os√≥b, a tak≈ºe otworzy≈Ça biura w siedmiu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. O projekcie: Jeste≈õ ekspertem w zakresie baz danych Oracle? Do≈ÇƒÖcz do nas! Dla naszego szwajcarskiego klienta z bran≈ºy bankowej poszukujemy Database Administratora , kt√≥ry bƒôdzie odpowiedzialny za zarzƒÖdzanie, administracjƒô oraz optymalizacjƒô baz danych Oracle. Je≈õli jeste≈õ otwarty/-a na wyjazd i pracƒô w Szwajcarii w siedzibie klienta , zostaw nam swoje CV! Z nami bƒôdziesz: ZarzƒÖdzaƒá i administrowaƒá bazami danych Oracle Monitorowaƒá wydajno≈õƒá i optymalizowaƒá bazy danych Implementowaƒá i zarzƒÖdzaƒá procedurami bezpiecze≈Ñstwa RozwiƒÖzywaƒá problemy i zapewniaƒá wsparcie techniczne Tworzyƒá dokumentacjƒô technicznƒÖ oraz utrzymywaƒá kopie zapasowe Aktualizowaƒá i migrowaƒá bazy danych do nowszych wersji Oracle Przeprowadzaƒá migracjƒô do chmury Oracle (OCI) Czekamy na Ciebie, je≈õli: Masz minimum 5 lat do≈õwiadczenia jako Oracle DBA Bardzo dobrze znasz SQL oraz PL/SQL Masz do≈õwiadczenie w administracji Oracle RAC, Data Guard, ASM Znasz narzƒôdzia do monitorowania i optymalizacji (np. Oracle Enterprise Manager ) Masz wiedzƒô z zakresu system√≥w Unix/Linux Masz znajomo≈õƒá skryptowania w pow≈Çoce shell Znasz Infrastructure as Code (Ansible, Terraform, git) Masz do≈õwiadczenie z CI/CD P≈Çynnie pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie min. B2 Jeste≈õ gotowy/wa do relokacji do Szwajcarii na czas kontraktu Dodatkowym atutem bƒôdzie: Do≈õwiadczenie jako MSSQL DBA Znajomo≈õƒá PostgreSQL DBA Do≈õwiadczenie z Exadata i RAC Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Mo≈ºliwo≈õƒá pracy zdalnej Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºyci","[{""min"": 250, ""max"": 300, ""type"": ""Net per hour - B2B""}]",Database Administration,250,300,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,334,Senior Data Analyst,Jit Team,"Salary: 900 - 1100 PLN/day net+vat on B2B Work model: hybrid from Gdynia / Gda≈Ñsk / Warszawa / ≈Å√≥d≈∫ (2 days per week from the office) Why choose this offer? You can expect a flexible work organization The international work environment will give you the opportunity to interact with the English language on a daily basis Scandinavian organizational culture will provide you with work-life balance, you will gain time for additional training (financed by Jit) The Jit community will bring you a nice time during regular integration meetings Project We are looking for a data-driven professional to join a strategic KYC transformation initiative focused on improving data quality, governance, and regulatory compliance across the organization. This is a cross-functional role collaborating with business, IT, and architecture teams to implement a robust data model and reporting capabilities supporting a new Group-wide KYC system. Responsibilities you'll have Analyse existing KYC data to identify gaps, inconsistencies, and areas for improvement Work collaboratively with business and IT partners across business units to develop and implement data standards to ensure data accuracy & completeness Be a part of major projects for delivery of Group KYC Tool Partner with Data Architects and business SMEs to create and maintain a Business Information Model (BIM) for each data domain that is aligned across the enterprise Monitor data quality metrics and report on findings to management Investigate and resolve data quality issues, escalating complex problems as needed Provide consultative services to agile delivery teams and ensure data governance best practices Support the implementation of new KYC systems and processes, ensuring data migration and integration are seamless and accurate Utilize enterprise approved tools to develop reproducible data-flows that ultimately fulfill reporting requirements Implement testing and monitor quality of developed data-flows and reporting outputs, performing upgrades as identified and maintenance as required Work with multiple customers to analyze and visualize complex data in simple and accurate ways Document data definitions, data lineage, and data quality rules Participate in data governance/stewardship communities of practice Contribute to the development and maintenance of data governance policies and procedures Stay up-to-date with industry best practices and regulatory requirements related to KYC data management Proactively identify opportunities to improve data quality and efficiency Expected competences and knowledge Proven experience in data management, data quality, or a related role, preferably within the financial services industry Experience with advanced SQL queries and familiarity with another programming language, e.g. Python, R, Java and Scala Familiar with data taxonomy and capable to build small data models Experience with Power BI to produce reports Excellent stakeholder management is mandatory as you will be working closely with multiple business areas and key business leads across IT, Architecture, Financial Crime, Compliance, Legal - and the Business in order to implement changes Experience within financial crime (AML/CTF, KYC, Transaction Monitoring and Sanctions), working agile and experience with SAFe is an advantage Professional, organised, and structured in approach and work produced to a high quality of standard, and you always strive to find the solution that is best for the bank Excellent analytical and problem-solving skills, strong attention to detail and a commitment to data accuracy Ability to communicate effectively with both technical and non-technical audiences Experience with data governance frameworks and methodologies is a plus Experience with data migration and integration projects is a plus Ability to work independently and as part of a team Fluency in English is a requirement (speaking and writing) Technologies you'll work with SQL Python Power BI Client ‚Äì why choose this particular client from the Jit portfolio? Jit Team has had an over-decade-long relationship with the leading financial group in the Nordic countries, and we are privileged to be our client's premier partner in Poland. At present, over 200 Jit personnel are engaged in the completion of more than 60 projects for this Norwegian major provider of financial services with a global presence and a strong focus on modern technology. Our customer's work atmosphere is epitomized by the Scandinavian culture , which is conducive to people who place emphasis on work-life balance and feedback culture . Furthermore, all projects are executed in international teams, giving constant exposure to the English language. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 18900, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,18900,23100,Net per month - B2B
Full-time,Senior,Permanent,Remote,335,Senior Azure Data Engineer,Link Group,"Data Engineer ‚Äì Azure üìç 100% remote | üïí Full-time | üåç International environment We are looking for an experienced Data Engineer with strong expertise in the Azure ecosystem to join a dynamic data team delivering scalable and high-performance data solutions. You‚Äôll play a key role in designing, building, and optimizing modern data pipelines and data lake architectures using cutting-edge cloud technologies. üîß Key Responsibilities: Design and develop robust and efficient data pipelines using Azure Databricks, Spark, and PySpark Work with Delta Lake architecture to manage structured and semi-structured data Perform data modeling, transformation, and performance tuning for large datasets Build and manage Azure Data Factory pipelines and Azure Functions for orchestrating workflows Integrate various data formats such as Parquet, Avro, and JSON Collaborate with cross-functional teams to understand data requirements and deliver optimal solutions Use Git for version control and manage code in a collaborative environment Write efficient Python and SQL code for data processing and querying Ensure data quality, consistency, and reliability across the platform ‚úÖ Core Requirements: Solid hands-on experience in Azure Databricks, Spark, and PySpark Deep knowledge of Delta Lake and modern data lakehouse architectures Proficiency in data modeling and performance optimization techniques Experience with ADF (Azure Data Factory) and Azure Functions Strong skills in Python, SQL, and data serialization formats (Parquet, Avro, JSON) Familiarity with version control systems, especially Git Ability to work independently in a fully remote, distributed team Good communication skills in English","[{""min"": 18000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,25000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,337,Data Analyst with Tableau,Altimetrik Poland,"2-3 day per week you need to be available until 9: 00 PM for meetings with the US team. Altimetrik Polandis a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are seeking a skilledData Analyst with TableauforAirbnb-our customer, an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. Together with them, we are building a world-class payments platform that moves billions of dollars, in 191 countries, with 75 currencies, through a complex ecosystem of payments partners. They are also reinventing how to serve users to improve performance, scalability and extensibility. Responsibilities: Lead data visualization strategy for Policy, maintaining existing dashboards and developing new self-service resources. Optimize for bringing relevant data to the Policy team and bringing data created by the Policy team to cross-functional partners. Build and manage the primary Policy metrics and insights dataset and visualizations. Aggregate critical metrics across lobbying efforts, grassroots advocacy, partnerships, social and advertising campaigns, news articles, research, and regulatory compliance efforts. Architect this data to be flexibly leveraged at many geographic levels, including government administrative boundaries, custom shapes, and business geographic definitions. Partner closely with the Economics and Research Data Science team to ensure the Policy team is consuming all available data and that it meets the highest quality standards. Elevate the legitimacy of Policy Comms data assets to pass our rigorous internal data certification process. . Leverage Minerva, Airbnb‚Äôs in house metrics management tool, to make our data accessible to other data practitioners. Develop experimentation and attribution framework for political advocacy and regulatory compliance marketing initiatives. Support ad hoc analytics needs to support Policy, including supply composition, economic impact, growth and regulatory compliance, etc. Build relationships with local and regional Policy Comms teams around the world to support their local data and analytics needs. Partner with the Comms Data Strategy Taskforce. And if you possess.. 7+ years experience in business intelligence, data analytics, or data science. Expert in SQL and Python languages. Expertise in building data visualization, preferably in Tableau. Experience working with complex geographic data. Experience with knowledge graphs is a plus. Talent for breaking down complex technical concepts into common language and acting as a bridge between technical and departmental stakeholders. Experience working with complex and big data systems across a multitude of relationships and metrics. Ability to apply a creative and nuanced perspective to look beyond common data indicators in order to meet business goals. Expertise designing and running marketing experiments. Ability to self-serve and take the initiative to find answers to technical questions. Experience building and implementing machine learning models is a plus. Experience in survey population sampling and survey response analysis is a plus. Expertise in R, Java, REACT is a plus. ‚Ä¶ then we are looking for you! We work 100% remotely or from our hub inKrak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 20000, ""max"": 24300, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20000,24300,Net per month - B2B
Full-time,Mid,B2B,Remote,340,Data Engineer (Azure),SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. bran≈ºy automotive. Wykorzystywany stos technologiczny: Azure, Databricks, PySpark, Azure Data Factory, Azure Synapse, Power BI, Spark, Python, MongoDB, Azure Functions, SQL Server, rozw√≥j nowych funkcjonalno≈õci oraz utrzymanie aplikacji, wsparcie u≈ºytkownik√≥w wewnƒôtrznych oraz proces√≥w biznesowych poprzez rozwiƒÖzywanie problem√≥w zwiƒÖzanych z dzia≈Çaniem system√≥w i jako≈õciƒÖ danych, zapewnienie wydajnego i niezawodnego dzia≈Çania system√≥w, stawka do 140z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz minimum 3 lata do≈õwiadczenia w pracy z danymi i in≈ºynieriƒÖ oprogramowania, posiadasz tytu≈Ç magistra z informatyki, in≈ºynierii danych lub pokrewnej dziedziny, pracujesz z us≈Çugami Azure, takimi jak Azure Data Factory, Azure Synapse, Azure Functions, masz praktyczne do≈õwiadczenie z Databricks i PySpark, bardzo dobrze znasz bazy danych (np. SQL Server, Netezza, MongoDB), znasz narzƒôdzia i technologie: Power BI, Spark, Python, biegle komunikujesz siƒô w jƒôzyku angielskim (min. B2). Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 21840, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Engineering,21840,23520,Net per month - B2B
Full-time,Senior,B2B,Hybrid,341,Data Engineer,Antal Sp. z o.o.,"Join Us as a Data Engineer Location: Krak√≥w, Poland (Hybrid ‚Äì 5 days/month in office) Industry: FinTech / Data / Cloud / AI Are you passionate about working with impactful data in a modern tech stack? Want to help build solutions that directly protect millions of people and institutions from financial crime? This is your opportunity. We're part of Risk & Compliance Technology , a global team designing and delivering innovative solutions to protect the bank and its customers from financial crime, sanctions risk, identity threats, unauthorized trading, and regulatory breaches. Our mission: empower global growth through intelligent, data-driven risk management. Right now, we‚Äôre building a secure, scalable platform powered by Generative AI to automatically generate concise, accurate insights‚Äîhelping investigators assess risk faster, spot suspicious patterns, and act quickly. You‚Äôll be at the heart of this transformation. Who We‚Äôre Looking For We‚Äôre seeking a Data Engineer to join our FinCrime IT team , working across regions and disciplines. You‚Äôll collaborate with developers, architects, and business stakeholders to shape a solution that makes a difference. This is a hands-on role in a forward-thinking, agile environment. We value curiosity, ownership, and a problem-solving mindset. What You‚Äôll Do Design, build, and maintain components of a cutting-edge investigation support platform Design, build, and maintain components of a cutting-edge investigation support platform Take part in the entire development lifecycle: design, implementation, testing, and deployment Take part in the entire development lifecycle: design, implementation, testing, and deployment Work closely with cross-functional and international teams‚Äîincluding architecture, security, and compliance Work closely with cross-functional and international teams‚Äîincluding architecture, security, and compliance Build scalable, cloud-native processes that empower intelligent decision-making Build scalable, cloud-native processes that empower intelligent decision-making What You Bring Experience: Proven experience with data analysis and data engineering (Python, SQL) Proven experience with data analysis and data engineering (Python, SQL) End-to-end development lifecycle experience in large-scale IT projects End-to-end development lifecycle experience in large-scale IT projects Background in financial services or compliance-related technology (preferred) Background in financial services or compliance-related technology (preferred) Experience building and deploying solutions in a cloud environment (ideally Google Cloud) Experience building and deploying solutions in a cloud environment (ideally Google Cloud) Technical Skills: Languages & Tools : Python, SQL, Bash Languages & Tools : Python, SQL, Bash Cloud : Google Cloud Platform (BigQuery, Dataproc, Airflow) Cloud : Google Cloud Platform (BigQuery, Dataproc, Airflow) Containers & Infra : Docker, Kubernetes, GKE, Cloud Run Containers & Infra : Docker, Kubernetes, GKE, Cloud Run Security : IAM, roles, service accounts, secure development practices Security : IAM, roles, service accounts, secure development practices DevOps : Terraform, Jenkins, Ansible, Nexus DevOps : Terraform, Jenkins, Ansible, Nexus OS : Linux / Unix OS : Linux / Unix Agile methodology knowledge (Scrum, Jira, Confluence) Agile methodology knowledge (Scrum, Jira, Confluence) Soft Skills: Strong communication skills, with the ability to explain complex ideas clearly Strong communication skills, with the ability to explain complex ideas clearly Collaborative spirit and openness to working across time zones Collaborative spirit and openness to working across time zones Eagerness to learn and adapt in a fast-changing environment Eagerness to learn and adapt in a fast-changing environment Accountability and drive for quality Accountability and drive for quality Why Join Us? Make a tangible impact by fighting financial crime at a global scale Make a tangible impact by fighting financial crime at a global scale Work with modern tech: AI, GCP, containers, and automation tools Work with modern tech: AI, GCP, containers, and automation tools Collaborate with a global team of experienced professionals Collaborate with a global team of experienced professionals Be part of a purpose-driven organization that invests in innovation and ethical tech Be part of a purpose-driven organization that invests in innovation and ethical tech Ready to Apply? Help us protect the financial world through smarter technology. Apply now and be part of something meaningful. To learn more about Antal, please visit www.antal.pl","[{""min"": 170, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,220,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,344,Senior Power Platform Developer,Onwelo Sp. z o.o.,"Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, powiƒôkszy≈Ça zesp√≥≈Ç do kilkaset os√≥b, a tak≈ºe posiada biura w sze≈õciu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. O projekcie: Projekt dotyczy rozwoju i implementacji rozwiƒÖza≈Ñ opartych na Microsoft Power Platform oraz us≈Çugach Azure. Celem jest stworzenie zintegrowanego ≈õrodowiska wspierajƒÖcego procesy wewnƒôtrzne klienta, z naciskiem na automatyzacjƒô, raportowanie i poprawƒô efektywno≈õci operacyjnej. Z nami bƒôdziesz: Tworzyƒá rozwiƒÖzania w Power Platform Pracowaƒá w ≈õrodowisku Azure Projektowaƒá architekturƒô i szacowaƒá czas realizacji Wsp√≥≈Çpracowaƒá z biznesem Doradzaƒá technologicznie i wspieraƒá rozw√≥j zespo≈Çu Czekamy na Ciebie, je≈õli: Masz min. 3 lata do≈õwiadczenia w pracy z Power Platform Znasz Power BI, Power Apps, Power Automate, Copilot Umiesz analizowaƒá wymagania biznesowe Czujesz odpowiedzialno≈õƒá za realizowane zadania Mile widziane: JavaScript/TypeScript, C#, React) Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Potrzebujesz pracowaƒá hybrydowo? Mo≈ºemy zaproponowaƒá 4 dni pracy zdalnej i 1 dzie≈Ñ w tygodniu pracy z biura w Warszawie Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 800, ""max"": 1100, ""type"": ""Net per day - B2B""}]",Data Engineering,800,1100,Net per day - B2B
Full-time,Senior,B2B,Remote,345,Implementation Developer (more technical background),Ligo Headhunters,"Implementation Specialist ‚Äì Remote | B2B | 25,000‚Äì28,000 PLN net/month I am currently recruiting for anImplementation Specialistposition for a US company.We are looking for someone withquick availability‚Äì a 3-month notice period is out of the question.We‚Äôre looking for someonemore technical than analytical. Location: Fully Remote Salary: 25,000‚Äì28,000 PLN net/month (B2B)We are flexible with salary for the right candidate Form of employment: B2B (Note: Since the client is based in the US, the0% VAT rateapplies under reverse charge rules.) Recruitment process: Two interviews via Zoom Vacation: 21 paid vacation days Company type: US Startup Blaze is building ano-code app building platform.We are looking for a person who will work directly with our customers to gather requirements and build apps for them using our platform. This roledoesn't involve writing any code, but requires the ability tothink like a developerand understand basic principles of software design. The role also involves customer interaction, soEnglish communication skills are essential. The successful candidates come from various backgrounds ‚Äî typically: Ajunior developer Or afreelancerused to interacting with customers You don‚Äôt need to be a strong developer in any specific technology or have years of experience.Youmust be comfortable working with pre-made componentsand focusing onhigh-level app building‚Äî not coding. ExcellentEnglish communicationskills Goodeye for design Knowledge ofSQLand principles ofrelational databases Familiarity withdatabase schema design Fullyremote position Flexible working hours Very competitive salary Work in aninternational team Use ofinnovative tools and technologies We‚Äôre flexible, but: We expectfull-time commitment(no other daytime jobs or demanding side gigs).If there‚Äôs a deadline ‚Äì we count on your extra effort. You should be able to: Schedule meetings with customers duringtheir working hours Attend team meetings with the US (usually around5‚Äì6 PM Polish time) Examples of working schedules within the team: A few hours in the morning + a few in the afternoon/evening Standard Polish working hours + short evening overlap Late afternoon start with evening/late-night work (for max US overlap) We‚Äôreflexible‚Äî as long as the work gets done. Seems like a place you‚Äôd like to be a part of?Our team is waiting for you! üì© Send yourCVandfinancial expectations, notice period.","[{""min"": 25000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,25000,28000,Net per month - B2B
Full-time,Mid,B2B,Remote,349,Data Engineer (MS Azure) - Future Opportunities,Avanade Poland,"This is ""future opportunities"" ad - we will begin recruitment process after summer (September). You can expect to hear from us at that time. The role 100% remote so it is available for Candidates from all over the Poland. Do you love making sure that information is available and consumable? So do we. Are you passionate about transforming raw data into actionable insights? We're seeking a Data Engineer with expertise in the Microsoft Azure Data realm, including Delta Lake, Databricks, and ETL processes. Key Responsibilities: Design, build, and maintain scalable, efficient, and reliable data pipelines using Databricks and Azure services (e.g., Azure Data Lake, Azure SQL Database, Azure Databricks, Azure Data Factory). Lead the end-to-end development of data architectures and ETL processes to support data-driven decision-making across the organization. Collaborate with data scientists, analysts, and other engineering teams to ensure data integration and optimization. Implement data governance, data quality, and data security best practices across data systems. Mentor and provide guidance to junior data engineers, fostering a culture of continuous learning and improvement. Optimize performance of data workflows, troubleshoot issues, and ensure the reliability of data solutions. Stay up-to-date with the latest developments in cloud technologies, data engineering best practices, and emerging tools and frameworks Required Skills and Qualifications: Extensive experience with Databricks and Azure services (e.g., Azure Data Lake, Azure SQL Database, Azure Databricks, Azure Data Factory). Strong expertise in SQL, Python and other data engineering programming languages. Proven experience in building and managing complex data pipelines and distributed data architectures. Solid understanding of data engineering concepts, such as ETL, data modeling, data warehousing, and data governance. Ability to optimize large-scale data workflows and troubleshoot complex data issues. Strong leadership and communication skills, with the ability to guide teams, collaborate with cross-functional stakeholders, and communicate technical concepts to non-technical audiences. Proven track record of successfully delivering large-scale data projects and systems. Preferred Skills: Experience with other Azure tools such as Azure Microsoft Fabric, Azure Synapse Analytics, Azure IoT Hub, Azure HDInsight, Kafka and Azure Stream Analytics. Experience with containerization (Docker/Kubernetes) and orchestration tools (e.g., Apache Airflow). Familiarity with DevOps principles and CI/CD pipelines for data engineering workflows. Experience with data warehouses and real-time data processing frameworks. Experience with LLMs and orchestration frameworks (e.g., LangChain, Semantic Kernel) is a plus. Exposure to machine learning concepts and frameworks (e.g., MLflow, TensorFlow) is a plus. We offer: Remote/hybrid work ‚Äì you choose! Access to Microsoft certifications, trainings and e-learning platforms (Pluralsight, Rosetta Stone) Career/development path ‚Äì every employee has his/her Mentor Chill rooms at the offices: table football, game console, board games Free parking spot at the offices We participate in charity programs: Szlachetna Paczka, Business Run Sports section on the Strava: Poland Club | Avanade Poland Running on Strava Company events (Team outings, Meetups) - check our Instagram!Avanade Poland These are just a few of the Avanade benefits, let's talk about the others!","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,110,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,351,LIMS Expert,Sii,"Minimum of 4 years of relevant experience in a regulated environment Hands-on laboratory experience and familiarity with analytical techniques (e.g., HPLC) Knowledge of GxP requirements and global regulatory standards Proficiency with LIMS (LabVantage experience preferred), eQMS, and document management systems Excellent communication and customer service skills, with the ability to translate technical information for diverse audiences Fluency in English (spoken and written) We are looking for a skilled, detail-oriented, and collaborative LIMS Expert to join our team in the medical and pharmaceutical industry, supporting global users of the Laboratory Information Management System. In this role, you will be part of a dynamic group committed to delivering both rapid and long-term solutions to enhance the laboratory user experience and ensure alignment with strategic goals. The ideal candidate will bring strong technical expertise, customer service skills, and a proactive attitude to a highly regulated environment. Support creation and updates of LIMS Master Data (e.g., specifications, sampling plans, LES worksheets) Troubleshoot and resolve user support tickets within the LIMS environment Communicate with QC departments to collect and verify data for LIMS integration Create and manage compliance documentation, including routing for approval Participate in training users and supporting routine QC operations Recommend process improvements and contribute to best practice development Rekrutacja online Jƒôzyk rekrutacji: polski&angielski Start ASAP Praca w pe≈Çni zdalna Darmowa kawa Darmowe ≈õniadanie Bez wymaganego dress code'u Nowoczesne biuro Szkolenia wewnƒôtrzne Pakiet sportowy Miƒôdzynarodowe projekty Prywatna opieka medyczna Bud≈ºet na szkolenia Ma≈Çe zespo≈Çy Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title ‚Äì get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers ‚Äì Power People. Learn more atsii.pl.","[{""min"": 22000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,22000,28000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,353,Senior DBA Consultant,Lumicode Sp. z o.o. (Pentacomp Group),"Kim jeste≈õmy Lumicode Sp. z o.o. nale≈ºy do Grupy Pentacomp, kt√≥ra jest producentem rozwiƒÖza≈Ñ informatycznych i dostawcƒÖ profesjonalnych us≈Çug IT dla du≈ºych przedsiƒôbiorstw i sektora publicznego. JakoPentacomptworzymy rozwiƒÖzania IT ≈ÇƒÖczƒÖce innowacyjno≈õƒá z latami do≈õwiadcze≈Ñ - a ich mamy ca≈Çkiem sporo. Istniejemy na rynku prawie 30 lat i mo≈ºemy pochwaliƒá siƒô wieloma zrealizowanymi projektami Aktualnie do projektunaszego Klienta poszukujemy Senior DBA Consultanta. Oferujemy: Praca hybrydowa (Warszawa, 2 razy w tygodniu w biurze) Stawka do190 pln/h netto + VAT B2B; Dofinansowanie do karty sportowej oraz mo≈ºliwo≈õƒá skorzystania z prywatnej opieki zdrowotnej; 10+ lat do≈õwiadczenia w zarzƒÖdzaniu bazƒÖ danych Oracle w wersji od 12c do 19c (RAC, Data guard) Do≈õwiadczenie z systemem operacyjnym Linux, Narzƒôdzia do zarzƒÖdzania Oracle (Data Guard, RMAN, Data pump), Znajomo≈õƒá zasad projektowania architektury, Obs≈Çuga r√≥≈ºnych typ√≥w problem√≥w Data Guard, Praktyczne do≈õwiadczenie ORACLE w produkcyjnym ≈õrodowisku RAC (ASM) (19c), Dobra znajomo≈õƒá Oracle Golden gate - jasne zrozumienie przep≈Çywu procesu, bazy danych Downstream, OGG start stop, Dobra znajomo≈õƒá Storage, Dobra znajomo≈õƒá rozwiƒÖzywania problem√≥w FSFO Dobra znajomo≈õƒá procesu - tj. zarzƒÖdzanie zmianƒÖ, zarzƒÖdzanie incydentami, Wiedza na temat obs≈Çugi Oracle SR, Wiedza na temat architektury OEM i zarzƒÖdzania macierzami OEM. Przygotowanie ≈õrodowiska prePROD w celu w≈ÇƒÖczenia FSFO Synchronizacja DataGuard w przypadku jakichkolwiek problem√≥w / b≈Çƒôd√≥w w DG, Przebudowa Standby zgodnie z potrzebami, Migracja starego PDB do nowego kontenera Aktywacja FSFO RozwiƒÖzywanie problem√≥w z FSFO Wykonywanie test√≥w na prePROD: Switchover i Failover","[{""min"": 140, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Unclassified,140,190,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,357,Data and Cloud Architect,B2Bnetwork,"We are looking for an experienced Data and Cloud Architect to support transformation initiatives in the areas of data, cloud, and AI. You will be working in the Data & Analytics domain within the Financial Crime Prevention space, helping to drive solution design and ensure architectural governance. The role involves close collaboration with both business and technology stakeholders. Responsibilities: Design and implement solution architecture for cloud, data, and AI initiatives Create and maintain architecture documentation and blueprints Support architectural governance and ensure alignment with enterprise standards Collaborate with business and IT teams to define scalable, modern architecture solutions Contribute to large-scale cloud and AI transformation efforts Requirements (Must-Have): Strong experience in integration architecture and data integration patterns (batch, real-time, near-real-time) Hands-on experience with cloud-based big data platforms, preferably AWS Background in AI and ML architecture (cloud and on-prem), including GenAI solutions Familiarity with big data technologies (e.g., Hadoop, Spark) and related toolchains Knowledge of enterprise solution architecture implementation and governance Experience with modern tech stacks (open-source and vendor-based) Solid understanding of data architecture, data modeling, and data management technologies Nice-to-Have: Experience in the banking sector Knowledge of Financial Crime Prevention processes or technologies Location: Gdynia, Gda≈Ñsk or Warsaw (hybrid model ‚Äì 2 days/week in the office) Contract Type: B2B Project Duration: Long-term","[{""min"": 170, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Architecture,170,185,Net per hour - B2B
Full-time,Mid,B2B,Remote,358,Data Engineer,emagine Polska,"Informacje o projekcie: Bran≈ºa: finanse/po≈ºyczki Lokalizacja: 100% zdalnie Umowa: B2B Stawka: 200 pln/h netto + VAT D≈Çugo≈õƒá projektu: d≈Çugoterminowy Poszukujemy do≈õwiadczonego Data Engineera do zespo≈Çu Data Platform, kt√≥ry bƒôdzie modelowaƒá dane oraz implementowaƒá procesy ELT w chmurze. ObowiƒÖzki: Modelowanie struktur bazodanowych w podej≈õciu DDD oraz tworzenie logicznych i fizycznych modeli danych. Opracowywanie warstwy Data Contracts na podstawie zamodelowanych struktur dla domen danych. Wsp√≥≈Çpraca w procesie ingerencji danych z system√≥w ≈∫r√≥d≈Çowych. Implementacja modeli danych w Data Platform na r√≥≈ºnych warstwach (Bronze, Silver, Gold) w ≈õrodowisku Azure Databricks. Wymagania: Do≈õwiadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejƒôtno≈õƒá implementacji proces√≥w ELT. Mile widziane: Umiejƒôtno≈õƒá tworzenia dokumentacji technicznej. Do≈õwiadczenie w mapowaniu danych ze ≈∫r√≥d≈Çowych do docelowych struktur. Znajomo≈õƒá narzƒôdzi do zarzƒÖdzania metadanymi (np. Azure Purview).","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,180,200,Net per hour - B2B
Full-time,Mid,B2B,Remote,359,Data Scientist,Experis Manpower Group,"Tasks: Analyze Standard Operating Procedures (SOPs), system logs, and structured datasets to support AI agent training and optimization. Validate AI-generated outputs and provide actionable feedback for continuous model improvement. Develop dashboards and define metrics to monitor and report on AI impact and performance. Collaborate closely with software developers and business analysts to align AI solutions with business needs. Requirements: Proven experience (4+ years) in data science, with a strong foundation in data analysis and machine learning. Proficiency in Python and SQL, with hands-on experience in data visualization tools (e.g., Power BI, Tableau, or similar). Familiarity with enterprise data systems such as SAP, OTM, or Denali is highly desirable. Strong analytical thinking, attention to detail, and ability to communicate technical insights to non-technical stakeholders. Ability to work independently in a remote or hybrid environment and manage multiple priorities effectively. Offer: 100% remote work MultiSport Plus Group insurance Medicover Premium e-learning platform","[{""min"": 190, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,190,200,Net per hour - B2B
Full-time,Mid,B2B,Remote,360,Data Engineer,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: Data Engineer Responsibilities: Develop, optimize & maintain big data (ELT/ETL) pipelines for purposes of business intelligence and statistical modelling Ensure & monitor data quality and integrity Set up and configure data storage systems (e.g. SQL databases, data lake) Manage and support production use of connections between key elements of data infrastructure Write and maintain secure, robust, scalable, and efficient code that turns business concepts into tangible solutions Drive engineering best practices like automation, CI/CD, and maintainability Support standardization and automation by following best-practices / common development standards Collaborate with BI analysts, data scientists, ML engineers and core IT in cross-functional teams delivering value through data solutions Requirements: Broad skills in Python, SQL, MDX and bash scripting 2+ years of hands-on experience within Azure data components (Data Factory, Synapse, Datalake, Blob Storage, Sharepoint, Databricks) Experience with data workflow orchestration (ADF, Databricks Jobs, possibly Airflow) Good command of version control and CI/CD pipelines using Azure DevOps or alike Experience with Snowflake and its use with Azure cloud Business-ready English (B2-C1) Familiarity with data governance concepts (e.g., catalogue & meta data, data lineage, master data, security & compliance) Nice to have: Familiarity with SAP Business Warehouse as a data source Azure Data Engineer / Data Analytics certificate Understanding of BI tools, eg. Power BI Experience with PySpark / Scala Offer: Private medical care Co-financing for the sport card Training & learning opportunities Constant support of dedicated consultant Employee referral program","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Mid,B2B,Remote,361,Administrator Baz Danych Oracle / Oracle DBA,Simora,"Do≈ÇƒÖcz do naszego zespo≈Çu jako Administrator Baz Danych Oracle! Poszukujemy osoby, kt√≥ra zajmie siƒô zarzƒÖdzaniem bazami danych naszych klient√≥w. Jeste≈õmy firmƒÖ, kt√≥ra rozwija nowoczesne rozwiƒÖzania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzƒôdzi wspierajƒÖcych naszych klient√≥w. Cenimy pozytywnƒÖ atmosferƒô, wzajemny szacunek i zaanga≈ºowanie ‚Äì to fundamenty naszego zespo≈Çu. Tw√≥j zakres obowiƒÖzk√≥w Tworzenie baz danych oraz ≈õrodowisk bazodanowych na ≈õrodowisku produkcyjnym i testowym Migrowanie baz danych na nowe ≈õrodowiska ZarzƒÖdzania i aktualizacja baz danych i ≈õrodowisk bazodanowych Konfigurowanie i optymalizacja ≈õrodowiska bazodanowego Automatyzacja zada≈Ñ za pomocƒÖ jƒôzyk√≥w skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jako≈õci i bezpiecze≈Ñstwa dla tworzonych rozwiƒÖza≈Ñ Wdra≈ºanie rozwiƒÖza≈Ñ opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Sta≈Çe doskonalenie sposobu Twojej pracy Nasze wymagania Do≈õwiadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomo≈õƒá jƒôzyka SQL i PLSQL Umiejƒôtno≈õƒá dbania o szczeg√≥≈Çy i jako≈õƒá rozwiƒÖza≈Ñ Komunikatywno≈õƒá Znajomo≈õƒá jƒôzyka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykszta≈Çcenie wy≈ºsze (preferowany kierunek: informatyka) Znajomo≈õƒá jƒôzyk√≥w programowania: Python, Java itp. Znajomo≈õƒá system√≥w operacyjnych Linux / Windows Znajomo≈õƒá system√≥w wirtualizacyjnych np.: OLVM Oferujemy CiekawƒÖ pracƒô w firmie o wysokiej dynamice rozwoju Mo≈ºliwo≈õƒá podniesienia kwalifikacji w obszarach zwiƒÖzanych z bazami danych, bezpiecze≈Ñstwem danych oraz sztucznƒÖ inteligencjƒÖ Benefity Karta Multisport Prywatna opieka zdrowotna ‚Äì Medicover Praca zdalna Brak dress code‚Äôu Dofinansowanie szkole≈Ñ i kurs√≥w Elastyczny czas pracy","[{""min"": 7000, ""max"": 13000, ""type"": ""Net per month - B2B""}]",Database Administration,7000,13000,Net per month - B2B
Full-time,Mid,Permanent,Remote,362,Adoption Engineer,Link Group,"We are looking for a skilledAdoption Engineerwith strong hands-on experience indbt (Core & Cloud),DevOps,Infrastructure as Code, andAirflow. This role is critical to support the successful rollout, scaling, and enablement of modern data transformation platforms within enterprise environments. As an Adoption Engineer, you will serve as a technical bridge between platform engineering and data users, ensuring smooth adoption of tools and best practices across teams. You will be responsible for building robust, reusable infrastructure components, supporting automation, and guiding users through optimal implementation patterns. Act as asubject matter expert (SME)indbt Core and Cloudimplementation, configuration, and adoption strategy Design and build scalable and reusableinfrastructure componentsusingIaC tools(Terraform, CloudFormation, etc.) Enable and supportAirflow DAGcreation and scheduling for orchestration of dbt models and other workflows Collaborate with data teams to improve workflows, CI/CD pipelines, and DevOps processes Define and enforce best practices indata modeling,code versioning, anddeployment automation Provide onboarding and enablement support to teams migrating to dbt Cloud Support observability, testing, and documentation standards for all data transformation processes Ensure security, scalability, and operational excellence across the data transformation stack Proven experience withdbt Core and dbt Cloud(advanced user-level, admin or platform support experience is a plus) Strong proficiency inSQLand data modeling principles Experience withDevOps toolingandCI/CD pipelines Hands-on experience withAirflowand workflow orchestration Experience withInfrastructure as Code(Terraform preferred, but others acceptable) Cloud platform familiarity (AWS, Azure or GCP) Strong communication skills, with the ability to support teams across different technical maturity levels Experience in data platforms such asSnowflake,BigQuery,Databricks, orRedshift Familiarity withmonitoring & observabilitytools for data workflows Understanding ofdata governanceandaccess controlconcepts Background in platform evangelism, internal consulting, or enablement is a plus","[{""min"": 85, ""max"": 105, ""type"": ""Gross per hour - Permanent""}]",Data Engineering,85,105,Gross per hour - Permanent
Full-time,Mid,B2B,Remote,364,Data Engineer (Databricks),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition byForbesas one of the top 10 AI companies. As aData Engineer, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of a universal data platform for global aerospace companies.This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Data Platform Transformation for energy management association body.This project addressed critical data management challenges, boosting user adoption, performance, and data integrity. The team is implementing a comprehensive data catalog, leveraging Databricks and Apache Spark/PySpark, for simplified data access and governance. Secure integration solutions and enhanced data quality monitoring, utilizing Delta Live Table tests, established trust in the platform. The intermediate result is a user-friendly, secure, and data-driven platform, serving as a basis for further development of ML components. Design of the data transformation and following data ops pipelines for global car manufacturer.This project aims to build a data processing system for both real-time streaming and batch data. We‚Äôll handle data for business uses like process monitoring, analysis, and reporting, while also exploring LLMs for chatbots and data analysis. Key tasks include data cleaning, normalization, and optimizing the data model for performance and accuracy. üöÄ Your main responsibilities: Design scalable data processing pipelines for streaming and batch processing using Big Data technologies like Databricks, Airflow and/or Dagster. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using Databricks/DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. üéØ What you‚Äôll need to succeed in this role: At least 3 years of commercial experienceimplementing, developing, or maintaining Big Data systems. Strong programming skills inPython: writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity withBig Datatechnologies likeAirfloworDagster,Databricks, SparkandDBT. Experience implementing and deploying solutions in cloud environments (with a preference forAzure). Knowledge of how to build and deployPower BIreports and dashboards for data visualization. Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master‚Äôs or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage withtop-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you towork remotelyor from modern offices and coworking spaces. Accelerate your professional growth throughcareer paths,knowledge-sharinginitiatives,languageclasses, and sponsoredtrainingorconferences, including a partnership withDatabricks, which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days offavailable for B2B contractors and individuals under contracts of mandate. Participate inteam-building eventsand utilize theintegration budget. Celebratework anniversaries, birthdays,andmilestones. Accessmedicalandsports packages, eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you canboostyourpersonal brandby speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website (career page) and social media (Facebook,LinkedIn,Instagram).","[{""min"": 15120, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Data Engineering,15120,21000,Net per month - B2B
Full-time,Mid,B2B,Remote,368,Data Engineer,KUBO,"We‚Äôre hiring a Data Engineer! Designing and building robust ETL/ELT data pipelines in Microsoft Fabric using PySpark and SQL Modeling data using approaches like star schema, data vault, and lakehouse Creating well-structured datasets, reports, and Power BI dashboards focused on business usability and self-service Implementing best practices around data governance, security, and documentation Automating tests, CI/CD workflows, and monitoring using Azure DevOps or GitHub Actions Collaborating closely with product owners, analysts, and fellow engineers in cross-functional teams 3+ years of experience as a Data Engineer or in a similar role Advanced SQL skills ‚Äì query optimization, indexing, partitioning Strong Python programming skills Solid knowledge of Apache Spark for batch & streaming data, Delta Lake Experience with Power BI ‚Äì data modeling, DAX, RLS, deployment pipelines Familiarity with cloud platforms ‚Äì ideally Azure (Data Lake Gen2, Data Factory, Synapse/Databricks) Proficiency with version control and DevOps tools (Git, pull requests, CI/CD basics) Fluency in English (minimum B2+) Work model: 100% remote Rate: 100‚Äì150 PLN/h net (B2B) Benefits: Private medical care, Life insurance, Multisport card","[{""min"": 100, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,100,150,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,369,Data Engineer,Tooploox,"We areTooploox üíé,an AI software development companyoffering custom AI solutions and services. We help innovative companies and startups design and build digital products with generative AI, mobile, and web technologies. Our team, consisting of nearly 200 experts including our R&D team of over 40 engineers, many with PhDs, has pioneered AI solutions across industries like healthcare, fashion, and e-commerce. We‚Äôve published over 15 research papers in top conferences like NeurIPS and ICML. We're on the lookout for aData Engineerüìä to take on a pivotal role in our team.You'll be at the heart of working with data, focusing on scalable batch and streaming data pipelines. If you're someone who loves to merge traditional software development with innovative AI technologies, this role is tailor-made for you. Design, develop, and maintain scalable batch and streaming data pipelines. Work withPythonto transform, process, and integrate data. Handle a mix of structured and unstructured data, including work withNoSQL and vector databases. Optimize performance acrossbig data workflows, including tuningHive and Sparkjobs. BS/BA in Data Engineering/Computer Science+ 2 years of experience or related field or 5 years of relevant experience. Extensive expertise withApache Spark (especially PySpark), Hadoop, and Apache Hivewith a proven track record of optimizing large-scale data systems. Strong programming skills inPython. Comprehensive understanding of database concepts, including experience withNoSQL databases (e.g., MongoDB, Redis)and ideally vector databases. Proven hands-on experience with stream processing, preferably usingApache Flink. In-depth knowledge of distributed computing, data warehousing, and performance optimization techniques. Exceptional problem-solving and communication skills, with experience working in cross-functional teams. Fluency in Polish and English. Experience with LLMs, prompt engineering, or machine learning workflows (we use this in conjunction with vector DBs). Experience in Java or Scala - useful for deeper Spark optimization or contributing to broader engineering projects. Familiarity with Spring Boot for building and deploying data applications. üèñÔ∏è26 days of annual service break. ü§íAn additional pool of 14 days per yearpaid at80% of your standard rate. üá¨üáßEnglish lessonsonce a week or more frequently, depending on your needs. üìöAccess to a curated libraryof books and e-books, regularly updated based on employee suggestions, plus recurringknowledge-sharing initiatives. üè°Flexible hoursand the option to work100% remotelyor from one of our offices inWroc≈Çaw or Warsaw. üíªTop-quality equipment‚Äì we provide MacBooks, new monitors, noise-cancelling headphones, and any additional gear you may need to work comfortably. üè•Group insurancewith Warta andprivate medical carewith Enel-Med for just 1 PLN. üß†Mental health support‚Äì we offer access to a psychologist with fully anonymous consultations if needed. üèãÔ∏è‚Äç‚ôÇÔ∏èMultisport card(we cover most of the cost ‚Äì your contribution is currently no more than 45 PLN, or less depending on the selected package), access togyms in our Wroc≈Çaw and Warsaw offices, andsports initiativeslike the annualBike 2 Work Challenge. üçïüéÆüï∫üèª We hostteam lunches, webinars, game nights, and social events. We enjoy the occasional barbecue, dance party, time on the terrace, foosball, or PlayStation session.","[{""min"": 18000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,22000,Net per month - B2B
Full-time,Senior,B2B,Remote,370,Manager Database Administrator,Link Group,"Role Overview We are looking for an experienced and versatileSenior Database Administratorto step into a managerial role within our growing IT team. This is an exciting opportunity for someone passionate about database technologies who is also ready to lead and develop a team. You will be responsible for ensuring the reliability, performance, and security of our database systems while driving strategic improvements and mentoring your team. Design, implement, and maintain robust, high-performance, and secure database environments. Monitor system performance, proactively troubleshoot issues, and optimize efficiency. Oversee backup strategies, ensure successful recovery processes, and manage disaster recovery plans. Lead, coach, and mentor a team of database professionals, supporting their growth and daily activities. Partner with cross-functional teams to understand and deliver on database requirements aligned with business objectives. Promote a culture of learning, innovation, and continuous improvement. Define and execute database strategies that align with broader IT and organizational goals. Stay updated on emerging technologies and best practices, recommending and implementing upgrades and enhancements. Maintain thorough documentation of database architecture, procedures, and configurations. Ensure adherence to security, data governance, and compliance standards. Provide technical guidance and training to IT staff and end-users. Work closely with development teams to optimize database integration and performance for business-critical applications. Bachelor‚Äôs degree in Computer Science, Information Technology, or a related field. At least5 years of experienceas a DBA, includingproven experience in a leadership or managerial role. Strong leadership and team management skills, with ability to mentor and inspire technical teams. Deep expertise with database systems such as Oracle, SQL Server, or MySQL. Strong background in database design, performance tuning, and optimization. Excellent problem-solving, analytical, and decision-making skills. Strong communication and interpersonal abilities to effectively collaborate across teams. Experience working with cloud database platforms (e.g., AWS, Azure). Understanding of data warehousing and business intelligence concepts. Familiarity with database security standards and best practices.","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Database Administration,150,180,Net per hour - B2B
Full-time,Senior,B2B,Remote,372,MOCNY GCP Data Engineer,Link Group,"Poszukujemy do≈õwiadczonego GCP Data Engineera do projektu realizowanego dla globalnej firmy konsultingowej w obszarze data & analytics. Praca dotyczy budowy i optymalizacji zaawansowanych pipeline‚Äô√≥w danych w ≈õrodowisku Google Cloud Platform (BigQuery, Airflow, DBT). Kluczowe bƒôdƒÖ umiejƒôtno≈õci projektowania architektury danych, integracji danych z r√≥≈ºnych ≈∫r√≥de≈Ç oraz optymalizacji wydajno≈õci system√≥w. Wymagania: Minimum 4 lata do≈õwiadczenia jako Data Engineer, w tym 3 lata z GCP Bardzo dobra znajomo≈õƒá BigQuery, Python, SQL Do≈õwiadczenie z Airflow, DBT/Dataform Znajomo≈õƒá zasad modelowania danych, optymalizacji zapyta≈Ñ Angielski B2/C1 Mile widziane: Certyfikaty GCP, do≈õwiadczenie z BI (Power BI, Tableau), znajomo≈õƒá Azure Informacje organizacyjne: üìç 100% zdalnie üóì Start: sierpie≈Ñ üíº Proces: HR + techniczne + opcjonalne spotkanie z klientem","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,373,Snowflake Data Engineer,Onwelo Sp. z o.o.,"Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Do≈ÇƒÖczysz do zespo≈Çu realizujƒÖcego projekt dla globalnej organizacji z sektoralife science i healthcare, specjalizujƒÖcej siƒô w rozwiƒÖzaniach laboratoryjnych i biotechnologicznych. Klient prowadzi dzia≈Çalno≈õƒá na wielu rynkach i obs≈Çuguje tysiƒÖce jednostek operacyjnych na ca≈Çym ≈õwiecie.Celem projektu jest budowa i rozw√≥j skalowalnej platformy danych w ≈õrodowisku chmurowym, kt√≥ra wspiera analitykƒô biznesowƒÖ, planowanie operacyjne oraz zaawansowane modele predykcyjne.Zesp√≥≈Ç Onwelo wspiera klientam.in. w rozwoju hurtowni danych, projektowaniu potok√≥w ETL, modelowaniu danych i zapewnieniu jako≈õci danych w ≈õrodowisku enterprise.Szukamy osoby, kt√≥ra wniesie swoje do≈õwiadczenie i wesprze zesp√≥≈Ç w rozwoju architektury danych, zapewniajƒÖc wydajno≈õƒá, jako≈õƒá i bezpiecze≈Ñstwo danych. Projektowaƒá, budowaƒá i rozwijaƒá hurtownie danych oparte na platformieSnowflake Tworzyƒá oraz optymalizowaƒápotoki danych (ETL/ELT)w ≈õrodowiskach chmurowych Wdra≈ºaƒá i zarzƒÖdzaƒá komponentami Snowflake: Snowpipe, Streams, Tasks, Secure Views Projektowaƒá i rozwijaƒámodele danychwspierajƒÖce analitykƒô biznesowƒÖ Wsp√≥≈Çpracowaƒá z zespo≈Çami Data Science i BI w zakresie zasilania modeli i dashboard√≥w Wspieraƒá automatyzacjƒô proces√≥w danych poprzez integracjƒô z narzƒôdziami CI/CD (GitLab, Jenkins) Posiadasz minimum 3-letnie do≈õwiadczenie jakoData Engineer‚Äì z naciskiem na Snowflake Znasz platformƒôSnowflake: strukturƒô danych, architekturƒô, optymalizacjƒô zapyta≈Ñ, zarzƒÖdzanie schematami i dostƒôpem Biegle pos≈Çugujesz siƒôSQL(w tym: CTE, window functions, UDF, optymalizacja zapyta≈Ñ) Masz do≈õwiadczenie z procesamiETL/ELT, r√≥wnie≈º z wykorzystaniem danych p√≥≈Çstrukturalnych (JSON, XML, Parquet) Znasz zasady projektowania nowoczesnych modeli danych (np. Kimball, Data Vault) Maszwy≈ºsze wykszta≈Çcenie techniczne(np. informatyka, matematyka, in≈ºynieria danych) Komunikujesz siƒôpo angielsku na poziomie min. B2 Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 16800, ""max"": 23100, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 17000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16800,23100,Net per month - B2B
Full-time,Mid,B2B,Remote,374,Data Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients‚Äô systems, departments and sites. We provide an open technology platform that‚Äôs shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience.Want to be part of it? Job Summary: As a Data Engineer at Keyloop you will work within the Analytics Engineering team and be responsible for maintaining and developing our Data Lake and existing Data Pipelines, as well as continually exploring, analysing and proposing improvements to existing processes and tooling. You will also be required to contribute to scoping and discovery exercises being conducted elsewhere in the business, leveraging your expert knowledge of the data and business intelligence offering and the associated data requirements. You will therefore be working in a busy and multi-functional team, planning and prioritising a variable workload and delivering to deadlines. You will report to the Lead Analytics Engineer, and you may provide mentorship to other analysts and data engineers in Analytics Engineering where appropriate to support their knowledge and skills growth. Key Responsibilities: ‚Ä¢ Design, build and maintain scalable and robust ETL/ELT pipelines using various data integration tools and programming languages (e.g. Python, SQL) ‚Ä¢ Collaborate with Analytics Engineers to design and implement optimised data models within our data warehouse, ensuring data quality, consistency and ease of use for analytics ‚Ä¢ Manage and optimise our data warehouse including cost optimisation and ensuring data governance best practices ‚Ä¢ Implement robust data validation, monitoring and alerting mechanisms to ensure accuracy and completeness of our data ‚Ä¢ Work closely with Analytics Engineers to understand their data requirements, provide technical guidance and ensure the efficient delivery of data products ‚Ä¢ Pre and post release communications where necessary to relevant stakeholders ‚Ä¢ Documenting processes (or SOPs) for commonly performed tasks to assist with the training of other Data Engineers, or to aid business continuity as a general theme ‚Ä¢ Fully understanding the data landscape in databases, analytics and applications and all the front-end and back-end products in the global portfolio ‚Ä¢ Understanding of data compliance and laws, as well as the full data collection to output technology stack Technical Competencies: ‚Ä¢ Experience with Amazon Web Services cloud platform and with knowledge in particular of data related services they offer ‚Ä¢ SQL for data manipulation, transformation and query optimisation ‚Ä¢ Experience in Python for data engineering tasks ‚Ä¢ Hands-on experience with a modern data warehouse platform (e.g. Amazon Redshift) ‚Ä¢ Solid understanding of data warehousing concepts, dimensional modelling (star schema approach), and ETL/ELT principles ‚Ä¢ Experience with data pipeline orchestration tools ‚Ä¢ Familiarity with version control systems (e.g. Git) ‚Ä¢ Understanding of data governance principles and tools Behavioural & Personality Competencies: ‚Ä¢ Analytical and logical mindset ‚Ä¢ Time management skills ‚Ä¢ High standard of problem-solving skills and attention to detail ‚Ä¢ Good communication and listening skills ‚Ä¢ Team player and collaborative ‚Ä¢ Ability to manage multiple different projects ¬∑ Organised & self-sufficient ¬∑ Logical, methodical approach to problem and issue solving ¬∑ Numerate, innovative and critical thinking Desirable Skills: ‚Ä¢ Familiarity with JIRA software ‚Ä¢ Experience, or an interest in, the automotive industry ‚Ä¢ Experience of Agile/SCRUM delivery environment Why join us? We‚Äôre on a journey to become market leaders in our space ‚Äì and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We‚Äôre committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles ‚Äì not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn‚Äôt require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply.","[{""min"": 18000, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Engineering,18000,23100,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,375,Data Scientist,Onwelo Sp. z o.o.,"Onweloto nowoczesna polska sp√≥≈Çka technologiczna, specjalizujƒÖca siƒô w budowie innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z r√≥≈ºnych sektor√≥w na ca≈Çym ≈õwiecie. Firma oferuje kompleksowe us≈Çugi z zakresu tworzenia, rozwoju i utrzymania oprogramowania, oraz silne wsparcie kompetencyjne. Do naszego zespo≈ÇuData & Analyticsposzukujemy Data Scientista, kt√≥ry bƒôdzie pracowa≈Ç nad budowƒÖ i rozwojem modeli analitycznych oraz uczenia maszynowego dla klient√≥w z r√≥≈ºnych bran≈º ‚Äì zar√≥wno z Polski, jak i z rynk√≥w zagranicznych. Bƒôdziesz wsp√≥≈Çpracowaƒá z zespo≈Çami analitycznymi i biznesowymi, wspieraƒá podejmowanie decyzji na podstawie danych i tworzyƒá rozwiƒÖzania, kt√≥re realnie wp≈ÇywajƒÖ na dzia≈Çalno≈õƒá naszych klient√≥w. Przeprowadzaƒá eksploracyjnƒÖanalizƒô danych (EDA) Poszukiwaƒázale≈ºno≈õci, wzorc√≥w i insight√≥w w danych biznesowych Budowaƒámodele klasyfikacyjne, regresyjne i klasteryzacyjne Przeprowadzaƒáfeature engineering i przygotowywaƒá dane do modelowania Wsp√≥≈Çpracowaƒá z zespo≈Çami analitycznymi, technologicznymi i biznesowymi Wizualizowaƒá wyniki analiz i przygotowywaƒá raporty oraz prezentacje Maszminimum 2-letnie do≈õwiadczeniew pracy jakoData Scientistlub na podobnym stanowisku Bardzo dobrze znaszPythona i pracowa≈Çe≈õ z bibliotekami: pandas, scikit-learn, numpy, matplotlib, seaborn Swobodnie pracujesz zdanymi tabelarycznymii znasztechniki EDA Potrafisz wyciƒÖgaƒá trafne wnioski z danych i prezentowaƒá je w przystƒôpny spos√≥b ZnaszSQLi masz do≈õwiadczenie z du≈ºymi zbiorami danych Dodatkowo docenimy, je≈õli: Korzysta≈Çe≈õ znarzƒôdzi BI i znasz metody interpretacji modeli Masz do≈õwiadczenie w automatyzacji proces√≥w analitycznych Znasz system kontroli wersjiGITi technologie konteneryzacji (Docker) Pracowa≈Çe≈õ z rozwiƒÖzaniami w chmurze obliczeniowej(Azure, AWS lub GCP) Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Wydarzenia firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu","[{""min"": 15750, ""max"": 21000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 15000, ""type"": ""Gross per month - Permanent""}]",Data Science,15750,21000,Net per month - B2B
Full-time,Mid,B2B,Remote,377,Spatial Data Engineer,Spyrosoft,"Project Description Are you passionate about geospatial data and eager to work on cutting-edge GIS solutions? Join our team as a Spatial Data Engineer and help our clients implement and maintain advanced spatial data processing systems using ESRI products or Open-Source equivalents. Key Responsibilities Process and manage geospatial data from diverse sources Design and maintain ETL pipelines Integrate spatial data from multiple systems Develop automation solutions for data workflow Build GIS applications (web and mobile) Technical Requirements Solid understanding of spatial data models and best practices in spatial data management Proficiency in Python at an intermediate level, especially with ArcGIS API for Python and arcpy Strong experience with ESRI products , particularly: ArcGIS Pro ArcGIS Online (including: Experience Builder, Field Maps, Hub, Map Viewer, Survey123, Workflow Manager) ArcGIS Notebooks Managing and administering ArcGIS Online/Portal Automating spatial data processing (using ESRI or Open-Source tools) Integrating cloud solutions (Azure, AWS) with GIS environments Working in Azure DevOps Familiarity with data sharing methodologies and cloud-based environments Nice to have: Experience with FME Form , FME Flow , ArcGIS Velocity , or Sweet for ArcGIS Ôªø üí° Soft Skills Passion for spatial data and strong analytical thinking Excellent communication skills and business-oriented mindset Fluency in English, both written and spoken","[{""min"": 70, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Data Engineering,70,120,Net per hour - B2B
Full-time,Mid,B2B,Remote,378,Data Engineer (Palantir+Snowflake),Onwelo Sp. z o.o.,"Poznaj Onwelo: Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Do≈ÇƒÖcz do zespo≈Çu Data, kt√≥ry wspiera jednƒÖ z najwiƒôkszych grup technologicznych na ≈õwiecie w budowie centralnej platformy danych. W projektach wykorzystujemy platformƒôPalantir Foundryjako warstwƒô aplikacyjnƒÖ orazSnowflakejako warstwƒô magazynowania i przetwarzania danych. Pracujemy nad integracjƒÖ danych z r√≥≈ºnych ≈∫r√≥de≈Ç (ERP, CRM, dane laboratoryjne) i tworzymy zaawansowane pipeline'y danych, modele analityczne i aplikacje wspierajƒÖce decyzje biznesowe. Tworzyƒá i utrzymywaƒá pipeline‚Äôy danych w platformiePalantir Foundry Projektowaƒá i wdra≈ºaƒá procesy przetwarzania danych oraz modele danych wSnowflake Integrowaƒá dane z r√≥≈ºnych ≈∫r√≥de≈Ç ‚Äì ERP, CRM, dane laboratoryjne Zapewniaƒá jako≈õƒá danych, monitorowaƒá i optymalizowaƒá procesy Wsp√≥≈Çpracowaƒá z architektami i analitykami danych przy projektach w r√≥≈ºnych obszarach biznesowych (produkcja, finanse, logistyka, R&D) Masz min. 3‚Äì5 lat do≈õwiadczenia jako Data Engineer lub Developer Pracowa≈Çe≈õ(a≈õ) z platformƒÖPalantir Foundry(np. Code Workbook, Pipelines, Ontology) Bardzo dobrze znaszSnowflakei jƒôzykSQL Programujesz wPythoni/lubPySpark Masz do≈õwiadczenie w integracji danych z system√≥w klasyERP/CRM(np. SAP, Salesforce, MS Dynamics) Znasz rozwiƒÖzania chmurowe, szczeg√≥lnieAWS Pos≈Çugujesz siƒô jƒôzykiem angielskim na poziomie min.B2(miƒôdzynarodowe ≈õrodowisko) Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 900, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Data Engineering,900,1000,Net per day - B2B
Full-time,Senior,B2B,Remote,379,Data Solution Architect (Azure),Scalo,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania tom.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! projekt budowy nowoczesnej Data Platform w chmurze Azure, obejmujƒÖcej warstwƒô Lakehouse (Bronze / Silver / Gold) oraz Analytical Platform (MLOps), modelowanie struktur bazodanowych w podej≈õciu DDD (Data Domain Driven Design), opracowywanie modeli danych na podstawie dokumentacji z obszaru Data Governance (glosariusz danych, modele konceptualne/logiczne), projektowanie przep≈Çywu danych ze ≈∫r√≥de≈Ç do warstw platformy danych (Ingest: direct query, API, event streaming), tworzenie i dokumentowanie Data Contracts, wsp√≥≈Çpraca z w≈Ça≈õcicielami system√≥w ≈∫r√≥d≈Çowych i docelowych w zakresie integracji danych i SLA, implementacja struktur danych w architekturze medallion (Bronze / Silver / Gold) przy u≈ºyciu ETL na Azure Data Platform, doradztwo w zakresie doboru narzƒôdzi, architektury integracyjnej i praktyk CI/CD, mo≈ºliwo≈õƒá realnego wp≈Çywu na standardy technologiczne i projektowe, praca 100% zdalna, a dla chƒôtnych mo≈ºliwo≈õƒá pracy z biura we Wroc≈Çawiu, stawka do 240 z≈Ç/h przy B2B, w zale≈ºno≈õci od do≈õwiadczenia. masz do≈õwiadczenie w modelowaniu danych (ERD), przygotowywaniu Data Contracts i implementacji struktur domenowych w ≈õrodowisku Data Warehouse znasz procesy Data Ingestion i architekturƒô nowoczesnych DWH w Azure (Azure Synapse, Data Lake, Databricks), masz do≈õwiadczenie z platformami MLOps / Analytical Platform, mile widziane do≈õwiadczenie w: tworzeniu dokumentacji mapowania danych ≈∫r√≥d≈Çowych, zarzƒÖdzaniu metadanymi i jako≈õciƒÖ danych (np. Azure Purview), komunikujesz siƒô p≈Çynnie w jƒôzyku angielskim (B2/C1). d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 33600, ""max"": 40320, ""type"": ""Net per month - B2B""}]",Data Architecture,33600,40320,Net per month - B2B
Full-time,Mid,B2B,Hybrid,382,BI Developer (WCL Qlik),ITDS,"BI Developer (WCL Qlik) Join us, and turn data into powerful business decisions! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As aBI Developer (WCL Qlik),you will be working for our client, a leading global financial institution undergoing a major digital transformation to enhance its data analytics capabilities. You will be part of a dynamic team focused on developing and maintaining scalable Qlik dashboards and reports, aimed at improving decision-making across various business units. The project involves managing complex data sets, implementing infrastructure best practices, and ensuring compliance within a fast-paced, highly regulated environment. Your role plays a key part in optimizing how data is shared, visualized, and utilized to deliver impactful business insights. Your main responsibilities: Developing advanced dashboards and reports using QlikSense Ensuring data integrity and managing updates to data sources Managing project sites and content on Qlik Server Documenting data sources, processes, and dashboards clearly Analyzing data sharing policies and promoting compliance best practices Collaborating with cross-functional teams and stakeholders at all levels Supporting Qlik deployment by following infrastructure best practices Enhancing dashboard performance for large data sets Influencing stakeholders through thoughtful data presentations Following established internal control standards and audit requirements You're ideal for this role if you have: Proficiency in Qlik with strong dashboard development experience 3+ years of experience in data analysis roles Practical knowledge of QlikSense and Qlik architecture Ability to work with large data sets while maintaining performance High level of mathematical and analytical skills Proven experience in stakeholder management and communication Strong collaboration skills within cross-functional teams Experience documenting technical processes and data sources Ability to adapt quickly in fast-changing environments Understanding of regulatory requirements for data sharing It is a strong plus if you have: Experience with Qlik administration Familiarity with financial services or highly regulated industries Knowledge of best practices for offshore project coordination Prior involvement in change or transformation projects Awareness of risk management and internal control standards We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #...7316 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere.","[{""min"": 21000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,28000,Net per month - B2B
Full-time,Manager / C-level,B2B,Remote,383,AI/ML Data Practice Lead,Knowit Poland sp. z o.o.,"Role Overview: As the AI/ML & Data Practice Lead, you will be responsible for designing, implementing, and delivering AI/ML solutions that drive value for our clients. You will also play a key and active role in pre-sales activities, defining company offering, lead discovery workshops, and provide hands-on consulting. Your expertise will help establish and grow our AI/ML and Data Management practice, positioning Knowit Poland as a leader in the field. Key Responsibilities: Lead the development and delivery of AI/ML and Data Management solutions. Engage in pre-sales activities, including participation in defining company offer, client consultations, discovery workshops, and inspiration events. Design and implement AI/ML and Data solutions based on public cloud infrastructure (preferably Azure). Establish and grow Knowit Poland's AI/ML and Data practices. Serve as the technical and professional lead in AI/ML and Data areas.","[{""min"": 150, ""max"": 200, ""type"": ""Net per day - B2B""}]",Data Science,150,200,Net per day - B2B
Full-time,Senior,Permanent or B2B,Hybrid,384,DATA Architect,Power Media,"Nasz klient to firma, kt√≥ra odwa≈ºnie podchodzi do wyzwa≈Ñ technologicznych i nie boi siƒô szukaƒá nieszablonowych rozwiƒÖza≈Ñ. Innowacyjno≈õƒá ≈ÇƒÖczy tu siƒô z solidnƒÖ wiedzƒÖ technicznƒÖ oraz doskona≈ÇƒÖ znajomo≈õciƒÖ reali√≥w bran≈ºy. SpecjalizujƒÖ siƒô w projektach z pogranicza eCommerce i Business Intelligence, oferujƒÖc kompleksowe rozwiƒÖzania oparte na sprawdzonych technologiach. Co wiƒôcej ‚Äì umiejƒôtnie ≈ÇƒÖczƒÖ te dwa ≈õwiaty, tworzƒÖc narzƒôdzia, kt√≥re zapewniajƒÖ pe≈Çen wglƒÖd w dane i realne wsparcie dla biznesu. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozw√≥j rozwiƒÖza≈Ñ Data Platform w ≈õrodowisku chmurowym. Kluczowym zadaniem bƒôdzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejs√≥w w z≈Ço≈ºonym krajobrazie systemowym klienta. Projekt obejmuje budowƒô hurtowni danych oraz platform danych od podstaw. Zesp√≥≈Çsk≈Çada siƒô z 22 os√≥b: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiƒÖzk√≥w: Rozw√≥j i utrzymanie Data Warehouse oraz Data Platform przy u≈ºyciu narzƒôdzi ETL i SQL, Analiza wymaga≈Ñ biznesowych i proponowanie rozwiƒÖza≈Ñ technicznych, Wsp√≥≈Çpraca z zespo≈Çem w celu zapewnienia dostƒôpno≈õci rozwiƒÖza≈Ñ biznesowych, Wykorzystanie swojej wiedzy i do≈õwiadczenia do tworzenia innowacyjnych rozwiƒÖza≈Ñ. G≈Ç√≥wne wymagania: Bardzo dobra znajomo≈õƒá metod i technik projektowania oprogramowania, Ponad 10-letnie do≈õwiadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych, Znajomo≈õƒá ekosystem√≥w Big Data, Znajomo≈õƒározwiƒÖza≈Ñ chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejƒôtno≈õci analityczne (analiza i dokumentowanie wymaga≈Ñ biznesowych oraz specyfikacji technicznych) Do≈õwiadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomo≈õƒá SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomo≈õƒá rozwiƒÖza≈Ñ ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (min. B2+/C1)‚Äì codzienna praca w miƒôdzynarodowym ≈õrodowisku, Gotowo≈õƒá do podr√≥≈ºy s≈Çu≈ºbowych. Mile widziane: Znajomo≈õƒánarzƒôdzi Informatica, Znajomo≈õƒá Machine Learning oraz framework√≥w ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomo≈õƒá jƒôzyk√≥w programowania (Java, Scala, C++, Python). Znajomo≈õƒá jƒôzyka niemieckiego. Firma oferuje: Stabilne, d≈Çugofalowe zatrudnienie w oparciu o B2B lub UoP, Mo≈ºliwo≈õƒá pracy w wiƒôkszo≈õci zdalnej(praca z biura 1 raz w tygodniu, we wtorek) P≈Çaska struktura, anty korporacyjne podej≈õcie do pracy i zespo≈Çu, Ciekawe, miƒôdzynarodowe projekty, Zgrany zesp√≥≈Ç chƒôtnie uczestniczƒÖcy w aktywno≈õciach sportowo ‚Äì rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team ‚Äì building), Dodatkowe zajƒôcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajƒôƒá sportowych, Nowoczesne biuro ze strefƒÖ relaksu, Co roczny 5 dniowy wyjazd firmowy (ca≈Ça firma), warsztaty kulinarne, wyj≈õcia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywno≈õciami ≈öwietna atmosfera, partnerskie podej≈õcie, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa ‚Äûmiƒôkko-techniczna‚Äù. GorƒÖca pro≈õba o CV w j. angielskim : )","[{""min"": 18000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,18000,30000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,385,Data Engineer,Antal Sp. z o.o.,"For Our Client in the Banking Industry, we are looking for a Data Engineer. Salary Range: 180-200 PLN/hour Work Model: Hybrid (6 times a month in the Krakow office) Responsibilities: This role involves: Responsibility for metrics implementation: data ingest, data quality, refinement, and presentation - design, development, testing, coding, and deploying to production Operating and iterating a cloud data platform capability to support our internal goals Focus on quality, time, and budget constraints Communicating efficiently within the team and business stakeholders Working with global diverse teams Working in an Agile product-oriented culture Skills Desired: Experience in data engineering, ETL processes At least 7 years of professional experience in SQL development Experience in GCP, Big Query Experience with data build tools (DBT) Experience with Airflow, Cloud Composer Experience in data modeling Experience with data streaming, e.g., Kafka Experience with BI tools ‚Äì mainly Looker Studio Experience making trade-offs when designing schemas and marts Experience with implementing automated data quality solutions Experience in orchestration and scheduling Experience working in a DevOps environment, i.e., think, build, and run Experience with Continuous Integration, Continuous Delivery","[{""min"": 180, ""max"": 200, ""type"": ""Net per day - B2B""}]",Data Engineering,180,200,Net per day - B2B
Full-time,Senior,B2B,Remote,387,Senior Data Engineer with AWS and Snowflake,Sii,"Strong expertise in cloud technologies and the Snowflake ecosystem, particularly AWS, Data One Platform, Immuta, Collibra, and cloud security aspects Minimum 5 years of experience in Python programming for building and maintaining data pipelines Advanced knowledge of databases, including query optimization, relational schema design, and MPP using SQL Expertise in data storage technologies, including files, relational databases, MPP, NoSQL, and various data types (structured, unstructured, metrics, logs) Deep familiarity with Data Vault, Data Mesh, dimensional modeling, and metadata management Understanding of business and analytical processes, as well as integration with analytics and reporting systems Experience working in Agile environments, coaching development teams with the use of fluent English and Polish Residing in Poland required Nice-to-have requirements Knowledge of Pharma data formats is a big plus We are looking for a Data Engineer with expertise in Snowflake, AWS, and ETL processes, who will work closely with AI scientists and data analysts to design, develop, and maintain data pipelines and systems that support clinical and operational data use cases. Design and develop data architecture incorporating Snowflake, Immuta, Collibra, and cloud security Implement and optimize ETL/ELT processes, manage raw data, automate workflows, and ensure high performance and reliability of data processing systems Manage data quality and security, monitor quality, and implement metadata management strategies Collaborate with analytics and business teams to identify user needs and deliver comprehensive solutions supporting analytics and reporting Optimize and migrate data systems through data and process conversion from existing systems to Data Vault Define and enforce coding standards, data modeling, and ETL/ELT architecture, ensure compliance with Data Mesh and other modern approaches Coach and mentor the data engineering team, conduct code reviews, participate in architectural discussions, and initiate innovative technological solutions Recruitment language: Polish Start ASAP Permanent contract Fully remote Free coffee Free breakfast No dress code Modern office Sport subscription Training budget Private healthcare Small teams International projects Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title ‚Äì get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers ‚Äì Power People. Learn more atsii.pl.","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,26000,Net per month - B2B
Full-time,Manager / C-level,Permanent,Hybrid,389,Big Data Staff Engineer,Relativity,"Job Overview At Relativity, we make software to help users organize data, discover the truth, and act on it. Our e-discovery platform is used by more than 13,000 organizations around the world to manage large volumes of data and quickly identify key issues during litigation, internal investigations, and compliance projects. We are seeking a Staff Data Engineer to join our Relativity Data Services organization, a team dedicated to developing data and AI infrastructure that powers AI-driven applications in support of our mission to drive pursuit of justice. Relativity‚Äôs scale and breadth provide significant opportunities for rich data exploration and insights. Our market position and advanced products ensure that our latest models and insights can quickly benefit our users. Great insights stem from excellent data, and the best insights arise from substantial data. Our data infrastructure and engineering guarantee that Relativity's vast data is accessible for insights, confidential data remains secure, and data protection is always upheld. We are making substantial investments in data pipeline and data lake technology for the future. In this role, you will partner with teams across the Data Services organization to scale and optimize our data platforms, advance tooling for large-scale distributed processing, and enable critical use cases such as reporting, analytics, and audit. Additionally, you will lead efforts to redefine and strengthen our approach to audit and behavioral analytics. Role overview: Staff Engineer serves as a technical liaison between his or her teams and other internal and external development teams to identify and resolve dependencies, to identify, improve, and apply software engineering best practices and processes, and to identify and mitigate risks to the on-time delivery of software. Staff Engineer ‚Äì thinks what to buy or what to build, designs the architecture to serve the user needs and support the system scale. Understands the trade-offs to be made and that there are no silver bullets. But ultimately builds systems that work, deliver value in time and are predictable to operate and extend. Staff Engineer serves as a mentor to other team members to improve technical and process expertise and promotes collaboration. Job Description and Requirements Responsibilities Lead design of software using abstraction, low coupling and high cohesion, modularization, encapsulation and information hiding, and separation of concerns. Lead implementation of software using practical application of algorithms, defensive programming and exception handling, fault tolerance, design patterns. Be pragmatic ‚Äì in using object-oriented principles, applying SOLID principles and design patterns in a variety of languages. Build systems that are low maintenance but not overengineered ‚Äì balancing security, observability and extensibility with time-to-market and user value. Specify non-functional software requirements and analyze all requirements to determine design feasibility within time and cost constraints. Test and lead test of software emphasizing the practice of Test-Driven Design and the use of autonomous frameworks and Continuous Integration. Identify and offer solutions to reduce technical debt. Display an ownership mindset; be accountable for and beyond the features your team and larger organization develops. Provide solutions to varied and ambiguous issues, utilizing judgment to select methods and techniques for obtaining solutions. Offer coaching to ensure the team stays focused and delivers against the goals, adapting to changing business requirements. Advocate for and ensure adherence to best practices in coding standards, quality assurance, and security, while aligning solution with company-wide architectural principles. Your Skills 10+ years of professional software development experience on commercial-grade systems and applications with a proven track record of building and shipping successful software. 6+ years of hands-on experience with large-scale data infrastructure and cloud-native distributed systems. Proven proficiency in multiple programming languages, with a strong aptitude for rapidly learning and adapting to new technologies. Experience with at least two of the following is required: Java, Python, Scala, Rust and C#. Experience building and optimizing data pipelines using Apache Spark for large-scale batch workloads. Familiarity with deploying and managing data workloads on Kubernetes, including containerization and orchestration best practices. Extensive knowledge of and adherence to SDLC (Software Development Life Cycle) standards and best practices. Ability to consistently identify and deliver technical improvement feedback to team members in a supportive and constructive manner, to achieve demonstrable results over time. Excellent problem solving-solving skills, with a clear ability to present trade-offs, make informed decisions, and drive strategic execution. Excellent verbal and written communication to clearly, succinctly, and completely communicate intent (both technical and non-technical) in interactions with team members and management. Nice to have: Experience working with Data Lake and Lakehouse architectures on cloud storage platforms like ADLS. Nice to have: Hands-on experience or practical understanding of machine learning systems and their integration into production environments. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law. #LI-MM5 Relativity is committed to competitive, fair, and equitable compensation practices. This position is eligible for total compensation which includes a competitive base salary, an annual performance bonus, and long-term incentives. The expected salary range for this role is between following values: 300 000 and 450 000PLNThe final offered salary will be based on several factors, including but not limited to the candidate's depth of experience, skill set, qualifications, and internal pay equity. Hiring at the top end of the range would not be typical, to allow for future meaningful salary growth in this position.","[{""min"": 300000, ""max"": 450000, ""type"": ""Gross per year - Permanent""}]",Data Engineering,300000,450000,Gross per year - Permanent
Full-time,Senior,Permanent,Hybrid,392,"Director, Data Engineering & Analytics",Optiveum,"Director, Data Engineering & Analytics Location: Warsaw (remote for now, hybrid soon) Salary: up to 33 000 PLN/month Contract type: Employment contract (UoP only) Our client is a US-based technology company headquartered in New York City, delivering innovative digital solutions and cloud-based platforms for private capital markets. With offices in multiple countries, the company is now investing in a new engineering centre in Warsaw, highly valuing the technical expertise and strong work ethic of software engineers in Poland. Currently, our client is building a new team in Warsaw. While the work is fully remote for now, within a few months, the role will require working from the Warsaw office 3 days per week. About the role As Director of Data Engineering & Analytics , you will lead a high-performing team of data engineers and be responsible for building and maintaining scalable data platforms and architectures. You will play a key role in shaping data-driven strategies and empowering analytics and data science teams with robust tools and pipelines. What you will do Lead, recruit, mentor, and develop a world-class data engineering team Design and maintain scalable data pipelines and architectures Optimize infrastructure for data extraction, transformation, and loading (ETL) Model data for optimal performance and insights Build and implement tools and systems that empower analytics and data science functions What we are looking for 8+ years of experience in data engineering or cloud/web engineering 5+ years in leadership roles with direct client communication Hands-on experience with ETL tools (AWS Glue, Azure Data Factory) Knowledge of orchestration tools (Airflow, Prefect) Proficiency with DBT, SQL, and data modeling techniques Experience with data lakes, data warehouses, and data mesh architecture Programming skills in Python or R Knowledge of reporting best practices and tools (Power BI, Tableau, Redash, etc.) Experience working with cloud platforms like AWS, Azure, or GCP Bachelor‚Äôs degree in Computer Science or a related field what is offered Competitive salary (up to 33 000 PLN/month) Employment contract (UoP only) Flexible work arrangements Great paid time off policy Comprehensive benefits package Opportunities for professional growth and leadership Supportive, inclusive, and fast-paced work environment","[{""min"": 28000, ""max"": 33000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,28000,33000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,398,Data Engineer with Databricks,Margo,"We are looking for data engineers perfectly familiar with data literacy and proficient with data platform architecture patterns. The data platform is leveraging MS Azure services, including ingestion, staging, processing, and serving. The client now entering a phase of scaling Databricks solution, then we are looking for resources with relevant skills on this technology. The position is ideal for you if you have: 3+ years of experience as a Data Engineer Experience with Azure Databricks and Power BI Fluent in English (written and spoken) Joining Margo you can expect: Ability to work in an international consulting company on ambitious projects, Permanent contract or B2B cooperation, Benefits such as medical care and sports card, Co-finantrainings, certification exams and post-graduate studies, Internal training and the possibility of using our know-how, Possibility to use our library free of charge, Individual approach and development opportunities (career path planning, ability to change the project and position, possibility to get involved in outside-project activities with additional remuneration), Possibility to influence the shape of the company, openness to your ideas and willingness to implement them, Excellent working atmosphere, integration events. Data engineer, with proven skills with Databricks. Missions : support our dev team to develop our data pipelines & data transformation & data exposure services Actively Contribute to the continuous improvement of our dev patterns Document developments & unit testing Improve platform observability capabilities Key skills : Proficient in SQL, Python and PySpark At least 2 years of experience in Databricks, with proven experience deploying Unity Catalog solutions Proven experience working in Azure, and more specifically with the following services within Azure: Data Factory, SQL Database, Data Lake, Logic Apps Skilled in Data modelization, whatever the supporting tool. Comfortable with business-specific discussions.","[{""min"": 170, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,200,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,399,Regulatory Reporting IT Analyst / DevOps,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: üåçhybrid work model from Warsaw - 3 days HO, 2 days from the office üìù ASAP project start Responsibilities: Design and develop solutions for regulatory reporting on the Snowflake platform Collaborate with developers and SMEs to gather requirements and translate them into technical designs Build and maintain ETL pipelines and orchestration workflows Apply DevOps practices to ensure high code quality, automation, and efficient deployment Support testing, troubleshooting, and documentation of the implemented solutions Requirements: Minimum 3 years of a professional experience Solid experience with DevOps practices and tools Strong hands-on expertise in Snowflake Proficiency in Python development Experience designing and building ETL pipelines Familiarity with orchestration tools (e.g., Airflow, Prefect, etc.) Very good command of English (oral and written) Nice to have: Knowledge of DORA (DevOps Research and Assessment) principles Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 140, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,140,150,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,402,Data Engineer,dotLinkers,"Type of contract: contract of employment (UoP) / B2B Salary ranges: up to 24 000 PLN a month Working model: 100% remote Join our client, one of the leading logistics and transport solutions providers. About the role: Looking for a skilled Lead Data Engineer to drive the design and implementation of robust data systems, while effectively connecting technical teams with business goals. This role involves hands-on development, strategic planning, and cross-team collaboration. Responsibilities: Designing scalable data systems and tools to support analytics, modeling, and decision-making Building advanced data pipelines and platforms on Azure Collaborating closely with technical and business teams Establishing best practices and reusable standards in data engineering Engaging with international projects and external partners Requirements: 5+ years in IT, data engineering, or information systems Expertise in Azure, Spark (Scala/PySpark), Databricks, Kafka, Event Hubs, Python, Java, SQL/NoSQL Experience with DevOps tools and practices (CI/CD, Terraform, Kubernetes) Skilled in streaming data and big data environments Strong leadership and communication skills (English and Polish) Experience working with distributed teams The offer: Flexible remote work with occasional office visits Benefits include private healthcare, insurance, and a sports card High-end equipment Development budget for learning and growth","[{""min"": 20000, ""max"": 24000, ""type"": ""Net per month - B2B""}, {""min"": 20000, ""max"": 24000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,24000,Net per month - B2B
Full-time,Senior,B2B,Remote,403,Senior Data Analyst,Kodiak Hub,"Join Kodiak HubüêªüöÄ Senior Data Analyst ‚Äì Drive Insights for a Smarter, Sustainable Future Who We Are At Kodiak Hub , we‚Äôre revolutionizing how companies manage supplier relationships and elevate procurement strategies. Through our dynamic SaaS platform and international team spirit, we tackle today‚Äôs global supply chain challenges head-on. Guided by our values ‚Äî Caring, Curious, Cool, and Courageous ‚Äî we build a future where strategic, sustainable, and smart procurement is the standard. About the Role We‚Äôre looking for a Senior Data Analyst who‚Äôs passionate about turning complex data into actionable insights ‚Äî for both our internal teams and our global end-users. In this role, you‚Äôll bridge raw data and real-world decisions: optimizing Kodiak Hub‚Äôs SaaS platform, shaping our internal analytics landscape, and delivering smart reporting that drives user success. You‚Äôll collaborate cross-functionally, working closely with our Engineering, Product, and Customer Success teams to unlock the full power of data. What You‚Äôll Do Advanced Analytics : Dive deep into SQL/NoSQL data structures to extract, clean, and analyze key datasets. Insight for Action : Generate analytics that inform internal business development and enhance the end-user experience within the Kodiak Hub platform. Data Storytelling : Build dashboards and reports that tell compelling stories through tools like Power BI , Looker , or Tableau . Platform Optimization : Contribute to smarter, more intuitive analytics features within Kodiak Hub‚Äôs SaaS modules, driving greater value for users. Stakeholder Collaboration : Partner with Engineering, Product, and Customer Success to translate needs into smart, actionable data solutions. Cloud Proficiency : Work within a cloud-native environment (primarily Azure) , leveraging ready-pulled datasets to support analysis. What We‚Äôre Looking For Strong hands-on experience with SQL/NoSQL and database optimization. Expertise in Power BI and/or other visualization tools ( Looker, Tableau ). Familiarity with cloud-native environments, especially Azure (AWS knowledge a plus but not required). Experience translating business and product needs into actionable analytics. Understanding of SaaS platforms and how analytics can drive platform and user success. A collaborative, solutions-focused mindset that aligns with Kodiak Hub‚Äôs values . Excellent communication and data storytelling skills ‚Äî the ability to turn complexity into clarity. Why Join Kodiak? Impact at Scale : Help shape the future of sustainable supply chains and empower users worldwide. Team Spirit : Join a caring, curious, and courageous team with a passion for innovation. Growth Opportunities : Take a senior role with room to grow and make strategic impacts. Modern Ways of Working : Flexibility, ownership, and a commitment to your well-being. Ready to turn data into decisions that matter? Apply now and join us in building the future of smart, sustainable procurement!","[{""min"": 20000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20000,29000,Net per month - B2B
Full-time,Senior,B2B,Remote,405,Data Architect (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Data Architect , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company. This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. üöÄ Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Hadoop, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. üéØ What you'll need to succeed in this role: 5+ years of proven commercial experience in implementing, developing, or maintaining Big Data systems. Strong programming skills in Python or Java/Scala : writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity with Big Data technologies like Spark , Cloudera, Airflow , NiFi, Docker , Kubernetes , Iceberg , Trino or Hudi. Proven expertise in implementing and deploying solutions in cloud environments (with a preference for AWS ). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master‚Äôs or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. Fluent English (C1 level) is a must. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn , Instagram ).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Architecture,21000,31920,Net per month - B2B
Full-time,Mid,B2B,Hybrid,406,SAS/Oracle Data Warehouse Engineer,Onwelo Sp. z o.o.,"Poznaj Onwelo: Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Do≈ÇƒÖcz do projektu realizowanego dla klienta z bran≈ºy ubezpieczeniowej, gdzie bƒôdziesz czƒô≈õciƒÖ zespo≈Çu odpowiedzialnego za rozw√≥j narzƒôdzi wspierajƒÖcych Hurtowniƒô Danych. Projekt obejmuje budowƒô narzƒôdzi automatyzujƒÖcych, integracyjnych i raportowych, jak r√≥wnie≈º eksploracjƒô nowych technologii i podej≈õƒá architektonicznych. Tworzyƒá narzƒôdzia wspierajƒÖce pracƒô developer√≥w Hurtowni Danych (SAS, SAS Viya, Oracle i inne technologie) Budowaƒá API umo≈ºliwiajƒÖce integracjƒô r√≥≈ºnych system√≥w (REST, SAS, Office365) Projektowaƒá rozwiƒÖzania wspierajƒÖce wymianƒô danych pomiƒôdzy systemami (np. z wykorzystaniem Apache Kafka) Realizowaƒá POC nowych technologii i narzƒôdzi (np. SAS Viya, Exadata w chmurze Azure) Opracowywaƒá komponenty optymalizujƒÖce przetwarzanie danych i wydajno≈õƒá proces√≥w Projektowaƒá techniczne data mart-y i raporty BI (SAS Viya, Power BI) Monitorowaƒá aktywno≈õƒá u≈ºytkownik√≥w oraz tworzyƒá narzƒôdzia raportowe Wspieraƒá strategiczne projekty poprzez dostarczanie rozwiƒÖza≈Ñ technologicznych Masz do≈õwiadczenie w projektowaniu rozwiƒÖza≈Ñ dla Hurtowni Danych i system√≥w BI Znasz narzƒôdzia i ≈õrodowisko SAS oraz SAS Viya (DI Studio, Enterprise Guide, SAS Studio, Visual Analytics) Pracujesz z bazami danych Oracle oraz jƒôzykiem PL/SQL (mile widziane do≈õwiadczenie z Oracle Exadata) Potrafisz pracowaƒá z narzƒôdziami CI/CD (Git, Bitbucket, Bamboo, JIRA) Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 700, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Data Engineering,700,1000,Net per day - B2B
Full-time,Senior,B2B,Remote,407,Senior Cloud Data Engineer (GCP),Future Processing,"Do naszej linii biznesowej Data Solutions poszukujemy osoby na stanowisko Senior Cloud Data Engineer ze znajomo≈õciƒÖ GCP masz min. 5 lat do≈õwiadczenia w IT, w tym min.3,5 roku w pracy z danymi w chmurze GCP(potwierdzone projektami komercyjnymi wdro≈ºonymi na produkcje), budowa≈Çe≈õ(a≈õ) i utrzymywa≈Çe≈õ(a≈õ)hurtownie/data lake w BigQuery, ≈ÇƒÖczƒÖc wiele ≈∫r√≥de≈Ç danych, korzystasz zSQLiPythonna poziomie zaawansowanym, znasz metodyki i stosujesz biegleGit oraz CI/CD, tworzysz i optymalizujesz rozwiƒÖzania przetwarzajƒÖce dane (ETL,ELT, itp.) poprzedzone projektem technicznym oraz alternatywami rozwiƒÖza≈Ñ, masz praktyczne do≈õwiadczenie w przetwarzaniu danych przy u≈ºyciuDataflow / Dataprocoraz orkiestracjiAirflow (Cloud Composer), monitoring, diagnostyka oraz rozwiƒÖzywanie problem√≥w w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanowaƒá infrastrukturƒô oraz obliczyƒá jej koszt, pracujesz w duchuDevSecOps & FinOps, znasz koncepcjeBigLake / Lakehouse / Data Mesh, znaszarchitekturƒô SMP oraz MPPwraz z przyk≈Çadami rozwiƒÖza≈Ñ opartych o te architektury, potrafisz zaplanowaƒá infrastrukturƒô GCP, estymowaƒá i optymalizowaƒá jej koszty, masz do≈õwiadczenie wmigracji rozwiƒÖza≈Ñ on-premise do chmuryi w ochronie danych (IAM, DLP, GDPR), swobodnie wsp√≥≈Çpracujesz z klientemi interdyscyplinarnymi zespo≈Çami, pos≈Çugujesz siƒôj. angielskimna poziomie ≈õredniozaawansowanym (min. B2). odpowiedzialno≈õƒá end-to-endza rozwiƒÖzania data-platformowe tworzone wsp√≥lnie z zespo≈Çem, tworzenie i optymalizacjƒô potok√≥wETL/ELT w GCP(BigQuery, Dataflow, Dataproc, Cloud Composer), tworzenie i modyfikowanie dokumentacji, budowanie i utrzymywaniekatalogu oraz modeli danychzgodnie z najlepszymi praktykami Data Governance, analizƒô wymaga≈Ñ biznesowych i dob√≥r optymalnych rozwiƒÖza≈Ñ technologicznych, analizowanie potencjalnych zagro≈ºe≈Ñ, monitoring, diagnostykƒô i FinOps-owe optymalizacjekoszt√≥w chmury.","[{""min"": 135, ""max"": 200, ""type"": ""Net per month - B2B""}]",Data Engineering,135,200,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,408,Sr Cloud Engineer - Data Integration Platform,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Sr. Cloud Engineer - Data Integration Platform The Data, Analytics and AI Organization of the Digital Transformation Unit of Bayer Consumer Health operates a global transactional data integration backbone which is used to connect internal and external applications and systems into a uniform experience for our customers, consumers and employees. The backbone (platform) provides composable infrastructure modules, development frameworks, CI/CD automations and developer tooling to internal and external software engineering teams that build integrations on top of the platform. As Sr. Cloud Engineer - Data Integration Platform you are responsible for planning, designing and implementing platform capabilities, steering external software engineering teams and overseeing operations of the cloud infrastructure. To do so, you will engage with Enterprise Architects, Integration Architects and Software Engineers from Bayer and partner companies as well as incorporate industry best practices and trends into your designs. Key Tasks & Responsibilities: Contribute to platform roadmap together with Enterprise and Integration Architects, derive requirements for missing capabilities and actively shape the future of the platform Plan, design and implement capabilities on Amazon Web Services and/or the GitHub ecosystem based on derived requirements in line with security guidelines & non-functional requirements like cost efficiency, elasticity & scalability, maintainability, developer experience and documentation guidelines. Engage and consult with software engineering teams building integrations on the platform on design, best practices and integration patterns Document and share newly built capabilities within the corporate software engineering community Observe and assess platform compliance and implement solutions to ensure platform operations within compliance guidelines such as security or regulatory aspects Engage with technology and implementation partners to overcome issues if required Engage with central cloud teams, network and security teams, cyber security teams to overcome issues if required and follow company best practices Qualifications & Competencies (education, skills, experience): Master‚Äôs degree in computer science, Engineering, or similar Proficient knowledge of distributed systems and their challenges such as eventual consistency, horizontal scaling and parallel processing, consensus, operations and maintenance of a microservices architecture Good knowledge of data integration technologies and concepts such as message brokers, streaming systems, batch processing, API design and - management 3-5 years of working experience in AWS cloud development with focus on serverless services such as AWS Lambda, AWS SQS, AWS DynamoDB, AWS API Gateway and augmenting services such as AWS CloudWatch, AWS IAM. Experience with AWS networking services, AWS DocumentDB (MongoDB) and AWS container services are a plus. Proficient knowledge of infrastructure as code tooling (preferred: Terraform) to automate infrastructure deployment. Additional coding skills such as shell scripting for task automation are a plus. 2-3 years of experience of software development with JavaScript/ECMAScript (preferred: server-side development with NodeJS). Good knowledge of the GitHub ecosystem including the core VCS, GitHub Actions for CI/CD and working with tickets/issues and pull requests Problem solving and analysis skills, combined with profound business judgement as well as good documentation and communication skills (active listening, consulting, challenging). Intercultural awareness and willingness to travel from time to time. Fluent in English, both written & spoken What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,28500,Gross per month - Permanent
Full-time,Mid,B2B,Remote,410,"BI Developer (Snowflake, Matillion)",Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä Design, build, and develop data warehouses based on Snowflake; Create and optimize ETL/ELT processes using Matillion; Integrate data from various sources (databases, APIs, files); Develop reports and dashboards in Power BI for internal clients; Maintain, monitor, and further develop existing BI solutions; Collaborate with project teams and business stakeholders to gather and analyze requirements; Participate in data migration from legacy systems (e.g., Oracle, SSIS) to Snowflake; Ensure high-quality technical and business documentation; Implement best practices for data management, security, and version control; Actively participate in Agile team meetings (e.g., daily stand-ups, sprint planning, retrospectives); 3-5 years' experiencea BI Developer role; At least 2 years‚Äô hands-on with Snowflake; Strong experience withMatillion; Solid skills inPower BI; Eager to work as afull-stack BI Developer(both backend and frontend); Strong English skills (min. C1 level, daily communication with international clients); Proactive and creative - skills to drive improvements and engage with both technical and non-technical stakeholders; Background inManaged Servicesdelivery models - you know how to take end-to-end ownership of BI solutions, ensuring their reliability, scalability, and alignment with client needs throughout the entire lifecycle; Experience working inAgileteams; Strong documentation skills and a knack for business analysis. Experience withOracle(we‚Äôre migrating to Snowflake, but legacy knowledge is a plus); Experience withSSAS/SSIS/SSRS; Familiarity withVisual Studio; Understanding ofversion control concepts(branching, merging, pushing; Git integrated with Matillion). Background in procurement, supply chain, or business data analysis related to orders and internal corporate stakeholders; By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Fully remotework or in our office in Wroc≈Çaw; B2B Contract: 120 ‚Äì 140 PLN net/hour + VAT Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories.","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Senior,B2B,Remote,411,Senior Data Scientist,hubQuest,"We are a team of tech enthusiasts on a mission to bring together the best minds in IT services and analytics. Our goal? To create cutting-edge IT and Analytical Hubs that empower our partners to become truly data-driven organizations. Tackle real-world challenges with advanced analytics and machine learning Lead impactful projects that influence decision-making across global markets Flexible remote or hybrid work model (with a modern office in Warsaw) Join a diverse and experienced international team No red tape ‚Äì just real tech challenges, autonomy, and ownership Enjoy private medical care, Multisport, access to online learning platforms and certifications Supportive, collaborative, and relaxed work culture About the Team What You‚Äôll Do Take technical ownership of an end-to-end data science project ‚Äì from business understanding to deployment Develop predictive and prescriptive models that support strategic and operational goals Design, test, and implement scalable data pipelines and modeling workflows (Python, PySpark) Work closely with stakeholders to identify opportunities for impact through data science Lead experimentation, model validation, and performance monitoring in production Contribute to the design of robust and maintainable analytics architecture usingAzure cloud services What We‚Äôre Looking For Hands-on experience with the Azure ecosystem(e.g., Azure Databricks, Azure ML, Azure Data Factory, Azure DevOps) AdvancedPythonandPySparkskills, including writing production-level code Experience with large-scale data processing and analytics Good understanding of CI/CD pipelines and deployment automation Strong background in building and deploying machine learning models 5+ years of data science experience in production environments 2+ years in senior or lead roles with proven project ownership Strong portfolio of deployed ML models used in production at scale Ability to independently lead projects from conception to launch Excellent communication and collaboration skills across technical and non-technical teams Adaptability to new tools, processes, and environments Self-driven learner with a proactive approach to skill development Ready to take ownership and build data science solutions that drive global decisions?Join us and help shape the future of data-driven transformation. Please add to your CV the following clause: ""I hereby agree to the processing of my personal data included in my job offer by hubQuest sp√≥≈Çka z ograniczonƒÖ odpowiedzialno≈õciƒÖ located in Warsaw for the purpose of the current recruitment process.‚Äù If you want to be considered in the future recruitment processes please add the following statement: ""I also agree to the processing of my personal data for the purpose of future recruitment processes.‚Äù","[{""min"": 28500, ""max"": 37000, ""type"": ""Net per month - B2B""}]",Data Science,28500,37000,Net per month - B2B
Full-time,Mid,B2B,Remote,412,Reporting Developer (Power BI/Tableau),Britenet,"About the role Project carried out for the lottery industry.The work is conducted remotely, with occasional visits to the office in Warsaw. Our expectations Minimum 3 years of experience in a similar position Proficient inPower BIand familiar withTableau Basic knowledge of project management principles, especially using Tableau/Power BI Understanding of best practices in data visualization Experience working with SQL for data extraction and manipulation Experience with Microsoft SQL Server and ETL processes Knowledge of data preparation, modeling, and visualization techniques Strong communication and teamwork skills Ability to translate business requirements into effective solutions Willingness to learn and adapt to new technologies and tools Very good command of English (minimum B2 level) Openness to occasional on-site work in the Warsaw office Main responsibilities Managing projects related to PowerBI/Tableau implementation, data visualization, and analytics Collaborating with other teams to gather requirements, design solutions, and deliver Tableau dashboards and reports Analyzing data trends and providing strategic insights to support business decision-making Working with team members on project tasks and deliverables Participating in project meetings and contributing to project status updates","[{""min"": 90, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,90,140,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,415,Tester Data&BI,summ-it,"Uprzejmie informujemy, ≈ºe z uwagi na sezon urlopowy czas odpowiedzi w procesie rekrutacji mo≈ºe siƒô wyd≈Çu≈ºyƒá. Je≈õli chcesz nauczyƒá siƒô nowych technologii, a nastƒôpnie pracowaƒá w projektach dla marek znanych na ca≈Çym ≈õwiecie, a jednak wciƒÖ≈º w ramach niekorporacyjnej struktury, to idealne miejsce dla Ciebie! W summ-it stwarzamy przestrze≈Ñ na zg≈Çaszanie swoich pomys≈Ç√≥w i dbamy o Tw√≥j rozw√≥j. Nasi pracownicy biorƒÖ udzia≈Ç w konferencjach bran≈ºowych i wydarzeniach dotyczƒÖcych szeroko pojƒôtej tematyki IT, a tak≈ºe udoskonalajƒÖ swoje umiejƒôtno≈õci dziƒôki r√≥≈ºnego rodzaju szkoleniom i warsztatom. Pracujemy z bazami danych o ≈ÇƒÖcznej wielko≈õci liczonej w PB, i optymalizujemy systemy walczƒÖc o ka≈ºdƒÖ ms. Obecnie zarzƒÖdzamy ponad 10 000 system√≥w baz dla naszych klient√≥w. Zatrudnienie na podstawie: umowy o pracƒô, umowy B2B, umowy zlecenie Pracƒô w atmosferze kole≈ºe≈Ñstwa i zaufania ‚Äì w ankiecie satysfakcji ponad 92% pracownik√≥w zgadza siƒô z tym stwierdzeniem Odpowiednie wsparcie i wsp√≥≈Çpracƒô w swoim zespole ‚Äì w ankiecie satysfakcji 100% pracownik√≥w zgadza siƒô z tym stwierdzeniem Elastyczne godziny pracy oraz pracƒô w modelu hybrydowym Pracƒô z biura w centrum Poznania Dostƒôp do najnowszych technologii IT Mo≈ºliwo≈õƒá rozwoju w miƒôdzynarodowej firmie Szkolenia zewnƒôtrzne i wewnƒôtrzne: Miƒôkka ≈õroda, Bezpieczne czwartki, Science Friday Spotkania firmowe: Summer Party, Winter Party, AllHands, Talk to Your Boss, Lokalne ≈õrody, summ‚Äëitowe ≈õniadania Programy doceniania pracownik√≥w: summ-it heores i nagrody za przyznane kudosy Program polece≈Ñ pracowniczych Mo≈ºliwo≈õƒá do≈ÇƒÖczenia do benefit√≥w (opieka medyczna, karta Multisport, ubezpieczenie grupowe) Pracƒô w zr√≥wnowa≈ºonym zespole ‚Äì 3 generacji: X, Y, Z Przeprowadzanie test√≥w w obszarze danych (Azure Databricks, Azure Data Factory, Azure Synapse, Azure Analysis Services, Power BI, MDS) zgodnie z opisem w User stories Rejestrowanie wynik√≥w test√≥w w Azure DevOps zgodnie z procesem Analiza wyniku test√≥w i informacja zwrotna dla developer‚Äô√≥w i SME Opracowywanie scenariuszy testowych (unit, integration, regression) Analiza test√≥w, identyfikacja i wdra≈ºanie automatyzacji test√≥w oraz usprawnie≈Ñ w procesach QA Proponowanie usprawnie≈Ñ w zakresie test√≥w Identyfikacja wzorc√≥w na powtarzajƒÖce siƒô b≈Çƒôdy Min. 3 lata do≈õwiadczenia w testowaniu danych Znajomo≈õƒá Azure Databricks (notebook, job, cluster Spark) Znajomo≈õƒá Azure Data Factory (pipeline‚Äôs datasets, linked services, monitor) Znajmo≈õƒá Azure Synapse (Dedicated pool) Znajomo≈õƒá modeli tabularnych (Analysis Services) i serwisu Power BI (data lineage, model semantyczny, po≈ÇƒÖczenia) Bieg≈Ço≈õƒá w SQL (weryfikacja jako≈õci danych, przygotowywanie zapyta≈Ñ testowych) Zrozumienie proces√≥w ETL Znajomo≈õƒá narzƒôdzi do automatyzacji test√≥w i CI/CD (w szczeg√≥lno≈õci Azure DevOps, TestPlans) Znajomo≈õƒá jƒôzyka angielskiego na poziomie min. B2 Dziƒôkujemy za zainteresowanie naszƒÖ ofertƒÖ i nades≈Çanie aplikacji. Uprzejmie informujemy, ≈ºe skontaktujemy siƒô z wybranymi kandydatami.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 10000, ""type"": ""Gross per month - Permanent""}]",Unclassified,10000,13000,Net per month - B2B
Full-time,Senior,B2B,Remote,421,Senior Backend Engineer (with Data Science Skills) at PeakData,PeakData,"PeakData provides AI-powered market intelligence to optimize drug launch execution and resource allocation for pharmaceutical companies. Our platform delivers actionable insights on healthcare professionals (HCPs) and healthcare organizations (HCOs), empowering commercial leaders with real-time, data-driven decision-making. We're looking for aSenior Engineerwith strongdata science capabilitiesto join our Data Platform team. In this role, you‚Äôll design and build cloud-native data solutions that support large-scale processing, analytics, and AI-powered automation across our platform. This is ahands-on, senior-levelrole. You will be expected to work independently, own end-to-end pipelines and infrastructure, and drive initiatives forward both individually and within the team. You should have a strong foundation inPython, SQL, AWS, and/orGCP, with experience using or integratingLLMsinto data workflows. You‚Äôll work with and expand upon: Pythonfor data pipelines and automation SQL(PostgreSQL) for transformation and analytics AWS (S3, Glue, Lambda, ECS, Bedrock)as primary cloud environment GCP (Vertex AI)for select workloads and integrations Medallion architecturewith RAW/CLEANED/CURATED layers LLM integrations for automation, enrichment, and insight generation Data quality frameworks and orchestration tools (e.g., Argo) Design, implement, and maintain scalable and efficient data pipelines across AWS and GCP Build data products and services supporting both internal analytics and client-facing insights Own ETL/ELT workflows from ingestion to curation Implement observability and alerting for pipeline health and data integrity Integrate LLMs into workflows to support enrichment, automation, or intelligent data handling Act as a technical lead for data engineering projects, driving execution independently Collaborate cross-functionally with Data Science, Product, and Engineering teams Contribute to architectural decisions and long-term data platform evolution Champion best practices for performance, security, and scalability Apply data science techniques where appropriate (e.g., clustering, statistical inference) Prototype and validate LLM-powered solutions using tools like AWS Bedrock or Vertex AI Use prompt engineering and evaluation frameworks to fine-tune LLM interactions Help bridge engineering and AI innovation across the platform 6+ years of experience indata engineeringor back-end systems with data-heavy workloads Strong hands-on skills withPythonandSQL Deep understanding ofAWScloud data tooling (S3, Lambda, Glue, Step Functions, etc.) Working experience withGCPservices, especially BigQuery and Vertex AI Exposure toLLMsand how they integrate into data workflows Experience buildingdata pipelinesat scale with monitoring and alerting Ability to work independently and take ownership of technical topics Experience withArgo, Airflowor similar orchestration frameworks Familiarity withIaC tools(Terraform) for deploying infrastructure Experience withdata quality monitoring, validation frameworks, or anomaly detection Previous work in healthcare, life sciences, or regulated data environments Proactive: You take initiative and don‚Äôt wait for tasks to be assigned Autonomous: You can own projects from design to production with minimal oversight Curious: You explore new approaches (especially LLMs/AI) and bring them to the table Collaborative: You work well with cross-functional teams Customer-aware: You understand the real-world impact of your pipelines and models Purpose-driven work: support pharmaceutical innovation and better patient outcomes Ownership: real autonomy in shaping our data systems and how they scale Innovation: work on LLM integration and next-gen data workflows A collaborative, fast-moving environment Competitive compensation Access to both AWS and GCP ecosystems in production If you're a hands-on data engineer who enjoys owning end-to-end systems, loves solving real business problems, and thrives in a hybrid cloud + AI environment ‚Äî we want to talk to you.","[{""min"": 28000, ""max"": 36000, ""type"": ""Net per month - B2B""}]",Data Science,28000,36000,Net per month - B2B
Full-time,Mid,B2B,Remote,422,Data Migration Specialist with German,in4ge sp. z o.o.,"Key Responsibilities: Performing data migration from Dynamics 365 system. Analyzing data migration requirements. Creating a comprehensive data migration plan. Preparing and cleaning data (data remediation) to ensure high data quality. Testing and verifying migrated data. Executing and monitoring the end-to-end data migration process. Minimum 1 year of experience working with Dynamics 365, especially in the area of data migration. Strong knowledge of SQL and Microsoft Excel. Good understanding of business processes. Fluent English and German (C1) and ability to work effectively in a distributed, international team. Fully remote work with flexible working hours -EMEA Timezone. Long-term collaboration on B2B contract. Opportunity to work on complex cloud projects for international clients. Professional growth in a highly skilled and supportive team. Collaborative and open working culture.","[{""min"": 18000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Unclassified,18000,29000,Net per month - B2B
Full-time,Mid,B2B,Remote,425,Insurance Data Analyst,Link Group,"Opis stanowiska: Poszukujemy do≈õwiadczonego Insurance Data Analyst do pracy w miƒôdzynarodowych projektach technologicznych. Osoba na tym stanowisku bƒôdzie odpowiedzialna za analizƒô danych, przygotowanie wymaga≈Ñ biznesowych oraz wsp√≥≈Çpracƒô z klientami z sektora ubezpiecze≈Ñ. Szukamy specjalisty o podej≈õciu technologicznym, kt√≥ry wesprze nasze zespo≈Çy w realizacji ambitnych projekt√≥w. Zakres obowiƒÖzk√≥w: Analiza biznesowa oraz zbieranie wymaga≈Ñ funkcjonalnych i niefunkcjonalnych Budowanie i utrzymywanie relacji z klientami oraz onsite koordynatorami Wsparcie w planowaniu i realizacji projekt√≥w Analiza, przygotowanie i eksploracja danych Tworzenie raport√≥w oraz wizualizacji danych przy u≈ºyciu narzƒôdzi no-code/low-code Przestrzeganie standard√≥w jako≈õci oraz wspieranie inicjatyw organizacyjnych Wsp√≥≈Çpraca w ≈õrodowisku Agile (Scrum, Kanban) Wymagania: Minimum 3 lata do≈õwiadczenia w pracy na podobnym stanowisku w ≈õrodowisku korporacyjnym Do≈õwiadczenie w bran≈ºy ubezpieczeniowej (min. 2 lata, aktualne) Bieg≈Ça znajomo≈õƒá SQL oraz technik projektowania relacyjnych baz danych, data mart√≥w, hurtowni danych i data lakes Umiejƒôtno≈õci w zakresie czyszczenia, eksploracji i analizy danych Znajomo≈õƒá narzƒôdzi do wizualizacji danych Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (w mowie i pi≈õmie) Wykszta≈Çcenie wy≈ºsze techniczne (In≈ºynieria, IT, Nauki ≈öcis≈Çe) Mile widziane: Znajomo≈õƒá OpenL Tablets Certyfikaty potwierdzajƒÖce umiejƒôtno≈õci (Java, Spring, SQL, AWS lub Azure Cloud, Angular, React) Do≈õwiadczenie w bezpo≈õredniej pracy z klientem","[{""min"": 95, ""max"": 125, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,95,125,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,426,Data Engineer,Tesco Technology,"Tesco Technology is multi-functional and specialist team that drives operational excellence of services improves scale for our systems and processes globally and creates business leading capabilities. We are an agile team of an industry-leading team of engineers. We create the future continuous integration and delivery tools for Colleague and Customer & Loyalty areas, solving problems, and developing new features through quality, scalable, performant, and maintainable technical solutions. The solutions that we are responsible for will have a global reach, impacting hundreds of thousands of Tesco colleagues worldwide. We operate in a DevOps philosophy. We take responsibility for the software through its entire lifecycle. We practice continuous integration, delivery, and support of our code through to production and beyond. As Tech Hub we cooperate within the group of Tesco Technology Hubs located in the UK, Poland, Hungary, and India. We always welcome conversations about flexible working, so feel free to talk to us during your application about how we can support you.We value connecting, collaborating, and innovating with our colleagues in person. At Tesco Technology, we work in a hybrid model. This role requires you to be based in or near Krak√≥w, as we currently meet in the office three days a week. The Data Engineering department at Tesco Technology is at the forefront of data processing within the retail and technology industry. This vital department handles a range of responsibilities, including: Analyzing order and delivery data to optimize logistics processes and enhance delivery efficiency. Managing critical data related to customer orders, suppliers, and products to ensure the seamless flow of our fulfillment operations. Upholding data integrity and security during the processing of order and delivery-related information. As we continue to expand, we are actively seeking a skilled Data Engineer to join our team of analytics experts. In this role, you will take charge of expanding and refining our data and data pipeline architecture. Additionally, you will be instrumental in optimizing data flow and collection to cater to the needs of cross-functional teams. Our ideal candidate is an experienced data pipeline builder and data enthusiast who relishes the opportunity to optimize data systems and construct them from the ground up. As a Data Engineer, you will collaborate closely with software developers, database architects, data analysts, and data scientists on various data-driven initiatives. You will play a crucial role in ensuring that the optimal data delivery architecture remains consistent across all ongoing projects. This role calls for a high level of self-direction and the ability to effectively support the data requirements of multiple teams, systems, and products. If you are enthusiastic about the prospect of optimizing, and possibly even redesigning, our company's data architecture to support our next generation of products and data initiatives, we encourage you to apply and be part of our dynamic team shaping the future of our data operations! Responsibilities Create and maintain optimal data pipeline architecture Assemble large complex data sets that meet functional / non-functional business requirements. Identify design and implement internal process improvements: automating manual processes optimising data delivery re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction transformation and loading of data from a wide variety of data sources Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition operational efficiency and other key business performance metrics. Work with stakeholders including the Executive Product Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure Create data tools for analytics and data scientist team members that assist them in building and optimising our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Mandatory skills: Data Processing: Apache Spark - Scala or Python Data Storage: Apache HDFS or respective cloud alternative Resource Manager: Apache Yarn or respective cloud alternative Lakehouse: Apache Hive/Kyuubi or alternative Workflow Scheduler: Airflow or alternative Nice to have skills: Functional programming Apache Kafka Kubernetes Stream processing CI/CD Unsure if you fit all the criteria? Apply and give us the chance to evaluate your potential ‚Äì you could be the perfect fit! We value flexibility at Tesco; therefore, this position is also available for candidates who are interested in working part time ‚Äì about 120 hours a month or more. Please let us know what would work for you. Hybrid work We know life looks a little different for each of us. That‚Äôs why at Tesco, we always welcome chats about different flexible working options. Some people are at the start of their careers, some want the freedom to do the things they love. Others are going through life-changing moments like becoming a carer, adapting to parenthood, or something else. So, talk to us throughout your application about how we can support.This role requires you to be based in or near Krak√≥w, as you will spend 60% (3 days) of your week collaborating with colleagues at our office locations or local sites and the rest remotely. Benefits Tesco is a diverse and exciting employer, dedicated to being #aplacetogeton, providing career-defining opportunities to all of our colleagues. If you choose to join our business, we will provide you with (for all): MacBook as your tool for work Learning opportunities - certified technical training and learning platforms like Udemy, Pluralsight and O‚Äôreily Referral Bonus Sports activities with a personal trainer in the office Benefits for colleagues on employment of contract only: Additional 4 days of paid leave to support your well-being and family life Up to 20% yearly salary bonus ‚Äì based on both individual and business performance Private healthcare (LuxMed) Cafeteria & Multisport Supporting those, who are not yet eligible for full holiday entitlement, by expanding their pool from 20 to 25 days Relocation Help IP Tax Deductible Costs If that sounds exciting, then we'd love to hear from you. Tesco is committed to celebrating diversity and everyone is welcome at Tesco. As a Disability Confident Employer, we‚Äôre committed to providing a fully inclusive and accessible recruitment process, allowing candidates the opportunity to thrive and inform us of any reasonable adjustments they may require.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 18500, ""max"": 26000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,30000,Net per month - B2B
Full-time,Senior,B2B,Remote,429,Data Engineer MS Fabric,1dea,"Dla jednego z du≈ºych klient√≥w poszukujemy osoby do roli: Data Engineer MS Fabric! Warunki zaanga≈ºowania: Lokalizacja: 100% zdalnie Start: ASAP (akceptujemy kandydatury z max 1msc okresem wypowiedzenia) Stawka (ustalana indywidualnie): 130 - 160 PLN net / h Zaanga≈ºowanie: B2B (outsourcing z 1dea), full-time, d≈Çugofalowo Minimum 3 lata do≈õwiadczenia jako Data Engineer, Znajomo≈õƒá technologii: Azure Data Factory, Azure, PySpark, Microsoft Fabric, Solidne podstawy w zakresie modelowania danych oraz architektury hurtowni danych. Jƒôzyk angielski na poziomie min. B2 Przejrzysty model wsp√≥≈Çpracy: Zatrudnienie przez 1dea na podstawie umowy B2B Stabilne i bezpieczne ≈õrodowisko pracy: Do≈ÇƒÖczysz do firmy z solidnƒÖ pozycjƒÖ na rynku Nowoczesne wyposa≈ºenie: Firma zapewnia nowoczesny sprzƒôt, oprogramowanie i konfiguracjƒô Elastyczny czas pracy: Mo≈ºliwo≈õƒá pracy w elastycznych godzinach Praca zdalna: mo≈ºliwa w 100% Profesjonalne doradztwo i wsparcie: Profesjonalne doradztwo i wsparcie w rozwoju kariery od do≈õwiadczonego zespo≈Çu specjalist√≥w 1dea Przyjemna atmosfera w zespole: Cenimy sobie kole≈ºe≈Ñsko≈õƒá, otwarto≈õƒá, szacunek, wzajemnƒÖ pomoc i wsparcie w rozwijaniu kompetencji zar√≥wno w≈Çasnych, jak i koleg√≥w i kole≈ºanek z zespo≈Çu Kultura kreatywno≈õci: Wspieramy kulturƒô kreatywno≈õci. Ka≈ºdy cz≈Çonek zespo≈Çu ma mo≈ºliwo≈õƒá proponowania w≈Çasnych pomys≈Ç√≥w i rozwiƒÖza≈Ñ, a jego g≈Ços jest zawsze brany pod uwagƒô","[{""min"": 130, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,130,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,430,ETRM Data Scientist,INFOPLUS TECHNOLOGIES,"We are looking for a highly skilled ETRM Data Scientist with 7+ years of experience to join our advanced analytics team. The ideal candidate will have deep expertise in time-series forecasting, scalable ML systems, and MLOps, with hands-on experience in energy trading (ETRM) environments. This is a high-impact role for someone passionate about building intelligent solutions in a cloud-native setting. Mandatory Skills Data Science & Machine Learning Expertise in time-series forecasting , predictive modelling , and deep learning . Experience with ML algorithms such as ARIMA , LSTM , Prophet , Linear Regression , Random Forest . Strong proficiency in scikit-learn , XGBoost , Darts , TensorFlow , PyTorch , Pandas , and NumPy . Proven knowledge of ensemble techniques ( stacking , boosting , bagging ) for robust model design. Ability to optimize and retrain production ML models based on evolving data and business needs. MLOps Implementation Proficiency in Python-based MLOps frameworks for automated pipelines, monitoring, and retraining. Hands-on experience with Azure Machine Learning Python SDK : Designing parallel model training workflows. Utilizing distributed computing for large-scale data processing. Big Data Analytics Strong experience with PySpark for distributed data processing and large-scale analytics. Azure Cloud Expertise Azure Machine Learning: Model deployment, training orchestration, lifecycle management. Azure Databricks: Data engineering and collaborative development using PySpark/Python. Azure Data Lake: Managing scalable storage and analytics pipelines for large datasets. Preferred Skills K-Means Clustering for segmentation and pattern analysis. Bottom-Up Forecasting for hierarchical business insight generation. Azure Data Factory for pipeline orchestration and ETL integration. Basic understanding of power/energy trading and ETRM systems. Exposure to Generative AI (GenAI) such as GPT or similar technologies.","[{""min"": 32000, ""max"": 36800, ""type"": ""Net per month - B2B""}]",Data Science,32000,36800,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,432,Senior Data Science/AI Engineer,N-iX,"(3767) About client: Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management.Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact. Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company‚Äôs R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 22164, ""max"": 25859, ""type"": ""Net per month - B2B""}, {""min"": 18470, ""max"": 21056, ""type"": ""Gross per month - Permanent""}]",Data Science,22164,25859,Net per month - B2B
Full-time,Mid,B2B,Remote,433,Data Scientists with Generative AI (mid/senior),Yosh.AI,"We are excited to announce an open position for: Data Scientists with Generative AI (mid/senior) - Location: Warsaw/remote We are looking for a hands-on generative AI engineer to architect, build, and deploy the next generation of autonomous and agentic AI systems. In this role, you will bridge the gap between rapid prototypes and robust, production-ready solutions that solve complex business challenges for our enterprise clients. You will be at the forefront of developing innovative technologies on a global scale, leveraging the full power of Google's Generative AI stack, LangChain, and other advanced models to create solutions that think, reason, and act. This is a unique opportunity to own end-to-end projects and work with a team of passionate experts, in close collaboration with Google, to drive real-world impact. Key Responsibilities: Architect and build enterprise-grade, agentic AI systems and conversational agents Engineer and deploy scalable AI/ML solutions on Google Cloud Platform (GCP) Develop and maintain serverless systems and containerized Python REST APIs Leverage LLMs to perform deep analysis on structured and unstructured data, enhancing our big data analysis platforms Drive innovation by rapidly prototyping new solutions and championing out-of-the-box thinking Required Experience: Proven experience building and deploying AI/ML models in a production environment Excellent programming skills in Python and experience building and deploying REST APIs Hands-on experience with a major cloud platform (GCP, AWS, or Azure). Practical experience with LLM orchestration frameworks Deep knowledge of Generative AI concepts Understanding of machine learning concepts Proficiency with version control systems (Git) Highly Desirable: Specific, in-depth experience with the Google Cloud Platform (GCP) AI/ML stack Experience designing and building fully autonomous or agentic AI systems Knowledge of technologies related to LLM models (e.g., LangChain, ADK, Vertex AI) Practical experience with Conversational AI platforms (e.g., Google Dialogflow CX/Playbooks) Experience with containerization technologies, specifically Docker Salary Range: 10.000-20.000 PLN gross per month (full time contract equivalent, depending on experience) We offer: Opportunity for professional development in the area of GenAI Private Medical insurance Multisport card Google certification paths Hybrid or remote location - our office is located in the center of Warsaw Cooperation with a great team of energetic and open-minded people","[{""min"": 10000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Science,10000,20000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,436,Middle/Senior Data Engineer,N-iX,"#3682 We are seeking aMiddle/Senior Data Engineerwith proven expertise inAWS, Snowflake, and dbtto design and build scalable data pipelines and modern data infrastructure. You'll play a key role in shaping the data ecosystem, ensuring data availability, quality, and performance across business units. 4+ years of experience in Data Engineering roles. Experience with theAWScloud platform. Proven experience withSnowflakein production environments. Hands-on experience building data pipelines usingdbt. Pythonskills for data processing and orchestration. Deep understanding of data modeling and ELT best practices. Experience with CI/CD and version control systems (e.g., Git). Strong communication and collaboration skills. Strong experience withSnowflake(e.g., performance tuning, storage layers, cost management) Production-level proficiency withdbt(modular development, testing, deployment).. Experience developingPythondata pipelines. Proficiency in SQL (analytical queries, performance optimization). Experience with orchestration tools like Airflow, Prefect, or Dagster. Familiarity with cloud platforms (e.g., GCP, or Azure). Knowledge of data governance, lineage, and catalog tools. Experience in working in Agile teams and CI/CD deployment pipelines. Exposure to BI tools like Tableau or Power BI. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 110, ""max"": 195, ""type"": ""Net per hour - B2B""}, {""min"": 81, ""max"": 162, ""type"": ""Gross per hour - Permanent""}]",Data Engineering,110,195,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,437,üëâ Data & AI Enterprise Architect,Xebia sp. z o.o.,"üü£You will be: leading and inspiring cross-functional architecture efforts across our client‚Äôs organization, filling an internal strategic role designed to bring added value to the client by aligning data architecture with business innovation and long-term growth, acting as a thought leader and enabler, supporting dedicated architects and teams across multiple domains, including: AI & ML Projects (40‚Äì60%), B2B & B2C E-commerce Platforms (20%), DataOps & Data Engineering (20%). üü£Your profile: proven experience as an Enterprise or Lead Data Architect in complex, enterprise-scale environments, deep expertise in Microsoft Azure and Databricks, strong understanding of data architecture principles, data governance, and modern data platforms, ability to work across business and technical domains, with a focus on value creation and business impact, willingness to occasionally work on site in Amsterdam, very good command of English (min. C1). üü£Nice to have: familiarity with technologies used across the client‚Äôs ecosystem, such as: Salesforce, Event Hubs / Kafka, Contentful or other headless CMS platforms, experience in customer-facing roles, pre-sales, or innovation consulting.in. Work from the European Union region and a work permit are required. üü£Recruitment Process: CVreview ‚ÄìHRcall ‚ÄìTechnical Interview‚ÄìClientInterview (with Live-coding) ‚ÄìHiring ManagerInterview ‚ÄìDecision üéÅBenefits üéÅ ‚úçDevelopment: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏èWe are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 43500, ""type"": ""Net per month - B2B""}, {""min"": 26850, ""max"": 35500, ""type"": ""Gross per month - Permanent""}]",Data Architecture,33500,43500,Net per month - B2B
Full-time,Senior,B2B,Remote,438,Senior Backend Engineer (Cloud & Data Infrastructure Focus) at PeakData,PeakData,"PeakData provides AI-powered market intelligence to optimize drug launch execution and resource allocation for pharmaceutical companies. Our platform delivers actionable insights on healthcare professionals (HCPs) and healthcare organizations (HCOs), empowering commercial leaders with real-time, data-driven decision-making. We are seeking aSenior Backend Engineerwho is passionate about building scalable, cloud-native systems with a focus ondata pipelines,infrastructure, andbackend services‚Äî not traditional frontend or UI-heavy applications. This role blends backend engineering, cloud architecture, and infrastructure thinking. You‚Äôll work alongside data engineers and AI specialists to develop services, APIs, and frameworks that power our platform. If you have strong Python skills, experience in AWS and/or GCP, and love solving deep technical problems in distributed systems, this is your opportunity. You‚Äôll be contributing to systems using: Python(FastAPI, asyncio, pandas, etc.) SQLandNoSQLdata stores (PostgreSQL, DynamoDB, BigQuery) AWS(Lambda, ECS, S3, Step Functions, CDK) ‚Äì core platform GCP‚Äì supporting workflows Event-driven architecture and async processing LLM integrations viaAWS BedrockorVertex AI Infra-as-code withCDKorTerraform Containerized services via Docker, ECS, and Kubernetes (limited scope) Design and implement scalable backend services and APIs that support platform data flow and automation Build resilient infrastructure components to handle large-scale data and AI-driven workloads Support data ingestion, transformation, and LLM integration with performant back-end systems Contribute to the modularization and standardization of internal backend tooling Collaborate with data engineers to bridge platform and data systems effectively Own infrastructure components through IaC (e.g., CDK, Terraform) Drive observability (logging, tracing, alerting) for backend and data systems Participate in performance tuning, scalability planning, and cost optimization Ensure reliability and resilience across critical services and pipelines Lead by example in delivering high-quality code and stable services Work independently on initiatives and drive solutions end-to-end Collaborate cross-functionally with Data Science, DevOps, and Product Engineering Mentor peers and support a culture of technical excellence 6+ years of experience in backend or infrastructure-focused engineering roles Strong Python backend development skills (FastAPI, Flask, asyncio) Deep experience withAWS(and familiarity withGCP) Comfortable with SQL and working knowledge of NoSQL datastores Solid understanding of distributed system design and asynchronous processing Experience working with infrastructure-as-code (e.g., CDK, Terraform) Experience with containerized deployment (Docker, ECS, or K8s) Exposure toLLM integration, prompt engineering, or AI-supported pipelines Experience with event-driven architecture (e.g., SQS, Pub/Sub, Kafka) Familiarity with data workflows, orchestration tools (e.g., Airflow), and data validation frameworks Previous work in regulated or high-sensitivity data environments Experience bridging backend systems and AI/data platforms Regular visits to ourWroc≈Çaw office (ideally twice per month)arevery welcome and positively received‚Äî we value face-to-face collaboration when possible! Self-driven: You take ownership and move projects forward independently Infrastructure-minded: You enjoy making systems robust, scalable, and observable Curious: You‚Äôre eager to learn about emerging tech like LLMs and AI ops Team player: You work well across disciplines and share knowledge openly Pragmatic: You solve problems efficiently and avoid over-engineering Opportunity to shape the technical foundation of our platform infrastructure Work that contributes to faster, more effective healthcare outcomes A collaborative, agile environment that values autonomy Exposure to both AWS and GCP in real-world deployments Hands-on work with LLMs and cutting-edge AI applications Competitive compensation and real responsibility from day one If you‚Äôre a backend engineer who thrives on owning infrastructure and scaling real-time, data-powered systems ‚Äî we‚Äôd love to hear from you.","[{""min"": 28000, ""max"": 36000, ""type"": ""Net per month - B2B""}]",Data Engineering,28000,36000,Net per month - B2B
Full-time,Mid,B2B,Remote,439,Data Engineer,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Poszukujemy do≈õwiadczonego Data Engineer do wsp√≥≈Çpracy przy tworzeniu i rozwijaniu rozwiƒÖza≈Ñ przetwarzania danych dla naszych klient√≥w. Twoim zadaniem bƒôdzie projektowanie i wdra≈ºanie skalowalnych system√≥w przetwarzania danych, a tak≈ºe wsp√≥≈Çpraca z zespo≈Çami Data Science, Analityki i IT. Twoje zadania Projektowanie, rozw√≥j i utrzymanie hurtowni danych, pipelines ETL/ELT i system√≥w przetwarzania du≈ºych zbior√≥w danych. Optymalizacja wydajno≈õci proces√≥w przetwarzania danych. Tworzenie i zarzƒÖdzanie infrastrukturƒÖ w chmurze (AWS, GCP, Azure) lub on-premise. Wsp√≥≈Çpraca z zespo≈Çami ds. analizy danych i biznesu w celu identyfikacji potrzeb i wymaga≈Ñ. Implementacja rozwiƒÖza≈Ñ monitorujƒÖcych jako≈õƒá danych i ich bezpiecze≈Ñstwo. Wymagania Min. 3 lata do≈õwiadczenia na stanowisku Data Engineer. Znajomo≈õƒá przynajmniej jednego narzƒôdzia chmurowego (AWS, GCP, Azure). Do≈õwiadczenie z systemami big data, np. Apache Spark, Kafka, Hadoop. Umiejƒôtno≈õƒá pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL) i bazami NoSQL (np. MongoDB, Cassandra). Praktyczna znajomo≈õƒá narzƒôdzi ETL/ELT (np. Airflow, dbt). Nice-to-have : Certyfikaty z zakresu technologii chmurowych. Do≈õwiadczenie z modelowaniem danych i optymalizacjƒÖ zapyta≈Ñ SQL. Znajomo≈õƒá narzƒôdzi monitorujƒÖcych (Prometheus, Grafana). Zrozumienie zasad CI/CD oraz praktyczne do≈õwiadczenie z narzƒôdziami DevOps. Oferujemy Mo≈ºliwo≈õƒá bycia czƒô≈õciƒÖ miƒôdzynarodowych projekt√≥w. Ambitne i rozwojowe projekty. Wsparcie merytoryczne na ka≈ºdym etapie wdro≈ºenia. Dostƒôp do najnowszych technologii. Praca w systemie zdalnym lub hybrydowym, w zale≈ºno≈õci od projektu. Nasza oferta Mo≈ºliwo≈õƒá bycia czƒô≈õciƒÖ miƒôdzynarodowych projekt√≥w. Ambitne i rozwojowe projekty. Wsparcie merytoryczne na ka≈ºdym etapie wdro≈ºenia. Dostƒôp do najnowszych technologii. Praca w systemie zdalnym lub hybrydowym, w zale≈ºno≈õci od projektu. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. We connect you with the right people","[{""min"": 18000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,18000,28000,Net per month - B2B
Full-time,Mid,B2B,Remote,445,Data Management Engineer,Experis Manpower Group,"Join a dynamic team of engineers responsible for the development, maintenance, and enhancement of technologies in the Data Analytics Catalog and Data Management domains. The scope includes tools such as Collibra, Schedulix, Trillium, Tibco CIM, Tibco EBX, Informatica MDM, and Informatica SaaS MDM. The role involves software development, configuration, AWS infrastructure setup, and quality assurance. Tasks: Develop, configure, and maintain Collibra and MDM/RDM tools. Implement and manage AWS-based serverless solutions using Infrastructure as Code (Terraform preferred). Design and maintain scalable applications with REST and Graph APIs. Integrate and manage metadata, data lineage, profiling, and quality processes. Collaborate in Agile/Scrum or Kanban teams under the guidance of a Technical Product Manager. Conduct code reviews and ensure adherence to best practices. Create and maintain technical documentation with high attention to detail. Support DevOps practices including CI/CD pipelines (GitHub Actions preferred). Work with containerization technologies like Docker and Kubernetes. Perform data transformation tasks (ETL/ELT) and manage relational and NoSQL databases. Requirements: 5+ years of experience in Python. Proficiency in Groovy (especially for Collibra-related tasks). Experience with AWS, especially serverless architecture and Terraform. Understanding of microservices architecture and design patterns. Deep knowledge of Collibra Data Governance (configuration, metadata ingestion, workflows, etc.). Experience connecting to diverse data sources for metadata and data quality management. Strong skills in designing and maintaining scalable applications. Experience with REST and Graph APIs. Proficiency in Git. Experience working in Agile/Scrum environments. Familiarity with DevOps methodologies and CI/CD pipelines (GitHub Actions preferred). Experience with Docker and Kubernetes. Knowledge of testing frameworks like pytest or unittest. Ability to conduct thorough code reviews. Experience with ETL/ELT processes. Familiarity with relational (PostgreSQL, MySQL) and NoSQL (DynamoDB) databases. Strong documentation skills and attention to detail. Offer: 100% remote work MultiSport Plus Group insurance Medicover Premium e-learning platform","[{""min"": 160, ""max"": 172, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,172,Net per hour - B2B
Full-time,Senior,B2B,Remote,447,Lead Data Engineer,Link Group,"About the Role We‚Äôre looking for aLead Data Engineerto take ownership of the architecture and delivery of data solutions in a modern, cloud-based environment. You will lead technical direction, mentor team members, and collaborate closely with cross-functional teams to ensure scalable, reliable, and high-performance data platforms. Design and build robustdata pipelines,ETL/ELT workflows, and data architecture Lead the engineering team in best practices, code quality, and solution design Collaborate with stakeholders, product owners, and architects to define data strategies Set and enforce standards around data governance, quality, and performance Review code, provide technical mentorship, and support engineering growth Take ownership of end-to-end solution delivery ‚Äî from data ingestion to consumption Support DevOps, CI/CD, and cloud infrastructure related to data workloads 6+ years of experience in Data Engineering Proven experience in atechnical leadershiporlead engineerrole Expert inPython,SQL, and cloud-native data processing tools Strong knowledge ofAWS(preferred), GCP or Azure also welcome Experience with orchestration tools (Airflow,Dagster, etc.) Proficiency with modern data platforms likeDatabricks,Snowflake,Redshift Familiar withdata modeling,data lakes,lakehouse architectures, andstreaming(Kafka, Kinesis, etc.) Knowledge ofCI/CD,Terraform, andGitOpsworkflows Experience withteam managementor mentoring junior engineers Knowledge ofdbt,Spark,Delta Lake,Glue, or similar Exposure toMLOps,analytics engineering, orBI/data visualizationtools Understanding ofdata security,compliance, andprivacy regulations","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,180,Net per hour - B2B
Full-time,Mid,B2B,Remote,450,Analytics Engineer,Datumo,"We‚Äôre looking for a Analytics Engineer ready to push boundaries and grow with us. Datumo specializes in providing Data Engineering and Cloud Computing consulting services to clients from all over the world, primarily in Western Europe, Poland and the USA. Core industries we support include e-commerce üõí, telecommunications üì° and life sciences üß¨. Our team consists of exceptional people whose commitment allows us to conduct highly demanding projects. Our team members tend to stick around for more than 3 years, and when a project wraps up, we don't let them go - we embark on a journey to discover exciting new challenges for them. It's not just a workplace; it's a community that grows together! Must-have: ‚úÖ at least 3 years of commercial experience in programming ‚úÖ proven record with a selected cloud provider GCP (preferred), Azure or AWS ‚úÖ good knowledge of JVM languages (Scala or Java or Kotlin), Python, SQL ‚úÖ experience in one of data warehousing solutions: BigQuery/Snowflake/Databricks or similar ‚úÖ in-depth understanding of big data aspects like data storage, modeling, processing, scheduling etc. ‚úÖ data modeling and data storage experience ‚úÖ ensuring solution quality through automatic tests, CI/CD and code review ‚úÖ proven collaboration with businesses ‚úÖ English proficiency at B2 level, communicative in Polish Nice to have: üåü knowledge of dbt, Docker and Kubernetes, Apache Kafka üåü familiarity with Apache Airflow or similar pipeline orchestrator üåü another JVM (Java/Scala/Kotlin) programming language üåü experience in Machine Learning projects üåü understanding of Apache Spark or similar distributed data processing framework üåü familiarity with one of BI tools: Power BI/Looker/Tableau üåü willingness to share knowledge (conferences, articles, open-source projects) What‚Äôs on offer: üî• 100% remote work, with workation opportunity üî• 20 free days üî• onboarding with a dedicated mentor üî• project switching possible after a certain period üî• individual budget for training and conferences üî• benefits: Medicover Private Medical Care , co-financing of the Medicover Sport card üî• opportunity to learn English with a native speaker üî• regular company trips and informal get-togethers Development opportunities in Datumo: üöÄ participation in industry conferences üöÄ establishing Datumo's online brand presence üöÄ support in obtaining certifications (e.g. GCP, Azure, Snowflake) üöÄ involvement in internal initiatives, like building technological roadmaps üöÄ training budget üöÄ access to internal technological training repositories Discover our exemplary project: üîå IoT data ingestion to cloud The project integrates data from edge devices into the cloud using Azure services. The platform supports data streaming via either the IoT Edge environment with Java or Python modules, or direct connection using Kafka protocol to Event Hubs. It also facilitates batch data transmission to ADLS. Data transformation from raw telemetry to structured tables is done through Spark jobs in Databricks or data connections and update policies in Azure Data Explorer. ‚òÅÔ∏è Petabyte-scale data platform migration to Google Cloud The goal of the project is to improve scalability and performance of the data platform by transitioning over a thousand active pipelines to GCP. The main focus is on rearchitecting existing Spark applications to either Cloud Dataproc or Cloud BigQuery SQL, depending on the Client‚Äôs requirements and automate it using Cloud Composer. üìà Data analytics platform for investing company The project centers on developing and overseeing a data platform for an asset management company focused on ESG investing. Databricks is the central component. The platform, built on Azure cloud, integrates various Azure services for diverse functionalities. The primary task involves implementing and extending complex ETL processes that enrich investment data, using Spark jobs in Scala. Integrations with external data providers, as well as solutions for improving data quality and optimizing cloud resources, have been implemented. üõí Realtime Consumer Data Platform The initiative involves constructing a consumer data platform (CDP) for a major Polish retail company. Datumo actively participates from the project‚Äôs start, contributing to planning the platform‚Äôs architecture. The CDP is built on Google Cloud Platform (GCP), utilizing services like Pub/Sub, Dataflow and BigQuery. Open-source tools, including a Kubernetes cluster with Apache Kafka, Apache Airflow and Apache Flink, are used to meet specific requirements. This combination offers significant possibilities for the platform. Recruitment process: 1Ô∏è‚É£Quiz - 15 minutes 2Ô∏è‚É£ Soft skills interview - 30 minutes 3Ô∏è‚É£ Technical interview - 60 minutes Find out more by visiting our website - https: //www.datumo.io If you like what we do and you dream about creating this world with us - don‚Äôt wait, apply now!","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,14000,25000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,451,Senior Database Engineer,AUCTANE Poland,"About us At Auctane, we are united by a passion to help sellers ‚Äî wherever they are, however they operate ‚Äî fulfill the promises they make to consumers. The Auctane mission is to fuel commerce through exceptional delivery. We make it possible for businesses to meet the ever rising expectations of their customers, and we make the world smaller and more accessible to consumers everywhere. Auctane brands enable hundreds of thousands of merchants to annually deliver billions of products ‚Äî over $200 billion worth ‚Äî to customers around the globe. And Auctane is just getting started. Auctane is a team of shipping and software experts with a passion for helping merchants move their ideas, dreams and innovations around the globe. The Auctane family includes ShipStation, ShipWorks, ShipEngine, ShippingEasy, Stamps, Endicia, Metapack, GlobalPost, and Packlink. Our partners include Amazon, UPS, USPS, eBay, BigCommerce, Shopify, WooCommerce, and Walmart. Our values Salary range for this role: 18 900- 23 600 PLN gross/month About the role As Senior Database Engineer, you will be responsible for the design, implementation, maintenance, and optimization of the organization's relational database systems. This role will ensure the integrity, availability, and performance of critical data to support business operations and decision-making processes. The DBE will work closely with IT teams, developers, and business stakeholders to deliver efficient and scalable database solutions that meet the organization's current and future needs. The ideal candidate will have strong technical skills in SQL Server and Oracle database management systems, excellent problem-solving abilities, and the capacity to translate business requirements into effective relational database solutions. This role will be critical in maintaining the organization's data infrastructure, enabling data-driven decision-making, and ensuring the optimal performance of mission-critical applications across the enterprise. What will you be doing? Develop database schemas, tables, and indexes to optimize performance and ensure data integrity. Implement and maintain database security measures, including user authentication, access controls, and data encryption. Analyze and interpret database performance metrics to proactively identify areas for improvement Monitor database performance and troubleshoot issues such as slow queries, database locks, and resource contention. Identify and address performance bottlenecks through query optimization, index tuning, and database configuration adjustments. Perform regular backups and disaster recovery procedures to ensure data availability and integrity. Optimize database indexes, query execution plans, and caching strategies to improve application responsiveness and scalability. Write and optimize SQL queries and stored procedures for efficient data retrieval and manipulation. Configure and maintain database replication, clustering, or failover mechanisms to provide resilience against hardware failures, network outages, or natural disasters. Stay abreast of emerging database technologies, trends, and best practices through self-study, training, and participation in industry forums and conferences. Collaborate with software developers to design database schemas that align with application requirements and development workflows. Participate in code reviews to ensure adherence to database best practices, performance guidelines, and data access patterns. Work closely with application developers to optimize SQL queries, data access patterns, and database interactions for maximum efficiency. What are we looking for? Bachelor's degree in Computer Science, Information Technology, or a related field In-depth knowledge and proficiency in SQL (Structured Query Language) Proficiency in managing SQL database systems Strong understanding of database design principles, including normalization, indexing, and data modeling Experience in database administration tasks such as installation, configuration, backup and recovery, security management, and performance tuning is important Knowledge of database security best practices and experience in implementing security measures such as user authentication, access controls, encryption, and auditing are important Strong troubleshooting and problem-solving skills Effective communication and collaboration skills Communicative level of English Tech Stack SQL Oracle Amazon Web Services C# .Net What do we offer? üóìÔ∏è Annual Salary Review: We are reviewing the salaries of all our teams annually in order to evaluate an increase according to individual performance and the business results. üìô Personal Training Budget. Up to 7.000 PLN/year training budget (certifications, conferences attendance, etc.) to invest in your professional development. We want to help you improve your technical skills, feel involved in the product community, and develop your soft skills to lead teams and manage other stakeholders. üåÖ Up to 30 days of vacation per year ( additional days are granted along with seniority at AUCTANE). üíô Up to 500 PLN/year to match your NGO donations ! We are happy to support your initiatives by duplicating the amount donated. üíú Lunch card üòä Volunteer day. You can take 1 day off per year in order to participate in volunteering activities! üîó Referral Fee We need your support in hiring top-class talent! We offer a referral bonus of 4k-20k PLN, depending on the complexity of the role and the hiring process. üë©‚Äç‚öïÔ∏èWe have an Employee Assistance Program with psychological assistance free of charge. üó∫Ô∏è Languages classes every week. Thirsty for knowledge? Learn a new language by joining our free English/Spanish/German classes. You can connect and enjoy taking up a new language or improving your current skills with one of our great instructors. üè• Free private medical insurance üìÑ Attractive life insurance üèê Gym membership co-financing üè¢ Nice office in Zielona Gora (free drinks, snacks‚Ä¶) or new office in Wroc≈Çaw (in both locations 3 days remotely, 2 days in the office/per week) ‚öñÔ∏è Great work-life balance We offer a flexible work schedule and will do our best to adapt to your personal situation. Working in a fast-paced environment can be intense, but that doesn‚Äôt mean you shouldn‚Äôt enjoy your free time! üíú An inclusive and upbeat work environment Leave your suit behind... we‚Äôre a t-shirt and converse kind of place! More importantly, our company culture promotes diversity and inclusion. The personality and opinions of each of our team members are important and valid, and we aim to offer all employees a safe environment where they can be themselves and thrive. üåç A cross-cultural atmosphere We are a truly international team of 20 nationalities that speak 10 languages. Our company language is English and all internal communication and company-wide meetings are in English. üèüÔ∏è Company events Work hard, play hard! We do our best every day, even at our regular team-building events. üì∫ Internal and external training, free access to online training platforms such as Linkedin Learning üè† Possibility to work in a home-office using equipment provided by AUCTANE, or in our office prepared in accordance with all safety requirements. Do you want to know a little bit more about the team? Please, don‚Äôt hesitate to reach out to Amazing Auctane-PL and our Instagram","[{""min"": 18900, ""max"": 23600, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18900,23600,Gross per month - Permanent
Full-time,Senior,B2B,Hybrid,452,Data Business Analyst,emagine Polska,"Project Information: Industry : Banking Location : Gda≈Ñsk ( 2 day per week in office) Rate : up to 155 z≈Ç/h netto + VAT (B2B) Summary: The Senior Data Business Analyst will serve as a crucial liaison between the business and IT development teams, primarily focusing on advanced data analysis to ensure business requirements are accurately translated into effective solutions. Main Responsibilities: Conduct advanced data analysis using SQL. Manage and analyze large datasets efficiently. Demonstrate strong analytical and problem-solving skills. Understand business terminology to effectively bridge gaps between business needs and IT. Utilize a functional background in Credit Risk (if applicable). Independently convert business requirements into high-level and low-level solution designs. Support unit testing (UT), system testing (ST), and user acceptance testing (UAT). Create functional test cases and validate results. Exhibit strong communication skills and team collaboration in an agile environment. Leverage a minimum of 8-10 years of IT experience and 3-4 years of analytical experience. Key Requirements: Proficient in SQL data analysis Experience with large datasets Strong analytical and problem-solving skills Business terminology comprehension Minimum 8-10 years of IT experience Minimum 3-4 years of analyst experience Nice to Have: Knowledge of SAS/Python Functional background in Credit Risk Experience creating functional test cases","[{""min"": 120, ""max"": 155, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,120,155,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,454,"Data Engineer - Spark, Scala/Java",Link Group,"Developing, managing, and optimizing data pipelines. Engaging with team developers and architects to shape the process implementation framework. Estimating workload and regularly updating progress on solution development. Cooperating closely with stakeholders to refine solution details and validate requirements. Supporting business teams in understanding technical possibilities, offering optimal solutions, and clarifying constraints. Establishing efficient workflows, proposing, and executing process enhancements. Proficiency in Scala or Java with substantial experience. Minimum 4‚Äì5 years of relevant experience Hands-on expertise in working with Spark. Familiarity with CI/CD methodologies and tools such as GitHub Actions. Knowledge of cloud platforms (GCP) and infrastructure as code (Terraform) is a plus. Experience with Airflow is an added advantage. Background in data pipeline testing is beneficial. A business- and product-centric approach with direct experience in stakeholder collaboration. Banking sector experience is a plus. Prior work experience in Scrum and agile methodologies. Strong analytical skills with the ability to address intricate technical challenges. Passion for tackling challenges that foster both personal and professional development. A collaborative mindset with a willingness to learn and grow. Proficiency in English is essential.","[{""min"": 22000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,30000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,461,Senior Data Scientist - ML&AI,VISA,"Company Description Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose ‚Äì to uplift everyone, everywhere by being the best way to pay and be paid. Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa. Visa‚Äôs Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world‚Äôs most sophisticated processing networks capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. While working with us you‚Äôll get to work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cyber security, and B2C platforms. The Opportunity: We are looking to hire a Data Scientist to lead the AI/ML initiatives in the Cybersecurity Products team. This position will be based out of Warsaw, Poland. In this role, you will: Be responsible for driving, designing, and building cutting edge innovation in the space of cybersecurity through Artificial Intelligence and Machine Learning - current scope of problems includes behavior biometrics, risk-based authentication, Account Takeover protection, advanced threat detection, smart incident response, AI model threat analysis, and many more. Drive the continued innovation and engineering of our existing behavior-based adaptive authentication product and bot/fraud protection product with a high-performance team of data scientists and engineers. Build innovative solutions and collaborate with engineering and product partners across the global Visa organization that help secure Visa against a variety of threats and attacks. Provide consultation to more experienced leaders in order to recommend solutions which solve security & other business challenges. You must have strong technical depth and experience in application of Machine Learning, Deep Learning, and Data Science techniques. On top of that, you should also have a genuine interest in cybersecurity and a desire to build solutions that deliver real impacts to the world. We will rely on your leadership to establish a roadmap and vision as this team engages in existing and new emerging areas. Support transfer technical knowledge to facilitate implementation of the business solution provided. Document all projects developed, including clear and efficient coding, and write other documentation as needed. Identify relevant market trends by country, based on a deep analysis of payment industry information. Interacting with several internal and external stakeholders for the strategic definition of analysis and initiatives. Continuously develop and present innovative ideas to improve current business practices within Visa. Essential Functions: Cyber Analytics Product: Research innovation in digital authentication using behavior biometrics capabilities, build applied AI based models and engineer them into the product called Visa Behavior Analytics. You will engage in data science and applied AI related activities for a Visa engineered product to protect against account takeover related threats, continuously enhancing it to combat threats in the secure authentication and perimeter defense space. Cyber/AI R&D: Research innovation in applying AI to the more general field of cybersecurity, including the protection with and against AI driven technologies, as well as the AI models themselves. You will be using your core competencies around AI and data science and help drive the teams to build models and solutions that work at scale, harnessing Petabytes of data while applying it to products that need to respond with cyber analytics in milliseconds. Influence & Collaborate: Be able to present results to a cross section of employees, including C-Level and other senior leadership at Visa. You will engage with internal technology, and cyber teams along with global product orgs. In addition, you will collaborate with colleagues in technology and product offices to establish effective, productive business relationships. This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs. Basic Qualifications: 2+ years of relevant work experience and a Bachelor‚Äôs degree, OR 5+ years of relevant work experience. Preferred Qualifications: 3 or more years of work experience with a Bachelor‚Äôs Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD,MD). 3+ years of experience in modern data mining and data science techniques(e.g., regressions, decision trees, ensemble algorithms, neural networks, time series analytics, clustering, anomaly detection, text analytics, etc.). Candidates with a PhD in a quantitative field, such as Statistics, Mathematics, Operational Research, Computer Science, Economics, or engineering preferred. Experience in developing and deploying products using Docker, Kubernetes, and the containerization technology stack. Experience in development of advanced machine learning and deep learning models such as RNN, LSTM, Graph Neural Networks. Experience and proficiency in working with large language models (LLMs), and familiarity with the associated solution architecture and infrastructure, including Nvidia GPUs. Experience in leading, building, and supporting scalable and reliable AI/ML-powered systems, enabling rapid prototyping and advanced analytics using modern big data and AI/ML technologies (e.g., Spark, Kafka, TensorFlow, PyTorch) in an agile environment. Software Engineering background: The candidate must be proficient in Python and at least one object-oriented programming language (Java, Golang, C++,etc.) Golang experience is desired. Candidate must possess software engineering skills and be able to take end-to-end ownerships of analytical models. Data Wrangling: The candidate must have proficient data wrangling skills with Python, SQL, and other data processing tools/scripts. Experience with the end-to-end machine learning lifecycle and MLOps, including data preprocessing and feature extraction, model training and evaluation, deployment, and monitoring of AI models in production environments. Experience working with Docker in both development and deployment workflows, ensuring smooth transitions from development to production environments. Distributed Systems: practical experience with NoSQL data platforms (e.g., Cassandra, Lakehouse, DynamoDB) and caching technologies like Redis is a plus. A solid understanding of the Linux networking subsystem, contributing to the stability and performance of deployed AI/ML systems. A solid understanding of the web applications and APIs, contributing to the front-end accessibility and integration of AI-driven solutions. Cloud domain: Familiarity with infrastructure and analytics services on cloud(e.g., AWS, Azure) is a plus. Domain Knowledge - Candidate with background in one or multiple of the following domains is a plus: Cybersecurity, AI security/privacy research and Biometrics Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","[{""min"": 16000, ""max"": 26000, ""type"": ""Gross per month - Permanent""}]",Data Science,16000,26000,Gross per month - Permanent
Full-time,Senior,B2B,Hybrid,463,Data Engineer Hadoop,Antal Sp. z o.o.,"Hadoop Data Engineer (GCP, Spark, Scala) ‚Äì Krak√≥w / Hybrid We are looking for an experienced Hadoop Data Engineer to join a global data platform project built in the Google Cloud Platform (GCP) environment. This is a great opportunity to work with distributed systems, cloud-native data solutions, and a modern tech stack. The position is based in Krak√≥w (hybrid model ‚Äì 2 days per week in the office). Your responsibilities: Design and build large-scale, distributed data processing pipelines using Hadoop, Spark, and GCP Develop and maintain ETL/ELT workflows using Apache Hive, Apache Airflow (Cloud Composer), Dataflow, DataProc Work with structured and semi-structured data using BigQuery, PostgreSQL, Cloud Storage Manage and optimize HDFS-based environments and integrate with GCP components Participate in cloud data migrations and real-time data processing projects Automate deployment, testing, and monitoring pipelines (CI/CD using Jenkins, GitHub, Ansible ) Collaborate with architects, analysts, and product teams in Agile/Scrum setup Troubleshoot and debug complex data logic at the code and architecture level Contribute to cloud architecture patterns and data modeling decisions Must-have qualifications: Minimum 5 years of experience as a Data Engineer / Big Data Engineer Hands-on expertise in Hadoop, Hive, HDFS, Apache Spark, Scala, SQL Solid experience with GCP and services like BigQuery, Dataflow, DataProc, Pub/Sub, Composer (Airflow) Experience with CI/CD processes and DevOps tools: Jenkins, GitHub, Ansible Strong data architecture and data engineering skills in large-scale environments Experience working in enterprise environments and with external stakeholders Familiarity with Agile methodologies such as Scrum or Kanban Ability to debug and analyze application-level logic and performance Nice to have: Google Cloud certification (e.g., Professional Data Engineer) Experience with Tableau, Cloud DataPrep, or Ansible Knowledge of cloud design patterns and modern data architectures Work model: Hybrid ‚Äì 2 days per week from the Krak√≥w office (rest remotely) Opportunity to join an international team and contribute to global-scale projects To learn more about Antal, please visit www.antal.pl","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,180,220,Net per hour - B2B
Full-time,Senior,Permanent,Hybrid,464,Senior Data Architect,KUBO,"We are looking for a Senior Data Architect to join our global Data & Analytics team and lead the design and implementation of scalable, reusable, and high-performing data and AI solutions. This role is critical to delivering analytical products that empower data-driven decision-making across various business domains. As a Senior Data Architect, you will take ownership of architecture and implementation, ensuring solutions are scalable, cost-efficient, and aligned with data governance and security standards. Key responsibilities: Lead the technical implementation of analytical and AI solutions in e.g. finance, controlling Own the development of harmonized, enriched, and reusable data models and KPIs based on data from multiple sources Translate business and functional requirements into technical architecture, considering scalability, reusability, performance, security, and cost efficiency Lead and mentor internal and external development teams Design and orchestrate end-to-end data pipelines, implement data quality and governance processes, and ensure delivery of production-ready data assets Ideal candidate profile: At least 5+ years of experience in Data & Analytics or AI, ideally in supply chain Proven experience in leading technical teams Strong technical knowledge in data architecture and engineering using technologies such as Azure Data Lake, Azure Synapse, Databricks Experience with BI tools like Tableau or Power BI and understanding of data cataloging and data quality management Fluent English Conditions: Work model: Hybrid ‚Äì 1 day per week in the Warsaw office Salary: 20 000-28 000 PLN gross/month Employment type: Full-time employment contract (UoP) directly with the client Business trips to Germany 1-2 times a year Benefits: VIP Medical Care Package, Life & Travel Insurance, Company Bonus, Holiday allowance, Co-financed sport card *Relocation package and full support with relocation to Warsaw Recruitment steps: Phone call with a Recruiter (20 - 30 min.) First interview with a Manager (1h) Second interview with a technical team (1h) Feedback and decision","[{""min"": 20000, ""max"": 28000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,20000,28000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,467,üëâ GenAI Lead,Xebia sp. z o.o.,"üü£About project: GenAI Lead will take ownership of designing, developing, and deploying advanced AI solutions, specializing in machine learning, deep learning, and generative AI technologies. This role demands a strategic leader with deep technical expertise to architect scalable GenAI solutions, guide a team of AI professionals, and deliver transformative business outcomes through innovative AI applications. üü£You will be: designing and implementing scalable Generative AI (GenAI) systems using Large Language Models (LLMs), vision models, and vector databases, leading the design, development, and deployment of AI-driven solutions, including machine learning models, deep learning frameworks, and generative AI applications, overseeing seamless integration of AI solutions with Azure AI Studio, SharePoint, and Power BI for deployment, reporting, and visualization, collaborating with cross-functional teams to shape AI strategies and deliver high-impact solutions, providing technical leadership in implemention of Agentic frameworks and tools like LlamaIndex to advance AI workflows, mentoring and empowering a team of AI developers, fostering a culture of innovation, collaboration, and technical excellence, staying at the forefront of AI advancements, proposing creative and practical solutions to address complex challenges, enforcing best practices in code quality, model optimization, and solution scalability to ensure robust production-ready systems. üü£Your profile: Bachelor‚Äôs or Master‚Äôs degree in computer science, Data Science, Engineering, or a related field (PhD preferred), 8+ years of hands-on experience in AI/ML development, including at least 3 years in leadership or solution architect capacity, demonstrated success in delivering machine learning, deep learning, and generative AI projects in production environments, exceptional programming proficiency in Python and experience with frameworks such as TensorFlow, PyTorch, or equivalent. in-depth expertise in LLMs, vision models, vector databases, and RAG applications, strong command of Azure AI Studio, SharePoint, and basic Power BI for integration and reporting purposes, proven experience with Agentic frameworks and tools like LlamaIndex for intelligent system development, outstanding problem-solving abilities, with a knack for translating business needs into technical solutions, superior communication and leadership skills to manage teams, align stakeholders, and drive project success, excellent verbal and written communication skills in English (min. C1). üü£Nice to have: experience with cloud platforms beyond Azure (e.g., AWS, GCP), knowledge of DevOps practices (e.g., CI/CD pipelines, Kubernetes), familiarity with AI ethics, governance, and compliance standards, exposure to advanced visualization tools or BI platforms. üü£Recruitment Process: CVreview ‚ÄìHRcall ‚ÄìInterview(with Live-coding) ‚ÄìClientInterview (with Live-coding) ‚ÄìHiring ManagerInterview ‚ÄìDecision üéÅBenefits üéÅ ‚úçDevelopment: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏èWe are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 40000, ""type"": ""Net per month - B2B""}, {""min"": 27000, ""max"": 32500, ""type"": ""Gross per month - Permanent""}]",Unclassified,33500,40000,Net per month - B2B
Full-time,Senior,B2B,Remote,470,Mid/Senior Data Engineer,CodiLime,"Get to know us better CodiLime is a software and network engineering industry expert and the first-choice service partner for top global networking hardware providers, software providers and telecoms. We create proofs-of-concept, help our clients build new products, nurture existing ones and provide services in production environments. Our clients include both tech startups and big players in various industries and geographic locations (US, Japan, Israel, Europe). While no longer a startup - we have 250+ people on board and have been operating since 2011 we‚Äôve kept our people-oriented culture.Our values are simple: Actto deliver. Disruptto grow. Team upto win. The project and the team The goal of this project is to build a centralized, large-scale business data platform for one of the biggest global consulting firms. The final dataset must be enterprise-grade, providing consultants with reliable, easily accessible information to help them quickly and effectively analyze company profiles during Mergers & Acquisitions (M&A) projects. You will contribute to building data pipelines that ingest, clean, transform, and integrate large datasets from over 10 different data sources, resulting in a unified database with more than 300 million company records. The data must be accurate, well-structured, and optimized for low-latency querying. The platform will power several internal applications, enabling a robust search experience across massive datasets and making your work directly impactful across the organization. The data will provide firm-level and site-level information, including firmographics, technographics, and hierarchical relationships (e.g., GU, DU, subsidiary, site). This platform will serve as a key data backbone for consultants, delivering critical metrics such as revenue, CAGR, EBITDA, number of employees, acquisitions, divestitures, competitors, industry classification, web traffic, related brands, and more. Technology stack: Languages: Python, SQL Data Stack: Snowflake + DBT, PostgreSQL, Elasticsearch Processing: Apache Spark on Azure Databricks Workflow Orchestration: Apache Airflow Cloud Platform: Microsoft Azure- Compute / Orchestration: Azure Databricks (Spark clusters), Azure Kubernetes Service (AKS), Azure Functions, Azure API Management.- Database & Storage: Azure Database for PostgreSQL, Azure Cosmos DB, Azure Blob Storage- Security & Configuration: Azure Key Vault, Azure App Configuration, Azure Container Registry (ACR)- Search & Indexing: Azure AI Search CI/CD: GitHub Actions Static Code Analysis: SonarQube AI Integration (Future Phase): Azure OpenAI What else you should know: Team Structure: Data Architecture Lead Data Engineers Backend Engineers DataOps Engineers Product Owner Work culture: Agile, collaborative, and experienced work environment. As this project will significantly impact the organization, we expect a mature, proactive, and results-driven approach. You will work with a distributed team across Europe and India. We work on multiple interesting projects at the time, so it may happen that we‚Äôll invite you to the interview for another project if we see that your competencies and profile are well suited for it. As a part of the project team, you will be responsible for: Data Pipeline Development: Designing, building, and maintaining scalable, end-to-end data pipelines for ingesting, cleaning, transforming, and integrating large structured and semi-structured datasets Optimizing data collection, processing, and storage workflows Conducting periodic data refresh processes (through data pipelines) Building a robust ETL infrastructure using SQL technologies. Assisting with data migration to the new platform Automating manual workflows and optimizing data delivery Data Transformation & Modeling: Developing data transformation logic using SQL and DBT for Snowflake. Designing and implementing scalable and high-performance data models. Creating matching logic to deduplicate and connect entities across multiple sources. Ensuring data quality, consistency, and performance to support downstream applications. Workflow Orchestration: Orchestrating data workflows using Apache Airflow, running on Kubernetes. Monitoring and troubleshooting data pipeline performance and operations. Data Platform & Integration: Enabling integration of 3rd-party and pre-cleaned data into a unified schema with rich metadata and hierarchical relationships. Working with relational (Snowflake, PostgreSQL) and non-relational (Elasticsearch) databases Software Engineering & DevOps: Writing data processing logic in Python. Applying software engineering best practices: version control (Git), CI/CD pipelines (GitHub Actions), DevOps workflows. Ensuring code quality using tools like SonarQube. Documenting data processes and workflows. Participating in code reviews Future-Readiness & Integration: Preparing the platform for future integrations (e.g., REST APIs, LLM/agentic AI). Leveraging Azure-native tools for secure and scalable data operations Being proactive and motivated to deliver high-quality work, Communicating and collaborating effectively with other developers, Maintaining project documentation in Confluence. As a Data Engineer, you must meet the following criteria: Strong experience with Snowflake and DBT (must-have) Experience with data processing frameworks, such as Apache Spark (ideally on Azure Databricks) Experience with orchestration tools like Apache Airflow, Azure Data Factory (ADF), or similar Experience with Docker, Kubernetes, and CI/CD practices for data workflows Strong SQL skills, including experience with query optimization Experience in working with large-scale datasets Very good understanding of data pipeline design concepts and approaches Experience with data lake architectures for large-scale data processing and analytics Very good coding skills in Python- Writing clean, scalable, and testable code (unit tests)- Understanding and applying object-oriented programming (OOP) Experience with version control systems: Git Good knowledge of English (minimum C1 level) Beyond the criteria above, we would appreciate the nice-to-haves: Experience with PostgreSQL (ideally Azure Database for PostgreSQL) Experience with GitHub Actions for CI/CD workflows Experience with API Gateway, FastAPI (REST, async) Experience with Azure AI Search or AWS OpenSearch Familiarity with developing ETL/ELT processes (a plus) Optional but valuable: familiarity with LLMs, Azure OpenAI, or Agentic AI system Flexible working hours and approach to work: fully remotely, in the office or hybrid Professional growth supported by internal training sessions and a training budget Solid onboarding with a hands-on approach to give you an easy start A great atmosphere among professionals who are passionate about their work The ability to change the project you work on","[{""min"": 16500, ""max"": 27500, ""type"": ""Net per month - B2B""}]",Data Engineering,16500,27500,Net per month - B2B
Full-time,Senior,B2B,Remote,474,Data Engineer,Link Group,"About the Role We're looking for aData Engineerto join a growing team working on modern data platforms. You will play a key role in designing, developing, and maintaining scalable data pipelines and infrastructure to support data analytics and reporting initiatives. This is a great opportunity to work with cutting-edge cloud and big data technologies. Design, build, and maintain scalableETL/ELT pipelines Work with structured and unstructured data from diverse sources Optimize data workflows for performance, reliability, and cost Implement data quality checks and monitoring Collaborate with analysts, architects, and other engineers to support data needs Build data integrations with internal and third-party APIs Support cloud data infrastructure and automation 3+ years of experience as a Data Engineer or similar role Strong knowledge ofSQLand data modeling principles Experience withPythonorScalafor data processing Hands-on experience with cloud platforms (ideallyAWS, but Azure/GCP also valuable) Familiarity with tools likeApache Spark,Airflow,Kafka, or similar Experience withdata lakes,data warehouses, orlakehouse architectures Git and CI/CD workflows Strong problem-solving skills and ability to work independently Experience withSnowflake,Redshift, orBigQuery Familiarity withdbt,Terraform, or other IAC tools Background indata governanceorsecurity Experience with real-time data processing (e.g., Flink, Kinesis) Exposure toML pipelinesorMLOps","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,476,Data Analyst,Vasco Electronics,"Miejsce pracy: Krak√≥w Tryb pracy: Hybrydowy, 3 dni z biura, 2 dni zdalnie Etat: Full time Rodzaj umowy: Umowa o Pracƒô, Umowa Zlecenie, B2B Wynagrodzenie: UoP (brutto): 9900 - 12000 PLN UZ (brutto): 59 - 71 PLN/h B2B (netto/h FV): 73 - 93 PLN/h Zakres obowiƒÖzk√≥w Kompleksowa analiza danych (marketingowych, sprzeda≈ºowych, finansowych, magazynowych) z wykorzystaniemGoogle BigQueryiSQL ≈ÅƒÖczenie, agregowanie, por√≥wnywanie i weryfikowanie danych z r√≥≈ºnych ≈∫r√≥de≈Ç Projektowanie, tworzenie i utrzymywanie interaktywnych dashboard√≥w oraz raport√≥w wPower BI T≈Çumaczenie wymaga≈Ñ biznesowych na specyfikacje techniczne, przygotowywanie analiz i prezentowanie rekomendacji dla interesariuszy ≈öcis≈Ça wsp√≥≈Çpraca zzespo≈Çem Data Engineer√≥woraz dba≈Ço≈õƒá o jako≈õƒá i sp√≥jno≈õƒá dokumentacji Wykorzystywanie LLM√≥w do automatyzacji swojej pracy Nasze oczekiwania 3 - 5 lat do≈õwiadczenia Zaawansowana, praktyczna znajomo≈õƒáSQLoraz ≈õrodowiska bazodanowegoGoogle BigQuerylub innych baz danych Praca z wersjonowaniem kodu (Dataform, dbt lub podobne) Bieg≈Ço≈õƒá w wizualizacji danych i budowaniu raport√≥w wPower BIlub podobne Do≈õwiadczenie w samodzielnym prowadzeniu analiz, od ekstrakcji danych po prezentacjƒô wniosk√≥w Wysoko rozwiniƒôte umiejƒôtno≈õci komunikacyjne i zdolno≈õƒá do efektywnej wsp√≥≈Çpracy z odbiorcami biznesowymi i technicznymi Proaktywne podej≈õcie, umiejƒôtno≈õƒá jasnego formu≈Çowania rekomendacji i otwarto≈õƒá na kulturƒô feedbacku Jƒôzyk angielski na poziomie B2 Mile widziane BigQuery Power BI Dataform Python Oferujemy ≈örodowisko oparte na warto≈õciach i przyjaznƒÖ, nieformalnƒÖ atmosferƒô ‚Äì bez nadƒôcia, z fajnymi lud≈∫mi i dobrƒÖ kawƒÖ Du≈ºy wp≈Çyw na kszta≈Çt pracy zespo≈Çu Bud≈ºet do wykorzystania na platformie Worksmile, kt√≥ra oferuje dostƒôp do takich benefit√≥w jak m.in. Multisport, Allianz, Luxmed, PZU oraz wiele innych Elastyczny czas pracy Inicjatywy rozwojowe Dofinansowanie okular√≥w korekcyjnych 800 z≈Ç Czƒôste integracje Atrakcje w biurze tj. PS5 + VR2, biuro przyjazne zwierzƒôtom, przekƒÖski i owoce w biurze Parking przy biurze","[{""min"": 9900, ""max"": 12000, ""type"": ""Gross per month - Permanent""}, {""min"": 73, ""max"": 93, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,9900,12000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,479,Senior Data Engineer,emagine Polska,"Summary: The role of the Senior Data Engineer focuses on enhancing a Data Lakehouse platform built on Snowflake and DBT, driving innovation, and ensuring the platform's operational excellence. Start: ASAP Duration: 3 months with possible extension until the end of 2025 Location: 100% Remote Salary: 170-190 z≈Ç/h Main Responsibilities: Design and implement solutions for the data platform using Snowflake and Azure. Enhance the platform with new features and optimize CI/CD pipelines. Automate processes and improve governance and monitoring. Collaborate within an international team and gather requirements from stakeholders. Work within Scrum and Kanban methodologies using Jira. Key Requirements: DBT Snowflake SQL Python or other object-oriented languages (Java, C#, etc.) Experience with relational databases CI/CD operations Version control using Git (preferably GitHub) Data architecture and integration skills English B2/C1 proficiency Nice to Have: PowerBI Other Details: Ability to work in Scrum and Kanban environments using Jira. Experience working independently with international stakeholders. Strong problem-solving skills. Experience working in an international team setting. Project Start Date: As soon as possible Team Composition: Three other data engineers Contract Type: B2B Work Type: Remote (with office options in Poznan and Warsaw) Travel: Unlikely but possible to Poznan, Warsaw, or Copenhagen Working Hours: Flexible; generally available from 9 AM - 4 PM.","[{""min"": 170, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,190,Net per hour - B2B
Full-time,Senior,B2B,Remote,480,Data Engineer with Palantir,Link Group,"About the Role We are looking for aData Engineer experienced with Palantir Foundryto join a cross-functional team working on large-scale data integration, modeling, and analytics platforms. The ideal candidate is hands-on, proactive, and capable of navigating complex data ecosystems in an enterprise environment. Design and build data pipelines and models usingPalantir Foundry Integrate multiple data sources (structured and unstructured) into usable, high-quality data assets Collaborate with data scientists, analysts, and business stakeholders to support advanced analytics initiatives Apply data governance, lineage, and cataloging principles within Foundry Develop and maintain Foundry ‚ÄúObjects‚Äù, Code Workbooks, and other tooling Ensure quality, performance, and scalability of the implemented data solutions Support and document platform usage and development best practices 3+ years of experience in Data Engineering Hands-on experience withPalantir Foundryin a commercial or enterprise setting Proficiency inSQL,Python, and data transformation techniques Good understanding ofdata modeling(dimensional, relational, and graph-based) Familiarity withdata governanceandmetadata management Experience working incloud-based environments(AWS, GCP, or Azure) Excellent communication skills and ability to work with cross-functional teams Previous experience in highly regulated industries (finance, pharma, defense, etc.) Experience integrating Foundry with external tools and systems via APIs Knowledge ofCI/CD,Git, and software engineering best practices Exposure to tools likeAirflow,dbt,Databricks,Snowflake, etc. Experience withdata privacy regulations(GDPR, HIPAA, etc.)","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,481,SAP Master Data Subject Matter Expert,emagine Polska,"Location: Poland, 100% Remote Expected Start Date: ASAP Contract Period: 12 months (+possible extension) Language Requirement: English B2/C1 Salary: 190-205 z≈Ç/h B2B contract Introduction & Summary: We are seeking a highly skilled Master Data Subject Matter Expert with over 5 years of relevant experience in Master Data Management, particularly with SAP ECC and/or S/4HANA. The ideal candidate will possess a deep understanding of Master Data processes, cross-functional knowledge across various business processes, including O2C, P2P, and R2R, and exceptional soft skills for effective communication and stakeholder engagement. Evaluate and improve current Master Data processes as part of SAP S/4HANA implementation. Participate in data clean-up and harmonization efforts. Design and document SAP Master Data configuration activities. Support implementation of new SAP features for Master Data enhancements. Assist in writing test plans and support end-user training. Create and maintain comprehensive Master Data documentation. Analyze large data sets for migration and validation purposes. Provide end-user support and troubleshooting for Master Data-related issues. Work closely with project teams to prepare for ERP rollout. Minimum of 5 years of experience in Master Data management. Operational experience with SAP ECC and/or S/4HANA. Deep knowledge of Business Partner setup. Cross-functional understanding of Master Data objects. Experience in the retail sector is a plus. Experience in SAP implementation or transformation programs is advantageous. Expertise in Microsoft Office tools, especially Excel and PowerPoint. Working knowledge of Jira is preferred. Nice to Have: Experience in retail industry operations. Participation in SAP implementation or rollout projects. Experience in data mapping and migration into SAP. This position offers an opportunity to contribute to significant ERP initiatives in a dynamic environment. The role is remote with flexible working hours, depending on project needs.","[{""min"": 190, ""max"": 205, ""type"": ""Net per hour - B2B""}]",Data Engineering,190,205,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,484,Data Modeller,Antal Sp. z o.o.,"Position: Data Modeller Location: Krak√≥w hybrid Employment Type: Full-time | B2B We are seeking aData Modellerto design enterprise-grade data models and lead a team of data modellers in shaping our data foundation. You'll collaborate closely with cross-functional teams to ensure consistency, scalability, and data quality across systems. Design and develop conceptual, logical, and physical data models to support business operations and analytics. Work with stakeholders to gather requirements and translate them into scalable, efficient data models. Define and enforce data modeling standards, best practices, and governance policies. Mentor and train the data modelling team in tools, techniques, and methodologies. Collaborate with data architects, engineers, and analysts to align data models with broader data strategies. Contribute to design and architecture discussions to ensure end-to-end data consistency. Stay up to date on data technologies, trends, and tools; evaluate and recommend improvements. Experience indata modeling for both transactional and analytical systems. Strong understanding ofmetadata, data analysis, and requirement gathering. Ability to clearly communicate complex data modeling concepts to technical and non-technical audiences. Leadership experience, including mentoring and guiding teams. Strong stakeholder management and consulting skills. Solid knowledge ofdata governance, data quality, and protection practices. Experience with cloud data platforms (AWS, Azure, GCP). Familiarity with Big Data ecosystems (Hadoop, Spark). Knowledge of industry models (BIAN, FSDM, BDW) or experience in designing enterprise-wide data models. To learn more about Antal, please visitwww.antal.pl","[{""min"": 130, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Science,130,180,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,487,üëâ Senior GCP Data Engineer,Xebia sp. z o.o.,"üü£ You will be: developing and maintaining data pipelines to ensure seamless data flow from the Loyalty system to the data lake and data warehouse, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, ensuring data integrity, consistency, and availability across all data systems, integrating data from various sources, including transactional databases, third-party APIs, and external data sources, into the data lake, implementing ETL processes to transform and load data into the data warehouse for analytics and reporting, working closely with cross-functional teams including Engineering, Business Analytics, Data Science and Product Management to understand data requirements and deliver solutions, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, optimizing data storage and retrieval to improve performance and scalability, monitoring and troubleshooting data pipelines to ensure high reliability and efficiency, implementing and enforcing data governance policies to ensure data security, privacy, and compliance, developing documentation and standards for data processes and procedures. üü£ Your profile: 7+ years in a data engineering role, with hands-on experience in building data processing pipelines, experience in leading the design and implementing of data pipelines and data products, proficiency with GCP services, for large-scale data processing and optimization, extensive experience with Apache Airflow, including DAG creation, triggers, and workflow optimization, knowledge of data partitioning, batch configuration, and performance tuning for terabyte-scale processing, strong Python proficiency, with expertise in modern data libraries and frameworks (e.g., Databricks, Snowflake, Spark, SQL), hands-on experience with ETL tools and processes, practical experience with dbt for data transformation, deep understanding of relational and NoSQL databases, data modelling, and data warehousing concepts, excellent command of oral and written English, Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Information Systems, or a related field. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ üü£ Nice to have: experience with ecommerce systems and their data integration, knowledge of data visualization tools (e.g., Tableau, Looker), understanding of machine learning and data analytics, certification in cloud platforms (AWS Certified Data Analytics, Google Professional Data Engineer, etc.). üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 21500, ""max"": 33000, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,21500,33000,Net per month - B2B
Full-time,Mid,B2B,Remote,491,Junior/ Mid Data Engineer,Altimetrik Poland,"Altimetrik Polandis a digital enablement company. In an agile way, we deliver bite-size outcomes to enterprises and startups from all industries, to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a big focus on core development, attacking challenging and complex problems of the biggest companies in the world. For our client, founded in 1884 as a Swiss luxury watchmaker, known for precision-made chronometers designed for aviators, we seek aJunior/ MidData Engineerto join our team and drive the development and success of the high-end timepieces. Responsibilities: Collaboration: collect and understand requirements and work closely with the business analysts, data architects, BI engineers and other data engineers. Developing data pipelines: develop, optimize, and maintain reliable and scalable ETL/ELT pipeline on a cloud platform through the collection, storage, processing, and transformation of large datasets Support production issues related to application functionality and integrations And if you possess.. Related Experience: 2+ years of experience in the role of data engineer Hands-on experience in gathering, cleaning, processing and visualizing data in Databricks with Python/Pyspark Proficient in SQL development skills with the ability to write complex, efficient queries for data integration Experience in Azure cloud ecosystem (Azure Data Factory, Azure Storage Account, Azure SQL database, . Experience in data modelling in a Data lake environment Must have strong experience in data warehouse concepts Analytical skills to support Business Analysts and the ability to translate user stories Good diagnosis skills for identifying job issues Ability to manage and complete multiple tasks within tight deadlines Proficient in spoken and written communication skills (verbal and non-verbal) ‚Ä¶ then we are looking for you! We work 100% remotely or from our hub in Krak√≥w We grow fast. We learn a lot. We prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 15000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,15000,20000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,494,Data Modeler - banking üí•,ITDS,"Join us, and turn complex data into powerful business insights! Krak√≥w-based opportunity with a hybrid work model (2 days/week in the office). As a Data Modeller, you will be working for our client, a global financial services institution undergoing a large-scale digital transformation to modernise and enhance its data infrastructure. The project focuses on building scalable, high-quality data models that support operational efficiency and strategic decision-making. You will be collaborating with cross-functional teams to design, standardise, and govern data solutions that drive business value across regulatory, analytical, and customer-facing initiatives. This is a key role where your expertise in data modelling will shape the foundation of a forward-looking data strategy. Your main responsibilities Designing and developing conceptual, logical, and physical data models Designing and developing conceptual, logical, and physical data models Collaborating with stakeholders to gather and translate data requirements Collaborating with stakeholders to gather and translate data requirements Establishing and maintaining data modelling standards and governance policies Establishing and maintaining data modelling standards and governance policies Participating in architecture and design discussions to align data models Participating in architecture and design discussions to align data models Working closely with data engineers and architects on data integration strategies Working closely with data engineers and architects on data integration strategies Creating documentation to support metadata management and data lineage Creating documentation to support metadata management and data lineage Reviewing and optimising existing data models for scalability and performance Reviewing and optimising existing data models for scalability and performance Advising teams on best practices for data modelling and data quality Advising teams on best practices for data modelling and data quality Supporting regulatory and compliance requirements through accurate modelling Supporting regulatory and compliance requirements through accurate modelling Staying informed on emerging data technologies and recommending improvements Staying informed on emerging data technologies and recommending improvements You're ideal for this role if you have: Demonstrated expertise in data modelling across operational and analytical systems Demonstrated expertise in data modelling across operational and analytical systems Proven experience eliciting, documenting, and verifying data requirements Proven experience eliciting, documenting, and verifying data requirements Strong communication skills for engaging both technical and non-technical stakeholders Strong communication skills for engaging both technical and non-technical stakeholders Excellent consulting skills to advise and guide teams on data architecture Excellent consulting skills to advise and guide teams on data architecture Solid relationship management abilities across complex organisational structures Solid relationship management abilities across complex organisational structures Deep knowledge of metadata management and data cataloguing Deep knowledge of metadata management and data cataloguing Strong problem-solving skills for analysing and resolving data challenges Strong problem-solving skills for analysing and resolving data challenges Experience applying data governance principles and standards Experience applying data governance principles and standards Ability to develop scalable and efficient data models for enterprise systems Ability to develop scalable and efficient data models for enterprise systems Proficiency in using data modelling tools such as Erwin, PowerDesigner, or similar Proficiency in using data modelling tools such as Erwin, PowerDesigner, or similar It is a strong plus if you have: Familiarity with financial services data domains and regulatory reporting needs Familiarity with financial services data domains and regulatory reporting needs Experience working in Agile or DevOps environments Experience working in Agile or DevOps environments Understanding of cloud-based data platforms (e.g., AWS, Azure, GCP) Understanding of cloud-based data platforms (e.g., AWS, Azure, GCP) Knowledge of data warehouse and data lake design principles Knowledge of data warehouse and data lake design principles Exposure to master data management and reference data modelling Exposure to master data management and reference data modelling We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to an attractive Medical Package Access to Multisport Program Access to Multisport Program #GETREADY Internal job ID #7435 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 1200, ""max"": 1400, ""type"": ""Net per day - B2B""}]",Data Science,1200,1400,Net per day - B2B
Full-time,Mid,B2B,Remote,495,Programista Baz Danych MS-SQL,People More P.S.A.,"Cze≈õƒá! Nazywamy siƒô People More bowiem traktujemy naszych wsp√≥≈Çpracownik√≥w z szacunkiem ale r√≥wnie≈º dlatego, ≈ºe projekty przy kt√≥rych pracujemy sƒÖ dla ludzi i powinny byƒá ≈Çatwe oraz przyjemne w u≈ºyciu. Jeste≈õmy technologiczni ale patrzymy szerzej : ) People More istnieje ju≈º ponad cztery lata i wywodzi siƒô z jednej z najstarszych agencji interaktywnych w kraju - Insignia. Sp√≥≈Çkƒô tworzƒÖ osoby z ogromnym zapleczem klient√≥w w kraju i zagranicƒÖ dla kt√≥rych budujemy projekty od zera (UX, UI, frontend, backend, mobile) lub w czƒô≈õci. Tworzymy bezpo≈õrednio dla naszych klient√≥w jak r√≥wnie≈º wspieramy naszych partner√≥w w pracy nad w≈Çasnymi rozwiƒÖzaniami. Gwarantujemy tym samym bogaty wachlarz projekt√≥w i mo≈ºliwo≈õƒá ich zmian! Pracujemy przy projektach z ca≈Çego ≈õwiata. Do projektu naszego bezpo≈õredniego klienta poszukujemy specjalisty na stanowiskoProgramista Baz Danych MS-SQL Zakres obowiƒÖzk√≥w: Projektowanie i rozw√≥j struktur baz danych MS SQL Tworzenie oraz optymalizacja zapyta≈Ñ i procedur sk≈Çadowanych (T-SQL) Wsparcie zespo≈Ç√≥w projektowych w obszarach zwiƒÖzanych z bazami danych ≈öcis≈Ça wsp√≥≈Çpraca z developerami .NET Diagnozowanie problem√≥w i wsparcie w zakresie dzia≈Çania aplikacji oraz modyfikacja danych Generowanie raport√≥w zgodnie z ustalonym harmonogramem Wymagania: Minimum 2 lata do≈õwiadczenia w programowaniu MS SQL, T-SQL Bardzo dobra znajomo≈õƒá SQL oraz procedur sk≈Çadowanych Umiejƒôtno≈õƒá priorytetyzacji zada≈Ñ i dobrej organizacji pracy w≈Çasnej Znajomo≈õƒá ≈õrodowiska .NET Znajomo≈õƒá systemu kontroli wersji GIT Znajomo≈õƒá narzƒôdzi RED GATE (SQL Source Control, SQL Prompt) Umiejƒôtno≈õƒá analizy potrzeb wewnƒôtrznego klienta i dostosowania rozwiƒÖza≈Ñ Znajomo≈õƒá jƒôzyka angielskiego i polskiego O nas / co oferujemy: Jeste≈õmy otwarci, szczerzy i rozwiƒÖzujemy problemy zamiast je generowaƒá. Mo≈ºe to oczywiste ale naprawdƒô szanujemy pracownik√≥w i wsp√≥≈Çpracownik√≥w. My te≈º byli≈õmy programistami i cenimy tƒô pracƒô Miƒôdzynarodowe ≈õrodowisko i projekty PrywatnƒÖ opiekƒô medycznƒÖ Kartƒô sportowƒÖ Praca w 100% zdalna (chyba ≈ºe preferujesz inny system) Mamy biuro w Krakowie, ale je≈õli lubisz pracowaƒá zdalnie, nie ma sprawy. Nie mamy nic przeciwko pracy w pe≈Çni zdalnej. Dla nas mo≈ºesz znajdowaƒá siƒô w dowolnym miejscu : ) Dlaczego warto pracowaƒá z People More? Je≈õli nie jeste≈õ zadowolony ze swojej pracy lub zada≈Ñ, wsp√≥lnie znajdziemy wyj≈õcie! Je≈õli siƒô znudzisz, zaproponujemy Ci nowy produkt i nowe, fascynujƒÖce zadania Wsp√≥lnie popracujemy nad TwojƒÖ markƒÖ: bƒôdziesz mia≈Ç okazjƒô uczestniczyƒá w konferencjach, w tym jako prelegent, pomo≈ºemy Ci publikowaƒá w uznanych czasopismach i online U≈Çatwimy Ci dostƒôp do wyzwa≈Ñ, kt√≥re zazwyczaj sƒÖ trudne do zdobycia. W ka≈ºdej chwili mo≈ºesz porozmawiaƒá bezpo≈õrednio z zarzƒÖdem People More - m√≥wimy Twoim jƒôzykiem, poniewa≈º za≈Ço≈ºyciele firmy sƒÖ programistami i projektantami! Jak wyglƒÖda proces rekrutacji? Przyjazna, zdalna rozmowa wstƒôpna Zdalna rozmowa techniczna Decyzja o podjƒôciu wsp√≥≈Çpracy!","[{""min"": 110, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Database Administration,110,170,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,496,Data & BI Software Engineer,Spyrosoft,"Requirements: Experience in any SQL (or similar) Knowledge of BI tools (particularly Power BI) Ability to design data models Optimizing interactive dashboards and reports Familiarity with ETL/ELT processes Experience with GCP, especially Looker and LookML Data governance principles and experience in monitoring data quality Fluency in English We are seeking a Data & BI Specialist responsible for designing, optimizing, and developing BI solutions. Depending on your experience level (Regular/Senior), your tasks will include advanced data modeling, performance optimization, and data visualization. Additionally, you may work on developing infrastructures and processing large datasets, ensuring their high quality and availability across the organization. Data Visualization: Creating and optimizing interactive dashboards and data visualizations. Performance optimization of reports. UX/UI development. Data Modeling & SQL: Designing scalable data models. Advanced SQL queries. Supporting Master Data Management (MDM). Infrastructure & Data Processing: Building and optimizing ETL/ELT processes. Incident management and monitoring cloud infrastructure. Data Governance & Quality: Implementing data governance principles. Monitoring data quality (KPIs). Managing data catalogs (e.g., Purview). Business Support: Supporting stakeholders through data analysis and delivering key insights. Gathering business requirements. Proficiency in SQL (or Python, Scala, etc.) and ability to design data models (Lookml, AAS) Hands-on experience with BI tools (Looker, Power BI) and optimizing interactive dashboards and reports. Knowledge of ETL/EL processes Strong expertise in Google Cloud Platform, particularly with Looker and LookML. Understanding of data governance principles and experience in monitoring data quality. Fluent communication in English, especially in the context of technical documentation and collaboration with international teams. Ability to create scripts for process automation (Python, Scala). Experience in Fabric, Databricks, Snowflake, etc. Basic knowledge of frameworks for processing large datasets, such as Hadoop or Spark. Experience with data management tools (e.g., Purview, Collibra).","[{""min"": 70, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,70,150,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,497,Senior Data Engineer (sta≈Çe zatrudnienie - sektor bankowy),DEVTALENTS Sp. z o.o.,"Senior Data Engineer | Tworzenie skalowalnych rozwiƒÖza≈Ñ chmurowych O DEVTALENTS oraz model zatrudnienia W DEVTALENTS ≈ÇƒÖczymy wybitnych specjalist√≥w IT z ambitnymi projektami, stosujƒÖc nasz unikalny model wsp√≥≈Çpracy ‚ÄûBuild-Operate-Transfer‚Äù. Jako cz≈Çonek zespo≈Çu DEVTALENTS bƒôdziesz pracowaƒá nad innowacyjnymi rozwiƒÖzaniami dla naszych klient√≥w, majƒÖc jasno okre≈õlonƒÖ ≈õcie≈ºkƒô prowadzƒÖcƒÖ do bezpo≈õredniego zatrudnienia u klienta. Prowadzenie projektowania, rozwoju i utrzymania potok√≥w danych oraz proces√≥w ETL/ELT obs≈ÇugujƒÖcych du≈ºe, zr√≥≈ºnicowane zbiory danych. Optymalizacja proces√≥w pobierania, transformacji i dostarczania danych z wykorzystaniem SQL, PySpark i Pythona. Wykorzystywanie framework√≥w takich jak Apache Airflow, AWS Glue, Kafka i Redshift w celu zapewnienia wydajnej orkiestracji danych, przetwarzania wsadowego/strumieniowego i wysokiej wydajno≈õci analiz. Wdra≈ºanie najlepszych praktyk w zakresie kontroli wersji (Git), infrastruktury jako kodu (Terraform, Ansible) oraz pipeline‚Äô√≥w CI/CD, aby zapewniƒá solidne, powtarzalne i skalowalne wdro≈ºenia. ≈öcis≈Ça wsp√≥≈Çpraca z zespo≈Çami Data Science, Analityki i Product Management przy projektowaniu modeli danych i architektur wspierajƒÖcych cele biznesowe. Monitorowanie, debugowanie i optymalizacja potok√≥w ETL, zapewnianie wysokiej niezawodno≈õci, niskich op√≥≈∫nie≈Ñ i efektywno≈õci kosztowej. Mentoring in≈ºynier√≥w na poziomie ≈õrednim i juniorskim oraz budowanie kultury dzielenia siƒô wiedzƒÖ, ciƒÖg≈Çego doskonalenia i innowacji. Du≈ºa bieg≈Ço≈õƒá w SQL, PySpark i Pythonie w zakresie transformacji danych oraz tworzenia skalowalnych potok√≥w danych (minimum 6 lat do≈õwiadczenia komercyjnego). Praktyczne do≈õwiadczenie w pracy z Apache Airflow, AWS Glue, Kafka i Redshift. Znajomo≈õƒá pracy z du≈ºymi wolumenami danych strukturalnych i czƒô≈õciowo strukturalnych. Mile widziane do≈õwiadczenie z DBT. Bieg≈Ço≈õƒá w korzystaniu z Gita do kontroli wersji. Airflow jest kluczowy do orkiestracji proces√≥w. Solidne do≈õwiadczenie w pracy z AWS (Lambda, S3, CloudWatch, SNS/SQS, Kinesis) oraz znajomo≈õƒá architektur serverless. Do≈õwiadczenie w automatyzacji i zarzƒÖdzaniu infrastrukturƒÖ za pomocƒÖ Terraform i Ansible. Umiejƒôtno≈õci w zakresie monitorowania potok√≥w ETL, rozwiƒÖzywania problem√≥w z wydajno≈õciƒÖ oraz utrzymywania wysokiej niezawodno≈õci operacyjnej. Znajomo≈õƒá proces√≥w CI/CD w celu automatyzacji test√≥w, wdro≈ºe≈Ñ i wersjonowania potok√≥w danych. Umiejƒôtno≈õƒá projektowania rozproszonych system√≥w, kt√≥re skalujƒÖ siƒô horyzontalnie dla du≈ºych wolumen√≥w danych. Wiedza o architekturach przetwarzania w czasie rzeczywistym (Lambda) i wsadowym (Kappa) bƒôdzie dodatkowym atutem. Do≈õwiadczenie w tworzeniu API (REST, GraphQL, OpenAPI, FastAPI) do wymiany danych. Znajomo≈õƒá zasad Data Mesh i narzƒôdzi self-service do danych bƒôdzie du≈ºym plusem. Wcze≈õniejsze do≈õwiadczenie w budowaniu skalowalnych platform danych i przetwarzaniu du≈ºych zbior√≥w danych jest wysoko cenione. Wy≈ºsze wykszta≈Çcenie w zakresie informatyki lub kierunk√≥w pokrewnych. Znajomo≈õƒá jƒôzyka angielskiego na poziomie co najmniej B2. Proaktywne podej≈õcie do rozwiƒÖzywania problem√≥w, pasja do podejmowania decyzji w oparciu o dane i nieustannego doskonalenia. Doskona≈Çe umiejƒôtno≈õci komunikacyjne pozwalajƒÖce przek≈Çadaƒá z≈Ço≈ºone koncepcje in≈ºynierii danych na zrozumia≈Çy jƒôzyk dla odbiorc√≥w technicznych i nietechnicznych. Umiejƒôtno≈õƒá wsp√≥≈Çpracy w ≈õrodowisku wielofunkcyjnym i zwinnym oraz gotowo≈õƒá do wspierania i mentorowania cz≈Çonk√≥w zespo≈Çu. Chƒôƒá ≈õledzenia trend√≥w bran≈ºowych, eksperymentowania z nowymi technologiami i wdra≈ºania innowacji w praktykach in≈ºynierii danych.","[{""min"": 25000, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Engineering,25000,32000,Net per month - B2B
Full-time,Senior,B2B,Remote,502,Senior Data Engineer,edrone,"We're a hard-working, fun-loving, get-things-done type of team dedicated to providing unique marketing automation solutions for clients. We understand the challenges of eCommerce and the importance of seamless customer service and satisfaction. We roll our sleeves up, act fast, and learn together. We're looking for a Senior Data Engineer who will do the same! üöÄ Who are we? Edrone is a SaaS-based product that helps thousands of small and medium-sized businesses compete with major brands. Our mission is to provide simple solutions to big challenges in eCommerce. We achieve results through a strong feedback culture and clearly defined, transparent expectations. Currently, we work with nearly 2,000 online stores ‚Äî primarily in Poland and Brazil. Brands such asBielenda,Mosquito,2005,orLilouhave placed their trust in our product. If you want to learn more about our culture and what it‚Äôs like to work at edrone, check it outhere. Our social media: LinkedIn,Instagram,YouTube. Sounds great? Keep on reading! üöÄ What‚Äôs in it for you: Be part of a small, fast-paced team that values innovation, efficiency, and a positive work culture. We thrive on challenges, embrace change, and keep things moving. We value initiative and ownership‚Äîif something makes sense, we act on it quickly and take full responsibility for delivering it. Direct responsibility for projects, regular 1: 1s with your leader with a blameless postmortems, code reviews B2B contract (20-25k) & covering all the costs of accounting services Hybrid or remote work or a modern, well-equipped office - whatever you prefer! 26 paid days off so you can relax properly! Benefits - MultiSport card, LuxMed medical package, group accident insurance, English and Portuguese classes, and Mindgram - a portal for mental health and development üöÄ How you will spend your time: Leadership, Innovation & Excellence Drive technical design discussions, lead critical implementations, and support decision-making across the team Mentor other engineers and raise the bar for code quality and system thinking Stay current with evolving technologies and proactively introduce improved tools, patterns, or approaches when beneficial. Champion engineering excellence by questioning the status quo and influencing product/technical direction Backend System Development Design, build, and maintain robust Python-based services and microservices Develop and optimize RESTful APIs and background services supporting core business logic and integrations Ensure code quality, reusability, and scalability through modular design and adherence to best practices Cloud-Native Application Engineering Develop serverless and containerized applications usingAWS Lambda,ECS, and other cloud-native tools LeverageAWS services(S3, RDS, DynamoDB, Step Functions, etc.) to support backend operations and workflows Collaborate with DevOps to provision, deploy, and monitor cloud infrastructure Automation and Task Orchestration Automate recurring tasks, background jobs, and workflows using Python scripts and AWS orchestration tools Build and maintain task schedulers and asynchronous workers for time-sensitive operations Implement monitoring, logging, and alerting systems for observability and proactive issue resolution Data Access and Integration Build data access layers and connectors for interfacing with relational and NoSQL databases Develop data integration scripts or services to move and sync data between systems when needed Write efficient, production-grade SQL and Python code to support internal tools and services Performance and Reliability Optimize application performance, API response times, and system throughput Implement retries, fallbacks, and circuit breakers to increase fault tolerance Continuously assess and improve system design for scalability and maintainability üöÄWho you are: 5+ years of professional experienceas a Data or Python Engineer Experience inData Engineering‚Äì including schema design, query optimization, and managing pipelines in production Experienced withAWS services (Redshift, Aurora, DynamoDB, S3, Glue, Lambda, Step Functions, etc.)to build data pipelines and scalable cloud-native applications. You‚Äôre familiar withdbtor eager to work with it Experience withdata orchestration tools(e.g., Airflow, Step Functions) ‚Äî scheduling, monitoring, and troubleshooting data pipelines. Experience in building and maintainingRESTful APIs, microservices, and backend systems. Have acted as atechnical/feature leadon multiple projects, owning solutions from design through production operation. Comfortableworking closely with the Product team, aligning goals, making independent technical decisions, and challenging assumptions when needed. Experienced inleading design reviews, engaging in pair programming, and documenting key decisions for long-term clarity. üöÄ It‚Äôs nice if you have: Experience in Java üìùHow does the recruitment process look like: A 30-minute phone interview with thePeople and Culture Partner-Milena Micor, where we aim to get to know you a little better! A technical online interview with theData Team Lead - Krystian Andruszek and another panelist 30-minute conversation with ourCTO Maciej Mendrelato align on vision, culture, and expectations Decision regarding the offer and welcome on board! After each stage, you will always receive feedback regarding your candidacy.","[{""min"": 20000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,20000,25000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,503,üëâ Senior Azure Data Engineer,Xebia sp. z o.o.,"üü£ You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. üü£ Your profile: ready to start immediately , 3+ years‚Äô experience with Azure (Data Factory, SQL, Data Lake, Power BI, Devops, Delta Lake, CosmosDB), 5+ years‚Äô experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools ‚Äì Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, experience with CI/CD tooling (GitHub, Azure DevOps, Harness etc), working knowledge of Git, Databricks will be benefit, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ Please note that we are currently looking to expand our talent pool for future opportunities within the IT industry. While we may not have an immediate project for you at the moment, we are proactively recruiting to ensure that we have the right expertise when new projects arise. We will contact you when a potential project matching your skills and experience becomes available. Thank you for your interest in joining our team. üü£ Nice to have: Ôªø experience with Azure Event Hubs, Azure Blob Storage, Azure Synapse, Spark Streaming, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification. üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Technical Interview (with live-coding elements) ‚Äì Client Interview (live-coding)‚Äì Hiring Manager call ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,22300,33700,Net per month - B2B
Full-time,Mid,B2B,Remote,504,Power BI Developer,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmƒÖ rekrutacyjnƒÖ, w kt√≥rej wierzymy, ≈ºe wszystko jest mo≈ºliwe dziƒôki odpowiednim ludziom. Naszym celem jest po≈ÇƒÖczenie najbardziej utalentowanych pracownik√≥w z odpowiednimi firmami, tworzƒÖc synergiczne relacje, kt√≥re przyczyniajƒÖ siƒô do wzrostu i sukcesu ka≈ºdej ze stron. Uwa≈ºamy, ≈ºe prawdziwƒÖ warto≈õƒá stanowiƒÖ ludzie pracujƒÖcy wsp√≥lnie w atmosferze wzajemnego szacunku i zaufania. Dla naszego Klienta, miƒôdzynarodowej organizacji, poszukujemy os√≥b na stanowisko Power BI Developer. Zakres obowiƒÖzk√≥w: Projektowanie, rozw√≥j i utrzymanie raport√≥w oraz dashboard√≥w w Power BI. Wsp√≥≈Çpraca z zespo≈Çami analitycznymi i biznesowymi w celu identyfikacji wymaga≈Ñ oraz przekszta≈Çcania danych w warto≈õciowe informacje. Integracja danych z r√≥≈ºnych ≈∫r√≥de≈Ç (SQL, Excel, API, itd.) i ich przetwarzanie. Praca z zapytaniami SQL w celu ekstrakcji, transformacji i ≈Çadowania danych (ETL). Optymalizacja raport√≥w i zapyta≈Ñ w Power BI pod kƒÖtem wydajno≈õci. Tworzenie i wdra≈ºanie modeli danych w Power BI i SQL. Wspieranie proces√≥w analizy danych i podejmowania decyzji biznesowych dziƒôki interaktywnym dashboardom i raportom. Wymagania: Minimum 3-letnie do≈õwiadczenie w pracy z Power BI, w tym tworzeniu raport√≥w, dashboard√≥w i modeli danych. Umiejƒôtno≈õƒá pracy z SQL (zapytania, agregacje, joiny, optymalizacja zapyta≈Ñ). Do≈õwiadczenie w pracy z danymi: przetwarzanie, analiza i modelowanie danych. Znajomo≈õƒá narzƒôdzi ETL i integracji r√≥≈ºnych ≈∫r√≥de≈Ç danych. Umiejƒôtno≈õƒá tworzenia rozwiƒÖza≈Ñ frontendowych w Power BI (dynamiczne raporty, wykresy, tabele). Znajomo≈õƒá podstawowych zasad analizy danych oraz tworzenia wizualizacji, kt√≥re wspierajƒÖ procesy podejmowania decyzji. Dobra znajomo≈õƒá jƒôzyka angielskiego w mowie i pi≈õmie. Dodatkowe atuty: Do≈õwiadczenie z Power Query i DAX. Znajomo≈õƒá narzƒôdzi do automatyzacji proces√≥w analitycznych. Zrozumienie proces√≥w biznesowych i zdolno≈õƒá do przekszta≈Çcania danych w u≈ºyteczne informacje. Oferujemy: Udzia≈Ç w dynamicznych, miƒôdzynarodowych projektach, Mo≈ºliwo≈õƒá pracy w pe≈Çni zdalnej (sporadyczne wizyty w zale≈ºno≈õci od potrzeb biznesowych), Elastyczne godziny pracy oraz przyjaznƒÖ atmosferƒô w zespole, Mo≈ºliwo≈õƒá rozwoju w nowoczesnych technologiach i realny wp≈Çyw na innowacyjne projekty. Jak wyglƒÖda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klient√≥w. Sk≈ÇadajƒÖc aplikacjƒô, mo≈ºesz liczyƒá na nasz obiektywizm, szacunek i pe≈Çny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 14000, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,14000,24000,Net per month - B2B
Full-time,Mid,B2B,Remote,506,Data Engineer,DataMass,"About the Role: We are looking for an experiencedData Engineerto join our delivery team working for aleading financial client in the UK. This is an exciting opportunity to contribute to the development of a scalable and secure data platform leveragingDatabricks,Apache Spark, andAzure. The ideal candidate will be comfortable working in regulated environments and building robust pipelines that support critical analytics and reporting needs. Build, optimize, and maintain data pipelines usingApache SparkandDatabricks Develop and deploy ETL workflows usingPySparkandPython Write clean, optimizedSQLqueries for complex data transformations Work closely with data analysts, data scientists, and business users to deliver well-structured, reliable datasets Implementdata modelingbest practices for both batch and streaming data LeverageAzureservices to manage data storage, orchestration, and compute workloads Ensure data quality, lineage, and compliance within afinancial servicescontext Contribute to solution architecture and participate in code reviews and Agile ceremonies Strong experience withApache Spark(Advanced) Proficient inDatabricks,PySpark,Python, andSQL(Regular) Hands-on experience working withAzurecloud services Solid understanding ofdata modelingand data warehousing principles Experience working with financial or highly regulated datasets (preferred) Strong communication and stakeholder collaboration skills Experience withDelta Lake,Unity Catalog, orAzure Synapse Familiarity with data governance, security, and compliance standards (e.g., GDPR, FCA) Exposure toCI/CD,Git, or workflow/orchestration tools (e.g., Azure Data Factory) Knowledge ofdata quality frameworks(e.g., Great Expectations) Opportunity to work on a high-impact project in the financial domain Access to cutting-edge technologies and cloud infrastructure Collaborative, diverse, and inclusive team environment Flexible working model (remote)","[{""min"": 20300, ""max"": 25700, ""type"": ""Net per month - B2B""}]",Data Engineering,20300,25700,Net per month - B2B
Full-time,Mid,B2B,Remote,507,Data Engineer,Datumo,"We‚Äôre looking for a Data Engineer ready to push boundaries and grow with us. Datumo specializes in providing Data Engineering and Cloud Computing consulting services to clients from all over the world, primarily in Western Europe, Poland and the USA. Core industries we support include e-commerce üõí, telecommunications üì° and life sciences üß¨. Our team consists of exceptional people whose commitment allows us to conduct highly demanding projects. Our team members tend to stick around for more than 3 years, and when a project wraps up, we don't let them go - we embark on a journey to discover exciting new challenges for them. It's not just a workplace; it's a community that grows together! Must-have: ‚úÖ at least 3 years of commercial experience in programming ‚úÖ proven record with a selected cloud provider GCP (preferred), Azure or AWS ‚úÖ good knowledge of JVM languages (Scala or Java or Kotlin), Python, SQL ‚úÖ experience in one of data warehousing solutions: BigQuery/Snowflake/Databricks or similar ‚úÖ in-depth understanding of big data aspects like data storage, modeling , processing , scheduling etc. ‚úÖ data modeling and data storage experience ‚úÖ ensuring solution quality through automatic tests, CI/CD and code review ‚úÖ proven collaboration with businesses ‚úÖ English proficiency at B2 level, communicative in Polish Nice to have: üåü knowledge of dbt, Docker and Kubernetes, Apache Kafka üåü familiarity with Apache Airflow or similar pipeline orchestrator üåü another JVM (Java/Scala/Kotlin) programming language üåü experience in Machine Learning projects üåü understanding of Apache Spark or similar distributed data processing framework üåü familiarity with one of BI tools: Power BI/Looker/Tableau üåü willingness to share knowledge (conferences, articles, open-source projects) What‚Äôs on offer: üî• 100% remote work, with workation opportunity üî• 20 free days üî• onboarding with a dedicated mentor üî• project switching possible after a certain period üî• individual budget for training and conferences üî• benefits: Medicover Private Medical Care , co-financing of the Medicover Sport card üî• opportunity to learn English with a native speaker üî• regular company trips and informal get-togethers Development opportunities in Datumo: üöÄ participation in industry conferences üöÄ establishing Datumo's online brand presence üöÄ support in obtaining certifications (e.g. GCP, Azure, Snowflake) üöÄ involvement in internal initiatives, like building technological roadmaps üöÄ training budget üöÄ access to internal technological training repositories Discover our exemplary project: üîå IoT data ingestion to cloud The project integrates data from edge devices into the cloud using Azure services. The platform supports data streaming via either the IoT Edge environment with Java or Python modules, or direct connection using Kafka protocol to Event Hubs. It also facilitates batch data transmission to ADLS. Data transformation from raw telemetry to structured tables is done through Spark jobs in Databricks or data connections and update policies in Azure Data Explorer. ‚òÅÔ∏è Petabyte-scale data platform migration to Google Cloud The goal of the project is to improve scalability and performance of the data platform by transitioning over a thousand active pipelines to GCP. The main focus is on rearchitecting existing Spark applications to either Cloud Dataproc or Cloud BigQuery SQL, depending on the Client‚Äôs requirements and automate it using Cloud Composer. üìà Data analytics platform for investing company The project centers on developing and overseeing a data platform for an asset management company focused on ESG investing. Databricks is the central component. The platform, built on Azure cloud, integrates various Azure services for diverse functionalities. The primary task involves implementing and extending complex ETL processes that enrich investment data, using Spark jobs in Scala. Integrations with external data providers, as well as solutions for improving data quality and optimizing cloud resources, have been implemented. üõí Realtime Consumer Data Platform The initiative involves constructing a consumer data platform (CDP) for a major Polish retail company. Datumo actively participates from the project‚Äôs start, contributing to planning the platform‚Äôs architecture. The CDP is built on Google Cloud Platform (GCP), utilizing services like Pub/Sub, Dataflow and BigQuery. Open-source tools, including a Kubernetes cluster with Apache Kafka, Apache Airflow and Apache Flink, are used to meet specific requirements. This combination offers significant possibilities for the platform. Recruitment process: 1Ô∏è‚É£Quiz - 15 minutes 2Ô∏è‚É£ Soft skills interview - 30 minutes 3Ô∏è‚É£ Technical interview - 60 minutes Find out more by visiting our website - https: //www.datumo.io If you like what we do and you dream about creating this world with us - don‚Äôt wait, apply now!","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,14000,25000,Net per month - B2B
Full-time,Senior,B2B,Remote,508,Workplace SCCM/Intune Specialist,emagine Polska,"We are looking for an experiencedWorkplace SCCM/Intune Specialist, specializing in Modern Workplace to support the carve-out project. The focus of the role is to support the transition team with modern end-user technology services, with a strong focus on SCCM and Intune. This role is pivotal in ensuring a seamless, secure, and efficient digital workplace experience for our users. Strong documentation and communication skills are essential. Start: 1st September Duration: first contract for 6 months + possible extension Location: Fully Remote (Poland) Salary: B2b contract- 150-170 z≈Ç/hour Skills and Experience: Strong experience with Microsoft Intune, SCCM, Endpoint Manager (policy management, app deployments, reporting). Knowledge of hybrid-joined and Azure AD-joined device scenarios. Familiarity with Windows Autopilot, co-management, compliance policies, and Conditional Access. Good understanding of security controls on endpoints (AV, encryption, firewall, patching). Strong knowledge of end user devices, printers, device inventory, policies. Previous experience with Microsoft Exchange ‚Äì important nice to have. Previous involvement in large-scale device migration or M&A carve-out projects is a plus. Key Responsibilities: Perform complete device inventory for in-scope users, validating ownership, management method, and compliance state. Plan and execute endpoint separation strategy (re-provisioning or re-enrollment into buyer‚Äôs management platform). Adjust Intune MDM and SCCM policies, profiles, and configurations for separation readiness. Manage application packaging and redeployment for devices transitioning to the buyer. Handle BitLocker, encryption keys, and endpoint security policies to ensure secure handover. Support co-management scenarios (SCCM and Intune) for hybrid-managed devices during migration. Provide technical guidance to user support teams for device wipe/rebuild/re-enrollment procedures. Document device management changes and handover processes for operational continuity.","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Unclassified,150,170,Net per hour - B2B
Full-time,Senior,B2B,Office,509,Data Engineer,1dea,"Dla jednego z du≈ºych klient√≥w poszukujemy osoby do roli: Data Engineer Warunki zaanga≈ºowania: Obszar: Finansowy Lokalizacja: Irlandia, 5 dni w tygodniu Start: ASAP (akceptujemy kandydatury z max 1msc okresem wypowiedzenia) Stawka (ustalana indywidualnie): 150 - 170 PLN net / h Zaanga≈ºowanie: B2B (outsourcing z 1dea), full-time, d≈Çugofalowo Zakres obowiƒÖzk√≥w Projektowanie, tworzenie i utrzymywanie wysokiej jako≈õci potok√≥w danych (data pipelines). Analiza danych oraz modelowanie danych zgodnie z wymaganiami biznesowymi. Tworzenie i utrzymywanie proces√≥w ETL do efektywnego pobierania i transformacji danych (znajomo≈õƒá SnapLogic bƒôdzie dodatkowym atutem). Zapewnienie wydajno≈õci, bezpiecze≈Ñstwa i skalowalno≈õci rozwiƒÖza≈Ñ danych. Wsp√≥≈Çpraca z analitykami, naukowcami danych i innymi interesariuszami w zakresie projektowania i wdra≈ºania rozwiƒÖza≈Ñ. Udzia≈Ç w przeglƒÖdach kodu i projekt√≥w w celu utrzymania wysokiego poziomu standard√≥w in≈ºynierii danych. Tworzenie i utrzymywanie proces√≥w CI/CD dla p≈Çynnej integracji i dostarczania danych. Wsp√≥≈Çpraca z liderami technicznymi i architektami nad ulepszaniem rozwiƒÖza≈Ñ. Wymagania Minimum 5 lat do≈õwiadczenia na stanowisku Data Engineera w ≈õrodowisku korporacyjnym. Zaawansowana znajomo≈õƒá architektury danych i technologii bazodanowych. Bieg≈Ço≈õƒá w pracy z MS SQL, w tym ze ≈õrodowiskiem SQL MI w chmurze Azure . Do≈õwiadczenie w analizie danych, modelowaniu danych oraz procesach ETL. Znajomo≈õƒá koncepcji hurtowni danych. Do≈õwiadczenie w pracy z narzƒôdziami CI/CD (Git, Jenkins, Docker). Zrozumienie i do≈õwiadczenie w metodykach Agile. Oferujemy Zatrudnienie na podstawie umowy B2B na czas nieokre≈õlony Do≈ÇƒÖczysz do firmy z solidnƒÖ pozycjƒÖ na rynku Firma zapewnia nowoczesny sprzƒôt, oprogramowanie i konfiguracjƒô Profesjonalne doradztwo i wsparcie w rozwoju kariery od do≈õwiadczonego zespo≈Çu specjalist√≥w 1dea Cenimy sobie kole≈ºe≈Ñsko≈õƒá, otwarto≈õƒá, szacunek, wzajemnƒÖ pomoc i wsparcie w rozwijaniu kompetencji zar√≥wno w≈Çasnych, jak i koleg√≥w i kole≈ºanek z zespo≈Çu Wspieramy kulturƒô kreatywno≈õci. Ka≈ºdy cz≈Çonek zespo≈Çu ma mo≈ºliwo≈õƒá proponowania w≈Çasnych pomys≈Ç√≥w i rozwiƒÖza≈Ñ, a jego g≈Ços jest zawsze brany pod uwagƒô","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Manager / C-level,B2B,Remote,513,Head of Data & Quant Engineering,RedStone,"RedStoneis a fast-growing Polish blockchain startup revolutionizing oracle infrastructure. With a team of 40, over half being senior engineers and technical experts, we deliver scalable, secure, and low-latency off-chain data to smart contracts across multiple chains. We‚Äôve secured over $6B in TVS, raised $15M in Series A funding, and were recognized by Forbes asthe top VC-backed startup in Poland. Backed by leading Web3 names like Arrington Capital, Stani Kulechov (Aave), and Gnosis, our team includes alumni from Google, OpenZeppelin, and major crypto projects. We work remotely across time zones, with a Warsaw HQ for deep work and collaboration. JOIN THE REDSTONE TEAM! As Head of Data, you'll own the strategy and execution of RedStone‚Äôs data architecture. You‚Äôll lead a high-performing technical team, define standards for data quality and reliability, and help deliver real-time intelligence for smart contracts across multiple chains and financial ecosystems. This is a high-impact leadership role at the intersection of DeFi, infrastructure, and big data. Lead theData Engineering and Analytics functionacross the company Architect robust systems for collecting, validating, and aggregating off-chain and on-chain data Define and enforcedata reliability, accuracy, and uptime standards Design monitoring systems to detect anomalies, manipulation attempts, and cross-source inconsistencies Work cross-functionally with Engineering, Product, and Business teams to turn raw data into mission-critical insights Grow and mentor a team of high-caliber data and backend engineers Research and prototype advanced data products (e.g., asset pricing models, latency-adjusted feeds, cross-DEX aggregation) Own the roadmap and delivery of the data infrastructure used by our oracle network Strong backend engineering background withPython, Go, or Rust Experience working withAWS (especially Lambda), event-driven architectures, andmessage queueslike RabbitMQDeep knowledge oftime-series databases(InfluxDB, TimescaleDB) and monitoring systems (Grafana, CloudWatch) Familiarity withon-chain data structures, smart contract logs, block timing, and decentralized data fetching Advanced understanding offinancial or trading data, including anomaly detection, latency compensation, and pricing validation Quality-Driven Mindset‚Äì extreme attention to detail; cares deeply about data integrity in high-stakes environments Leadership & Mentoring‚Äì experience managing or mentoring technical teams, building culture, and scaling impact Ownership & Execution‚Äì strong project management and execution skills, especially in unstructured and fast-moving environments Experience withDEX protocols(Uniswap, GMX, dYdX, Curve, etc.) or DeFi lending platforms (Aave, Compound, Euler) Knowledge ofCEX infrastructure, order books, matching engines, and arbitrage mechanics Understanding ofliquidations,slippage, and pricing risk in lending/AMM environments Contributions toWeb3 projects, DAOs, or open-source blockchain tooling Exposure toquantitative financeor financial modeling Hands-on experience withsmart contracts, oracles, and price feed architecture The role can be fully remote but we also have an office in the center of Warsaw at Zgoda 3. Competitive salary (based on skills and experience) +token allocation after 3 months Multisport card and private healthcare Long-term stability and growth (Series A closed, backed by top global investors) Flexible hours A fixed, pre-agreed number of paid service break days. Top-tier equipment (MacBooks, external displays) Regular team off-sites, hackathons, and Web3 events Full ownership over your work A rare opportunity to build foundational infrastructure for the future of finance üåêWebsite üß†Docs üíªGitHub üê¶X Be part of a world-class team building mission-critical Web3 infrastructure. Apply now and redefine the future of decentralized data.","[{""min"": 25000, ""max"": 35000, ""type"": ""Net per month - B2B""}]",Data Engineering,25000,35000,Net per month - B2B
Full-time,Senior,B2B,Remote,514,Data Engineer,Haddad Brands Europe,"Building a scalable Data Warehouse that consolidates transactional data from our AS400 (IBM i) ERP and DB2 databases Integrating data from MS SQL-based applications to support analytics and reporting Automating ETL pipelines for nightly and real-time data ingestion Implementing data quality, lineage, and governance processes Design, develop, and maintain ETL processes to extract data from AS400 DB2 and MS SQL systems Model and optimize Data Warehouse schemas (star, snowflake) for performance and scalability Collaborate with Business Analysts and stakeholders to translate requirements into data solutions Implement and monitor data quality checks, alerts, and remediation workflows Automate deployments and version control of data pipelines using tools like Git Troubleshoot data issues and tune database performance for large datasets Proven experience as a Data Engineer or similar role in a corporate environment Strong SQL skills and hands-on experience with DB2 on AS400 (IBM i) and MS SQL Server Proficiency in ETL frameworks or orchestration tools Programming experience in Python, Shell scripting, or similar languages Solid understanding of data modeling, warehousing concepts, and performance tuning Familiarity with data governance, lineage, and quality best practices Excellent communication skills and ability to work with cross-functional teams English proficiency at C1 level or higher Initial meeting in Polish to get acquainted and discuss your background Technical interview within two weeks, focusing on your data architecture and ETL skills Brief English conversation to assess collaboration and communication abilities Private medical insurance 21 days of paid vacation per year (B2B contract) Company-provided hardware and software Training programs and clear career-development path Freedom to choose tools and technologies Supportive, trust-based work environment Get to know us video: -<http: //gofile.me/20s0Q/YMS1drNMp Password: HaddadFamilyB2BA@Retail","[{""min"": 17000, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Engineering,17000,24000,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,515,Middle Data Engineer (Databricks),N-iX,"#3821 Join our team to work on enhancing a robust data pipeline that powers ourSaaS product,ensuring seamless contextualization, validation, and ingestion of customer data. Collaborate withproduct teamsto unlock new user experiences by leveragingdata insights.Engage with domain experts to analyze real-world engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Clientwas established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client‚Äôs platform empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renownedScandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities: Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements: Programming: Minimum of3-4 yearsas data engineer, or in a relevant field. Python Proficiency: Advanced experience inPython, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights. Cloud: Familiarity with cloud platforms (preferablyAzure). Data Platforms: Experience withDatabricks, Snowflake, or similar data platforms. Database Skills: Knowledge of relational databases, with proficiency inSQL. Big Data: Experience using Apache Spark. Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability. Version Control: Experience withGitlabor equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have: Experience withDockerandKubernetes. Experience with document and graph databases. Ability to travel abroad twice a year for an on-site workshops.","[{""min"": 18101, ""max"": 21536, ""type"": ""Net per month - B2B""}, {""min"": 14591, ""max"": 17547, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18101,21536,Net per month - B2B
Full-time,Mid,Permanent or B2B,Hybrid,517,Data Scientist,NeuroSYS,"Nasz klient toglobalna firma farmaceutyczna, kt√≥ra rozbudowuje swoje systemy zarzƒÖdzania produkcji opierajƒÖc siƒô na analityce danych z u≈ºyciem Data Science i Machine Learning. Obecnie poszukujemy do≈õwiadczonego Data Scientista, kt√≥ry pomo≈ºe namwykorzystaƒá zgromadzone dane do optymalizacji proces√≥w produkcyjnych. W ramach projektu klient pracuje nad przygotowaniem zestawu narzƒôdzi analitycznych wykorzystujƒÖcych Power BI jako interfejs do wizualizacji danych oraz bazujƒÖcy na algorytmach Machine Learning do analizy trend√≥w, odkrywania nieoczywistych wzorc√≥w w celu usprawnienia procesu produkcyjnego i wykrywania wczesnych oznak zu≈ºycia lub nieprawid≈Çowego dzia≈Çania element√≥w maszyn (Predictive Maintenance). Twoje zadania: Wykorzystywanie technologiiOCR do ekstrakcji danychz dokument√≥w papierowych, ≈öcis≈Ça wsp√≥≈Çpraca z ekspertami bran≈ºowymi w celu zrozumienia proces√≥w i danych produkcyjnych Analiza potrzeb i wymaga≈Ñ biznesowych w zakresie analizy danych i oczekiwanych rezultat√≥w, Ocena wykonalno≈õci i szacowanie czasoch≈Çonno≈õci dla zg≈Çaszanych potrzeb, Dob√≥r, implementacja i usprawnianiemodeli analitycznych, Przetwarzanie i eksploracja danych pochodzƒÖcych zsystemu typu historian, Regularne raportowanie postƒôp√≥w prac i prezentacja wynik√≥w interesariuszom. Wymagane umiejƒôtno≈õci: Do≈õwiadczenie wMachine Learning i Data Science: min.3 lata do≈õwiadczeniaw komercyjnych projektach wykorzystujƒÖcych ML, umiejƒôtno≈õƒá trenowania i oceny modeli ML, do≈õwiadczenie z technikami uczenia nadzorowanego i nienadzorowanego. Umiejƒôtno≈õci programistyczne: znajomo≈õƒá narzƒôdziOCR(AWS Textract, Tesseract), bieg≈Ça znajomo≈õƒá jƒôzykaPythonoraz popularnych bibliotekML(m.in. aiohttp, scikit-learn, TensorFlow, PyTorch, XGBoost), do≈õwiadczenie w przetwarzaniu i analizie du≈ºych zbior√≥w danych (Big Data), znajomo≈õƒá narzƒôdzi doprzetwarzania danych, takich jak Pandas, NumPy czy SQL, do≈õwiadczenie w pracyz bazami danychSQL i noSQL, praktyczna znajomo≈õƒá GIT oraz GitFlow, podstawowa znajomo≈õƒá Power BI bƒôdzie dodatkowym atutem. Umiejƒôtno≈õci analityczne i komunikacyjne: zdolno≈õƒá do analizy wymaga≈Ñ biznesowych i przek≈Çadania ich na techniczne rozwiƒÖzania, umiejƒôtno≈õƒá prezentacji wynik√≥w oraz klarownego wyja≈õniania z≈Ço≈ºonych koncepcji technicznych osobom nietechnicznym, swoboda komunikacji wjƒôzyku angielskimw mowie i pi≈õmie na poziomie C1. Mile widziane: Znajomo≈õƒá koncepcji i metod Predictive Maintenance i Przemys≈Çu 4.0. (np. prognozowanie stanu maszyn, analiza drga≈Ñ, wykrywanie awarii), zrozumienie pojƒôƒá takich jak IoT, SCADA, DCS, IIoT, Znajomo≈õƒá system√≥w typu historian (np. AVEVA Historian, OSIsoft PI) lub protoko≈Ç√≥w przemys≈Çowych (OPC, Modbus), Do≈õwiadczenie z technologiami chmurowymi (np. Azure, AWS, Google Cloud), zw≈Çaszcza w kontek≈õcie ML Ops, Znajomo≈õƒá technik DevOps i CI/CD (np. Docker, Kubernetes) u≈ºywanych w ≈õrodowisku ML, Do≈õwiadczenie w analizie danych procesowych i integracji z systemami przemys≈Çowymi, Do≈õwiadczenie w pracy z danymi produkcyjnymi i procesowymi (np. dane z PLC, SCADA, Historian), Do≈õwiadczenie w tworzeniu dashboard√≥w Power BI i ich integracji ze ≈∫r√≥d≈Çami danych. Oferujemy: AtrakcyjnƒÖ, pe≈ÇnƒÖ wyzwa≈Ñ pracƒô w zgranym zespole pasjonat√≥w IT i lu≈∫nej atmosferze, Udzia≈Ç w innowacyjnych projektach realizowanych dla globalnego klienta, DowolnƒÖ formƒô zatrudnienia, elastyczne godziny pracy, PrywatnƒÖ opiekƒô medycznƒÖ, Mo≈ºliwo≈õƒá pracy zdalnej, jak i w biurze; ze sporadycznymi, obowiƒÖzkowymi spotkaniami w biurze, w kt√≥rym czekajƒÖ ≈õwie≈ºe owoce, przekƒÖski i pyszna kawa non stop!","[{""min"": 15000, ""max"": 21800, ""type"": ""Net per month - B2B""}, {""min"": 11200, ""max"": 16200, ""type"": ""Gross per month - Permanent""}]",Data Science,15000,21800,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,520,Lead Data Scientist,Bayer Sp. z o.o.,"Lead Data Scientist For Digital Hub Warsaw, we are looking for: Lead Data Scientist Are you ready to make a significant impact in the world of data science and AI agents? We are seeking a talented Lead Data Scientist to become a vital part of our dynamic Data Assets, Analytics and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in implementing global, cutting-edge AI solutions across commercial, product supply, marketing, and R&D domains. Our diverse, international team, spanning Poland and Germany, is dedicated to managing the entire product life cycle ‚Äì from proof of concept to the seamless operation of fully industrialized products. We pride ourselves on delivering innovative solutions that leverage traditional Machine Learning, Generative AI, and Mathematical Optimization, all powered by a modern tech stack featuring Python, Azure and Databricks. If you‚Äôre passionate about driving data-driven decision-making and AI driven process automation and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Creates, employs, evaluates, optimizes, and maintains generative AI, machine learning, time series forecasting, mathematical optimization, simulation, and NLP models and workflows that deliver actionable insights and meet business objectives. Participates in the development, delivery, and maintenance of Agentic AI Platform by implementing and proposing new features, designing evaluation experiments, and researching the latest advancements in the GenAI field. Extracts valuable information from large structured and unstructured datasets, identifying key variables and metrics that influence consumer behavior, commercial activities, product supply, R&D and RMSQC. Builds prototypes to prove modeling concepts. Industrializes and scales up successful advanced analytics prototypes into IT products. Creates intuitive and interactive data visualizations, reporting tools and dashboards to communicate complex analytical findings to technical and non-technical stakeholders as well as enable scenario creation and management. Develops generalized frameworks and libraries for repetitive data science activities. Leads exploratory projects that investigate new data sources, tools, and analytical techniques, keeping Bayer at the forefront of data science in the consumer health sector. Builds data processing pipelines, analyzes data used for modelling and performs other data related activities if required. Manages code in Github repositories and performs peer code reviews. Fosters a culture of continuous learning within the data science team, taking active part in workshops and training sessions to share knowledge and skills. Presents compelling data driven stories to all levels of the organization, including peers, senior management, and internal customers to drive both strategic and operational changes in the business. Acts as a subject matter expert in data science, advising on best practices and emerging technologies that can enhance Bayer Consumer Health‚Äôs data science capabilities. Develops and train other members of the Data Science team. Qualifications & Competencies (education, skills, experience): Master‚Äôs or PhD degree with 8+ years of experience in Data Science or related fields. Proven educational background or applied experience in at least one of the following: Machine Learning, Statistics, Mathematics, Computer Science, Quantitative Finance/Economics/Marketing, Biostatistics, Bioinformatics, or other related quantitative disciplines. Expert proficiency and practical experience in machine learning, generative AI, forecasting, and mathematical optimization. Expert knowledge of data science tools, libraries, frameworks, and platforms: Python, SQL, Vector Stores, Spark, Azure, Databricks, Github, LangChain, RAG, prompt and context engineering. Ability to write production grade code using object-oriented programming paradigm. Proven track record of developing advanced analytics products within a cloud environment and delivering valuable analysis through the application of domain and business knowledge. Ability to create architecture for advanced analytics products. Problem solving and analytical skills. Interpersonal and communication skills, active listening, consulting, challenges, presentation skills. Leadership, strategic and design thinking. Fluent in English, both written & spoken. What do We offer: A flexible, hybrid work model. Great workplace in a new modern office in Warsaw. Career development, 360¬∞ Feedback & Mentoring programm. Wide access to professional development tools, trainings, & conferences. Company Bonus & Reward Structure. VIP Medical Care Package (including Dental & Mental health). Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù). Life & Travel Insurance. Pension plan. Co-financed sport card - FitProfit. Meals Subsidy in Office. Additional days off. Budget for Home Office Setup & Maintenance. Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs. Tailored-made support in relocation to Warsaw when needed.","[{""min"": 26000, ""max"": 34000, ""type"": ""Gross per month - Permanent""}]",Data Science,26000,34000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Hybrid,522,Senior Data Engineer,Remodevs,"Please note it's now remote role but later turns into hybrid - so only candidates from Warsaw and surroundings are required. About: We are seeking a highly motivated and self-driven data engineer for our growing data team -who is able to work and deliver independently and as a team. In this role, you will play a crucial part in designing, building and maintaining our ETL infrastructure and data pipelines. Major Responsibilities: ‚óè Design, develop, and deploy Python scripts and ETL processes with Prefect and Airflow to prepare data for analysis. ‚óè Model dimensional and denormalized schemas for optimal performance reporting and discovery. ‚óè Design AI-friendly DB schemas and ontologies. ‚óè Architect cloud ops solutions for data topologies. ‚óè Transform and migrate data with Python, DBT, and Pandas. ‚óè Work with event-based/streaming technologies for real-time ETL. ‚óè Ingest and transform structured, semi-structured, and unstructured data. ‚óè Optimize ETL jobs for performance and scalability to handle big data workloads. ‚óè Monitor and troubleshoot ETL jobs to identify and resolve issues or bottlenecks. ‚óè Implement best practices for data management, security, and governance with Prefect, DBT, and Pandas. ‚óè Write SQL queries, program stored procedures, and reverse engineer existing data pipelines. ‚óè Perform code reviews to ensure fit to requirements, optimal execution pattern,s and adherence to established standards. ‚óè Assist with automated release management and CI/CD processes. ‚óè Validate and cleanse data and handle error conditions gracefully. Skills ‚óè 3+ years of Python development experience, including Pandas ‚óè 5+ years writing complex SQL queries with RDBMSes. ‚óè 5+ years of Experience with developing and deploying ETL pipelines using Airflow, Prefect, or similar tools. ‚óè Experience with cloud-based data warehouses in environments such as RDS, Redshift, or Snowflake. ‚óè Experience with data warehouse design: OLTP, OLAP, Dimensions, and Facts. ‚óè Experience with Cloud-based data architectures, messaging, and analytics. Pluses: Experience with ‚óè Docker ‚óè Kubernetes ‚óè CI/CD automation ‚óè AWS lambdas/step functions ‚óè Data partitioning ‚óè Databricks ‚óè Pyspark ‚óè Cloud certifications","[{""min"": 24012, ""max"": 25859, ""type"": ""Net per month - B2B""}, {""min"": 24012, ""max"": 25859, ""type"": ""Gross per month - Permanent""}]",Data Engineering,24012,25859,Net per month - B2B
Full-time,Senior,B2B,Remote,523,Senior Power BI Developer,Holisticon Insight,"Holisticon Insightis a division ofhttp: //nexergroup.comfocused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! https: //holisticon.pl/holisticon-insight/ üöÄ We are looking foraSenior Power BI Developerto lead the frontend development of a proprietary Business Intelligence (BI) and analytics tool within the Dealer Management System (DMS) stream to work on a project in a team of our client,(a Swedish-based leading provider of transport solutions.) In the role of Senior Power BI Developer, you willbe responsible for building modern BI solutions usingPower BI Premium (Cloud). You will play a key role in enhancing our data visualization and reporting capabilities across complex data environments. Develop and maintain dashboards, cubes, and reports usingPower BI Premium (Cloud) Collaborate with cross-functional teams to gather requirements and deliver data-driven insights Participate in the migration from on-premise solutions to cloud-based BI infrastructure Drive innovation and best practices in frontend data engineering and analytics Expert-level proficiency in Power BI and Power Automate StrongSQL skillsand hands-on experience with data modeling and BI tools Proven track record working oncomplex projectswith multiple reports Analytical thinker with a passion for solving challenges through data Experience collaborating inlarge, diverse development teams Excellentcommunication skillsto engage with both technical and non-technical stakeholders Experience withPythonandSnowflake Familiarity withcloud migration projects Life insurance Multisport card Fully remote job Private medical care Flexible working hours Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories) 1Ô∏è‚É£ Initial chat with Joanna, our recruiter ‚Äì 30 minutes 2Ô∏è‚É£ Technical interview with our expert ‚Äì 30-45 minutes 3Ô∏è‚É£ Final interview with the client ‚Äì 1 hour","[{""min"": 130, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,130,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,525,Senior Data Engineer,KMD Poland,"#Data Engineer #Apache Spark #Databricks #Java #Apache Kafka #Batch Processing #Structured Streaming #Azure #SQL #Microservices #CI/CD #Docker #DDD Are you ready to join our international team as aLead/Senior Data Engineer? We shall tell you why you should... What product do we develop? We are building an innovative solution,KMD Elements, on Microsoft Azure cloud dedicated to the energy distribution market (electrical energy, gas, water, utility, and similar types of business). Our customers include institutions and companies operating in the energy market as transmission service operators, market regulators,distribution service operators, energy trading, and retail companies. KMD Elements delivers components allowing implementation of the full lifecycle of a customer on the energy market: meter data processing, connection to the network, physical network management, change of operator, full billing process support, payment, and debt management, customer communication, and finishing on customer account termination and network disconnection. The key market advantage of KMD Elements is its ability to support highly flexible, complex billing models as well as scalability to support large volumes of data. Our solution enables energy companies to promote efficient energy generation and usage patterns, supporting sustainable and green energy generation and consumption. We work with always up-to-date versions of: ‚Ä¢ Apache Spark on Azure Databricks ‚Ä¢ Apache Kafka ‚Ä¢ Delta Lake ‚Ä¢ Java ‚Ä¢ MS SQL Server and NoSQL storages like Elastic Search, Redis, Azure Data Explorer ‚Ä¢ Docker containers ‚Ä¢ Azure DevOps and fully automated CI/CD pipelines with Databricks Asset Bundles, ArgoCD, GitOps, Helm charts ‚Ä¢ Automated tests How do we work? #Agile #Scrum #Teamwork #CleanCode #CodeReview #Feedback #BestPracticies ‚Ä¢ We follow Scrum principles in our work ‚Äì we work in biweekly iterations and produce production-ready functionalities at the end of each iteration ‚Äì every 3 iterations we plan the next product release ‚Ä¢ We have end-to-end responsibility for the features we develop ‚Äì from business requirements, through design and implementation up to running features on production ‚Ä¢ More than 75% of our work is spent on new product features ‚Ä¢ Our teams are cross-functional (7-8 persons) ‚Äì they develop, test and maintain features they have built ‚Ä¢ Teams‚Äô own domains in the solution and the corresponding system components ‚Ä¢ We value feedback and continuously seek improvements ‚Ä¢ We value software best practices and craftsmanship Product principles: ‚Ä¢ Domain model created using domain-driven design principles ‚Ä¢ Distributed event-driven architecture / microservices ‚Ä¢ Large-scale system for large volumes of data (>100TB data), processed by Apache Spark streaming and batch jobs powered by Databricks platform Your responsibilities: ‚Ä¢Develop and maintainthe leading IT solution for the energy market using Apache Spark, Databricks, Delta Lake, and Apache Kafka ‚Ä¢Have end-to-end responsibilityfor the full lifecycle of features you develop ‚Ä¢Designtechnical solutions for business requirements from the product roadmap ‚Ä¢Maintainalignment with architectural principles defined on the project and organizational level ‚Ä¢Ensure optimal performancethrough continuous monitoring and code optimization. ‚Ä¢Refactor existing codeandenhance system architectureto improve maintainability and scalability. ‚Ä¢Design and evolvethe test automation strategy, including technology stack and solution architecture. ‚Ä¢Preparereviews,participatein retrospectives,estimateuser stories, andrefinefeatures ensuring theirreadinessfor development. Personal requirements: ‚Ä¢ Have4+years ofApache Sparkexperience and have faced various data engineering challenges in batch or streaming ‚Ä¢ Have an interest instream processingwith Apache Spark Structured Streaming on top of Apache Kafka ‚Ä¢ Have experienceleading technical solution designs ‚Ä¢ Have experience withdistributed systems on a cloud platform ‚Ä¢ Have experience withlarge-scale systems in a microservice architecture ‚Ä¢ Are familiar withGit and CI/CD practices and can design or implement the deployment process for your data pipelines ‚Ä¢ Possess aproactive approachand can-do attitude ‚Ä¢ Are excellent inEnglish and Polish, bothwritten and spoken ‚Ä¢ Have ahigher educationin computer science or a related field ‚Ä¢ Are ateam playerwith strong communication skills Nice to have requirements: ‚Ä¢ Apache Spark Structured Streaming ‚Ä¢ Azure ‚Ä¢ Domain Driven Development ‚Ä¢ Docker containers and Kubernetes ‚Ä¢ Message brokers (i.e. Kafka) and event-driven architecture ‚Ä¢ Agile/Scrum Our offer: ‚Ä¢ Contract type: B2B ‚Ä¢Work Mode: Flexible ‚Äî this role supportson-site,hybrid, andremotearrangements, depending on your individual preferences. ‚Ä¢Occasional on-site presence may be required‚Äî for example, onboard new team members, explore new business domains, or refine requirements in close collaboration with stakeholders or team building activities. What does the recruitment process look like? ‚Ä¢ Phone conversation with Recruitment Partner ‚Ä¢ Technical interview with the Hiring Team ‚Ä¢ Cognitive test ‚Ä¢ Offer KMD (an NEC company) is committed to providing equal opportunities. Hence, we invite all qualified interested applicants to apply for career opportunities. At KMD all aspects of employment and cooperation including the decision to hire/cooperate with will be based on merit, competence, performance, and business needs without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other status protected under local anti-discrimination legislation.","[{""min"": 115, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,115,180,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,530,Senior Data Analyst,Jit Team,"Salary: 900-1050PLN/day on B2B Work model: hybrid from Gda≈Ñsk (at least 2 day per week from the office) Why choose this offer? You can expect aflexible work organization Theinternational work environmentwill give you the opportunity to interact with the English language on a daily basis Scandinavian organizational culturewill provide you with work-life balance, you will gain time for additional training (financed by Jit) TheJit communitywill bring you a nice time during regular integration meetings Project Join a team responsible foranalyzing and modernizing a complex data environmentwithin alarge organization,operating in thefinancial sector.The project focuses on understanding and documenting current data lineage across legacy systems, as well as designing afuture-state solution fordata acquisition, integration, and modeling from strategic sources. The goal is not only to map existing data processes, but also to propose optimized solutions that meet both business and technical needs. Responsibilities you'll have Analyze legacy systemsto identify data sources,transformations, and destinations Create detaileddocumentation ofdata flows (data lineage)anddata models Evaluate the qualityand relevance of existing data Collaborate withbusiness stakeholders and IT teams to understand data requirements Design and propose afuture-state data solutionand architecture Prepare technical documentation and project-related deliverables Expected competences and knowledge Min.4 years of experiencein data management, IT/business analysis, or data architecture Strong knowledge ofdata modeling, data flows, mapping techniques, and transformations Experience insolution design and system integration Experience withdata warehouses and databases(e.g., Snowflake, DB2) as well asETL/ELT tools(e.g., DBT, Alteryx) Familiarity withdata visualization tools(e.g., Power BI, QlikView) Excellent communication skills in English Self-driven withstrong stakeholder management skills, including communication, collaboration, requirement gathering, and expectation management Nice to have Experience in thebanking or financial sector Background in data programming Technologiesyou'll work with Databases & Data Warehouses: Snowflake, DB2 ETL / ELT Tools: DBT, Alteryx Data Visualization: Power BI, QlikView Languages & Techniques: SQL, data analysis, data mapping Documentation: UML, data lineage diagrams, data models Client‚Äì why choose this particular client from the Jit portfolio? Jit Team has had anover-decade-long relationshipwith the leading financial group in the Nordic countries, and we are privileged to be our client's premier partner in Poland. At present,over 200 Jit personnelare engaged in the completion ofmore than 60 projectsfor this Norwegian major provider of financial services with a global presence and a strong focus on modern technology. Our customer's work atmosphere is epitomized by theScandinavian culture, which is conducive to people who place emphasis onwork-life balance and feedback culture. Furthermore, all projects are executed in international teams, giving constant exposure to the English language. About Jit Team The Human factor of IT- it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employover 500 experienced experts. We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share whatwe have achieved over 15 years of activity. By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 17100, ""max"": 22050, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,17100,22050,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,533,"Senior Data Engineer, Active Journey Alliance (m/f/x)",HelloFresh,"HelloFresh Group, the world‚Äôs leading integrated food solutions provider, is expanding with a new R&D Tech office in Poland. With brands offering meal kits, ready-to-eat meals, and specialty products such as meat, seafood, and pet food, we are seeking individuals who are ready to make an impact from day one. Joining us in Wroc≈Çaw means shaping the culture, working on meaningful R&D Tech projects, and contributing to a global company changing how people eat. At HelloFresh Group, we are driven by a high-performance culture that values speed, agility, and continuous learning. We believe in hands-on contribution and fostering a truly collaborative, egoless environment where every team member contributes to our mission ofchanging the way people eat, foreverand welcome team players who thrive in a dynamic environment, lead with ownership, and bring diverse perspectives to the table. Our teams thrive on in-person collaboration, with an expectation to work from the office four days a week. This approach creates a dynamic space where ideas flourish, decisions are made efficiently, and our collective impact is accelerated. It's how we stay closely connected to our shared goals and drive swift execution. The position is part of the Active Journey Alliance, which has the mission to maximize retention of active customers. Active Journey Alliance helps customers to create habits with our products, manage their subscription, order the products they love and provide help in the face of errors. Our Data Engineers assume development and operational responsibility for the HelloFresh Platform that enables serving millions of customers globally by providing the best & efficient experience for our customers and internal users. Take ownership of the end-to-end process, including architecture, design, development, deployment, and operations of data pipelines, applying DevOps practices, pair programming, and other cutting-edge methodologies. Be active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Back-end Engineers, Front-end Engineers, Product Analysts, and Business Intelligence teams Demonstrate an in-depth understanding of HelloFresh‚Äôs core product and architecture, and act as ambassador for software solutions offering support and mentorship to colleagues Work with state-of-the-art technologies like AWS (EMR, Glue, s3, etc), Kafka, PySpark, Kubernetes, Airflow, Prefect, Tecton, Databricks, as well as our in-house Data Pipeline tools Strong background in software engineering with a focus on writing clean, maintainable Python code Proficient in working with cloud platforms and data technologies such as PySpark, Kubernetes, and Kafka Skilled in designing scalable data models and working with relational databases (e.g., PostgreSQL) and object stores (e.g., AWS S3) Track record of building and optimizing efficient, reliable data pipelines Committed to ensuring data quality, implementing monitoring practices, and maintaining high testing standards across the development lifecycle Comfortable in agile environments, embracing fast iterations, lean principles, and continuous delivery A collaborative team player who actively mentors others, stays curious, and is eager to learn and share new trends Health- You‚Äôre covered from your first day with private health insurance Hybrid Working Schedule- We work in-office 4 days a week to align on goals, with flexible hours to support work-life balance and personal needs Holidays- You receive 26 days of paid vacation each year, providing you time to rest and recharge Learning and Development- An annual Learning & Development budget and a Mentoring Program to support your ongoing professional growth Employee Referral Program- Our team members can participate in our internal employee referral program and receive a bonus for recommending successful candidates to open roles Daily Comforts- Free coffee, drinks, and fresh fruit are available to keep you refreshed throughout the day If you are passionate about making a tangible impact and thrive in a fast-paced environment where your work directly contributes to a global purpose, we encourage you to apply ‚Äì even if your experience doesn't tick every single box, we believe there are many ways to develop skills and grow with us.","[{""min"": 14700, ""max"": 22100, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14700,22100,Gross per month - Permanent
Full-time,Senior,B2B,Remote,535,Data Vault Expert with Snowflake,Experis Manpower Group,Main Responsibilities: Assess the current platform and propose a path forward Stakeholder and change management Define platform strategy and vision Coordinate delivery and planning Required Skills: Min. 5 years of experience Experience withData Vault 2.0 Proficiency inSnowflake Hands-on experience withDBT Familiarity withAWS What We Offer: B2B contract via Experis Sports card Life insurance Private medical care Fully remote work - candidate must be located in Poland,"[{""min"": 28560, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Architecture,28560,30240,Net per month - B2B
Full-time,Senior,B2B,Hybrid,536,Big Data Engineer,Link Group,"üöÄ Big Data Engineer Full-time | AdTech Platform | Python/Java/Scala We‚Äôre looking for an experienced Big Data Engineer to join our high-impact team building the backbone of a global advertising platform delivering personalized content to millions of media-enabled devices. Your work will directly influence data-driven decision making, real-time targeting, and analytics pipelines for one of the most advanced AdTech ecosystems in the industry. üîç What You‚Äôll Be Doing Build and maintain robust, scalable data pipelines to support attribution, targeting, and analytics Collaborate closely with Data Scientists, Engineers, and Product Managers Design and implement efficient storage and retrieval layers for massive datasets Optimize data infrastructure and streaming processing systems (e.g. Flink, Apache Ignite) Drive quality through unit tests, integration tests, and code reviews Develop and maintain Airflow DAGs and other orchestration pipelines Translate business needs into robust, technical data solutions Lead or support A/B testing and data-driven model validation Contribute to R&D initiatives around cloud services and architecture ‚úÖ What You Bring 5+ years of experience in backend/data engineering using Python, Java, or Scala Strong experience with Big Data frameworks (Hadoop, Spark, MapReduce) Solid knowledge of SQL/NoSQL technologies (e.g. Snowflake, PostgreSQL, DynamoDB) Hands-on with Kubernetes, Airflow, and AWS (or similar cloud platforms) Stream processing experience: Flink, Ignite Experience with large-scale dataset processing and performance optimization Familiarity with modern software practices: Git, CI/CD, clean code, Design Patterns Fluent in English (B2+) Degree in Computer Science, Telecommunications, or related technical field ‚≠ê Bonus Points Experience with GoLang or GraphQL Hands-on with microservices or serverless solutions Experience in container technologies (Docker, Kubernetes) Previous work in AdTech, streaming media, or real-time data systems","[{""min"": 100, ""max"": 130, ""type"": ""Net per hour - B2B""}]",Data Engineering,100,130,Net per hour - B2B
Full-time,Senior,Permanent,Remote,537,Senior Application Manager Microsoft M365,Simon-Kucher CBS,"Become part of a unique entrepreneurial team.Think independently, use your initiative, and take some risks. Entrepreneurship is a powerful force that drives the growth not only of our firm but our clients and people. Unlock the power of opportunity.Advance your career in a thriving company that creates positive impact. We invest in your professional development every step of the way. Enjoy balance and flexible working.Be empowered to do your best work ‚Äì we offer flexible and hybrid working, sabbaticals, and paid time off. Prioritize your health and wellbeing.No matter where you live, we offer a competitive suite of health benefits to help keep you and your loved ones safe. Work in a values-driven culture.At Simon-Kucher, our vision is to become the world's leading growth specialist. Our values guide the way we do business and communicate our distinctiveness. They sum up what we stand for, influence our culture, and drive how and why we do things. Be responsible for the planning, development, implementation and operation of Microsoft M365 tools (SharePoint, Teams) and Power Platform applications (Power Apps, Power Automate, etc.). Collaborate with and consult stakeholders to understand business needs, recommend application enhancements, and drive the adoption of best practices. Participate in strategic planning to align Microsoft services with business goals, staying abreast of industry trends and emerging technologies. Define operational level agreements within the IT organization and ensure compliance with them. Oversee configuration and maintenance of the Microsoft applications in close collaboration with the Simon-Kucher MS operations team. Ensure advanced technical support to end-users, addressing and resolving service-related issues according to agreed service level agreements. Liaise with software vendors for support, updates, enhancements, and developments, ensuring service level agreements are met. Support and enhance knowledge management and digital collaboration based on Microsoft tools and solutions. Develop and deliver training sessions for end-users (in collaboration with our Learning & Skill Development team), create detailed documentation, and maintain up-to-date knowledge bases. Manage incident resolution and problem-solving processes, conducting root cause analysis and implementing preventive measures. Apprenticeship as IT specialist or bachelor‚Äôs degree in information systems, computer science, IT, or a related field. Around 5 years of experience in Microsoft application management or a similar role. Proven experience in developing and customizing applications within the Microsoft 365 environment, including SharePoint, Power Apps, and Power Automate. Understanding of web technologies such as HTML5, CSS, and JavaScript. Experience with SAAS enterprise software solutions and cloud technologies. Familiarity with integration scenarios and process automation scenarios as well as API management. Advanced communication skills to effectively convey technical information to non-technical stakeholders and collaborate with cross-functional teams. A keen focus on user satisfaction, understanding their needs, and ensuring applications meet their expectations. Strong analytical, problem-solving, and organizational skills. Proficient in both spoken and written English to effectively communicate with international teams, vendors, and stakeholders.","[{""min"": 25000, ""max"": 28000, ""type"": ""Gross per month - Permanent""}]",Unclassified,25000,28000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,539,Python Developer with AI,Pretius,"At Pretius we are looking for Python Developer with AI to a project in the financial industry, international team. Location: remote Salary: 150-210 pln netto/h Project / Role Build and validate intelligent assistants with cutting-edge LLMs (e.g., GPT-4, Falcon 2, LLAMA 3, Mixtral) using techniques like RAG and agent frameworks (e.g., Langraph, CrewAI) Recommend technical approaches and architectures for business challenges Implement efficient Python-based data pipelines for AI model training and deployment Communicate insights to both technical and non-technical audiences Contribute to project documentation Requirements 4+ years of relevant experience Proficiency in Python and LLM frameworks (e.g., Langchain) Experience with transformer-based models and large datasets (e.g. Pandas, NumPy, SQL) Strong knowledge of ML/AI concepts: types of algorithms, machine learning frameworks,model efficiency metrics, model life-cycle, AI architectures Experience with data engineering, including preprocessing, transformation, and pipeline automation Fluent in English (C1) Nice to have: Familiarity with cloud-based ML services (AWS, Azure, GCP) What we offer? We focus on long-term relationships based on fair principles and reliability Co-financing of the Multisport card and Medicover private healthcare Modern office available Team bonding activities, internal courses, conferences, certifications","[{""min"": 25200, ""max"": 35280, ""type"": ""Net per month - B2B""}]",Data Science,25200,35280,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,540,Sr Data Engineer - Product Core Data Asset,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Senior Data Engineer - Product Core Data Asset The PS Data & Analytics team at Bayer Consumer Health focuses on driving digital transformation and innovation by creating best-in-class analytical solutions that enable data-driven decision making and performance optimization for Bayer Consumer Health‚Äôs Supply Chain organization. You will be part of the data & analytics organization and will be responsible for building data products in the area of product supply and supply chain. You partner with business stakeholders, data architects, data scientists, analytics leads as well as other engineers. You will build data pipelines, data models and provision data for Analytics products and data scientists. You will also make sure that proper development processes are followed within the team, enhance implementation frameworks and guide other team members in building scalable, secure and well performing data products. If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Ingest data from transactional systems and data warehouses of other business functions into the divisional data analytics platform. Key Tasks & Responsibilities: Integrate data from different sources (e.g. supply chain planning data, product hierarchy, quality data, inventory data, procurement data, distributor and transportation data) to develop globally harmonized data models and KPI calculations Contribute to define data management and data quality standards. Ensure that data is well-managed to build stable, reusable and quality assured data assets. Collaborate with other IT functions (enabling functions data asset teams, analytics teams, platform product managers & integration architects) to ensure the aforementioned activities are executed effectively. Ensure that product supply data products adhere to the data protections standards. Continuously enhance implementation frameworks based on the needs of the analytics products in your responsibility. Guide other data engineers in your team (internal or external engineers) and ensure that all engineers apply same design principles. Together with the assigned data architect, ensure that cost and time estimations are accurate, quality of delivery is assured, and deliverables are properly tested, documented and handed over to the operations team Qualifications & Competencies (education, skills, experience): Bachelor/Master‚Äôs degree in Computer Science, Engineering, or a related field. 5+ years of working experience in the field of Data & Analytics, preferably in the area of product supply and the CPG industry Excellent data engineering & technology knowledge (Azure Data Lake Gen2, Azure Synapse, Databricks, Snowflake, potentially also legacy stack SAP Hana, SQL, Python as well as data management knowhow (data cataloguing, data quality management) Knowledge of CI/CD processes and tools (GitHub, Azure DevOps Pipelines) Profound data content knowledge (Supply Chain, Logistics, Quality Management) and Product Supply process knowhow Experience in Agile methodologies (Scrum, Kanban) Strong problem solving and analytical skills, combined with impeccable business judgment. Excellent interpersonal and communication skills, active listening, consulting, challenging, presentation skills. Fluent in English, both written & spoken, intercultural awareness and willingness to travel What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,28500,Gross per month - Permanent
Full-time,Senior,B2B,Remote,541,Senior Data Engineer (Azure/Databricks),GS Services,Poszukujemy superbohater√≥w ‚Äì Senior Azure Data Engineer‚Äô√≥w! ü¶∏‚Äç‚ôÄÔ∏èü¶∏‚Äç‚ôÇÔ∏è,"[{""min"": 170, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,190,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,545,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for aData Engineerwho can help us to provide solutions for customers for making informed decisions in volatile short-term markets. You will design, implement, and maintain data pipelines and storages, become a data manager of our model inputs, and create insightful and powerful analysis and visualization for day-ahead, intraday and balancing data. In our day-to-day work, we include pair programming, joint learning sessions, and recurring hacking days to explore new ideas. We have very much an agile and digital way of working with fast feedback loops and embracing a culture of learning and personal growth. What you will be doing to make a difference? Thrive in an empowered, self-driven team where you take ownership across the entire data lifecycle: Work together with in-house analysts to understand the domain and the data in question. Design, build and maintain flexible and scalable end-to-end data pipelines together with software engineers. Monitor quality and reliability of data and implement required tooling in coordination with data scientists. Visualize data in a meaningful way for in-house analysis and together with product manager and UX designer, for customer facing dashboards. What do you need to succeed in the role? A Bachelor‚Äôs or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. A good understanding of database systems. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Why will you love being part of our team? Supportive Onboarding: Begin your journey with a thorough introduction and a steep learning curve. Room to Grow: Shape and develop your role with a large degree of influence. Mission-Driven Culture: Join one of Europe‚Äôs most exciting green tech companies and contribute to building a more sustainable future. Inclusive Environment: Work in an innovative, international, and supportive atmosphere. Competitive Benefits: Enjoy salaries that reflect your professional experience, flexible working hours, and a hybrid work model that fits your lifestyle. Team Spirit: Collaborate with talented, inspiring colleagues who believe in succeeding together. Attractive Perks: Benefit from our referral program and other employee-focused initiatives. We are looking to hire for Volue office in Gda≈Ñsk but will be ready to consider other locations for the right candidate. In Volue, we cherish each employee‚Äôs competence, ideas and personality. Let your skills and talent be a part of our team ‚Äì and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14000,22000,Net per month - B2B
Full-time,Mid,B2B,Remote,547,Data Engineer,edrone,"We're a hard-working, fun-loving, get-things-done type of team dedicated to providing unique marketing automation solutions for clients. We understand the challenges of eCommerce and the importance of seamless customer service and satisfaction. We roll our sleeves up, act fast, and learn together. We're looking for a Data Engineer who will do the same! üöÄ Who are we? Edrone is a SaaS-based product that helps thousands of small and medium-sized businesses compete with major brands. Our mission is to provide simple solutions to big challenges in eCommerce. We achieve results through a strong feedback culture and clearly defined, transparent expectations. Currently, we work with nearly 2,000 online stores ‚Äî primarily in Poland and Brazil. Brands such asBielenda,Mosquito,2005,orLilouhave placed their trust in our product. If you want to learn more about our culture and what it‚Äôs like to work at edrone, check it outhere. Our social media: LinkedIn,Instagram,YouTube. Sounds great? Keep on reading! ‚ú® What‚Äôs in it for you: Be part of a small, fast-paced team that values innovation, efficiency, and a positive work culture. We thrive on challenges, embrace change, and keep things moving. We value initiative and ownership‚Äîif something makes sense, we act on it quickly and take full responsibility for delivering it. Direct responsibility for projects, regular 1: 1s with your leader with a blameless postmortems, code reviews B2B contract (15-20k) & covering all the costs of accounting services Hybrid or remote work or a modern, well-equipped office - whatever you prefer! 26 paid days off so you can relax properly! Benefits - MultiSport card, LuxMed medical package, group accident insurance, English classes, and Hedepy - a portal for mental health and development üöÄ How you will spend your time: Backend System Development Design, build, and maintain robust Python-based services and microservices Develop and optimize RESTful APIs and background services supporting core business logic and integrations Ensure code quality, reusability, and scalability through modular design and adherence to best practices Cloud-Native Application Engineering Develop serverless and containerized applications usingAWS Lambda,ECS, and other cloud-native tools LeverageAWS services(S3, RDS, DynamoDB, Step Functions, etc.) to support backend operations and workflows Collaborate with DevOps to provision, deploy, and monitor cloud infrastructure Automation and Task Orchestration Automate recurring tasks, background jobs, and workflows using Python scripts and AWS orchestration tools Build and maintain task schedulers and asynchronous workers for time-sensitive operations Implement monitoring, logging, and alerting systems for observability and proactive issue resolution Data Access and Integration Build data access layers and connectors for interfacing with relational and NoSQL databases Develop data integration scripts or services to move and sync data between systems when needed Write efficient, production-grade SQL and Python code to support internal tools and services Contribute to Innovation and Excellence Stay informed on modern Python practices, libraries, and AWS developments Take initiative in proposing improvements and new ideas to enhance our platform üëÄWho you are: 3+ years of experienceas a Data Engineer. Hands-on experience withschema design, complex SQL/query optimization,and running data pipelines in production. Experienced withAWS services (Redshift, Aurora, DynamoDB, S3, Glue, Lambda, Step Functions, etc.)to build data pipelines and scalable cloud-native applications. dbtexperience (or strong SQL/ELT background and eagerness to learn dbt quickly). Familiarity withdata orchestration tools(e.g., Airflow, Step Functions) ‚Äî scheduling, monitoring, and troubleshooting data pipelines. Ability tobuild and maintain RESTful APIs/microservicesin Python (e.g., FastAPI/Flask) and understand basic backend architecture. üëÄ It‚Äôs nice if you have: Experience in Java is a plus. üìùHow does the recruitment process look like: A 30-minute phone interview with the recruiter -Milena Micor, where we aim to get to know you a little better! A technical online interview with theTeam Lead Krystian Andruszek and another panelist A short call with ourCTO ‚Äì Maciej Mendrela‚Äì where we‚Äôll share more about the direction of our organization and how we see this role evolving Decision regarding the offer and welcome on board! ‚≠ê After each stage, you will always receive feedback regarding your candidacy.","[{""min"": 15000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,15000,20000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,549,Senior Data Engineer,N-iX,"#3071 We are seeking a proactive Senior Data Engineer to join our vibrant team. As a Senior Data Engineer, you will play a critical role in designing, developing, and maintaining sophisticated data pipelines, Ontology Objects, and Foundry Functions within Palantir Foundry. The ideal candidate will possess a robust background in cloud technologies, data architecture, and a passion for solving complex data challenges. Key Responsibilities: Collaborate with cross-functional teams to understand data requirements, and design, implement and maintain scalable data pipelines in Palantir Foundry, ensuring end-to-end data integrity and optimizing workflows. Gather and translate data requirements into robust and efficient solutions, leveraging your expertise in cloud-based data engineering. Create data models, schemas, and flow diagrams to guide development. Develop, implement, optimize and maintain efficient and reliable data pipelines and ETL/ELT processes to collect, process, and integrate data to ensure timely and accurate data delivery to various business applications, while implementing data governance and security best practices to safeguard sensitive information. Monitor data pipeline performance, identify bottlenecks, and implement improvements to optimize data processing speed and reduce latency. Troubleshoot and resolve issues related to data pipelines, ensuring continuous data availability and reliability to support data-driven decision-making processes. Stay current with emerging technologies and industry trends, incorporating innovative solutions into data engineering practices, and effectively document and communicate technical solutions and processes. Tools and skills you will use in this role: Palantir Foundry Python PySpark SQL TypeScript Required: 5+ years of experience in data engineering, preferably within the pharmaceutical or life sciences industry; Strong proficiency in Python and PySpark; Proficiency with big data technologies (e.g., Apache Hadoop, Spark, Kafka, BigQuery, etc.); Hands-on experience with cloud services (e.g., AWS Glue, Azure Data Factory, Google Cloud Dataflow); Expertise in data modeling, data warehousing, and ETL/ELT concepts; Hands-on experience with database systems (e.g., PostgreSQL, MySQL, NoSQL, etc.); Proficiency in containerization technologies (e.g., Docker, Kubernetes); Effective problem-solving and analytical skills, coupled with excellent communication and collaboration abilities; Strong communication and teamwork abilities; Understanding of data security and privacy best practices; Strong mathematical, statistical, and algorithmic skills. Nice to have: Certification in Cloud platforms, or related areas; Experience with search engine Apache Lucene, Webservice Rest API; Familiarity with Veeva CRM, Reltio, SAP, and/or Palantir Foundry; Knowledge of pharmaceutical industry regulations, such as data privacy laws, is advantageous; Previous experience working with JavaScript and TypeScript.","[{""min"": 18470, ""max"": 19579, ""type"": ""Net per month - B2B""}, {""min"": 14776, ""max"": 15515, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18470,19579,Net per month - B2B
Full-time,Mid,B2B,Hybrid,550,GenAI Productivity Analyst,ITDS,"GenAI Productivity Analyst Join us, and lead the charge in AI-powered developer tools! Krak√≥w - based opportunity with hybrid work model (6 days/month in the office). As a GenAI Productivity Analyst, you will be working for our client, a global financial services leader pioneering developer productivity enhancements through innovative data science projects. Your role focuses on advancing the initial proof-of-concept linking GenAI coding assistant usage to developer productivity, operationalizing these insights, and identifying actionable patterns and use cases. Collaborating with data scientists and programme teams, you will help unlock industry-leading knowledge that supports developer performance improvements across thousands of users, setting new standards in GenAI adoption and value realization within a complex, fast-paced environment. Your main responsibilities: Reviewing and enhancing the existing GenAI CA Productivity Analysis proof-of-concept Developing a future roadmap and delivery strategy for GenAI CA productivity insights Identifying specific use cases and technical patterns where GenAI CAs impact productivity most Highlighting challenges, accelerators, and best practices for GenAI CA adoption Contributing to a data science strategy for measuring GenAI CA benefits Gathering, analyzing, and interpreting SDLC, Deployment, and DORA metrics data Collaborating with data analysts and scientists to refine analytical models and methodologies Defining delivery strategy, phased plans, and MVP implementation objectives Communicating actionable insights clearly to senior stakeholders Supporting delivery teams with inputs to develop initiatives improving GenAI CA benefits for developers You're ideal for this role if you have: Proven experience in data analysis within software development contexts Strong understanding of SDLC, Deployment, and DORA metrics and their impact on developer productivity Experience translating data analysis into actionable business or operational insights Familiarity with GenAI technologies, specifically coding assistants like GitHub Copilot Proficiency in data analytics and processing tools such as Python, R, SQL, or Jupyter notebooks Solid knowledge of data modeling concepts Experience working in global matrix organizations and cross-cultural environments Excellent written and verbal communication skills in English, including report and presentation creation Ability to work independently in a fast-paced, dynamic environment with tight deadlines Strong interpersonal and influential communication skills It is a strong plus if you have: Exposure to machine learning libraries such as Scikit-learn, XGBoost, Keras, or PyTorch Track record in knowledge acquisition, transfer, and community building processes Experience rewriting or refining English content authored by non-native speakers Willingness to explore and implement emerging technologies Prior experience contributing to developer productivity or adoption programs We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7159 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here .","[{""min"": 23100, ""max"": 29400, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,23100,29400,Net per month - B2B
Full-time,Senior,Permanent,Remote,552,Senior/Lead Data Scientist,Link Group,"Do≈ÇƒÖcz do zespo≈Çu, kt√≥ry wykorzystuje dane, by ulepszaƒá procesy logistyczne. Szukamy osoby o analitycznym umy≈õle, kt√≥ra potrafi przekszta≈Çcaƒá liczby w konkretne dzia≈Çania i wspieraƒá decyzje operacyjne. Twoje zadania: Oczyszczanie i przygotowanie danych do analiz i test√≥w Budowa i doskonalenie modeli uczenia maszynowego Weryfikacja modeli, wyciƒÖganie wniosk√≥w i prezentowanie wynik√≥w Wsp√≥≈Çpraca z zespo≈Çami biznesowymi i technologicznymi Proponowanie nowych sposob√≥w usprawnienia proces√≥w na podstawie danych Czego oczekujemy: Umiejƒôtno≈õci programowania w Pythonie lub R Znajomo≈õci SQL oraz ≈õrodowisk danych, takich jak Databricks lub Snowflake Do≈õwiadczenia w pracy z modelami ML i ich wdra≈ºaniem Podstaw matematycznych (algebra, statystyka, rachunek r√≥≈ºniczkowy) Zdolno≈õci analitycznych i praktycznej znajomo≈õci takich metod jak regresje, drzewa decyzyjne, prognozowanie szereg√≥w czasowych Umiejƒôtno≈õci wizualizacji danych (np. z matplotlib, seaborn, ggplot2) Nice to have: Do≈õwiadczenie w ≈õrodowisku produkcyjnym ML Wiedzƒô z zakresu ≈Ça≈Ñcucha dostaw lub logistyki","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Science,20000,28500,Gross per month - Permanent
Full-time,Mid,B2B,Remote,553,Programista Baz danych,Eyzee S.A.,"Poszukujemy Programist√≥w Baz danych, kt√≥rzy majƒÖ do≈õwiadczenie w projektowaniu i eksploatacji baz danych dla naszego klienta w bran≈ºy medycznej. Celem projektu jest opracowywanie i wdra≈ºanie innowacyjnych rozwiƒÖza≈Ñ, programowanie nowych funkcjonalno≈õci, rozwijanie, modyfikowanie i testowanie oprogramowania. Tworzymy przyjazne miejsce pracy i rozwoju dla specjalist√≥w w bran≈ºy IT, zapewniamy ciekawe wyzwania, dbajƒÖc o dobrƒÖ komunikacjƒô i atmosferƒô w zespole. Z nami przyspieszysz rozw√≥j swojej kariery! Praca zdalna, pe≈Çen etat. Zadania dla Ciebie: projektowanie, implementacja i utrzymanie struktur relacyjnych baz danych (np. PostgreSQL, MySQL, MSSQL, Oracle) dla systemu medycznego tworzenie i optymalizacja logiki biznesowej w bazie danych przy u≈ºyciu jƒôzyk√≥w proceduralnych (np. PL/SQL, PL/pgSQL), w tym procedur sk≈Çadowanych, funkcji i wyzwalaczy pisanie z≈Ço≈ºonych i wydajnych zapyta≈Ñ SQL oraz skrypt√≥w do zarzƒÖdzania danymi i ich przetwarzania zapewnienie integralno≈õci, bezpiecze≈Ñstwa i optymalnej wydajno≈õci bazy danych wsp√≥≈Çpraca z zespo≈Çem deweloperskim i analitykami w celu realizacji wymaga≈Ñ projektowych Wymagania: min. 5 lat do≈õwiadczenia w pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL, MSSQL, ORACLE) praktyczne do≈õwiadczenie w programowaniu w proceduralnych jƒôzykach bazodanowych (np. PL/SQL, PL/pgSQL) znajomo≈õƒá technik migracji danych pomiƒôdzy r√≥≈ºnymi platformami bazodanowymi i systemowymi bieg≈Çej znajomo≈õci SQL do≈õwiadczenie z bazƒÖ danych PostgreSQL. umiejƒôtno≈õƒá strojenia zapyta≈Ñ SQL i procedur PL/pgSQL. znajomo≈õƒá zasad zarzƒÖdzania, konfiguracji i optymalizacji PostgreSQL Mile widziane: do≈õwiadczenie w bran≈ºy medycznej Co oferujemy? stabilne zatrudnienie w oparciu o kontrakt B2B s≈Çu≈ºbowy laptop i monitor dofinansowanie prywatnej opieki medycznej sportowƒÖ kartƒô Multisport nauka jƒôzyka angielskiego omawianie postƒôp√≥w i rozwoju co p√≥≈Ç roku transparentna komunikacja z pracownikami mo≈ºliwo≈õƒá zaanga≈ºowania siƒô w rozw√≥j organizacji chƒôtnie dzielimy siƒô wiedzƒÖ - do≈ÇƒÖcz do Akademii Eyzee mocny kompetencyjnie zesp√≥≈Ç sk≈ÇadajƒÖcy siƒô w wiƒôkszo≈õci z senior√≥w praca z narzƒôdziami JIRA, Confluence, BitBucket dbamy o integracje i chƒôtnie wsp√≥lnie spƒôdzamy czas Kim jeste≈õmy? Jeste≈õmy polskƒÖ firmƒÖ specjalizujƒÖcƒÖ siƒô w realizacji z≈Ço≈ºonych projekt√≥w informatycznych oraz doradczych dla firm z sektora finansowego, telekomunikacyjnego i publicznego. Stanowimy zgrany zesp√≥≈Ç konsultant√≥w z wiedzƒÖ i wieloletnim do≈õwiadczeniem w tworzeniu i utrzymywaniu rozwiƒÖza≈Ñ. Wa≈ºne dla nas sƒÖ: doprecyzowanie wymaga≈Ñ przed napisaniem kodu, jako≈õƒá tworzonego kodu, testowanie oraz CI/CD. Nasze projekty to g≈Ç√≥wnie tworzenie nowych mikroserwis√≥w lub nowych funkcjonalno≈õci do istniejƒÖcych rozwiƒÖza≈Ñ. Dodatkowo rozwijamy w≈Çasne aplikacje i plugin‚Äôy, kt√≥re nie tylko usprawniajƒÖ pracƒô, ale te≈º pozwalajƒÖ rozwinƒÖƒá nasze do≈õwiadczenie. Jeden z nich mo≈ºesz pobraƒá tutaj (eZee Worklog). Jeste≈õmy partnerem Atlassian.","[{""min"": 15000, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Database Administration,15000,21000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,555,Developer Hurtowni Danych,Clurgo,"Clurgo to firma stworzona przez developer√≥w dla developer√≥w. OdnajdƒÖ siƒô u nas pasjonaci nowych technologii takich jak Java, Node, Frontend, jak i mikroserwis√≥w. Jeste≈õmy firmƒÖ projektowƒÖ skupionƒÖ na dostarczaniu rozwiƒÖza≈Ñ IT dla naszych Klient√≥w, przy zachowaniu dobrych praktyk programistycznych oraz work-life balance. Realizujemy projekty dla r√≥≈ºnych bran≈º - od ma≈Çych projekt√≥w start-upowych, przez wysokobud≈ºetowe projekty dla miƒôdzynarodowych firm. ‚úÖDo≈ÇƒÖcz do projektu dla klienta z bran≈ºy ubezpiecze≈Ñ, gdzie Zesp√≥≈Ç Rozwoju Narzƒôdzi HD/BI rozwija Hurtowniƒô Danych w oparciu o technologie SAS i Oracle, jednocze≈õnie eksplorujƒÖc nowe rozwiƒÖzania jak Azure, Kafka czy REST API. Bƒôdziesz wspieraƒá zespo≈Çy biznesowe Hurtowni Danych oraz prowadziƒá projekty Proof of Concept nowych narzƒôdzi ‚úÖTechnologie i narzƒôdzia: SAS, Oracle, Azure, Kafka, REST API ‚úÖPraca hybrydowa (1 raz w tygodniu w biurze w Warszawie) ‚úÖPraca w metodyce SCRUM Szukamy Ciebie, je≈õli: üëâposiadasz do≈õwiadczenie w projektowaniu rozwiƒÖza≈Ñ z zakresuHurtowni Danych i Business Intelligence üëâdobrze znasz narzƒôdzia / ≈õrodowiska SAS i SAS Viya (DI Studio, Enterprise Guide, SAS Studio, Visual Analytics) üëâmasz do≈õwiadczenie w pracy z Oracle (mile widziane do≈õwiadczenie z Oracle Exadata i PL/SQL) üëâznasz zagadnienia i technologie CI/CD (Git / Bitbucket / Bamboo / JIRA) Zadania: budowa narzƒôdzi wspomagajƒÖcych pracƒô developer√≥w Hurtowni Danych - m. in. wsp√≥lne re-u≈ºywalne komponenty (SAS / SAS Viya / Oracle / inne technologie) budowa API ≈ÇƒÖczƒÖcego wykorzystywane w HD technologie (SAS / SAS Viya/Oracle / Office365 / REST API / inne technologie) budowa narzƒôdzi wspomagajƒÖcych wymianƒô danych pomiƒôdzy systemami informatycznymi m. in. Kafka POC nowych narzƒôdzi/technologii do potencjalnego wdro≈ºenia w HD m. in. SAS Viya & Exadata w chmurze Azure budowa narzƒôdzi monitorujƒÖcych aktywno≈õƒá u≈ºytkownik√≥w HD udzia≈Ç w pracach optymalizujƒÖcych przetwarzania HD, poprzez budowƒô wspierajƒÖcych komponent√≥w a tak≈ºe punktowe analizy i optymalizacje przetwarza≈Ñ budowa technicznych data mart-√≥w oraz raport√≥w BI (m. in. w SAS Viya lub Power BI) wsparcie projekt√≥w strategicznych w aspektach technologicznych Czego mo≈ºesz siƒô spodziewaƒá? Wsp√≥≈Çpracy w oparciu o kontrakt B2B Onboarding w biurze w Warszawie (dobrze skomunikowana lokalizacja) Pracy hybrydowej 1 raz w tygodniu z biura w Warszawie Prawdziwego work-life balance - w tej kwestii nie wierzymy w p√≥≈Ç≈õrodki üëå P≈Çaskiej struktury i niekorporacyjnej atmosfery üèÉ‚Äç‚ôÄÔ∏èBenefit√≥w: opieka medyczna, karta multisport, lekcje jƒôzyka angielskiego üôå Elastycznych godzin pracy üéâIntegracji - lubimy spƒôdzaƒá razem czas Profesjonalnego i procesu rekrutacyjnego ‚Äì zawsze otrzymasz od nas feedback niezale≈ºnie od decyzji. Poznaj nas lepiejüëâhttps: //www.facebook.com/clurgo/","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,559,Senior Data Engineer,Jit Team,"Salary: 1100‚Äì1440 PLN net/day ( B2B ) Work mode: Hybrid ‚Äì 1 day/week in Warsaw or Gdynia office Why choose this offer? You‚Äôll join a long-term , high-impact project focused on modern data processing and software asset management You‚Äôll work with an international team in a cloud-native environment ( GCP , Cloud Run , Pub/Sub , FastAPI ) Opportunity to develop scalable backend systems using Apache Beam and modern serverless patterns Full autonomy in technical decision-making and architecture evolution Stable , long-term collaboration with a global HR industry leader Project You‚Äôll join the Global SAM (Software Asset Management) team responsible for developing a cloud-native , data-driven platform used to analyze and manage enterprise software usage. The system is built on Google Cloud Platform , processes data in near real-time , and integrates with multiple systems using an event-driven architecture . You‚Äôll be responsible for designing and implementing data pipelines , backend APIs, and scalable serverless functions. This is a hands-on engineering role with lots of space for ownership and technical influence . Expected competences and knowledge Solid Python backend development experience, ideally with FastAPI Experience building data pipelines using Apache Beam or equivalent frameworks Familiarity with GCP services, especially Dataflow, Pub/Sub, Cloud Run Understanding of event-driven architecture and observability best practices Experience with CI/CD tools, preferably GitLab Ability to work independently in a distributed team Optional but welcome: knowledge of Google Cloud Firestore Technologies you'll work with Python backend development (FastAPI, data pipelines) Experience with Apache Beam or streaming frameworks Strong knowledge of GCP services (Dataflow, Pub/Sub, Cloud Run) Understanding of event-driven architecture and observability Familiarity with CI/CD workflows (GitLab preferred) Ability to work in a distributed team environment Optional: experience with Firestore or other GCP data stores Client ‚Äì why choose this particular client from the Jit portfolio? We have partnered with our Client, a leading HR company with over 60 years of experience and a presence in multiple countries around the world. Our Client provides a wide range of recruitment and job placement services and is dedicated to helping individuals achieve their career goals. In Poland, our Client has been operating for over 20 years and is at the forefront of digitization in the HR services industry. Our team has been building the Work Time system for our Client since 2019 , a complex tool for registering and settling the working time of full-time and temporary employees working under different legal and organizational conditions, supporting on-boarding processes, and integrating with the employee portal and mobile app. As a member of Jit Team, you will have the opportunity to work on a modern, complex enterprise-class system built in modern technologies such as Java and the AWS cloud . With a business-critical operation, the Work Time system settles working time for tens of thousands of employees every month and contributes to our Client's revenue. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 22000, ""max"": 28800, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,28800,Net per month - B2B
Full-time,Senior,B2B,Hybrid,560,"Senior Data Engineer ‚Äì Python, Spark, AWS",Link Group,"We‚Äôre looking for aSenior Data Engineerto join a highly skilled team responsible for building a scalable, multitenant data platform that powers data ingestion, transformation, cataloging, and distribution for internal and external stakeholders. This role is central to designing robust data pipelines, integrating modern data lakehouse components, and supporting the company‚Äôs AWS-based infrastructure. Design and build scalabledata ingestion pipelinesin AWS Work extensively withDatabricks, including Unity Catalog, streaming, Delta Lake, and data transformation tools Lead technical delivery and implementation of core platform components acrossraw, normalized, operational, and historical layers Collaborate with engineering managers and cross-functional teams across the US and Poland Drive technical excellence by improving team processes, architecture standards, and engineering best practices Support and consult with stakeholders to ensure successful delivery of data-driven solutions Contribute to the growth and maturity of the team‚Äôs cloud and data engineering capabilities 5+ years of experience in Pythonand strong software engineering skills Solid knowledge ofAWS cloud servicesand best practices Experience building scalableSparkpipelines inPySparkorScala Practical experience withSpark Streamingfor low-latency pipelines Familiarity withDelta Lakeand modern data lakehouse architectures Hands-on experience withKubernetesand container orchestration Experience withMongoDBand other NoSQL data stores Understanding ofmicroservicesandevent-driven architecture Excellent communication skills and ability to work in a collaborative, global team Commitment to high standards ofethics, quality, and delivery Familiarity withMachine Learning,AI models, orData Science workflows","[{""min"": 35000, ""max"": 40000, ""type"": ""Net per month - B2B""}]",Data Engineering,35000,40000,Net per month - B2B
Full-time,Senior,B2B,Remote,561,Data Architect,Connectis,"Wraz z naszym partnerem z bran≈ºy ubezpieczeniowej, poszukujemy eksperta/ekspertkina stanowiskoData Architect, kt√≥ry do≈ÇƒÖczy do strategicznego projektu budowyDataHubu‚Äì rozwiƒÖzania klasy mini hurtowni danych, wspierajƒÖcego dzia≈Çalno≈õƒá operacyjnƒÖ, raportowƒÖ, analitycznƒÖ oraz rozwiƒÖzania z zakresu generatywnej sztucznej inteligencji. Projektowanie i nadz√≥r nad wdra≈ºaniem skalowalnych, wydajnych modeli danych integrujƒÖcych wiele ≈∫r√≥de≈Ç. Tworzenie dokumentacji architektonicznej i mentoring mniej do≈õwiadczonych in≈ºynier√≥w. Zapewnienie jako≈õci danych: automatyczna walidacja, wykrywanie anomalii, monitoring. Definiowanie standard√≥w, wzorc√≥w i najlepszych praktyk w zakresie in≈ºynierii danych. Projektowanie rozwiƒÖza≈Ñ do przetwarzania danych strumieniowego i wsadowego. Ustalanie polityk dostƒôpu, zasad zarzƒÖdzania danymi i ram bezpiecze≈Ñstwa. Bliska wsp√≥≈Çpraca z zespo≈Çami DevOps, Data Engineering oraz AI/ML. üîçCZEGO OCZEKUJEMY OD CIEBIE? Do≈õwiadczenie zAzure DatabricksorazAzure DevOps. Bardzo dobra znajomo≈õƒáMicrosoft Azure. Znajomo≈õƒáApache Spark/PySpark. Bieg≈Ço≈õƒá wPythoniSQL. Mile widziane: Do≈õwiadczenie zDBT (Data Build Tool). Praktyczna znajomo≈õƒá narzƒôdziCI/CD Znajomo≈õƒá modeluData Vault 2.0. ‚ú®OFERUJEMY: Tryb pracy zdalnej z okazjonalnymi spotkaniami zespo≈Çu w biurze w centrum Warszawy(raz w miesiƒÖcu). Udzia≈Ç w strategicznym projekcie realizowanym dla miƒôdzynarodowego Partnera Biznesowego. Realny wp≈Çyw na architekturƒô i rozw√≥j rozwiƒÖzania wdra≈ºanego w wielu krajach. Dedykowane wsparcie opiekuna Connectis przez ca≈Çy czas trwania wsp√≥≈Çpracy. Wsp√≥≈Çpracƒô z do≈õwiadczonym zespo≈Çem in≈ºynier√≥w danych i DevOps√≥w. Mo≈ºliwo≈õƒá przed≈Çu≈ºenia wsp√≥≈Çpracy w kolejnych fazach programu. 5000 PLN za polecenie znajomego do naszych projekt√≥w. Szybki, zdalny proces rekrutacyjny. Dziƒôkujemy za wszystkie zg≈Çoszenia. Pragniemy poinformowaƒá, ≈ºe skontaktujemy siƒô z wybranymi osobami. 12170/DK","[{""min"": 150, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Architecture,150,200,Net per hour - B2B
Full-time,Senior,B2B,Remote,563,Senior Data Engineer,7N,"O projekcie: Poszukujemy do≈õwiadczonych Senior Data Engineer'√≥w, kt√≥rzy do≈ÇƒÖczƒÖ do nowo tworzonego w Polsce zespo≈Çu specjalizujƒÖcego siƒô w migracji danych do chmury. Projekt realizowany jest dla miƒôdzynarodowej du≈Ñskiej organizacji z sektora finansowego. Kluczowym zadaniem bƒôdzie zapewnienie p≈Çynnej transformacji istniejƒÖcych rozwiƒÖza≈Ñ oraz stworzenie trwa≈Çych fundament√≥w pod rozw√≥j nowoczesnej platformy danych opartej na architekturze Lakehouse. Praca w 100% zdalna, wymagana otwarto≈õƒá na podr√≥≈º do Danii na poczƒÖtku projektu. Zakres obowiƒÖzk√≥w Migracja istniejƒÖcych rozwiƒÖza≈Ñ danych do ≈õrodowiska chmurowego Projektowanie oraz implementacja proces√≥w ETL Modelowanie danych oraz tworzenie transformacji danych w oparciu o wymagania u≈ºytkownik√≥w ko≈Ñcowych Implementacja kontrakt√≥w danych i domen danych (np. Umowy, Polisy, Klienci, Roszczenia) Wsp√≥≈Çpraca z interesariuszami w celu zrozumienia wymaga≈Ñ biznesowych Przeprowadzanie test√≥w jako≈õci danych, preferencyjnie automatycznych, i tworzenie mechanizm√≥w kontrolnych zapewniajƒÖcych sp√≥jno≈õƒá danych Tworzenie i utrzymywanie dokumentacji technicznej i biznesowej Udzia≈Ç w planowaniu prac z wykorzystaniem Azure DevOps Udzia≈Ç w analizach potrzeb biznesowych i wsp√≥≈Çpraca z lokalnymi przedstawicielami klienta w celu okre≈õlenia wymaga≈Ñ funkcjonalnych i niefunkcjonalnych. Dbanie o jako≈õƒá i sp√≥jno≈õƒá kodu ≈∫r√≥d≈Çowego, w tym przestrzeganie dobrych praktyk programistycznych i standard√≥w kodowania przy pracy z SQL i Pythonem. Udzia≈Ç w rozwoju i utrzymaniu fundament√≥w danych wspierajƒÖcych produkty biznesowe Wsp√≥≈Çpraca z zespo≈Çami miƒôdzynarodowymi Oczekiwania Minimum 5 lat do≈õwiadczenia komercyjnego w roli Data Engineera Do≈õwiadczenie w migracji danych i projektowaniu rozwiƒÖza≈Ñ chmurowych Bardzo dobra znajomo≈õƒá jƒôzyk√≥w SQL i Python, w tym umiejƒôtno≈õƒá tworzenia skalowalnych i czytelnych skrypt√≥w oraz test√≥w automatycznych Praktyczna znajomo≈õƒá narzƒôdzi z ekosystemu Azure, w szczeg√≥lno≈õci: - Azure Data Factory (ADF) -Databricks - Azure DevOps (zarzƒÖdzanie backlogiem i planowaniem pracy) - (mile widziana znajomo≈õƒá: Microsoft Purview w zakresie rozumienia, niekoniecznie rozwoju) Do≈õwiadczenie w budowaniu proces√≥w ETL oraz tworzeniu i implementacji data pipelines Zrozumienie zasad tworzenia data contracts i domen danych Wiedza z zakresu testowania jako≈õci danych w procesach przetwarzania danych (ETL/ELT), np. Great Expectations ‚Äì mile widziane Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z migracjƒÖ danych i integracjƒÖ danych legacy Umiejƒôtno≈õƒá pracy w miƒôdzynarodowym zespole oraz komunikacji z interesariuszami - zdolno≈õƒá zrozumienia ich potrzeb i przekszta≈Çcania ich w wymagania techniczne Mile widziane certyfikaty: Databricks Certified Data Engineer Associate Microsoft Azure Data Fundamentals (DP-900) Mile widziana wiedza domenowa z zakresu ubezpiecze≈Ñ (w szczeg√≥lno≈õci um√≥w, os√≥b, polis, roszcze≈Ñ itp.) Bardzo dobra komunikacja w jƒôzyku angielskim Oferujemy Sta≈Çe wsparcie osobistego agenta , dbajƒÖcego o TwojƒÖ ciƒÖg≈Ço≈õƒá projektowƒÖ, kontakt z klientem, niezbƒôdne formalno≈õci, komfort pracy oraz rozw√≥j, Consultant Development Program ‚Äì doradztwo w planowaniu rozwoju w oparciu o najnowsze trendy i potrzeby rynku IT, obejmujƒÖce m.in. konsultacje z agentami i mentorami rozwoju , Dostƒôp do 7N Learning & Development ‚Äì platformy rozwojowo-edukacyjnej z webinarami, bibliotekƒÖ artyku≈Ç√≥w i raport√≥w bran≈ºowych oraz regularnymi zaproszeniami na jednorazowe i cykliczne wydarzenia rozwojowe ‚Äì techniczne, biznesowe oraz life-stylowe, Spektakularne eventy integracyjne, zar√≥wno dla Ciebie (np. coroczny wyjazd Kick-Off , imprezy ≈õwiƒÖteczne czy sportowe Letnie Igrzyska), jak i dla Twoich bliskich (np. pikniki rodzinne), Rozw√≥j zawodowy nie tylko podczas projektu ‚Äì mo≈ºesz zaanga≈ºowaƒá siƒô w przekazywanie wiedzy innym w ramach oferty 7N Services kierowanej do klient√≥w 7N, Relacje i dostƒôp do wiedzy najbardziej do≈õwiadczonych ekspert√≥w IT na rynku ‚Äì ≈õredni sta≈º zawodowy naszego Konsultanta w Polsce to ponad 10 lat, Pakiet benefit√≥w zaplanowany od A do Z, czyli dofinansowanie do opieki medycznej, ubezpieczenia na ≈ºycie, karty sportowej dla Ciebie i Twoich bliskich, a tak≈ºe zni≈ºki do sklep√≥w w Polsce i za granicƒÖ. O 7N CiƒÖg≈Çe poszukiwanie projekt√≥w, trudne negocjacje stawek, brak wsparcia w rozwoju ‚Äì brzmi znajomo? W 7N zyskujesz nie tylko stabilno≈õƒá kontrakt√≥w, ale te≈º zaanga≈ºowanie osobistego opiekuna dbajƒÖcego o Tw√≥j komfort zawodowy i sta≈Çy dostƒôp do inicjatyw rozwojowych. Naszym celem jest zapewnienie Ci stabilnej i komfortowej wsp√≥≈Çpracy, kt√≥ra przyczyni siƒô do sukcesu Twojego jako eksperta IT oraz naszych klient√≥w. Budujemy trwa≈Çe relacje, bazujƒÖc na skandynawskich warto≈õciach i 30-letnim do≈õwiadczeniu w tworzeniu rozwiƒÖza≈Ñ IT dla ponad 200 organizacji.","[{""min"": 160, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,185,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,564,BI Developer / SQL Developer,Beesafe,"About Us: Join our trailblazing team as we expand our digital horizons. From our beginnings as visionary InsurTech to becoming a key player in the Polish digital insurance market, we are part of the esteemed Vienna Insurance Group. We're redefining the rules in the insurance industry with our innovative approach. Our hybrid working model supports both the collaborative energy of office work and the flexibility of remote work. About the Role: We're seeking passionate BI/SQL Developer to join our team. You'll be at the heart of developing and implementing high-quality application software, using state-of-the-art tools and technologies. This is your chance to make a significant impact on one of the most exciting and unique products in the Polish digital insurance market. Why it is worth to work with us? You‚Äôll be contributing to the reporting solution and data model design of our Data Platform with close cooperation with our Data Engineers You‚Äôll gather business requirements and work closely with business stakeholders You‚Äôll deliver end-to-end business intelligence/reporting solutions using SQL/PowerBI What you need to start the adventure with us: +1 year of commercial experience in data extraction, ETL, and report development Proficiency in SQL Understanding of Relational Database Management System and Business Intelligence concepts Business and collaboration skills, and responsive to service needs and operational demands Domain knowledge gained across the insurance or financial sector Nice to have: Experience with BI tools (Power BI would be a plus) Experience with cloud solutions (we use Azure) Familiarity with code version control systems such as GIT Understanding of the principles of Agile and Scrum (we work in Scrum) Enthusiastic approach to coffee breaks (we love informal discussions with a cup of favorite coffee or tea) Why Join Us? Be part of a dynamic team driving digital innovation in the insurance and eCommerce sectors Opportunity to work in a collaborative and forward-thinking environment Contract options: B2B cooperation Engage in meaningful work that directly impacts business success Join a company that values work-life balance and fosters a positive team culture Comprehensive onboarding, including a dedicated Buddy program Remote work flexibility with hybrid office visits Flexible working hours Access to the latest tools and cloud-native solutions A comprehensive benefits package, including health insurance and MultiSport card Employee discounts on insurance products Referral program and sports club memberships Sounds interesting? Join us and help shape the future! üöÄ","[{""min"": 9000, ""max"": 12000, ""type"": ""Net per month - B2B""}, {""min"": 9000, ""max"": 12000, ""type"": ""Gross per month - Permanent""}]",Database Administration,9000,12000,Net per month - B2B
Full-time,Senior,B2B,Remote,566,Data Engineer/Data Modeler,Altimetrik Poland,"5 days per week you need to be available until 10: 00pm due to meetings with the US team Altimetrik Polandis a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are looking forData Engineer/ Data Modelerfor our client which is an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. As a Data Engineer/Data Modeler you will be responsible for designing and implementing various scalable data solutions for our customers providing hospitality and travel services. This role requires a strong analytical mindset, technical expertise, and the ability to collaborate across teams to ensure data solutions align with business needs. Responsibilities: Build and maintain scalable data models and metrics to support variety of UC such as: fraud detection, safety incident tracking, and ID verification processes. Collaborate with Product Managers (PMs) to translate product requirements into comprehensive logging needs. Scope and implement metrics for experimentation, KPI tracking, and operational insights. Migrate existing data assets (pipelines, tables, metrics) to modern technology stacks. Identify, deprecate, and replace legacy data assets with optimized, scalable solutions. And if you possess... Advanced SQL skills for querying and data transformation. Proficiency in Python for data analysis, scripting, and automation. Hands-on experience with AWS (e.g., S3, Redshift, Athena) for cloud-based data management. Experience with Airflow for orchestration and Hive for large-scale data processing. Strong experience in data modelling, metric design, and data pipeline development. Proven ability to work cross-functionally with Product Managers, Engineers, and Analysts. Exceptional problem-solving skills and a data-driven decision-making approach. Experience in fraud detection, safety incident monitoring, or related domains. We work 100% remotely or from our hub inKrak√≥w. üî•We grow fast. ü§ìWe learn a lot. ü§πWe prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 21000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Science,21000,28000,Net per month - B2B
Full-time,Mid,B2B,Remote,568,Data Engineer/Analyst - Remote!,ITDS,"Join us, and develop your data competencies Opportunity with the possibility to work 100% remotely! As a Data Engineer/Analyst, you will be working for our client, a dynamic and rapidly expanding organization in the home and lifestyle sector. The client is embarking on an exciting project to build and optimize their data infrastructure to gain deeper insights into customer behavior and drive business growth. You will play a pivotal role in designing and implementing robust ETL/ELT pipelines, leveraging cutting-edge technologies to transform raw data into actionable intelligence. Your expertise will be crucial in shaping the future of their data-driven decision-making processes and contributing to the overall success of the company. Your main responsibilities: Design and implement ETL/ELT pipelines Work with Apache Spark, understanding cluster operation and resource management Configure the number of executors, memory, and partitions in Spark Utilize strong SQL skills for data manipulation and querying Apply understanding of data warehouse concepts and data modeling basics Build and monitor data flows using a data orchestration tool Employ practical knowledge of Python to write functions and work with data structures Create basic ETL processes involving reading, transforming, and writing data Work with a cloud platform for data storage and processing Apply basic knowledge of Git and understanding of CI/CD concepts You're ideal for this role if you have: 2 years of experience in designing and implementing ETL/ELT pipelines Knowledge of Apache Spark, including cluster operation Understanding of Spark resource management Good knowledge of SQL Understanding of data warehouse concepts and data modeling basics Knowledge of any data orchestration tool Practical knowledge of the Python language Experience working with a cloud platform Basic knowledge of Git Practical knowledge of Power BI","[{""min"": 15000, ""max"": 16800, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,15000,16800,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,569,Azure Data Engineer z kompetencjami Devopsowymi,UNIVIO,"Jeste≈õmy polskƒÖ firmƒÖ technologicznƒÖ z ponad 25-letnim do≈õwiadczeniem jako partner cyfrowej transformacji handlu. Realizujemy miƒôdzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocze≈õnie lu≈∫nƒÖ, niezobowiƒÖzujƒÖcƒÖ atmosferƒô. Nasza organizacja opiera siƒô na kulturze otwarto≈õci i dzielenia siƒô wiedzƒÖ. Dowiedz siƒô kogo szukamy i zaaplikuj, je≈õli spe≈Çniamy Twoje oczekiwania üòâ","[{""min"": 16800, ""max"": 23520, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 17300, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16800,23520,Net per month - B2B
Full-time,Mid,B2B,Hybrid,570,Cloud Data Engineer,Optiveum,"Data Engineer ‚Äì Cloud ETL & Platform Development üìç Hybrid in Warsaw (3 days/week onsite required) | üíº Full-time | B2B contract up to $7,000/month Our client is a US-based technology company headquartered in New York City, delivering digital solutions and consulting services that transform businesses and drive measurable value. With offices in multiple countries, the company is now investing in a new engineering centre in Warsaw, recognizing the strong talent and culture of Polish software professionals. We are looking for a highly motivated and self-drivenData Engineerto join a fast-growing data platform team. In this role, you will design, build, and maintain robust, scalable, and cloud-native ETL infrastructure and data pipelines, enabling real-time analytics and AI-ready architecture for high-performance business applications. This is a hybrid position ‚Äî you will be expected to work from the Warsaw office at least 3 days per week. Design, develop, and deploy Python-based ETL pipelines using Airflow and Prefect Build and optimize data warehouse structures for analytics, OLAP, and dimensional modeling Model AI-compatible DB schemas and ontologies for future-facing analytics Migrate and transform structured, semi-structured, and unstructured data using DBT and Pandas Work with streaming/event-driven architectures for real-time data processing Optimize pipeline performance and scalability for large data volumes Ensure data quality, validation, cleansing, and graceful error handling Perform code reviews to ensure standards, scalability, and best practices Collaborate with DevOps on CI/CD and automated release pipelines Implement data governance, security, and observability across ETL processes 5+ years of experience designing and building enterprise-scale ETL pipelines Strong Python skills, including use of Pandas for data processing Proficient in SQL ‚Äî writing complex queries and procedures across RDBMSes Experience with Airflow, Prefect, or similar workflow orchestration tools Solid understanding of data warehousing (OLTP, OLAP, Facts & Dimensions) Familiarity with cloud-based data platforms such as RDS, Redshift, or Snowflake Knowledge of cloud data architectures, messaging systems, and scalable design Hands-on experience with code versioning, deployment automation, and CI/CD Preferred Qualifications Experience with Docker, Kubernetes, AWS Lambdas, Step Functions Exposure to Databricks, PySpark, and advanced distributed data processing Cloud certifications are a plus What‚Äôs Offered B2B contract with monthly compensation up to $7,000 Strong career growth opportunities in a global fintech environment High-impact projects in a fast-growing sector Friendly, open, and ambitious team culture Hybrid model ‚Äì minimum 3 days/week in the Warsaw office","[{""min"": 16623, ""max"": 25859, ""type"": ""Net per month - B2B""}]",Data Engineering,16623,25859,Net per month - B2B
Full-time,Senior,B2B,Remote,571,Data Modeller,ITDS,"Join Us and Build a Cutting-Edge Data Platform! As aData Modeller,you will be working for our client in the debt collection sector, helping to build and maintain a robust, cloud-native data platform. The role focuses heavily on data modelling, requiring an expert who can translate conceptual business needs into logical and physical data models, build data contracts, and implement scalable ELT pipelines using Azure Databricks. Your main responsibilities: Design and maintain logical and physical data models based on DDD (Domain-Driven Design) principles Translate conceptual models and business glossaries into technical data structures for the Data Warehouse Perform data mapping and create data contracts between the Data Platform and source systems Collaborate with source system owners to define data contract requirements Work on data ingestion processes from source systems using various methods: Direct database queries (bulk read/CDC), API communication, Event streaming Implement ELT processes across Bronze, Silver, and Gold layers in Azure Databricks Ensure alignment of data models with business and analytical requirements You're ideal for this role if you have: Strong experience in Data Modelling (logical & physical), preferably in DDD-based environments Proven ability to work with Data Governance inputs: glossaries, conceptual models, HLD/LLD documentation Experience preparing and maintaining data contracts Solid knowledge of data ingestion techniques and working with source systems Experience with Azure Databricks (or similar cloud platforms like GCP) Ability to develop and maintain ELT pipelines in cloud-native environments Nice to have: Experience in writing clear technical documentation (e.g. data contracts, field definitions, extraction rules) Background in mapping source data to target DWH structures Ability to interpret and work with ERDs and relational models Knowledge of master data management practices Familiarity withdbdiagram.io Awareness of Data Quality, Data Lineage, and metadata management concepts Experience using tools like Azure Purview or other metadata management platforms","[{""min"": 1200, ""max"": 1500, ""type"": ""Net per day - B2B""}]",Data Science,1200,1500,Net per day - B2B
Full-time,Mid,B2B,Remote,574,Big Data Engineer,EndySoft,"Position Overview: We are seeking a skilled Big Data Engineer to join our data engineering team. The ideal candidate will have extensive experience in building and managing large-scale data processing systems. This role involves designing, implementing, and optimizing data pipelines and infrastructure to support analytics, machine learning, and business intelligence efforts. MD rate: 16600 ‚Äì 20000 PLN Roles and Responsibilities: Design, develop, and maintain big data pipelines to process and analyze large datasets. Implement data ingestion , processing , and storage solutions using big data frameworks such as Apache Spark , Hadoop , and Kafka . Optimize data pipelines for performance , scalability , and fault tolerance . Collaborate with data scientists, analysts, and other stakeholders to ensure data availability and usability. Develop and maintain data storage solutions such as HDFS , Amazon S3 , Google Cloud Storage , or Azure Data Lake . Ensure data quality and integrity through automated testing and validation processes. Monitor and troubleshoot big data infrastructure to ensure optimal performance and reliability. Document technical solutions, workflows, and best practices. Required Skills and Experience: Proficiency in big data technologies such as Apache Spark , Hadoop , Kafka , or Flink . Strong programming skills in languages like Python , Scala , or Java . Experience with SQL and NoSQL databases such as PostgreSQL , MongoDB , or Cassandra . Familiarity with cloud platforms such as AWS , Azure , or Google Cloud , including their big data services (e.g., EMR , BigQuery , Databricks ). Knowledge of data modeling , ETL processes , and data pipeline orchestration tools like Apache Airflow , Luigi , or Dagster . Strong understanding of distributed computing principles and parallel processing . Experience with containerization tools such as Docker and orchestration tools like Kubernetes . Strong problem-solving skills and ability to troubleshoot large-scale data systems. Nice to Have: Experience with real-time data processing and streaming platforms such as Apache Kafka Streams , Kinesis , or Pulsar . Familiarity with machine learning pipelines and integration with big data systems. Knowledge of data governance , security , and compliance in big data environments. Experience with CI/CD tools for automating data pipeline deployment and management. Exposure to Agile/Scrum methodologies. Understanding of data visualization tools such as Power BI , Tableau , or Looker . Additional Information: This role offers an opportunity to work on complex, large-scale data projects and help shape the future of data-driven decision-making. If you are passionate about big data technologies and thrive in a fast-paced, innovative environment, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,16600,20000,Net per month - B2B
Full-time,Senior,B2B,Remote,577,Starszy Programista Baz Danych,P&P Solutions,"üìçLokalizacja: Hybrydowo / Zdalnie üïíWymiar pracy: Pe≈Çny etat, B2BüìëWide≈Çki: 125-150 z≈Ç/h netto na b2b PoszukujemyStarszego Programisty Baz Danychdo realizacji strategicznego projektu dlainstytucji publicznej w obszarze ochrony zdrowia. Projekt obejmuje dzia≈Çania zwiƒÖzane z utrzymaniem, rozwojem i migracjƒÖ danych w ≈õrodowisku relacyjnych baz danych ‚Äì ze szczeg√≥lnym uwzglƒôdnieniem PostgreSQL. Do≈ÇƒÖczysz do zespo≈Çu ekspert√≥w odpowiedzialnych za projektowanie, optymalizacjƒô i transformacjƒô danych w ramach kluczowych system√≥w informatycznych. To doskona≈Ça okazja do zaanga≈ºowania siƒô w projekt o wysokim znaczeniu spo≈Çecznym i du≈ºej skali technologicznej. Minimum5 lat do≈õwiadczeniazawodowego w pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL, MSSQL, Oracle). Minimum4 lata do≈õwiadczeniaw programowaniu w proceduralnym jƒôzyku bazodanowym (np. PL/SQL, PL/PgSQL). Bardzo dobra znajomo≈õƒá jƒôzykaSQL. Do≈õwiadczenie z technologiƒÖPostgreSQL‚Äì mile widziane pe≈Çne zrozumienie architektury i konfiguracji tej bazy. Praktyka wstrojenie zapyta≈Ñ SQL oraz procedur PL/PgSQL. Wiedza z zakresuzarzƒÖdzania, konfiguracji i optymalizacji PostgreSQL. Do≈õwiadczenie wmigracji danych z system√≥w klasy enterprise. Znajomo≈õƒá proces√≥w analizy i transformacji danych na potrzeby migracji. Udzia≈Ç w projektach realizowanych w obszarze ochrony zdrowia ‚Äì zw≈Çaszcza rejestry i czƒô≈õƒá ‚Äûbia≈Ça‚Äù Projektowanie, implementacja i optymalizacja procedur oraz zapyta≈Ñ w relacyjnych bazach danych (PostgreSQL, PL/SQL, itp.); Udzia≈Ç w projektach migracji danych z system√≥w klasy enterprise ‚Äì analiza, transformacja i integracja danych; Strojenie zapyta≈Ñ SQL oraz procedur w celu poprawy wydajno≈õci system√≥w bazodanowych; Wsp√≥≈Çpraca z zespo≈Çami deweloperskimi oraz analitykami w zakresie integracji danych i wymaga≈Ñ systemowych; Utrzymanie, rozw√≥j i konfiguracja istniejƒÖcych struktur baz danych; Projektowanie i implementacja mechanizm√≥w przetwarzania danych zgodnie z wymaganiami klienta; Tworzenie i utrzymywanie dokumentacji technicznej; Zapewnienie zgodno≈õci z wewnƒôtrznymi standardami jako≈õci i bezpiecze≈Ñstwa danych (np. WCAG, RODO ‚Äì je≈õli dotyczy); Wsp√≥≈Çpraca z zespo≈Çami odpowiedzialnymi za architekturƒô danych i DevOps w zakresie ciƒÖg≈Ço≈õci dzia≈Çania oraz wdro≈ºe≈Ñ. Wide≈Çki do 150 z≈Ç/h netto na b2b. Mo≈ºliwo≈õƒá pracy w 100% zdalnie. Mo≈ºliwo≈õƒá udzia≈Çu wprojekcie o znaczeniu publicznymi realnym wp≈Çywie na funkcjonowanie sektora ochrony zdrowia. Wsp√≥≈Çpracƒô z do≈õwiadczonym zespo≈Çem ekspert√≥w IT. D≈ÇugoterminowƒÖ wsp√≥≈Çpracƒô i stabilno≈õƒá projektu (projekt publiczny).","[{""min"": 125, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,125,150,Net per hour - B2B
Full-time,Senior,B2B,Remote,579,Data Scientist,Britenet,"Projekt realizowany dla firmy z bran≈ºy e-commerce. Oczekiwania: Minimum 3‚Äì5 lat do≈õwiadczenia na stanowisku Data Scientist Do≈õwiadczenie we wdra≈ºaniu modeli uczenia maszynowego w ≈õrodowisku produkcyjnym Praktyczne do≈õwiadczenie w pracy z du≈ºymi zbiorami danych i modelami na du≈ºƒÖ skalƒô (BigQuery) Do≈õwiadczenie w ≈ÇƒÖczeniu danych z wielu ≈∫r√≥de≈Ç oraz pracy z r√≥≈ºnymi typami danych: tabelarycznymi, tekstowymi (NLP), obrazami i szeregami czasowymi Bardzo dobra znajomo≈õƒá jƒôzyk√≥w Python i SQL Znajomo≈õƒá Google Cloud Platform (GCP) Znajomo≈õƒá narzƒôdzi Apache Airflow i Apache Spark Do≈õwiadczenie w tworzeniu raport√≥w i dashboard√≥w w Looker Studio Dobra znajomo≈õƒá metod statystycznych oraz algorytm√≥w uczenia maszynowego Praktyczne do≈õwiadczenie z algorytmami opartymi na drzewach decyzyjnych (np. XGBoost, LightGBM, CatBoost) Umiejƒôtno≈õƒá przek≈Çadania wyzwa≈Ñ biznesowych na problemy z zakresu uczenia maszynowego Zdolno≈õƒá do samodzielnego definiowania, testowania i optymalizacji modeli ML Otwarto≈õƒá na proponowanie w≈Çasnych rozwiƒÖza≈Ñ i podej≈õƒá w analizie danych Bardzo dobre umiejƒôtno≈õci komunikacyjne ‚Äì umiejƒôtno≈õƒá wyja≈õniania z≈Ço≈ºonych zagadnie≈Ñ technicznych osobom nietechnicznym Umiejƒôtno≈õƒá pracy zespo≈Çowej, dzielenia siƒô wiedzƒÖ i wsp√≥≈Çpracy z innymi Zadania: Projektowanie, rozwijanie i wdra≈ºanie modeli uczenia maszynowego w ≈õrodowiskach produkcyjnych Analiza du≈ºych i zr√≥≈ºnicowanych zbior√≥w danych (tabelaryczne, tekstowe, obrazy, szeregi czasowe) Integracja danych z wielu wewnƒôtrznych i zewnƒôtrznych ≈∫r√≥de≈Ç w celu budowy solidnych pipeline‚Äô√≥w i cech dla modeli ML Przeprowadzanie analiz danych na du≈ºƒÖ skalƒô z wykorzystaniem narzƒôdzi takich jak BigQuery, z uwzglƒôdnieniem skalowalno≈õci i wydajno≈õci Wykorzystywanie zaawansowanych metod statystycznych i algorytm√≥w uczenia maszynowego CiƒÖg≈Çe usprawnianie i optymalizacja istniejƒÖcych modeli oraz rozwiƒÖza≈Ñ opartych na danych poprzez testowanie i eksperymenty Wsp√≥≈Çpraca z interesariuszami biznesowymi, product managerami i zespo≈Çami UX w celu definiowania przypadk√≥w u≈ºycia ML i dostosowania rozwiƒÖza≈Ñ technicznych do cel√≥w biznesowych Przek≈Çadanie z≈Ço≈ºonych danych i wynik√≥w modelowania na klarowne wnioski i rekomendacje dla odbiorc√≥w nietechnicznych Tworzenie i utrzymywanie dashboard√≥w oraz raport√≥w w Looker Studio w celu monitorowania wydajno≈õci modeli i jako≈õci danych Proponowanie nowych podej≈õƒá, narzƒôdzi i metodologii w celu rozwijania kompetencji data science w zespole","[{""min"": 90, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Science,90,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,580,Senior Data Engineer,j-labs,"Do≈ÇƒÖcz do zespo≈Çu, kt√≥ry buduje platformƒô do zarzƒÖdzania majƒÖtkiem opartym na ETF-ach ‚Äì skalowalne, nowoczesne narzƒôdzie majƒÖce realny wp≈Çyw na spos√≥b inwestowania. Team jest odpowiedzialny za przetwarzanie du≈ºej ilo≈õci danych potrzebnych do generowania r√≥≈ºnego rodzaju raport√≥w.‚Äã JednƒÖ z domen, kt√≥rƒÖ zajmuje siƒô zesp√≥≈Ç jest zarzƒÖdzaniem ryzykiem.‚Äã Szukamy osoby samodzielnej i zaanga≈ºowanej, kt√≥ra bƒôdzie: Rozwijaƒá skalowalnƒÖ infrastrukturƒô danych w chmurze, kt√≥ra stanowi podstawƒô dzia≈Çalno≈õci firmy opartej na danych, z wykorzystaniem najnowszych technologii. Tworzyƒá rozwiƒÖzania do przetwarzania danych opartych na AWS, integrujƒÖcych dane z us≈Çug wewnƒôtrznych i zewnƒôtrznych. Budowaƒá jezioro danych finansowych, ≈ÇƒÖczƒÖcej nowoczesne technologie z funkcjami wymaganymi przez przepisy regulacyjne. ≈öci≈õle wsp√≥≈Çpracowaƒá z data scientistami, zespo≈Çami produktowymi i deweloperskimi przy wdra≈ºaniu inteligentnych funkcji do naszego produktu. Dzieliƒá siƒô wiedzƒÖ eksperckƒÖ na temat najlepszych praktyk w zakresie danych wewnƒÖtrz firmy. Wymagania: Do≈õwiadczenie w projektowaniu i obs≈Çudze potok√≥w danych w ≈õrodowisku AWS (min. 5 lat). Znajomo≈õƒá SQL . Do≈õwiadczenie w Pythonie, w tym znajomo≈õƒá framework√≥w takich jak DBT. Do≈õwiadczenie w pracy z us≈Çugami AWS , takimi jak S3, Athena i Glue. Znajomo≈õƒá narzƒôdzi Infrastructure-as-Code, takich jak Terraform . Pasja do podej≈õcia ""everything-as-code"" i pisanie dobrze zaprojektowanego, testowalnego i udokumentowanego kodu. Do≈õwiadczenie w pracy w metodykach zwinnych, np. Scrum. Zainteresowanie us≈Çugami finansowymi i rynkami. Bieg≈Ço≈õƒá w jƒôzyku angielskim w mowie i pi≈õmie- min. B2. Mo≈ºliwa praca 100% zdalna, ale ze wzglƒôdu na potrzebƒô okazjonalnych spotka≈Ñ zespo≈Çu w biurze, w pierwszej kolejno≈õci bierzemy pod uwagƒô osoby mieszkajƒÖce w Warszawie lub Krakowie, gdzie mamy biura.","[{""min"": 190, ""max"": 210, ""type"": ""Net per hour - B2B""}]",Data Engineering,190,210,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,581,Machine Learning Engineer (LLM),UNIVIO,"Jeste≈õmy polskƒÖ firmƒÖ technologicznƒÖ z ponad 25-letnim do≈õwiadczeniem jako partner cyfrowej transformacji handlu. Realizujemy miƒôdzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocze≈õnie lu≈∫nƒÖ, niezobowiƒÖzujƒÖcƒÖ atmosferƒô. Nasza organizacja opiera siƒô na kulturze otwarto≈õci i dzielenia siƒô wiedzƒÖ. Dowiedz siƒô kogo szukamy i zaaplikuj, je≈õli spe≈Çniamy Twoje oczekiwania üòâ","[{""min"": 16800, ""max"": 20160, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 16500, ""type"": ""Gross per month - Permanent""}]",Data Science,16800,20160,Net per month - B2B
Full-time,Senior,B2B,Remote,583,Senior Data Engineer,Experis Manpower Group,"Join a dynamic and collaborative team working in a fast-paced Scrum environment, where agility and teamwork are key to delivering impactful solutions. Each day begins with a stand-up meeting alongside data & analytics engineers, the Scrum Master, and the Product Owner ‚Äî aligning on sprint goals, resolving blockers, and sharing progress. Your Responsibilities: Design and enhance scalable data pipelines and infrastructure. Monitor system performance and optimize for cost-efficiency. Model data using DBT based on existing pipelines. Collaborate with functional analysts to refine business requirements. Participate in sprint ceremonies including refinements and retrospectives. Stay connected with your team and stakeholders through Jira and Slack. What We‚Äôre Looking For: Strong command ofSQL. Hands-on experience withDBTfor data transformation and pipeline automation. Enthusiasm forAWStechnologies. AWS Certified Associate accreditation. Proficiency inone or more of the following: Scala, Python, Go, Java, Shell scripting ‚Äî or deep database expertise with a willingness to learn Python and Scala. Practical DevOps experience(CI/CD, system setup, monitoring). Excellent communication and analytical skills. Team-oriented approach and interest in pair programming. Nice to Have: Experience with Terraform or other Infrastructure as Code tools. Understanding of large-scale distributed systems. Familiarity with Domain-Driven Design. Knowledge of monitoring, logging, and security automation. What We Offer: B2B contract via Experis. Sports card. Life insurance. Private medical care. Fully remote work.","[{""min"": 160, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,190,Net per hour - B2B
Full-time,Senior,B2B,Remote,584,Engineering Manager (AI & Big Data),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As an Engineering Manager, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: AI optimization engine for airport operation management. This project involves creating an AI-driven optimization engine to streamline airport operations, using real-time data from sources like radar, air traffic, and passenger information. GenAI knowledge retrieval platform for the leading automotive company. This project involves the creation of an Azure-backed platform that leverages cutting-edge LLMs to deliver powerful insights from an enterprise-scale knowledge base. AI recommendation engine for the leading innovation company. This project involves the design of AWS-powered solutions to provide domain-specific recommendations by employing sophisticated LLMs. This role is ideal for a leader who combines technical expertise with strong leadership skills , ready to drive innovative projects in data science and big data. Discover our perks and benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. In this position, you will: Be responsible for project management and technical leadership for data science and big data projects. Maintaining top-quality and client-focused deliverables to meet and exceed their expectations towards delivered solutions. Manage and mentor a team of data scientists, ensuring they are engaged, motivated, productive, and are provided with regular feedback to support growth. Act as a trusted advisor, proposing effective approaches and data science architectures tailored to client needs. Building technical solutions that deliver measurable value. Identify opportunities to expand service offerings and project potential with clients. Take part in pre-sales activities, connecting technical solutions to client needs. Participate in the recruitment process of new talents. What you‚Äôll need to succeed in this role: Bachelor‚Äôs or higher in Computer Science, Mathematics, Physics, or related field. Proven track record in managing technical teams and leading end-to-end projects. Experience working with corporate clients. Hands-on experience with Data Science applications (NLP, Computer Vision, Generative AI, Machine Learning, Predictive Modeling). Hands-on experience in ETL, data preparation, and data wrangling techniques. Strong critical thinking and problem-solving abilities. Excellent communication and presentation skills. Advanced English Skills ‚Äì C1 proficiency level or higher. Proficiency in Python and cloud platforms (preferably Azure, AWS).","[{""min"": 24000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Science,24000,30000,Net per month - B2B
Full-time,Senior,B2B,Remote,585,ETL Developer ‚Äì Informatica BDM/DEI,Calimala.ai,"Calimala.aiis seeking an experienced ETL Developer with expertise in Informatica BDM/DEI to join our innovative team in the telecom sector. In this capacity, you will be at the forefront of designing, developing, and optimizing scalable ETL pipelines that integrate data from diverse sources. This role requires a deep technical foundation in ETL development and a passion for turning complex data challenges into reliable, high-performance solutions. Responsibilities As an ETL Developer, you will work closely with data architects, analysts, and other team members to understand mapping specifications and implement efficient data workflows. Key responsibilities include: Design and develop complex ETL pipelines using Informatica BDM/DEI. Integrate data from various sources, including big data environments like Hive and Spark. Optimize mappings and workflows to ensure high performance and reliability. Collaborate with cross-functional teams to align data integration strategies with business needs. Document and maintain best practices in ETL and data governance. Requirements Candidates must bring at least five years of hands-on experience in ETL development and demonstrable expertise using Informatica BDM/DEI. A strong technical background in SQL, Hive, and Spark is essential along with proven experience in performance tuning and data integration. Additionally, familiarity with the telecom industry will serve as a significant advantage. What We Offer AtCalimala.ai, we provide a dynamic work environment where innovation meets real-world data challenges. In addition to a competitive salary we offer opportunities for professional growth and learning. Join our team to play a pivotal role in transforming data integration processes and driving business success.","[{""min"": 12000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Engineering,12000,22000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,589,Data Engineer,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: projekt z bran≈ºy automotive, dotyczy przetwarza danych z maszyn i czƒô≈õci, aby wesprzeƒá klient√≥w na ca≈Çym ≈õwiecie i pom√≥c im w przekszta≈Çcaniu siƒô w pe≈Çni operacyjne, oparte na danych organizacje, projektowanie, rozw√≥j i utrzymanie rozwiƒÖza≈Ñ przetwarzania danych, tworzenie i optymalizacja przep≈Çyw√≥w danych w ≈õrodowisku Azure i Databricks, wsp√≥≈Çpraca z interesariuszami z dzia≈Ç√≥w IT i biznesu w celu dostarczania warto≈õciowych produkt√≥w danych, praca hybrydowa, 1 dzie≈Ñ w tygodniu z biura we Wroc≈Çawiu, stawka do 140 z≈Ç/h przy B2B. Ta oferta jest dla Ciebie, je≈õli: posiadasz minimum 4-letnie do≈õwiadczenie w roli Data Engineer, bardzo dobrze znasz: Databricks, Workflows, Python, Spark, SQL, masz do≈õwiadczenie z DevOps - praca z pipelineami i repozytoriami, dobrze znasz: Azure Data Factory, Azure Key Vault i Power BI, znasz jƒôzyk angielski na poziomie min. B2, cechujesz siƒô otwarto≈õciƒÖ umys≈Çu i zaanga≈ºowaniem. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 22000, ""max"": 23500, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,23500,Net per month - B2B
Part-time,Senior,B2B,Remote,590,Senior BI Consultant / Analityk Power BI,Ness Solution,"üéØ Poszukiwany Senior BI Consultant / Analityk Power BI Lokalizacja: 100% zdalnie opcjonalnie praca hybrydowa (Warszawa) Umowa B2B | 0,5 FTE Tech stack: Power BI, SQL, R (opcjonalnie), Tableau, Azure üíº O roli Szukamy do≈õwiadczonego konsultanta BI, kt√≥ry po≈ÇƒÖczy kompetencje analityczne, deweloperskie i konsultingowe, aby wspieraƒá kluczowe inicjatywy raportowe oraz strategiczne projekty IT i transformacji operacyjnej w jednej z najwiƒôkszych instytucji finansowych w Polsce. üß∞ Twoje zadania Tworzenie i rozw√≥j zaawansowanych dashboard√≥w w Power BI (DAX, M Query) Projektowanie warstw danych (Data Marts, KPI Engines) Wsp√≥≈Çpraca z zespo≈Çem IT i raportingu zarzƒÖdczego Automatyzacja proces√≥w raportowych (ETL, API, data flows) Udzia≈Ç w definiowaniu metryk, KPI, SLA oraz tworzeniu raport√≥w zarzƒÖdczych (Opcjonalnie) wsparcie w analizach w jƒôzyku R i skryptach w Pythonie / Linuxie ‚úÖ Wymagania Min. 4‚Äì5 lat do≈õwiadczenia na stanowiskach analitycznych, BI lub konsultingowych Bardzo dobra znajomo≈õƒáPower BI / Tableau Praktyczna znajomo≈õƒáSQL‚Äì umiejƒôtno≈õƒá pisania zapyta≈Ñ, ≈ÇƒÖczenia tabel, optymalizacji Do≈õwiadczenie w pracy z danymi biznesowymi i przygotowywaniu raport√≥w dla interesariuszy Komunikatywno≈õƒá i umiejƒôtno≈õƒá pracy w ≈õrodowisku biznesowym (kontakt z zespo≈Çem i zarzƒÖdem) üí° Mile widziane Znajomo≈õƒáR(np.dplyr,ggplot2,shiny) ‚Äì projekt nie wymaga R, ale mo≈ºe siƒô przydaƒá Znajomo≈õƒáAzureDev,Python,Linux(bash, crontab) üí¨ Oferujemy Wsp√≥≈Çpraca B2B w wymiarze 0,5 etatu (ok. 20h tygodniowo) Mo≈ºliwo≈õƒá elastycznego rozplanowania czasu (2‚Äì3 dni/tydzie≈Ñ lub podzia≈Ç godzinowy) Praca w zespole z do≈õwiadczonymi analitykami i architektami BI Projekt z du≈ºym wp≈Çywem na decyzje zarzƒÖdcze Je≈õli oferta jest dla Ciebie interesujƒÖca, prze≈õlij swoje CV!","[{""min"": 20160, ""max"": 26880, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20160,26880,Net per month - B2B
Full-time,Senior,B2B,Remote,592,Senior Data Engineer,co.brick,"We're Hiring: Data Engineers & Data Architects üìçLocation: Remote üí∞Rate: Up to 170 PLN/h üïíEngagement: Full-time üìÖStart: July üìÜDuration: Minimum 12 months (with possible extension) We're currently looking for experiencedData EngineersandData Architectsto join a long-term project for established UK-based client. About the role: You‚Äôll be working on a project focused ondecommissioning legacy software and migrating data to modern systems. Required experience: ‚úîÔ∏è 7+ years in data-related roles ‚úîÔ∏è Strong skills in: ‚Ä¢SnowflakeandDBT ‚Ä¢Python ‚Ä¢AWS(preferred) orAzure ‚úîÔ∏è Excellent communication skills (C1+ English level) ‚úîÔ∏è Open and collaborative mindset Nice to have: ‚ûï Experience withFivetran We‚Äôre looking for communicative, proactive individuals who thrive in remote environments and want to work with a solid, international team. Looking forward to hearing from you!","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,170,Net per hour - B2B
Full-time,Senior,B2B,Remote,594,"Data Scientist (Gen AI, LLM)",emagine Polska,"Summary: The Senior Data Scientist role focuses on leveraging advanced data science techniques to drive innovations. The primary objective is to apply generative AI and machine learning methods to solve practical challenges in collaboration with both startup and established technology practices. Responsibilities: Utilizing machine learning techniques to solve real-world problems. Applying expertise in Generative AI and Large Language Models (LLMs). Implementing and optimizing natural language processing (NLP) or computer vision techniques. Developing scalable end-to-end machine learning solutions including data preprocessing and model evaluation. Working with cloud platforms and integrating MLOps practices. Analyzing large datasets for insights and model performance. Communicating technical concepts to non-technical stakeholders effectively. Collaborating on innovative solutions to complex data challenges. Key Requirements: 5+ years of experience as a Data Scientist. Strong proficiency in Python and machine learning libraries (scikit-learn, TensorFlow, PyTorch). Experience with Generative AI and LLMs. Knowledge of statistics and mathematics. Excellent problem-solving and critical-thinking skills. Fluent in English (written and spoken). Nice to Have: Programming skills in R, SQL, Java. Experience with big data tools (e.g., Hadoop, Spark, Kafka). Other Details: This position is part of a modern software house focusing on GenAI, cloud computing, and advanced machine learning algorithms. B2B contract! Fully remote with occasional visits in the client‚Äôs office.","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,180,200,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,595,Data Engineer,emagine Polska,"PROJECT INFORMATION: Industry: Construction Assignment type: B2B Start: September Work model: Hybrid model (2 days/week in office - Warsaw) Project length: 12 months + extensions Project language: English We are looking for a dedicatedData Engineer/BI Developer to join our Danish client'sBI scrum teamof six members, who develop and maintain our BI back-end solutions. The team you will join works in close collaboration with our highly innovative departments and companies, where you identify and build the foundation for their new BI reports. Reports covering everything from working environment, diversity, IOT machine data to the more financial. Our team is at the forefront of digitization and automation, focused on streamlining the way we work by processing and presenting data through Business Intelligence tools. The role involves collaborating with various departments to develop insightful BI reports that enhance decision-making and operational efficiency. Main Responsibilities Develop and maintain BI back-end solutions. Collaborate with various teams to identify and implement new BI reporting frameworks. Analyze data trends and provide actionable insights to colleagues. Facilitate the transition to cloud environments and integrate new data sources. Contribute to the development and execution of our digital strategies. Identify and implement new data sources and improve existing data frameworks. Work closely with cross-functional teams to define requirements for BI initiatives. Analyze complex data sets and translate findings into actionable insights. Contribute to cloud integration and Data Lakehouse projects. Key Requirements +3 years of experience in BI area Experience in Business Intelligence and data analysis. Strong understanding of Python and SQL at an advanced level. Strong Experience with Databricks Basic experience with DevOps practices Knowledge of cloud-based solutions, particularly Azure. Capacity to perform dimensional data modeling. Experience with structured and unstructured data sources (API, SQL). Adept at communicating complex ideas effectively. Good problem-solving skills and a proactive attitude toward technological advancements. Nice to Have Familiarity Data Factory. Knowledge of MS Dynamics and process automation. Familiarity with Power BI and data transformation tools.","[{""min"": 170, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,180,Net per hour - B2B
Full-time,Senior,Permanent,Hybrid,597,Senior Big Data Engineer,Relativity,"Posting Type Hybrid Job Overview Here at Relativity we prioritize flexibility and work-life harmony. Our Hybrid work environment provides options tailored to your role and location, aiming to enhance engagement, connectivity, and productivity. Join us to experience a culture of collaboration and innovation, where connecting in-person adds value to our collective growth. Let's work together! Join our team as we innovate the future of data platform architecture, enabling massive scaling and data processing for ML and Gen AI projects. You'll be at the forefront of processing vast unstructured data, building high-throughput APIs, and supporting distributed compute frameworks for seamless model deployment. Ready to dive into the heart of cutting-edge tech? Job Description and Requirements Your role in action: Build our next-generation data platform tooling and services to support the ingestion and processing of billions of documents at scale. Improve and extend our Spark based distributed data processing pipeline. Improve and extend our Rust based distributed query engine used to request large amounts of document data. Create tools to automate and optimize processes across disciplines Actively participate in the on-call schedule to investigate and fix production issues related to our data processing pipeline or query engine. Participate in code reviews for projects written by your team Focus on quality through comprehensive unit and integration testing Your Skills: 4+ years of software development experience in writing performant, commercial-grade systems and applications Experience with monitoring and troubleshooting production environments ‚Ä¢ Proficiency in programming languages used in high volume data processing and applications like Java or Scala and Python Experience building data pipelines with distributed compute frameworks like Hadoop. Spark, or Dask Knowledge of Linux/Unix systems, Docker/Kubernetes and CI/CD including scripting in Python or other scripting languages to automate build and deployment processes Knowledge of professional software engineering practices & software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations Leverages best practices and past experiences to mentor and improve the productivity of the team We‚Äôd particularly love it if you have: Deep experience building and debugging distributed data pipelines Experience with columnar databases and storage formats like Delta Lake and Parquet Experience deploying and managing services on Kubernetes Experience building with Rust. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law. #LI-MM5 Relativity is committed to competitive, fair, and equitable compensation practices. This position is eligible for total compensation which includes a competitive base salary, an annual performance bonus, and long-term incentives. The expected salary range for this role is between following values: 181 000 and 271 000PLNThe final offered salary will be based on several factors, including but not limited to the candidate's depth of experience, skill set, qualifications, and internal pay equity. Hiring at the top end of the range would not be typical, to allow for future meaningful salary growth in this position.","[{""min"": 181000, ""max"": 271000, ""type"": ""Gross per year - Permanent""}]",Data Engineering,181000,271000,Gross per year - Permanent
Full-time,Manager / C-level,B2B,Hybrid,603,Lead Data Analyst,Experis Manpower Group,"We are looking for an experiencedData Analyst (Manager level)to lead analytical initiatives aimed at optimizing business performance and product growth. This role focuses on transforming raw data into actionable insights through advanced data analysis, visualization, and reporting. The ideal candidate combines strong technical proficiency with leadership capability to manage cross-functional collaboration and ensure the delivery of high-quality analytical outcomes. Lead data collection, cleaning, and organization efforts for structured data sets Optimize business and product growth scenarios through advanced data analysis Analyze large data sets to identify patterns, trends, and opportunities Build and maintain dashboards and reports to present key insights to technical and non-technical stakeholders Interpret data findings and provide actionable recommendations to leadership and product teams Collaborate closely with data scientists, data engineers, and cross-functional teams to ensure data integrity and alignment Oversee data-related initiatives, ensuring quality, timeliness, and business relevance Stay current with emerging tools, technologies, and methodologies in data analytics Mentor junior analysts and contribute to the development of data literacy across the organization 5+ years of experience in data analytics, with at least 2 years in a leadership or managerial role Proven ability to lead data-driven decision-making processes Strong proficiency inR, Python, SQL, SAS, and SAS Miner Experience working with structured data from relational databases, spreadsheets, and cloud sources Advanced skills in data visualization and reporting (e.g., Power BI, Tableau, or similar tools) Excellent communication skills with the ability to translate technical findings into business-friendly language Familiarity with data governance, data quality standards, and best practices Demonstrated ability to manage multiple projects and collaborate with cross-functional teams Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Data Science, Statistics, or a related field Our offer: Work from Warsaw, Katowice or Gdansk office MultiSport card Private Healthcare Life insurance","[{""min"": 170, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,170,190,Net per hour - B2B
Full-time,Senior,B2B,Remote,604,Senior Oracle Database Administrator,Hirexa,"Job Title: Oracle DBA Location: Remote Employment Type: B2B About Hirexa Solutions: Hirexa Solutions is a leading player in the recruitment ecosystem across the United States, United Kingdom, Europe, and India. As the fastest-growing next-generation provider of technology talent, we empower our clients to become resourceful, achieve higher productivity, adopt agile structures, and effectively execute project deliverables. Envisioned and co-founded by veterans of the Information Technology industry, our mission is to make recruitment efficient, flawless, and cost-effective. Our unwavering commitment to strategic investments in intelligent technology underscores our passion for people and our dedication to helping organizations realize their true potential. Job Description Key Responsibilities: Design, install, configure, and maintain database systems (e.g., SQL Server, Oracle, MySQL, PostgreSQL). Monitor database performance and tune complex queries for optimal performance. Manage database backup, recovery, high availability (HA), and disaster recovery (DR) strategies. Implement security measures to protect sensitive data and ensure compliance with data governance policies. Perform database upgrades, patching, and migration activities. Collaborate with development and DevOps teams to support application deployment and database changes. Troubleshoot database-related issues and provide 24/7 support as needed. Create and maintain documentation, standards, and procedures. Automate routine tasks using scripts or tools (e.g., PowerShell, Bash, Python). Mentor junior DBAs and support database-related aspects of software development lifecycle (SDLC). Required Skills: 8 to 10 years of experience in a DBA role with enterprise-level databases. Expertise in at least one major RDBMS (e.g., Microsoft SQL Server, Oracle, PostgreSQL). Strong knowledge of SQL, T-SQL/PL-SQL, indexing, and query optimization. Experience with database monitoring and performance tuning tools. Proven experience with backup and recovery strategies and tools. Knowledge of cloud database platforms (e.g., AWS RDS, Azure SQL Database, Google Cloud SQL). Familiarity with replication, clustering, and HA/DR configurations. Strong scripting skills (e.g., PowerShell, Shell, Python) Position Overview: For one of our partners, we are seeking a Oracle DBA who will be responsible for MSSQL,Oracle DBA. The ideal candidate will possess the necessary skills and experience to contribute to the success of our partner organization. How to Apply: If you are interested in this opportunity, please submit your resume. We look forward to hearing from you!","[{""min"": 939, ""max"": 1024, ""type"": ""Net per day - B2B""}]",Database Administration,939,1024,Net per day - B2B
Full-time,Senior,B2B,Remote,605,AI/ML Principal Software Engineer,Sii,"Minimum of 7 years of experience building and deploying complex, production-grade software systems Proficient in backend technologies (e.g., Python, Java, Node.js) and frameworks (e.g., Django, Flask, Spring Boot) Skilled in frontend frameworks such as React or Angular Strong background in containerization (Docker) and orchestration (Kubernetes) Proven expertise with AWS and scalable cloud-based architectures Fluency in English, both spoken and written Residing in Poland required Join our client's team in the medical industry as an AI/ML Principal Software Engineer, and help build cutting-edge software solutions powered by machine learning for real-world applications. In this role, you will play a key part in designing and building scalable, high-performance systems that bring machine learning models into production environments. This is an exciting opportunity to lead critical initiatives at the intersection of software engineering and AI, while working remotely within a collaborative and forward-thinking team. Take ownership of designing and developing reliable, scalable software solutions that incorporate machine learning models Work closely with data scientists and ML engineers to transform experimental prototypes into fully operational, production-grade systems Contribute to key architectural decisions, infrastructure planning, and the selection and implementation of development tools Assist in building and maintaining cloud-native environments and solutions on AWS Apply industry best practices for automated testing, continuous integration and delivery (CI/CD), and system observability Develop and sustain backend infrastructures and user-facing applications with a strong focus on performance, security, and maintainability Start ASAP Praca w pe≈Çni zdalna Darmowe ≈õniadanie Bez wymaganego dress code'u Darmowa kawa Szkolenia wewnƒôtrzne Nowoczesne biuro Pakiet sportowy Bud≈ºet na szkolenia Miƒôdzynarodowe projekty Ma≈Çe zespo≈Çy Prywatna opieka medyczna Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title ‚Äì get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers ‚Äì Power People. Learn more atsii.pl.","[{""min"": 24000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Science,24000,28000,Net per month - B2B
Full-time,Senior,B2B,Remote,606,Senior Data Engineer,Link Group,"Maintain and optimizeAzure SQL DatabaseandAzure SQL Managed Instances Design, implement, and manage data pipelines to support real-time and near real-time data exchange Analyze and improve complex SQL queries and performance bottlenecks Work closely with solution architects and product teams to propose architectural changes and data model improvements Monitor and troubleshoot data issues, proposing sustainable and scalable fixes Contribute to the overall data architecture strategy and ensure alignment with business needs Proven experience as aSenior Data EngineerorData Architect Strong expertise inAzure SQL DatabaseandAzure SQL Managed Instance Deep knowledge ofSQL performance tuningandquery optimization Experience withdata modeling,ETL/ELT pipelines, anddata integration High-level understanding ofdata architectureprinciples and patterns Hands-on experience withAzure Data Factory,Azure Data Lake, or other Azure data services Familiarity withCI/CD,Git, and agile workflows Experience withPower BI,Databricks, orSynapse Analytics Familiarity withsecurity and compliancestandards in the data domain Knowledge ofAI/MLintegration in data pipelines Previous work onmaturity or assessment platformsis a plus 100% remote work with flexible hours Opportunity to shape a high-impact platform used by global organizations Long-term project with potential for growth and ownership Competitive compensation based on experience","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,608,Senior Data Analyst,PeakData,"About Peak Data: Peak Data is a Swiss-based startup revolutionizing the pharmaceutical industry through innovative data solutions. Our mission is to empower pharmaceutical companies with actionable insights, driving better decision-making and improving patient outcomes. Join our dynamic team and be a part of a company that is at the forefront of data-driven healthcare innovation. Position Overview: We are seeking a highly skilled and motivated Data Analyst to join our Operations Department. The ideal candidate will have a strong background in data analysis, with a particular focus on the pharmaceutical industry. You will be responsible for analyzing complex data sets, developing insights, and providing recommendations to support our operational strategies. Proficiency in SQL, Python, and AWS is essential for this role. Key Responsibilities: ‚Ä¢ Collect, process, and analyse large datasets to identify trends, patterns, and insights related to pharmaceutical operations. ‚Ä¢ Develop and maintain dashboards and reports to monitor key performance indicators (KPIs) and operational metrics. ‚Ä¢ Collaborate with cross-functional teams to define data requirements and ensure data accuracy and integrity. ‚Ä¢ Utilize SQL to query databases and extract relevant data for analysis. ‚Ä¢ Apply Python for data manipulation, statistical analysis, and automation of data workflows. ‚Ä¢ Leverage AWS services to manage and analyse data in the cloud environment. ‚Ä¢ Provide actionable insights and recommendations to support decision-making processes within the operations department. ‚Ä¢ Stay updated with industry trends, best practices, and emerging technologies in data analysis and the pharmaceutical sector. Qualifications: ‚Ä¢ Bachelor‚Äôs degree in Data Science, Statistics, Computer Science, or a related field. A Master‚Äôs degree is a plus. ‚Ä¢ Proven experience (4y or more) as a Data Analyst, preferably within the pharmaceutical industry. ‚Ä¢ Strong proficiency in SQL for data querying and database management. ‚Ä¢ Advanced skills in Python for data analysis, statistical modelling, and automation. ‚Ä¢ At least 3y of experience with AWS services, including data storage, processing, and analytics. ‚Ä¢ Solid understanding of pharmaceutical industry operations, regulations, and data requirements. ‚Ä¢ Excellent analytical and problem-solving skills with the ability to interpret complex data sets. ‚Ä¢ Strong communication skills, with the ability to present findings and insights to both technical and non-technical stakeholders. ‚Ä¢ Detail-oriented, with a commitment to accuracy and data integrity. ‚Ä¢ Ability to work independently and collaboratively in a fast-paced startup environment. What We Offer: ‚Ä¢ Competitive salary and benefits package. ‚Ä¢ Opportunity to work with a passionate and innovative team in a growing startup. ‚Ä¢ Flexible working hours and remote work options. ‚Ä¢ Professional development opportunities and support for continuous learning. ‚Ä¢ A dynamic and inclusive work environment that values creativity and diversity.","[{""min"": 12808, ""max"": 19213, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,12808,19213,Net per month - B2B
Full-time,Senior,B2B,Remote,609,Big Data Engineer,ITDS,"Big Data Engineer Join us, and build data solutions that drive global innovation! Krak√≥w - based opportunity with hybrid work model (2 days month in the office). As aBig Data Developer,you will be working for our client, a leading global financial institution, contributing to the design and development of cutting-edge data solutions for risk management and analytics. The client is undergoing a strategic digital transformation, focusing on scalable, cloud-based big data platforms that support advanced analytics and regulatory compliance. You will be part of a high-performing Agile team, collaborating closely with business stakeholders and technical teams to build and maintain robust distributed systems that process large volumes of data efficiently. Designing and developing distributed big data solutions using Spark Implementing microservices and APIs for data ingestion and analytics Managing cloud-native deployments primarily on GCP Writing and maintaining test automation frameworks using tools like JUnit, Cucumber, or Karate Collaborating with cross-functional teams to translate business requirements into technical specifications Developing and scheduling data workflows using Apache Airflow Maintaining and optimizing existing big data pipelines Utilizing DevOps tools such as Jenkins and Ansible for CI/CD automation Participating in Agile ceremonies and contributing to sprint planning and retrospectives Monitoring, troubleshooting, and improving data systems and services A degree in Computer Science, IT, or a related discipline Proven experience in designing and developing big data systems Hands-on experience with Spark and distributed computing SolidJava,Python, andGroovydevelopment skills Strong knowledge of the Spring ecosystem (Boot, Batch, Cloud) Familiarity with REST APIs, Web Services, and API Gateway technologies Practical experience in DevOps tooling like Jenkins and Ansible Proficiency in using RDBMS, especially PostgreSQL Hands-on experience with public cloud platforms, particularly GCP Excellent communication in English Experience with streaming technologies like Apache Beam or Flink Knowledge of OLAP solutions and data modeling Background in financial risk management or the banking industry Exposure to container technologies such as Docker and Kubernetes Familiarity with Traded Risk domain concepts Experience with RPC frameworks like gRPC Knowledge of data lakehouse tools like Dremio or Trino Hands-on experience with BI or UI development We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7225 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere.","[{""min"": 28000, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Engineering,28000,31500,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,612,Senior Data Engineer,XTB,"Tworzymy XTB ‚Äì globalnƒÖ firmƒô inwestycyjnƒÖ, oferujƒÖcƒÖ innowacyjne rozwiƒÖzania technologiczne, kt√≥re pozwalajƒÖ naszym klientom skutecznie zarzƒÖdzaƒá swoimi finansami na wiele sposob√≥w. Wszystko to w jednej intuicyjnej aplikacji XTB, z kt√≥rej korzysta ju≈º ponad milion u≈ºytkownik√≥w na ca≈Çym ≈õwiecie! Jeste≈õmy certyfikowanƒÖ firmƒÖGreat Place to Work. Poszukujemy osoby, kt√≥ra do≈ÇƒÖczy do naszego zespo≈Çu Data Platform w roli Data Engineer. G≈Ç√≥wnym zadaniem zespo≈Çu jest utrzymanie i rozw√≥j hurtowni danych on premise i jej docelowa migracja na ≈õrodowisko chmurowe. Wsp√≥≈Çpracujemy z zespo≈Çami produktowymi w celu dostarczenia danych potrzebnych do podejmowania kluczowych decyzji biznesowych. Pracujemy w frameworku Scrum. Do Twoich codziennych obowiƒÖzk√≥w bƒôdzie nale≈ºa≈Ço: Projektowanie i utrzymywanie hurtowni danych oraz migracja danych i proces√≥w na ≈õrodowisko chmure, Tworzenie i utrzymywanie modeli danych do wspierania kluczowych decyzji biznesowych, Integracja danych w celu wytworzenia wymaganych modeli danych, Wdra≈ºanie kontroli jako≈õci danych i proces√≥w walidacji, aby zapewniƒá dok≈Çadno≈õƒá, sp√≥jno≈õƒá i kompletno≈õƒá, Wsp√≥≈Çpraca z innymi zespo≈Çami, aby tworzyƒá zestawy danych spe≈ÇniajƒÖce potrzeby raportowania i rozwiƒÖzujƒÖce problemy systemowe, Projektowanie i wykonywanie test√≥w wydajno≈õciowych i integracyjnych, Raportowanie kluczowych wska≈∫nik√≥w w firmie w narzƒôdziu BI Wymagania: Co najmniej 4-letnie do≈õwiadczenie w pracy na stanowisku SQL Developer / Data Engineer Znajomo≈õƒá Python, SQL w tym T-SQL, pisanie z≈Ço≈ºonych procedur sk≈Çadowanych, optymalizacja wydajno≈õci, Umiejƒôtno≈õƒá tworzenia proces√≥w ETL (SSIS, Airflow), Do≈õwiadczenie zawodowe w eksploracji danych, analizie i modelowaniu z≈Ço≈ºonych zbior√≥w danych na du≈ºƒÖ skalƒô; Do≈õwiadczenie w pracy z narzƒôdziami do zarzƒÖdzania kodem ≈∫r√≥d≈Çowym, takimi jak GIT Dobra znajomo≈õƒá zasad standard√≥w integracyjnych: REST, gRPC Umiejƒôtno≈õƒá tworzenia rozwiƒÖza≈Ñ w oparciu o serwisy w Snowflake Do≈õwiadczenie w tworzeniu, wdra≈ºaniu i rozwiƒÖzywaniu problem√≥w z aplikacjami danych na platformie Microsoft Azure Znajomo≈õƒá rozwiƒÖza≈Ñ chmurowych (Azure, GCP), Silne umiejƒôtno≈õci rozwiƒÖzywania problem√≥w i dba≈Ço≈õƒá o szczeg√≥≈Çy. Umiejƒôtno≈õƒá skutecznej komunikacji i wsp√≥≈Çpracy w zespole. Chƒôƒá uczenia siƒô i dostosowywania do nowych technologii i koncepcji. Rozumienie zasad Agile i Scrum (pracujemy w Scrumie) Dodatkowe atuty: Do≈õwiadczenie w budowaniu skalowalnych, dzia≈ÇajƒÖcych w czasie rzeczywistym rozwiƒÖza≈Ñ typu Data Lake, Do≈õwiadczenie ze strumieniowym przesy≈Çaniem danych (Kafka), Do≈õwiadczenie w pracy z Kubernetes Znajomo≈õƒá koncepcji Data Mesh. Znajomo≈õƒá podej≈õcia DevOps Oferujemy Realny wp≈Çyw na rozw√≥j firmy i produktu Pracƒô w do≈õwiadczonym zespole, kt√≥ry chƒôtnie dzieli siƒô wiedzƒÖ JasnƒÖ wizjƒô rozwoju dziƒôki regularnym feedbackom i klarownym ≈õcie≈ºkom karier Bud≈ºet szkoleniowy na interesujƒÖce Ciƒô kursy i konferencje Dodatkowy dzie≈Ñ wolny z okazji Twoich urodzin Dodatkowy dzie≈Ñ wolny dla rodzic√≥w Sprzƒôt dopasowany do Twoich potrzeb PrywatnƒÖ opiekƒô medycznƒÖ i ubezpieczenie grupowe Dostƒôp do platformy e-learningowej do nauki jƒôzyka angielskiego oraz platformy benefitowej Dostƒôp do platformy wellbeingowej i mo≈ºliwo≈õƒá skorzystania z warsztat√≥w oraz prywatnych sesji terapeutycznych Pracƒô zdalnƒÖ, z biura w Warszawie lub z coworku w Twoim mie≈õcie Regularne spotkania integracyjne","[{""min"": 15000, ""max"": 19000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15000,19000,Net per month - B2B
Full-time,Senior,B2B,Remote,613,Database Administrator,Experis Manpower Group,"We are looking for aDatabase Administratorwho will take part in planning and developing the database, while also proactively addressing and resolving user-related issues. Responsibilities: ‚Ä¢ Install, configure, and maintain database management systems (DBMS) ‚Ä¢ Monitor database performance and implement changes to improve efficiency ‚Ä¢ Ensure data integrity and security by implementing appropriate measures ‚Ä¢ Perform regular database backups and recovery operations ‚Ä¢ Collaborate with developers to design and optimize database structures ‚Ä¢ Troubleshoot database issues and provide technical support to users ‚Ä¢ Maintain documentation related to database configurations and procedures ‚Ä¢ Stay updated with the latest database technologies and best practices Requirements: ‚Ä¢ Bachelor's degree in Computer Science, Information Technology, or a related field ‚Ä¢ Proven experience as a Database Administrator or similar role ‚Ä¢ Proficiency in database management systems such as Oracle, SQL Server, or MySQL ‚Ä¢ Strong understanding of database design and data modelling ‚Ä¢ Knowledge of backup and recovery procedures ‚Ä¢ Excellent problem-solving skills and attention to detail ‚Ä¢ Strong communication and teamwork abilities ‚Ä¢ Experience with cloud-based database solutions ‚Ä¢ Familiarity with data warehousing and ETL processes ‚Ä¢ Knowledge of database security practices Our offer: ‚Ä¢ B2B via Experis ‚Ä¢ 100% remote work ‚Ä¢ MultiSport Plus ‚Ä¢ Group insurance ‚Ä¢ Medicover Premium ‚Ä¢ e-learning platform","[{""min"": 172, ""max"": 183, ""type"": ""Net per hour - B2B""}]",Database Administration,172,183,Net per hour - B2B
Full-time,Senior,B2B,Remote,614,Senior Data Engineer (2328),N-iX,"About us: N-iXis a software development service company that helps businesses across the globe develop successful software products. Founded in 2002 in Lviv,N-iXhas come a long way and increased its presence in eight countries Poland, Ukraine, Sweden, Bulgaria, Malta, the UK, the US, and Colombia. Today, we are a strong community of 2,000+ professionals and a reliable partner for global industry leaders and Fortune 500 companies About the position: Our customer is seeking to expand its Data Engineering team to stand up modern data platform for one of its portfolio companies in the financial services sector. The role requires expertise in Python, SQL (PostgreSQL, MySQL), Airflow, Snowflake, and AWS cloud experience. This project involves managing financial assets owned by the company. It utilizes machine-learning models that operate on data ingested from third-party APIs. The process includes ELT (extract, load, transform), data modeling in Snowflake using DBT, training ML models using AWS SageMaker, running predictions, and storing predictions back in Snowflake. As a Senior Data Engineer, you will design, build, and maintain scalable data pipelines and architectures for our cloud-based analytical platforms. You will collaborate closely with data scientists, analysts, and software engineering teams to deliver robust, high-quality data solutions that drive business decisions. Project involvement plans: Initially 5 months with the possibility of extension. Start in July 2025. Responsibilities: Design, implement, and maintain scalable data pipelines that support business analytics, reporting, and operational needs. Collaborate cross-functionally with analysts, engineers, and product teams to translate data requirements into efficient data models and pipelines. Ensure reliability and performance of data workflows by proactively monitoring, debugging, and optimizing data processes. Drive automation and testing practices in data workflows to maintain high code quality and deployment confidence. Contribute to architectural decisions involving cloud infrastructure, data warehousing strategies, and data governance policies. Required Skills and Qualifications: Key Skills: Python AWS Snowflake Airflow DBT data modeling PostgreSQL, MySQL (or similar) Technical Expertise: Programming Languages: Advanced proficiency in Python for data engineering, data wrangling, and pipeline development. Cloud Platforms: Hands-on experience working with AWS (S3, Glue, Redshift, Lambda, etc.). Data Warehousing: Proven expertise with Snowflake ‚Äì schema design, performance tuning, data ingestion, and security. Workflow Orchestration: Production experience with Apache Airflow (Prefect, Dagster or similar), including authoring DAGs, scheduling workloads and monitoring pipeline execution. Data Modeling: Strong skills in DBT (Data Build Tool), including writing modular SQL transformations, building data models, and maintaining DBT projects. SQL Databases: Extensive experience with PostgreSQL, MySQL (or similar), including schema design, optimization, and complex query development. Additional Competencies: Version Control and CI/CD: Familiarity with Git-based workflows and continuous integration/deployment practices to ensure seamless code integration and deployment processes. Communication Skills: Ability to articulate complex technical concepts to technical and non-technical stakeholders alike. Tools: Git JIRA Confluence Must have: Extensive experience with Python for data analysis Declarative Data Modeling: Experience with modern tools like DBT for streamlined and efficient data modelling. Minimum 5 years of professional experience in production environments, emphasizing performance optimization and code quality. We offer: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits","[{""min"": 19209, ""max"": 24750, ""type"": ""Net per month - B2B""}]",Data Engineering,19209,24750,Net per month - B2B
Full-time,Senior,B2B,Remote,615,Senior Data Engineer,INFOPLUS TECHNOLOGIES,"Job Title: Senior Data Engineer üìç Location: Poland (Remote) üí∞ Rate: 1000 PLN/day üïí Seniority Level: Senior Required Skills & Experience: 5+ years of experience in data engineering roles Strong proficiency in Python for data engineering tasks Hands-on experience with AWS services such as: S3, Glue, Lambda, Step Functions, Redshift, Athena, CloudFormation Experience with CI/CD tools (e.g., GitLab CI, Jenkins, GitHub Actions) Proficiency with AWS CDK or other Infrastructure as Code frameworks Solid understanding of data warehousing , data lakes , and ETL best practices Familiar with version control (Git), unit testing, and agile delivery practices Excellent problem-solving skills and ability to work independently Strong communication skills with the ability to explain technical topics to stakeholders 5+ years of experience in data engineering roles Strong proficiency in Python for data engineering tasks Hands-on experience with AWS services such as: S3, Glue, Lambda, Step Functions, Redshift, Athena, CloudFormation Experience with CI/CD tools (e.g., GitLab CI, Jenkins, GitHub Actions) Proficiency with AWS CDK or other Infrastructure as Code frameworks Solid understanding of data warehousing , data lakes , and ETL best practices Familiar with version control (Git), unit testing, and agile delivery practices Excellent problem-solving skills and ability to work independently Strong communication skills with the ability to explain technical topics to stakeholders","[{""min"": 800, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Data Engineering,800,1000,Net per day - B2B
Full-time,Senior,B2B,Remote,619,Data Platform Engineer - available ASAP,ITDS,"As aData Platform Engineer, you will be working for our client in the debt collection sector, helping to build and maintain a robust, cloud-native data platform. The role focuses heavily on data modelling, requiring an expert who can translate conceptual business needs into logical and physical data models, build data contracts, and implement scalable ELT pipelines using Azure Databricks. Your main responsibilities: Design and maintain logical and physical data models based on DDD (Domain-Driven Design) principles Translate conceptual models and business glossaries into technical data structures for the Data Warehouse Perform data mapping and create data contracts between the Data Platform and source systems Collaborate with source system owners to define data contract requirements Work on data ingestion processes from source systems using various methods: Direct database queries (bulk read/CDC), API communication, Event streaming Implement ELT processes across Bronze, Silver, and Gold layers in Azure Databricks Ensure alignment of data models with business and analytical requirements You're ideal for this role if you have: Strong experience in Data Modelling (logical & physical), preferably in DDD-based environments Proven ability to work with Data Governance inputs: glossaries, conceptual models, HLD/LLD documentation Experience preparing and maintaining data contracts Solid knowledge of data ingestion techniques and working with source systems Experience with Azure Databricks Ability to develop and maintain ELT pipelines in cloud-native environments Nice to have: Experience in writing clear technical documentation (e.g. data contracts, field definitions, extraction rules) Background in mapping source data to target DWH structures Ability to interpret and work with ERDs and relational models Knowledge of master data management practices Familiarity withdbdiagram.io Awareness of Data Quality, Data Lineage, and metadata management concepts Experience using tools like Azure Purview or other metadata management platforms We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #6787 You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere.","[{""min"": 25200, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,31500,Net per month - B2B
Full-time,Senior,B2B,Remote,620,Data Engineering Analyst,TechTorch,"About Us TechTorchis a fast-growing consultancy at the intersection of enterprise tech, AI, and private equity. We partner with top-tier PE funds and their portfolio companies to deliver AI-powered platforms, data accelerators, and digital transformation projects that generate measurable value ‚Äî fast. Founded by former Bain consultants, CIOs, and enterprise tech leaders, we blend startup agility with big-firm experience. We‚Äôre not a typical startup ‚Äî we were built to deliver. As aData Analyst, you‚Äôll be a key contributor to building a modern Azure-native data platform for one of our private equity-backed clients. You‚Äôll analyze current data sources and legacy reports, reverse-engineer business logic, and translate complex reporting requirements into structured data specifications. You‚Äôll also support data profiling, validation, and the design of a Common Data Model (Bronze/Silver/Gold). Your work will directly influence dbt model development, data pipelines, and reporting layers ‚Äî all serving as the organization‚Äôs single source of truth. SQL (advanced querying, troubleshooting) dbt, ETL, data modeling (dimensional/relational) Data profiling, validation, quality checks Azure Data Services (preferred), AWS/GCP also welcome Familiarity with Medallion architecture Bonus: Python (Pandas), Salesforce data Analyze raw and semi-structured datasets to understand key metrics and usage Reverse-engineer legacy SQL, stored procedures, reports Perform data profiling and assess quality issues Translate business needs into clear data specs, rules, and mappings Support Common Data Model design (Bronze/Silver/Gold) Define data validation, error handling, and quality checks Collaborate closely with Data Architects and engineers Support testing and debugging through validation reports and business logic guidance Engage with business stakeholders to clarify reporting needs 5+ years of experience in enterprise data analysis Strong SQL skills (querying, analysis, debugging) Deep experience in data profiling and validation Familiarity with relational and dimensional modeling Ability to understand and document transformation logic Experience working across technical and business teams Knowledge of modern data architectures (Medallion, warehouse design) Clear, structured communication skills English: fluent spoken and written Python (Pandas) for exploratory analysis Experience with Salesforce data Cloud experience (Azure preferred, AWS/GCP okay) Projects with high-impact PE-backed companies Work with ex-Bain, top-tier CIOs, and data leaders Real ownership and visibility in project delivery Remote-first culture built on speed, clarity, and results A fast-paced environment for high performers who want to grow Client First‚Äì Value and outcomes over slide decks We, Not Me‚Äì We move faster as a team Get Stuff Done‚Äì Execution over bureaucracy AI First‚Äì AI is embedded into what we build Own It‚Äì We take full responsibility Agile Mindset‚Äì We adapt fast and improve constantly","[{""min"": 16000, ""max"": 23500, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,16000,23500,Net per month - B2B
Full-time,Senior,B2B,Remote,621,Data Engineering Architect (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition byForbesas one of the top 10 AI consulting companies. As aData Engineering Architect, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company.This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies.This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. This role represents a gradual shift away from hands-on coding towards a more strategic focus on system design, business consultation, and creative problem-solving. It offers an opportunity to engage more deeply with architecture-level decisions, collaborate closely with clients, and contribute to building innovative data-driven solutions from a broader perspective. üöÄ Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Airflow, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. üéØ What you'll need to succeed in this role: 5+ years of proven commercial experiencein implementing, developing, or maintaining Big Data systems. Strong programming skills inPythonorJava/Scala: writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Hands-on experience withBig Datatechnologies likeSpark,Airflow,Iceberg, CI/CD, Kafka. Proven expertise in implementing and deploying solutions in cloud environments (with a preference forAWS). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master‚Äôs or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. FluentEnglish(C1 level) is a must. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage withtop-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you towork remotelyor from modern offices and coworking spaces. Accelerate your professional growth throughcareer paths,knowledge-sharinginitiatives,languageclasses, and sponsoredtrainingorconferences, including a partnership withDatabricks, which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days offavailable for B2B contractors and individuals under contracts of mandate. Participate inteam-building eventsand utilize theintegration budget. Celebratework anniversaries, birthdays,andmilestones. Accessmedicalandsports packages, eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you canboostyourpersonal brandby speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website (career page) and social media (Facebook,LinkedIn,Instagram).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Architecture,21000,31920,Net per month - B2B
Full-time,Mid,B2B,Hybrid,626,Operation Center Engineer (DBA),emagine Polska,"üåç Work model: hybrid work ‚Äì 2 times a week from the office in Warsaw + business travels to Oslo from time to time. üìë Assignment type: B2B - 125-145 zl/h. ‚è≥ Project length: Long term (24 months contract + extensions). üìï Project language: English. ‚öôÔ∏è Industry: finance. Are you a skilled IT consultant looking for your next challenge? We are on the lookout for a Frontline Senior Operation Center Engineer to support our dynamic client base. You will manage, optimize, and maintain complex database technologies while collaborating closely with a specialized team of professionals. Your problem-solving skills and technical knowledge will be crucial in our mission to deliver top-notch solutions and services. Responsibilities: Managing, optimizing, and scaling customer databases on-premises and in Microsoft Azure. Participating in onboarding new customer databases. Troubleshooting and resolving database and application-related incidents. Implementing change requests from customers. Proactively improving customer delivery. Taking ownership of problem tickets and implementing solutions. Key Requirements: Bachelor‚Äôs degree in Computer Science, Information Technology, or a related field (equivalent experience is acceptable). Strong analytical and problem-solving abilities. Independent worker with team collaboration skills. Experience with PostgreSQL and MySQL database engines. Proficiency in Atlassian products (Confluence, Jira). Competence in Linux, Windows, and Azure. Knowledge of Elastistack and ElastiCloud. Nice to Have: Experience with RavenDB and Redis database engines. Ability to troubleshoot LDAP/SSO/TLS issues. Basic understanding of virtualization technologies. Join us for a professional journey where innovation and technology meet expertise and creativity in the finance industry.","[{""min"": 125, ""max"": 145, ""type"": ""Net per hour - B2B""}]",Data Engineering,125,145,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,627,Data Architect / Data Warehouse Specialist,Power Media,"Nasz klient kompleksowo realizuje us≈Çugi w zakresie konsultingu, projektowania i zarzƒÖdzania projektami. Specjalizuje siƒô g≈Ç√≥wnie w obszarach e-commerce, data management (Agile Data Warehouse, Data Governance) oraz Content Management. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozw√≥j rozwiƒÖza≈Ñ Data Platform w ≈õrodowisku chmurowym. Kluczowym zadaniem bƒôdzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejs√≥w w z≈Ço≈ºonym krajobrazie systemowym klienta. Projekt obejmuje budowƒô hurtowni danych oraz platform danych od podstaw. Zesp√≥≈Çsk≈Çada siƒô z 22 os√≥b: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiƒÖzk√≥w: Rozw√≥j i utrzymanie Data Warehouse oraz Data Platform przy u≈ºyciu narzƒôdzi ETL i SQL, Analiza wymaga≈Ñ biznesowych i proponowanie rozwiƒÖza≈Ñ technicznych, Wsp√≥≈Çpraca z zespo≈Çem w celu zapewnienia dostƒôpno≈õci rozwiƒÖza≈Ñ biznesowych, Wykorzystanie swojej wiedzy i do≈õwiadczenia do tworzenia innowacyjnych rozwiƒÖza≈Ñ. G≈Ç√≥wne wymagania: Bardzo dobra znajomo≈õƒá metod i technik projektowania oprogramowania, Ponad 10-letnie do≈õwiadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych, Znajomo≈õƒá ekosystem√≥w Big Data, Znajomo≈õƒározwiƒÖza≈Ñ chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejƒôtno≈õci analityczne (analiza i dokumentowanie wymaga≈Ñ biznesowych oraz specyfikacji technicznych) Do≈õwiadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomo≈õƒá SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomo≈õƒá rozwiƒÖza≈Ñ ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (min. B2+/C1)‚Äì codzienna praca w miƒôdzynarodowym ≈õrodowisku, Gotowo≈õƒá do podr√≥≈ºy s≈Çu≈ºbowych. Mile widziane: Znajomo≈õƒánarzƒôdzi Informatica, Znajomo≈õƒá Machine Learning oraz framework√≥w ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomo≈õƒá jƒôzyk√≥w programowania (Java, Scala, C++, Python). Znajomo≈õƒá jƒôzyka niemieckiego. Firma oferuje: Stabilne, d≈Çugofalowe zatrudnienie w oparciu o B2B lub UoP, Mo≈ºliwo≈õƒá pracy w wiƒôkszo≈õci zdalnej(praca z biura 1 raz w tygodniu, we wtorek) P≈Çaska struktura, antykorporacyjne podej≈õcie do pracy i zespo≈Çu, Ciekawe, miƒôdzynarodowe projekty, Zgrany zesp√≥≈Ç chƒôtnie uczestniczƒÖcy w aktywno≈õciach sportowo ‚Äì rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team ‚Äì building), Dodatkowe zajƒôcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajƒôƒá sportowych, Nowoczesne biuro ze strefƒÖ relaksu, Co roczny 5 dniowy wyjazd firmowy (ca≈Ça firma), warsztaty kulinarne, wyj≈õcia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywno≈õciami ≈öwietna atmosfera, partnerskie podej≈õcie, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa ‚Äûmiƒôkko-techniczna‚Äù. CV w j. angielskim.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,20000,30000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,629,Senior Data Engineer,Harvey Nash Technology,"3+ years of strong Python development experience and solid engineering practices Expertise in microservices frameworks and event-driven architectures Advanced Spark skills (PySpark, Scala) with experience in scalable, maintainable pipelines Experience with Spark Streaming and Delta Lake Familiarity with Kubernetes and MongoDB Proven AWS cloud experience Design, build, and enhance data pipelines for streaming and batch processing Extend and support our AWS cloud data platform Develop features using Databricks pipelines, Unity Catalog, and Spark Streaming Lead and mentor the team, promoting best practices and process improvements Gather requirements from stakeholders and deliver high-quality solutions Partner with customers to ensure project success What You‚Äôll Bring 3+ years of strong Python development experience and solid engineering practices Expertise in microservices frameworks and event-driven architectures Advanced Spark skills (PySpark, Scala) with experience in scalable, maintainable pipelines Experience with Spark Streaming and Delta Lake Familiarity with Kubernetes and MongoDB Proven AWS cloud experience Strong communication and a commitment to high ethical standards","[{""min"": 32000, ""max"": 40000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,32000,40000,Gross per month - Permanent
Full-time,Mid,B2B,Remote,633,Data Engineer,Ework Group,"üíª Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. üîπ For our Client we are looking forData Engineerüîπ ‚úîÔ∏è VCE team is looking for a skilled Data Engineer. We process data coming from machines and factories in order to provide customized data products to our customers around the world. Our mission is to expose value of the data and support clients to become fully operational data driven company. At our company, data practitioners have the opportunity to work with various types of data coming through diverse channels with different frequency. Preferred hybrid work from the Wroc≈Çaw office, but candidates from outside Wroc≈Çaw will also be considered. ‚úîÔ∏è Competences and skills : Informatica Power Center Azure Databricks (Pyspark, Spark SQL, Unity Catalog, Jobs/Workflows) Advanced SQL Practical experience with at least one relational DBMS (SQL Server / Oracle / PostgreSQL) Azure DevOps (Repos, Pipelines, YAML) Azure Key Vault Azure Data Factory (optional) DBT (optional) ‚úîÔ∏è Soft skills: Open-minded Engaged and flexible Driver of topics - ready to find his/her way to develop or proceed with topics Good at collaboration with stakeholders from IT and business side Working in the past in the data mash environment (as a bonus) ‚úîÔ∏è We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 120, ""max"": 128, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,128,Net per hour - B2B
Full-time,Mid,B2B,Remote,636,Data Engineer,emagine Polska,"PROJECT INFORMATION: Industry: Banking Location: Remote Type of assignment: B2B RESPONSIBILITIES: As a Data Engineer, your core responsibility will be to manage data artifacts for integration. Develop Component Data Artifacts (CDAs). Break down Master Data Artifacts (MDAs) and Global Data Artifacts (GDAs) into CDAs for integration. Focus on data modeling, data reuse, and understanding data domains. Implement data normalization concepts. Efficiently query data and prepare it for integration into the data lake using Juniper pipelines. REQUIREMENTS: Strong technical understanding of Google Cloud Platform (GCP). Experience with Tableau or Looker. Experience with Apache Airflow or Hadoop. Proficiency in data modeling concepts. Experience with data normalization techniques. Knowledge of data integration methodologies. NICE TO HAVE: Understanding of Juniper pipelines. Knowledge of refinery data processes. OTHER DETAILS: The Data Engineer will collaborate closely with GCP engineers and platform teams. This role focuses on ensuring effective management of data modeling and reuse across various platforms.","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,180,200,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,637,Big Data Developer,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: üíªHybrid work (2 days from the office) - Warszawa/Gdansk or Gdynia Responsibilities: Responsible for a successful design and implementation of a high-performing, flexible, robust, scalable and easily maintainable global reporting solution for the Bank Actively cooperating with Product Owner, Solution Architects, Analysts, Testers and Developers Working in an Agile team Requirements: 3+ years of experience with functional programming in Scala Experience with writing Spark-based applications in Scala Experience in containerized technologies including Docker and Kubernetes Deep understanding of Hadoop Technology Stack: Hive, Oozie, Kafka Strong communication skills and fluency in spoken and written English Experience within Agile ways of working Nice to have: Deep understanding of the principles of distributed systems Experience in data engineering and building ETL/ELT pipelines Experience with performance tuning of Hadoop/Spark solutions Experience in developing RESTful services and web applications with Bootstrap and ReactJS Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Senior,Permanent,Hybrid,638,Data Engineer,Vaillant Group Business Services,"What we achieve together We are looking for an experienced Data Engineer to join our innovative team at Vaillant. In this role, you will be instrumental in advancing our data infrastructure, driving analytics excellence, and leveraging cutting-edge technologies to transform data into actionable insights. If you are passionate about making a meaningful impact and eager to collaborate with a team that appreciates your expertise, we encourage you to apply. You will design, build, and maintain robust infrastructure and programs for efficient data extraction, transformation, loading, and serving. You will leverage Azure Cloud services, Spark technologies, and DataOps practices to handle large data volumes from diverse sources such as IoT, CRM, ERP, and PLM. You will play a pivotal role in supporting the design, development, maintenance, and automation of data products on our data platform, contributing significantly to data governance and security compliance. Ensuring data quality and accuracy is your priority, and you will implement validation and cleansing processes to achieve this. You will collaborate with a dynamic team of DevOps engineers, data scientists, and data engineers, and you will engage in advanced analytics of machine data, business process-related data, and customer data, enhancing Vaillant Group‚Äôs digital service portfolio. Your effective communication skills will shine as you work with cross-functional colleagues, sharing innovative ideas and fostering a collaborative environment. Agile methodologies are your preferred approach, allowing you to work seamlessly with the team and stakeholders. What makes us successful together Qualification: You hold a university degree, preferably in Computer Science or a related field, with a focus on Data Science, Data Mining, or Data Analytics. Experience: You bring at least 3 years of relevant experience in data analytics, particularly within a Big Data context. Know-how and skills: Your proficiency in Python (PySpark) and SQL is exceptional, and you are adept at working with cloud services and related components, ideally within the Microsoft Azure ecosystem and Databricks. Nice to have: Experience with additional data platforms and tools will be a plus. Personality: You thrive in a team setting, showcasing a high degree of initiative and motivation to excel in an agile and interdisciplinary environment. Language skills: Fluency in English is essential, enabling effective communication and collaboration across our global teams. What you can count on Hybrid work and environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee. Package of additional benefits: private medical care, multi-sport card. Onboarding: our clearly structured onboarding process, including an Onboarding App, enables us to integrate new employees into Vaillant Group quickly and in a targeted manner. Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings.","[{""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16000,20000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,639,Senior Azure Data Engineer with Databricks,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: Senior Azure Data Engineer with Databricks Responsibilities: Being responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems Building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies Evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards Driving creation of re-usable artifacts Establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation Working closely with analysts/data scientists to understand impact to the downstream data models Writing efficient and well-organized software to ship products in an iterative, continual release environment Contributing and promoting good software engineering practices across the team Communicating clearly and effectively to technical and non-technical audiences Defining data retention policies Monitoring performance and advising any necessary infrastructure changes Requirements: 3+ years‚Äô experience with Azure Data Factory and Databricks 5+ years‚Äô experience with data engineering or backend/fullstack software development Strong SQL skills Python scripting proficiency Experience with data transformation tools - Databricks and Spark Experience in structuring and modelling data in both relational and non-relational forms Experience with CI/CD tooling Working knowledge of Git English level: B2, C1 Nice to have: Experience with Azure Event Hubs, CosmosDB, Spark Streaming, Airflow Experience in Aviation Industry and Copilot Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Senior,Permanent,Remote,641,Principal Software Engineer,dotLinkers,"Salary: up to 49 000 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Principal Software Engineer you will serve as the key strategic and technical leader shaping the next generation of compute infrastructure. You‚Äôll design scalable execution platforms supporting microservices, batch processing, streaming data pipelines, and long-running workflows‚Äîleveraging Azure technologies including AKS (Kubernetes), KEDA, Temporal, and Apache Spark. You‚Äôll help define the broader platform architecture, influencing areas such as compute, storage, monitoring, and developer enablement. Working closely with engineering leaders and platform stakeholders, you‚Äôll lead the evolution toward a scalable, cloud-native, and developer-friendly compute environment that supports a variety of workloads across the organization. Responsibilities: Develop and refine the technical roadmap for compute infrastructure, enabling scalability, flexibility, and reliability across multiple workload types. Design core cloud-native execution frameworks for workflows, stream processing, and high-volume batch processing. Lead the adoption of Kubernetes, KEDA, and Temporal to orchestrate compute workloads with strong observability and fault tolerance. Integrate advanced data processing tools like Apache Spark, Azure Stream Analytics, and event-driven architectures across product services. Guide multi-team transformations of legacy systems into scalable microservices, containerized workloads, or serverless solutions. Align compute architecture with business priorities in collaboration with product and platform teams, ensuring compliance and operational reliability. Mentor Staff and Lead Engineers, promoting best practices in scalable architecture, cloud infrastructure, and modern compute strategies. Participate in architectural reviews, contribute to design documentation, and support long-term technical planning. Advocate for platform quality, security, and a streamlined developer experience. Required Qualifications: 10+ years of experience in software engineering, infrastructure, or platform development with demonstrated leadership in architecture. Hands-on experience running large-scale, production-grade Kubernetes-based compute platforms. Strong knowledge of orchestration technologies like KEDA, Temporal, or similar workflow/job orchestration engines. Practical experience with both batch (e.g., Spark) and streaming (e.g., Kafka, Azure Event Hubs) data processing systems. Proficiency in multiple programming languages (Go, Python, C#, Rust) and infrastructure-as-code tools (Terraform, Pulumi). In-depth understanding of distributed systems, autoscaling strategies, and compute security standards. Proven ability to work cross-functionally with multiple engineering teams and contribute to broader platform strategy. Preferred Qualifications: Background in building internal developer platforms or compute services offered as products. Knowledge of Azure serverless technologies such as Azure Functions or Azure Container Apps. Familiarity with tools like Dapr, KEDA Scalers, and modern runtime technologies such as Wasm, Nomad, or OpenFaaS. Contributions to open-source projects within the cloud-native or CNCF ecosystems. Experience designing systems that are multi-region, multi-tenant, and support zero-downtime deployments. Leadership Expectations: Define the long-term vision for compute infrastructure, ensuring alignment with business and growth objectives. Provide architectural leadership and technical mentorship across engineering and product teams. Drive complex, multi-disciplinary initiatives covering compute, data, security, and reliability. Cultivate a culture of technical excellence, innovation, and continuous improvement. Mentor senior engineers and encourage platform thinking and service-oriented design. Core Skills: Visionary Architecture: Ability to craft and communicate future-proof compute strategies. Technical Authority: Expertise in modern compute technologies and the ability to resolve complex technical challenges. Cross-Functional Leadership: Comfortable working with technical leaders across infrastructure, platform, and product domains. Cloud-Native Expertise: Mastery of compute orchestration, workload management, and event-driven architectures on Azure. Balanced Innovation: Ability to combine cutting-edge technologies with pragmatic, reliable solutions. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 39000, ""max"": 49000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,39000,49000,Gross per month - Permanent
Full-time,Mid,B2B,Remote,643,Technical Application Support Engineer,SCALO,"W Scalo zajmujemy siƒô dostarczaniem projekt√≥w software'owych i wspieraniem naszych partner√≥w w rozwijaniu ich biznesu. Tworzymy oprogramowanie, kt√≥re umo≈ºliwia ludziom dokonywanie zmian, dzia≈Çanie w szybszym tempie oraz osiƒÖganie lepszych rezultat√≥w. Jeste≈õmy firmƒÖ, kt√≥ra wykorzystuje szerokie spektrum us≈Çug IT, ≈ºeby pomagaƒá klientom. Obszary naszego dzia≈Çania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiƒÖzania chmurowe, zarzƒÖdzanie danymi, dedykowane zespo≈Çy developerskie. Cze≈õƒá! U nas znajdziesz to, czego szukasz - przekonaj siƒô! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. bran≈ºy Fintech. Wykorzystywany stos technologiczny: SQL, T-SQL, Neo4J, SOAP/XML, REST API, Tableau, SSRS, Power BI, Java, konfiguracja rozwiƒÖza≈Ñ i produkt√≥w dla klient√≥w, wsparciem drugiej linii (2nd level support) oraz modyfikacja skrypt√≥w, zapewnienie jako≈õci przed przekazaniem klientowi: planowanie, definiowanie i realizacja test√≥w, tworzenie dedykowanych raport√≥w i dashboard√≥w dla klient√≥w, wsp√≥≈Çpraca z zespo≈Çami developerskimi przy diagnozowaniu i rozwiƒÖzywaniu problem√≥w, tworzenie i utrzymanie funkcjonalno≈õci opartych o bazƒô danych Neo4J z wykorzystaniem jƒôzyka Cypher, komunikacja z klientami w celu diagnozowania problem√≥w i proponowania rozwiƒÖza≈Ñ, wsparcie po sprzeda≈ºy i po wdro≈ºeniu ‚Äì rozwiƒÖzywanie z≈Ço≈ºonych problem√≥w, analiza i modyfikacja skrypt√≥w w jƒôzyku Java (korekta b≈Çƒôd√≥w i tworzenie nowych), udzia≈Ç w rozwoju system√≥w w ramach zmian zg≈Çaszanych przez obecnych klient√≥w, stawka do 75 z≈Ç/h przy B2B w zale≈ºno≈õci od do≈õwiadczenia. Ta oferta jest dla Ciebie, je≈õli: masz wykszta≈Çcenie wy≈ºsze w zakresie informatyki, ekonomii lub matematyki, posiadasz minimum 2 lata do≈õwiadczenia zawodowego na podobnym stanowisku, posiadasz do≈õwiadczenie z Microsoft SQL, w tym czytania istniejƒÖcego kodu (T-SQL), masz do≈õwiadczenie z bazami danych Neo4J, posiadasz podstawowƒÖ znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z sieciami komputerowymi, najlepiej z do≈õwiadczeniem w pracy z VPN, FTP i SFTP, znasz SOAP/XML i/lub REST/JSON, znasz narzƒôdzia, np. Tableau, SSRS, Power BI, biegle komunikujesz siƒô w jƒôzyku angielskim (min. B2), mile widziana podstawowa znajomo≈õƒá Java. Co dla Ciebie mamy: d≈ÇugofalowƒÖ wsp√≥≈Çpracƒô - r√≥≈ºnorodne projekty (dzia≈Çamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), mo≈ºliwo≈õƒá rozwoju kompetencji we wsp√≥≈Çpracy z naszym Center of Excellence, kafeteryjny system benefit√≥w ‚Äì Motivizer, prywatnƒÖ opiekƒô medycznƒÖ ‚Äì Luxmed. Brzmi interesujƒÖco? Aplikuj ‚Äì czekamy na Twoje CV!","[{""min"": 11760, ""max"": 12600, ""type"": ""Net per month - B2B""}]",Data Engineering,11760,12600,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,648,Senior Data Engineer with Snowflake (greenfield),N-iX,"#3359 Join an exciting journey to create a greenfield, cutting-edge Consumer Data Lake for a leading global organization based in Europe. This platform will unify, process, and leverage consumer data from various systems, unlocking advanced analytics, insights, and personalization opportunities. As a Senior Data Engineer, you will play a pivotal role in shaping and implementing the platform's architecture, focusing on hands-on technical execution and collaboration with cross-functional teams. Your work will transform consumer data into actionable insights and personalization on a global scale. Using advanced tools to tackle complex challenges, you‚Äôll innovate within a collaborative environment alongside skilled architects, engineers, and leaders. Key Responsibilities: Hands-On Development : Build, maintain, and optimize data pipelines for ingestion, transformation, and activation. Create and implement scalable solutions to handle diverse data sources and high volumes of information. Data Modeling & Warehousing : Design and maintain efficient data models and schemas for a cloud-based data platform. Develop pipelines to ensure data accuracy, integrity, and accessibility for downstream analytics. Collaboration : Partner with Solution Architects to translate high-level designs into detailed implementation plans. Work closely with Technical Product Owners to align data solutions with business needs. Collaborate with global teams to integrate data from diverse platforms, ensuring scalability, security, and accuracy. Platform Development : Enable data readiness for advanced analytics, reporting, and segmentation. Implement robust frameworks to monitor data quality, accuracy, and performance. Testing & Quality Assurance : Implement robust security measures to protect sensitive consumer data at every stage of the pipeline Ensure compliance with data privacy regulations (e.g., GDPR, CCPA ..) and internal policies. Monitor and address potential vulnerabilities, ensuring the platform adheres to security best practices. Requirements: Over 4+ years of experience showcasing technical expertise and critical thinking in data engineering. Hands-on experience with DBT and strong Python programming skills. Proficiency in Snowflake and expertise in data modeling are essential. Demonstrated experience in building consumer data lakes and developing consumer analytics capabilities is required. In-depth understanding of privacy and security engineering within Snowflake , including concepts like RBAC, dynamic/tag-based data masking, row-level security/access policies, and secure views. Ability to design, implement, and promote advanced solution patterns and standards for solving complex challenges. Familiarity with multiple cloud platforms ( Azure or GCP preferred, with a focus on Azure). Practical experience with Big Data batch and streaming tools. Competence in SQL, NoSQL, relational database design (SAP HANA experience is a bonus), and efficient methods for data retrieval and preparation at scale. Proven ability to collect and process raw data at scale, including scripting, web scraping, API integration, and SQL querying. Experience working in global environments and collaborating with virtual teams. A Bachelor‚Äôs or Master‚Äôs degree in Data Science, Computer Science, Economics, or a related discipline. We offer*: Flexible working format - remote, office-based or flexible. A competitive salary and good compensation package. Personalized career growth. Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more). Active tech communities with regular knowledge sharing. Education reimbursement Memorable anniversary presents. Corporate events and team building. Other location-specific benefits. *not applicable for freelancers.","[{""min"": 22164, ""max"": 29553, ""type"": ""Net per month - B2B""}, {""min"": 17731, ""max"": 24566, ""type"": ""Gross per month - Permanent""}]",Data Engineering,22164,29553,Net per month - B2B
Full-time,Senior,B2B,Remote,652,Lead Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition byForbesas one of the top 10 AI consulting companies. As aLead Data Engineer, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company.This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies.This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. This role represents a gradual shift away from hands-on coding towards a more strategic focus on system design, business consultation, and creative problem-solving. It offers an opportunity to engage more deeply with architecture-level decisions, collaborate closely with clients, and contribute to building innovative data-driven solutions from a broader perspective. üöÄ Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Hadoop, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. üéØ What you'll need to succeed in this role: 5+ years of proven commercial experiencein implementing, developing, or maintaining Big Data systems. Strong programming skills inPythonorJava/Scala: writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity withBig Datatechnologies likeSpark, Cloudera,Airflow, NiFi,Docker,Kubernetes,Iceberg, Trino or Hudi. Proven expertise in implementing and deploying solutions in cloud environments (with a preference forAWS). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master‚Äôs or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. FluentEnglish(C1 level) is a must. üéÅ Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage withtop-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you towork remotelyor from modern offices and coworking spaces. Accelerate your professional growth throughcareer paths,knowledge-sharinginitiatives,languageclasses, and sponsoredtrainingorconferences, including a partnership withDatabricks, which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days offavailable for B2B contractors and individuals under contracts of mandate. Participate inteam-building eventsand utilize theintegration budget. Celebratework anniversaries, birthdays,andmilestones. Accessmedicalandsports packages, eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you canboostyourpersonal brandby speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website (career page) and social media (Facebook,LinkedIn,Instagram).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,31920,Net per month - B2B
Full-time,Senior,B2B,Remote,653,Platform Integration Engineer (Salesforce & Qualtrics),ITDS,"As aPlatform Developer, you will be working for our client, a global leader in healthcare and agriculture. The client is dedicated to revolutionizing the digital experience across global markets by integrating state-of-the-art SaaS platforms into their customer and internal operations. Your contributions will directly support strategic initiatives, streamline business processes, and enable data-driven decision-making at a global scale. Working closely with IT, business stakeholders, and external vendors, you will architect scalable solutions that elevate both user engagement and data management capabilities. This is an opportunity to make a tangible impact in a fast-paced, mission-driven environment that‚Äôs transforming the future of crop science through digital platforms. Develop and distribute surveys using multiple channels including WhatsApp, SMS, and email Design and implement automated workflows for survey logic and data processes Support integration between Salesforce and Qualtrics based on business events Manage Salesforce workflows, rules, and data objects related to survey responses Collaborate with Salesforce development teams to address evolving integration needs Integrate survey platforms with Google Cloud to support data lakes and warehouse strategies Work with data teams to design ingestion strategies and perform gap analysis Configure AI-powered survey analytics and insights for reporting Partner with cross-functional teams to gather requirements and deliver technical solutions 5‚Äì7 years of experience in SaaS development and platform integration Proven expertise in Qualtrics platform administration and workflow configuration Strong knowledge of Salesforce configuration, workflows, and data architecture Basic knowledge of Salesforce Service Cloud Experience with integration tools, APIs, and cloud platforms like Google Cloud Solid understanding of data modeling, ingestion strategies, and architecture Proficiency in Apex, Lightning Components, SOQL, and Salesforce permissions management Knowledge of XML, HTML, CSS, SOAP/REST Ability to translate business requirements into scalable technical solutions Familiarity with AI analysis tools within survey platforms Strong collaboration and communication skills in cross-functional environments Experience working in Agile development frameworks We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #7440 üìå You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure availablehere.","[{""min"": 25200, ""max"": 29400, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,29400,Net per month - B2B
Full-time,Senior,B2B,Hybrid,654,Data Engineer (Cybersecurity),Antal Sp. z o.o.,"Data Engineer ‚Äì B2B Contract (Hybrid ‚Äì Krak√≥w or Warsaw) Location: Hybrid ‚Äì 6 days per month on-site (preferably Krak√≥w or Warsaw) Contract type: B2B Workload: Full-time We are looking for an experienced Data Engineer to join a team focused on data processing and analytics in the cybersecurity domain. This role involves designing and building robust data architecture and scalable data pipelines that power reporting, data analytics, and future machine learning models. Design, build, and test multi-layered data architecture and efficient, scalable data pipelines. Design, build, and test multi-layered data architecture and efficient, scalable data pipelines. Perform deep analysis of large, complex datasets to extract insights and generate statistical metrics for business reporting and data estate assessment. Perform deep analysis of large, complex datasets to extract insights and generate statistical metrics for business reporting and data estate assessment. Develop systems and software for data acquisition, aggregation, and refinement. Develop systems and software for data acquisition, aggregation, and refinement. Integrate data across various sources, systems, and platforms. Integrate data across various sources, systems, and platforms. Optimize query performance and overall data processing efficiency. Optimize query performance and overall data processing efficiency. Work on data ingestion, sourcing, aggregation, API integration, and feature engineering. Work on data ingestion, sourcing, aggregation, API integration, and feature engineering. Collaborate with stakeholders to align deliverables with business needs and better understand the data context. Collaborate with stakeholders to align deliverables with business needs and better understand the data context. Write clean, reusable, and maintainable code using the team's DevOps practices. Write clean, reusable, and maintainable code using the team's DevOps practices. Follow Agile methodologies, including test-driven development (TDD). Follow Agile methodologies, including test-driven development (TDD). Continuously grow technical and domain-specific skills in collaboration with the team. Continuously grow technical and domain-specific skills in collaboration with the team. At least 12 months of hands-on experience with Spark, PySpark, and/or Databricks (or a comparable modern data platform). At least 12 months of hands-on experience with Spark, PySpark, and/or Databricks (or a comparable modern data platform). Strong hands-on experience with Python and SQL in end-to-end data engineering workflows. Strong hands-on experience with Python and SQL in end-to-end data engineering workflows. Experience building and maintaining data pipelines and ETL workflows across disparate datasets. Experience building and maintaining data pipelines and ETL workflows across disparate datasets. Working knowledge of Azure DevOps, scripting (Azure CLI), Git/version control, and CI/CD processes. Working knowledge of Azure DevOps, scripting (Azure CLI), Git/version control, and CI/CD processes. Practical experience with Databricks. Practical experience with Databricks. Ability to communicate complex technical topics clearly and effectively to diverse audiences. Ability to communicate complex technical topics clearly and effectively to diverse audiences. A proactive, eager-to-learn attitude, especially regarding cybersecurity. A proactive, eager-to-learn attitude, especially regarding cybersecurity. Hands-on experience working within Agile teams. Hands-on experience working within Agile teams. B2B contract and support of the Contractor Care Team Private Medical Care Cafeteria system Life insurance Zapraszamy do odwiedzenia naszej strony www.antal.pl","[{""min"": 30000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Engineering,30000,33000,Net per month - B2B
Full-time,Mid,B2B,Remote,656,Senior BI Developer with Snowflake,Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! üòä Design, build, and develop data warehouses based on Snowflake; Create and optimize ETL/ELT processes using Matillion; Integrate data from various sources (databases, APIs, files); Develop reports and dashboards in Power BI for internal clients; Maintain, monitor, and further develop existing BI solutions; Collaborate with project teams and business stakeholders to gather and analyze requirements; Participate in data migration from legacy systems (e.g., Oracle, SSIS) to Snowflake; Ensure high-quality technical and business documentation; Implement best practices for data management, security, and version control; Actively participate in Agile team meetings (e.g., daily stand-ups, sprint planning, retrospectives); At least 4 years' experiencein a BI Developer role; Advanced and proven experience as a Snowflake developer,responsible for building ETL processes and creating tables and views within the Snowflake environment; Solid skills inPower BI; Eager to work as afull-stack BI Developer(both backend and frontend); StrongEnglish skills (min. C1 level, daily communication with international clients); Proactive and creative- skills to drive improvements and engage with both technical and non-technical stakeholders; Strong documentation skills and a knack for business analysis. Experience withMatillion; Experience withOracle(we‚Äôre migrating to Snowflake, but legacy knowledge is a plus); Experience withSSAS/SSIS/SSRS; Familiarity withVisual Studio; Understanding ofversion control concepts(branching, merging, pushing; Git integrated with Matillion). Background in procurement, supply chain, or business data analysis related to orders and internal corporate stakeholders; Background inManaged Servicesdelivery models - you know how to take end-to-end ownership of BI solutions, ensuring their reliability, scalability, and alignment with client needs throughout the entire lifecycle; Experience working inAgileteams. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad ‚Äì so far we've been in Cape Town, Are, and Barcelona). Fully remotework or in our office in Wroc≈Çaw; B2B Contract: 135 ‚Äì 150 PLN net/hour + VAT Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories.","[{""min"": 135, ""max"": 155, ""type"": ""Net per hour - B2B""}]",Data Engineering,135,155,Net per hour - B2B
Full-time,Senior,Permanent,Remote,657,Staff Data Engineer,Dropbox,"Dropbox is a special place where we are all seeking to fulfill our mission to design a more enlightened way of working. We‚Äôre looking for innovative talent to join us on our journey. The words shared by our founders at the start of Dropbox still ring true today. Wouldn‚Äôt it be great if our working environment‚Äîand the tools we use‚Äîwere designed with people‚Äôs actual needs in mind? Imagine if every minute at work were well spent‚Äîif we could focus and spend our time on the things that matter. This is possible, and Dropbox is connecting the dots. The nearly 3,000 Dropboxers around the world have helped make Dropbox a living workspace - the place where people come together and their ideas come to life. Our 700+ million global users have been some of our best salespeople, and they have helped us acquire customers with incredible efficiency. As a result, we reached a billion dollar revenue run rate faster than any software-as-a-service company in history. Dropbox is making the dream of a fulfilling and seamless work life a reality. We hope you‚Äôll join us on the journey. Dropbox is rebuilding the foundation of its monetization and financial data systems - and at the center of that is our revenue and growth data platform. We‚Äôre looking for aStaff Data Engineerto lead the design and delivery of this critical foundation, enabling insights and systems that drive ARR tracking, financial reporting, and product monetization. Architect the next-generation data platform for ARR, revenue attribution, and growth analytics - setting vision, driving alignment, and delivering at scale. Own and evolve core data models and systems used across Finance, Product, and Analytics teams, ensuring accuracy, trust, and accessibility. Lead platform modernization, including the adoption of scalable lakehouse architectures (Databricks, Spark, Delta), CI/CD for data, and observability frameworks. Drive adoption of scalable data practices across Dropbox through reusable tooling, process improvements, and cross-team collaboration. Partner with stakeholders (e.g., Finance, Product, Data Science, and Infrastructure) to understand data needs and deliver solutions that drive real business outcomes. Mentor and grow junior engineers, and cultivate a high-performing, innovation-driven team culture. BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience. 10+ years building large-scale data systems, with a demonstrated track record of technical leadership, including ownership of architectural direction and cross-team platform work. Proven ability to set architectural direction, lead platform evolution, and influence technical strategy across teams. Deep hands-on expertise with Spark, Spark SQL, and Databricks, along with experience orchestrating data pipelines using Apache Airflow, and writing performant, maintainable Python and SQL code. Track record of implementing data quality, testing, and observability systems at scale. Experience supporting monetization, financial, or product growth analytics with trusted and governed data models. Familiarity with cloud platforms (AWS, GCP, or Azure) and lakehouse paradigms (e.g., Delta Lake, Iceberg). Experience leading data migrations or platform transformations (e.g., from on-prem to cloud, Hadoop to Databricks). Familiarity with tools for data contracts, lineage, and governance (e.g., dbt, Monte Carlo, Great Expectations). Understanding of data privacy and compliance frameworks, including GDPR, SOX, and audit-readiness. Dropbox applies increased tax deductible costs to remuneration earned by certain qualifying employees (to the extent an employee will be involved in the creation of the software as an ‚Äúauthor‚Äù) for the transfer of copyrights, in accordance with the relevant provisions of the Personal Income Tax Act. Poland Annual Pay Range 249 900 z≈Ç‚Äî338 100 z≈Ç The range listed above is the expected annual base salary/OTE (On-Target Earnings) for this role, subject to change. Please note, OTE are for sales roles only. Salary/OTE is just one component of Dropbox‚Äôs total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs). Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to: Competitive medical, dental and vision coverage* Retirement savings through a defined contribution pension or savings plan** Flexible PTO/Paid Time Off policy in addition to statutory holidays, allowing you time to unplug, unwind, and refresh Income Protection Plans: Life and disability insurance* Business Travel Protection: Travel medical and accident insurance* Perks Allowance to be used on what matters most to you, whether that‚Äôs wellness, learning and development, food & groceries, and much more Parental benefits including: Parental Leave, Fertility Benefits, Adoptions and Surrogacy support, and Lactation support Mental health and wellness benefits Additional benefits details are available upon request. Where group plans are not available, allowances may be provided Benefit, amount, and type are dependent on geographical location, based upon applicable law or company policy Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. A big part of that effort is our support for members and allies of internal groups like Asians at Dropbox, BlackDropboxers, enABLE, TODOS (Latinx), Pridebox (LGBTQ), Vets at Dropbox, and Women at Dropbox.","[{""min"": 20825, ""max"": 28175, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20825,28175,Gross per month - Permanent
Full-time,Senior,Permanent,Hybrid,658,Sr Data Engineer - Product Supply Analytics,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Senior Data Engineer - Product Supply Analytics The PS Data & Analytics team at Bayer Consumer Health focuses on driving digital transformation and innovation by creating best-in-class analytical solutions that enable data-driven decision making and performance optimization for Bayer Consumer Health‚Äôs Supply Chain organization. You will be part of the data & analytics organization and will be responsible for building data products in the area of product supply and supply chain. You partner with business stakeholders, data architects, data scientists, analytics leads as well as other engineers. You will build data pipelines, data models and provision data for front end developers and data scientists. You will also make sure that proper development processes are followed within the team, enhance implementation frameworks and guide other team members in building scalable, secure and well performing data products. If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Key Tasks & Responsibilities: Integrate data from different sources (e.g. supply chain planning data, financial data, quality data, distributor and transportation data, market data (sell-in, sell-out)) to develop globally harmonized data models & KPIs. Continuously enhance implementation frameworks based on the needs of the analytics products in your responsibility. Ensure that product supply data products adhere to the analytics data architecture guidance and deliver fit-for purpose & scalable analytical solutions. Guide other data engineers in your team and ensure that all engineers apply same design principles. Ensure that data is well-managed to build stable, reusable and quality assured data assets. Collaborate with other IT functions (enabling functions data asset teams, analytics teams, platform product managers & integration architects) to ensure the aforementioned activities are executed effectively. Together with the assigned data architect, ensure that cost and time estimations are accurate, quality of delivery is assured, and deliverables are properly handed over to the operations team Qualifications & Competencies (education, skills, experience): Bachelor/Master‚Äôs degree in Computer Science, Engineering, or a related field. 5+ years of working experience in the field of Data & Analytics, preferably in the area of product supply and the CPG industry Excellent data engineering & technology knowledge (Azure Data Lake Gen2, Azure Synapse, Databricks, Snowflake, potentially also legacy stack SAP Hana, SQL, Python as well as data management knowhow (data cataloguing, data quality management) Knowledge of CI/CD processes and tools (GitHub, Azure DevOps Pipelines) Profound data content knowledge (Supply Chain, Logistics, Quality Management) and Product Supply process knowhow Experience in Agile methodologies (Scrum, Kanban) Strong problem solving and analytical skills, combined with impeccable business judgment. Excellent interpersonal and communication skills, active listening, consulting, challenging, presentation skills. Fluent in English, both written & spoken, intercultural awareness and willingness to travel What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360¬∞ Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (‚ÄúWczasy pod gruszƒÖ‚Äù) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn‚Äôt mean you aren‚Äôt the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,20000,28500,Gross per month - Permanent
Full-time,Senior,B2B,Remote,662,Senior Data Engineer (SSAS / BI),P&P Solutions,"üìç Warszawa lub Katowice / hybrydowo lub 100% zdalnie üí∞ 130‚Äì160 PLN/h netto B2B Do≈ÇƒÖcz do rozbudowanego HUB-u danych w sektorze bankowym , kt√≥ry wspiera kluczowe decyzje zarzƒÖdcze i tworzy nowoczesne rozwiƒÖzania BI dla jednej z najwiƒôkszych instytucji finansowych. Szukamy do≈õwiadczonego Senior Data Engineer‚Äôa , kt√≥ry specjalizuje siƒô w SSAS, OLAP i Power BI i wniesie realnƒÖ warto≈õƒá do istniejƒÖcego zespo≈Çu. O projekcie Zasilisz zesp√≥≈Ç Reporting Enablement Product , kt√≥ry odpowiada za rozw√≥j i utrzymanie data martu MI (Management Information) ‚Äì centralnego ≈∫r√≥d≈Ça danych dla zarzƒÖdu i kadry mened≈ºerskiej. To miƒôdzynarodowy, multidyscyplinarny zesp√≥≈Ç (10 os√≥b: data engineers, analitycy biznesowi, PO, Scrum Master), pracujƒÖcy w modelu Agile. Do≈ÇƒÖczysz jako specjalista odpowiedzialny za rozw√≥j modeli SSAS OLAP i tabular , optymalizacjƒô wydajno≈õci oraz wsparcie rozwiƒÖza≈Ñ raportowych i analitycznych. Wymagania (must have) Min. 7 lat do≈õwiadczenia w pracy z SSAS (OLAP, tabular) Bardzo dobra znajomo≈õƒá SQL, MDX i DAX Do≈õwiadczenie w data warehousing, modelowaniu danych, procesach ETL Znajomo≈õƒá narzƒôdzi wizualizacji danych: Power BI Umiejƒôtno≈õƒá optymalizacji wydajno≈õci i analizy danych Komunikatywno≈õƒá i umiejƒôtno≈õƒá pracy w zespole Angielski na poziomie min. B2 Praktyka w pracy w metodyce Agile/Scrum Tech stack w projekcie SSAS (SQL Server Analysis Services) SQL, MDX, DAX OLAP, tabular models, Power BI ETL, Data Warehousing, Performance tuning, Data security ObowiƒÖzki Projektowanie, wdra≈ºanie i zarzƒÖdzanie kostkami OLAP oraz modelami tabularnymi Optymalizacja zapyta≈Ñ i monitorowanie wydajno≈õci systemu Wsp√≥≈Çpraca z zespo≈Çami danych w celu projektowania rozwiƒÖza≈Ñ BI RozwiƒÖzywanie problem√≥w z modelami danych i SSAS Implementacja polityk bezpiecze≈Ñstwa i procedur backupu Bie≈ºƒÖce ≈õledzenie trend√≥w w technologiach SSAS i BI Dlaczego warto? ‚úî Stabilny, rozbudowany projekt o du≈ºym wp≈Çywie biznesowym ‚úî Realny rozw√≥j kompetencji w obszarze BI i danych korporacyjnych ‚úî Praca z do≈õwiadczonym zespo≈Çem i nowoczesnymi narzƒôdziami ‚úî Mo≈ºliwo≈õƒá pracy hybrydowej lub zdalnej (dla topowych kandydat√≥w) Oferujemy Stawka do 160 z≈Ç/h netto na b2b Przelew w dogodnej formie Kr√≥tki 14-dniowy termin p≈Çatno≈õci faktury Bogaty pakiet us≈Çug prywatnej opieki medycznej Dostƒôp do platformy kafeteryjnej MyBenefit (umo≈ºliwiajƒÖcej zamawianie kart Multisport, kart przedp≈Çaconych do Ikea, Zalando, Notino i wielu innych)","[{""min"": 130, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,130,160,Net per hour - B2B
Full-time,Mid,B2B,Remote,663,Data Engineer with Blockchain,DCG,"Responsibilities: Design, implement, and maintain scalable data pipelines using Azure Databricks, Spark, and PySpark Work with Delta Lake to manage large-scale data storage and optimize performance Develop robust data integration solutions using Azure Data Factory and Azure Functions Build and maintain structured and semi-structured data models, leveraging formats such as Parquet, Avro, and JSON Ensure efficient and secure data processing through proper performance tuning and code optimization Collaborate with development and analytics teams to support business data needs Apply version control best practices using Git and follow coding standards in Python and SQL Requirements: Strong hands-on experience with Azure Databricks, Spark, and PySpark Proficiency in building and tuning data pipelines with Delta Lake Solid understanding of data modeling and performance optimization techniques Practical experience with Azure Data Factory, Azure Functions, and Git Competence in working with data formats such as Parquet, Avro, and JSON Strong programming skills in Python and SQL Ability to work effectively in a fast-paced, enterprise-level environment Strong communication skills and fluency in spoken and written English (C1) Nice to have: Understanding of blockchain-related concepts and data structures Offer: Private medical care Co-financing for the sports card Training & learning opportunities Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,665,Senior Data Engineer,SIX,"Senior Data Engineer Warsaw | working from home up to 40% | Reference 7218 Are you passionate about the stock market and cutting-edge technology? At SIX, we operate one of the most advanced, innovative, and stable stock exchanges in the world ‚Äî and we‚Äôre looking for a Senior Data Engineer to join our Data Management team. If you have a strong background in Python, SQL, databases, and cloud-based analytics, this is your opportunity to design and deliver high-impact data products and pipelines that power critical financial services. design, develop, and maintain our cloud-based data platform (Azure) for the Swiss Stock Exchange, aligned with our architecture standards lead the migration of legacy data solutions to modern cloud-based infrastructure conceptualize and implement transformation projects in the DWH and Big Data ecosystems collaborate with and support team members in building data-centric solutions, including direct involvement in solution design work cross-functionally with developers, testers, product owners, and business stakeholders in an agile setup (SCRUM & SAFe) 4+ years of experience building data pipelines on public cloud platforms such as Microsoft Azure, or with Hadoop (e.g., Hortonworks, Cloudera) proficiency in relational databases and SQL/NoSQL datastores; hands-on experience with data modeling, Spark, and preferably Databricks familiarity with agile methodologies (SCRUM, Kanban) and collaboration tools like Jira and Confluence a proactive, open mindset with a strong drive to learn and adapt to new technologies; excellent teamwork and communication skills strong customer focus with the ability to work independently and collaborate across diverse teams and cultures; fluency in English (German is a plus) sharing the costs of sports activities private medical care & life insurance sharing the costs of foreign language classes sharing the costs of professional training & courses remote work opportunities &flexible working time integration events &charity initiatives fruits and popcorn in the office video games at work, no dress code & leisure zone extra social benefits & holiday funds (Christmas/Easter gifts) meal and transportation allowance employee referral program Employee Assistance Program Day for U (Day for Medical Checkup) My Benefit Cafeteria Udemy for Business days for remote work from abroad If you have any questions, please call Gabriela Swiatek at +48 22 104 67 70. For this vacancy we only accept direct applicationsin English. Diversity is important to us. Therefore, we are looking to receiving applications regardless of any personal background.","[{""min"": 19000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,19000,27000,Gross per month - Permanent
Full-time,Mid,B2B,Remote,666,Data Engineer (pharma),7N,"Work Mode: Remote from the territory of Poland Expectations 3+ years of experience indata engineering,1+ year of experience inPythonwith data frameworks likePandas Proficiency inSQL,Python, andPandasfor data manipulation Experience indbtfor transformations Strong understanding ofdata modelling,ETL processes, anddata warehousing concepts Ability to set up and manage automated data pipelines usingGitLab CI/CD Experience withSnowflake's architecture, including warehouses, schemas, and security Ability to write efficientPythonscripts for data processing, leveragingPandasfor data wrangling and analysis Experience documenting data workflows Ability to collaborate effectively in cross-functional, international teams Sc., B.Eng. (or higher) in Computer Science, Data Engineering or related fields Excellentverbal and written communication skills (in English and Polish) Ongoing support from adedicatedagent, taking care of your project continuity, client contact, necessary formalities, work comfort and development, Consultant Development Program‚Äì advice on growth planning based on the latest trends and market needs in IT, including consultations withagents and growth mentors, Access to7N Learning & Development‚Äì a development and educational platform with webinars, a library of articles and industry reports, and regular invitations to one-time and recurring development events ‚Äì technical, business, and lifestyle, Spectacular integration events, both for you (e.g.,annual Kick-Off trip, Christmas parties, or Summer Olympics sports events) and for your loved ones (e.g., family picnics, movie premieres), Professional developmentnot only during the project ‚Äì you can get involved in knowledge transfer to others within the7N Servicesoffering directed at 7N clients, Relationships and access to the knowledge ofthe most experienced IT expertsin the market ‚Äì the average professional tenure of our consultants in Poland is over 10 years, A complete benefits package, including funding for medical care, life insurance, sports cards for you and your loved ones, as well as discounts in stores in Poland and abroad.","[{""min"": 130, ""max"": 145, ""type"": ""Net per hour - B2B""}]",Data Engineering,130,145,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,667,IPC Developer,ITDS,"Drive Innovation in Data Warehousing: IPC Developer wanted! ≈Å√≥d≈∫ based opportunity with remote work model (2 days in the office/month). As an IPC Developer , you will be working for our client, a leading player in the online banking sector, on the development and optimization of data warehouse solutions. This includes designing and implementing ETL processes, enhancing data architecture, and supporting the business intelligence environment to ensure effective data reporting and analysis. The project involves using cutting-edge technologies like Informatica Power Center and Oracle-based systems to support complex financial systems. You‚Äôll be part of a dynamic IT team that ensures data integrity, performance, and scalability of large-scale data systems. Your main responsibilities: Design and implement ETL processes using Informatica Power Center Develop and maintain data warehouses and reporting data marts Participate in the implementation and integration of Informatica tools Design, implement and develop BI-class analytical environments Create and maintain Oracle databases and applications Define and document technical specifications and administrative documentation Test and validate software solutions created by others Prepare software installation packages Support the software release and handover to production teams You're ideal for this role if you have: Strong knowledge of Informatica Power Center for ETL process development Solid understanding of RDBMS Oracle 9i/10g and database design Excellent command of SQL and good knowledge of PL/SQL Understanding of information systems engineering and development methodologies At least 6 months of experience designing and implementing ETL solutions At least 1 year of experience with Oracle-based systems in information environments Practical experience designing data warehouses for large-scale institutions Ability to read and write technical documentation in English Strong analytical thinking and problem-solving skills Team-oriented mindset with attention to quality and detail Nice to have: Theoretical knowledge and practical experience with Big Data, Python, and Spark Familiarity with Business Intelligence (BI) concepts Ability to work using Agile methodologies (Scrum, Kanban) Knowledge of tools such as SQL Developer, SVN, GitHub, JIRA, and Confluence We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS‚Äôs Whistleblower Procedure available here . Ref. number 7154","[{""min"": 16800, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Engineering,16800,22000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,668,Ekspert ds. Hurtowni Danych,ITDS,"Do≈ÇƒÖcz do nas i zarzƒÖdzaj danymi, kt√≥re napƒôdzajƒÖ biznes! Lokalizacja: Gda≈Ñsk z mo≈ºliwo≈õciƒÖ pracy hybrydowej (8 dni/miesiƒÖc w biurze) JakoStarszy Ekspert ds. Hurtowni Danych, bƒôdziesz pracowaƒá dla naszego klienta, du≈ºej instytucji finansowej ≈õwiadczƒÖcej us≈Çugi bankowe na rynku krajowym, zorientowanej na rozw√≥j nowoczesnych rozwiƒÖza≈Ñ raportowo-analitycznych. Do≈ÇƒÖczysz do zespo≈Çu odpowiedzialnego za utrzymanie i rozw√≥j hurtowni danych oraz system√≥w wspierajƒÖcych procesy raportowania wewnƒôtrznego i zewnƒôtrznego. Twoja praca bƒôdzie mieƒá bezpo≈õredni wp≈Çyw na jako≈õƒá danych, efektywno≈õƒá proces√≥w analitycznych oraz zgodno≈õƒá z wymaganiami regulator√≥w. Twoje g≈Ç√≥wne obowiƒÖzki: Analizowanie wymaga≈Ñ biznesowych dotyczƒÖcych system√≥w raportowo-analitycznych Opracowywanie specyfikacji biznesowych i technicznych na potrzeby rozwoju hurtowni danych Tworzenie i optymalizowanie proces√≥w ETL w istniejƒÖcych rozwiƒÖzaniach Przygotowywanie i wdra≈ºanie raport√≥w przy u≈ºyciu narzƒôdzi BI, w tym MS Power BI Przeprowadzanie test√≥w funkcjonalnych i integracyjnych wdra≈ºanych rozwiƒÖza≈Ñ Utrzymywanie serwis√≥w zgodnie z wymaganiami SLA RozwiƒÖzywanie incydent√≥w i zg≈Çosze≈Ñ u≈ºytkownik√≥w ko≈Ñcowych Wsp√≥≈Çpracowanie z zespo≈Çami audytu oraz przedstawicielami instytucji nadzorczych Idealnie pasujesz do tej roli, je≈õli: Masz co najmniej 4 lata do≈õwiadczenia w obszarze system√≥w raportowo-analitycznych Znasz systemy BI, w szczeg√≥lno≈õci MS Power BI Znasz bazy danych Oracle i potrafisz efektywnie korzystaƒá z ich zasob√≥w Potrafisz pisaƒá i optymalizowaƒá zapytania SQL Swobodnie pos≈Çugujesz siƒô pakietem MS Office My≈õlisz analitycznie i cechujesz siƒô du≈ºƒÖ dok≈Çadno≈õciƒÖ Potrafisz dzia≈Çaƒá pod presjƒÖ czasu i dotrzymywaƒá termin√≥w Posiadasz dobrƒÖ organizacjƒô pracy w≈Çasnej Znasz jƒôzyk angielski na poziomie co najmniej B2 Dodatkowym atutem bƒôdzie, je≈õli masz: Do≈õwiadczenie w pracy z narzƒôdziami ETL, w szczeg√≥lno≈õci Informatica Do≈õwiadczenie w prowadzeniu projekt√≥w lub koordynowaniu zada≈Ñ zespo≈Çu Znajomo≈õƒá proces√≥w raportowania regulacyjnego w instytucjach finansowych Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z hurtowniami danych w sektorze bankowym Do≈õwiadczenie we wsp√≥≈Çpracy z audytorami wewnƒôtrznymi i zewnƒôtrznymi","[{""min"": 900, ""max"": 1100, ""type"": ""Net per day - B2B""}]",Data Engineering,900,1100,Net per day - B2B
Full-time,Mid,Permanent or B2B,Hybrid,9,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for aData Engineerto join us to provide value to our customers in line with the Volue mission. In Volue Insight we enable our customers to make data driven decisions ‚Äì spanning from creating pricing models for their energy products, when to buy energy, to invest in renewable power plants or power-consuming industry. In the fuels team, we provide actuals and forecasts for prices, production levels, flows for the gas markets and conventional power plant operators. We build and maintain data pipelines, collect and process data from external sources, craft mathematical models, analyze time series, train machine learning models, build web applications, and enjoy working together. What you will be doing to make a difference: Design, build and maintain flexible and scalable end-to-end data pipelines for forecasting and prediction models. Contribute to our continuous push towards highly scalable and automated data pipelines and forecasting models where performance is monitored, quality is continuously evaluated, and experimentation is easy. Take part in the entire lifecycle of our models, from initial concept to deployment and ongoing maintenance, ensuring reliability and performance. Implement automated tests, participate in peer code reviews, and embrace continuous integration practices to ensure robust, maintainable code. What you need to succeed: A Bachelor‚Äôs or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Reasons to join Volue team and what we offer: Large degree of influence in shaping and developing the role further Great colleagues in one of Europe‚Äôs most exciting green tech companies with innovative and international work environment Flexible working hours and competitive compensation package, which includes a Multisport card, group life insurance, private healthcare, English classes, memorable offsite events, outstanding referral programme and access to various sports groups. In Volue, we cherish each employee‚Äôs competence, ideas and personality. Let your skills and talent be a part of our team ‚Äì and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,12000,20000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,46,Data Privacy and Compliance Specialist,OChK,"Data Privacy and Compliance Specialist Miejsce pracy: Warszawa / hybrydowo Poziom stanowiska: Intermediate 10.000 - 13.000 brutto UoP lub umowa B2B Tw√≥j zakres obowiƒÖzk√≥w: udzia≈Ç w negocjacjach i opiniowaniu um√≥w z klientami oraz dostawcami, w zakresie zapis√≥w dotyczƒÖcych ochrony danych i bezpiecze≈Ñstwa informacji, wsparcie w ocenie i monitorowaniu bezpiecze≈Ñstwa ≈Ça≈Ñcucha dostaw, ze szczeg√≥lnym uwzglƒôdnieniem aspekt√≥w zwiƒÖzanych z przetwarzaniem danych osobowych, udzia≈Ç w kontrolach zgodno≈õci, audytach wewnƒôtrznych i zewnƒôtrznych oraz wdra≈ºanie dzia≈Ça≈Ñ korygujƒÖcych i zapobiegawczych, przygotowywanie, aktualizacja i rozw√≥j dokumentacji z zakresu bezpiecze≈Ñstwa informacji (m.in. polityki, procedury, klauzule, rejestry), wsparcie zespo≈Ç√≥w biznesowych w analizie ryzyka oraz ocenie i zapewnianiu zgodno≈õci planowanych dzia≈Ça≈Ñ z obowiƒÖzujƒÖcymi regulacjami, weryfikacja zgodno≈õci przetwarzania danych osobowych w procesach biznesowych i systemach IT, prowadzenie szkole≈Ñ i dzia≈Ça≈Ñ edukacyjnych dla pracownik√≥w w obszarze bezpiecze≈Ñstwa informacji, w tym ochrony danych, monitorowanie zmian w przepisach prawa i regulacjach oraz inicjowanie i wdra≈ºanie niezbƒôdnych dzia≈Ça≈Ñ dostosowawczych. Nasze wymagania: wykszta≈Çcenie wy≈ºsze (preferowane: prawnicze, cyberbezpiecze≈Ñstwo), doskona≈Ça znajomo≈õƒá przepis√≥w oraz norm i standard√≥w dotyczƒÖcych bezpiecze≈Ñstwa informacji oraz ochrony danych osobowych, z uwzglƒôdnieniem zagadnie≈Ñ dotyczƒÖcych chmury obliczeniowej oraz sztucznej inteligencji, znajomo≈õƒá jƒôzyka angielskiego na poziomie minimum B2, co najmniej 3-letnie do≈õwiadczenie na stanowisku zwiƒÖzanym z bezpiecze≈Ñstwem informacji/ochronƒÖ danych osobowych, certyfikaty po≈õwiadczajƒÖce wiedzƒô z zakresu bezpiecze≈Ñstwa informacji oraz ochrony danych osobowych ‚Äì mile widziane. W OChK: pracujemy zadaniowo w trybie hybrydowym (nowoczesne biuro przy ul. Grzybowskiej), dzia≈Çamy w zwinnym ≈õrodowisku, z wykorzystaniem aplikacji zwiƒôkszajƒÖcych efektywno≈õƒá (m. in. Google Workspace, Slack, GitHub, Jira), inwestujemy w Tw√≥j rozw√≥j poprzez finansowanie szkole≈Ñ i cert√≥w, a od pierwszego dnia pracy udostƒôpniamy platformy edukacyjne Google i Microsoft, oferujemy prywatne ubezpieczenie medyczne, preferencyjne warunki ubezpieczenia grupowego oraz kartƒô Multisport organizujemy i wsp√≥≈Çfinansujemy naukƒô jƒôzyka angielskiego, udostƒôpniamy program polece≈Ñ, dziƒôki kt√≥remu pracownicy zyskujƒÖ dodatkowe bonusy za skutecznƒÖ rekomendacjƒô kandydat√≥w do pracy, cenimy proaktywno≈õƒá i inicjatywƒô w≈ÇasnƒÖ, dlatego wspieramy autonomiƒô w podejmowaniu decyzji, budujemy kulturƒô organizacyjnƒÖ na warto≈õciach takich jak profesjonalizm, wsp√≥≈Çodpowiedzialno≈õƒá i wzajemny szacunek, przyk≈Çadamy du≈ºƒÖ wagƒô do efektywnego onboardingu, podczas kt√≥rego w lu≈∫nej atmosferze i ze wsparciem Twojego CloudBuddiego poznajesz zesp√≥≈Ç, firmƒô i swoje obowiƒÖzki, stawiamy na integracjƒô zespo≈Ç√≥w podczas r√≥≈ºnorodnych inicjatyw, zar√≥wno firmowych jak i oddolnych, kt√≥re pomagajƒÖ nam lepiej siƒô poznawaƒá oraz budowaƒá i utrzymywaƒá dobrƒÖ atmosferƒô wsp√≥≈Çpracy.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 13000, ""type"": ""Gross per month - Permanent""}]",Unclassified,10000,13000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,70,Senior Data Engineer,N-iX,"We are seeking a Senior Data Engineer specializing in Databricks to join our global team. You will be instrumental in setting up and maintaining our Databricks platform, building robust data pipelines, and collaborating closely with our solution architects and data scientists. Your expertise will directly support our mission to leverage data and AI effectively within a cutting-edge automotive claims management environment. Key Responsibilities: Design, build, and maintain robust data pipelines within Databricks. Collaborate closely with international teams, including data scientists and architects, to develop scalable data solutions. Debug complex issues in data pipelines and proactively enhance system performance and reliability. Set up Databricks environments on cloud platforms (Azure/AWS). Automate processes using CI/CD practices and infrastructure tools such as Terraform. Create and maintain detailed documentation, including workflows and operational checklists. Develop integration and unit tests to ensure data quality and reliability. Migrate legacy data systems to Databricks, ensuring minimal disruption. Participate actively in defining data governance and management strategies. What We Expect from You (Requirements): 5+ years of proven experience as a Data Engineer. Advanced proficiency in Python for developing production-grade data pipelines. Extensive hands-on experience with Databricks platform. Strong knowledge of Apache Spark for big data processing. Familiarity with cloud environments, specifically Azure or AWS. Proficiency with SQL and experience managing relational databases (MS SQL preferred). Practical experience with Airflow or similar data orchestration tools. Strong understanding of CI/CD pipelines and experience with tools like GitLab. Solid skills in debugging complex data pipeline issues. Proficiency in structured documentation practices. B2 level or higher proficiency in English. Strong collaboration skills, ability to adapt, and eagerness to learn in an international team environment. Nice to have: Experience with Docker and Kubernetes. Familiarity with Elasticsearch or other vector databases. Understanding of DBT (data build tool). Ability to travel abroad twice a year for on-site workshops. Why Join Us Work on impactful projects with cross-functional teams. Opportunity to grow your BI and analytics career in a data-driven organization. Flexible working hours and remote work options. Competitive compensation and benefits. Opportunity to work on presales We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 29553, ""max"": 30661, ""type"": ""Net per month - B2B""}, {""min"": 24381, ""max"": 25489, ""type"": ""Gross per month - Permanent""}]",Data Engineering,24381,25489,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,93,Senior Data Engineer,N-iX,"#3204 Join our team to work on enhancing a robust data pipeline that powers our SaaS product, ensuring seamless contextualization, validation, and ingestion of customer data. Collabd engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Client was established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client‚Äôs platform rates with product teams to unlock new user experiences by leveraging data insights. Engage with domain experts to analyze real-world empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renowned Scandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities : Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements : Programming: Minimum of 3-4 years as a data engineer, or in a relevant field Python Proficiency: Advanced experience in Python, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights Cloud: Familiarity with cloud platforms (preferably Azure) Data Platforms: Experience with Databricks, Snowflake, or similar data platforms Database Skills: Knowledge of relational databases, with proficiency in SQL. Big Data: Experience using Apache Spark Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability Version Control: Experience with Gitlab or equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have : Experience with Docker and Kubernetes Experience with document and graph databases Ability to travel abroad twice a year for an on-site workshops","[{""min"": 18359, ""max"": 30993, ""type"": ""Net per month - B2B""}, {""min"": 14776, ""max"": 26228, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14776,26228,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Hybrid,117,Data Scientist,Link Group,"Poszukujemy do≈õwiadczonego Data Scientista do zespo≈Çu in≈ºynieryjnego miƒôdzynarodowej firmy technologicznej specjalizujƒÖcej siƒô w rozwiƒÖzaniach automatyzujƒÖcych procesy finansowe i ksiƒôgowe. Firma od ponad 20 lat rozwija w≈ÇasnƒÖ platformƒô SaaS wykorzystywanƒÖ globalnie przez dzia≈Çy finansowe du≈ºych organizacji. Projekt skupia siƒô na budowaniu nowoczesnych, skalowalnych rozwiƒÖza≈Ñ AI/ML wspierajƒÖcych procesy ksiƒôgowe i finansowe, z wykorzystaniem Python, TensorFlow lub PyTorch, Google Cloud Platform oraz MLOps. Na co dzie≈Ñ Data Scientist w tym zespole odpowiada za projektowanie, trenowanie i wdra≈ºanie modeli uczenia maszynowego, budowanie pipelines danych, integracjƒô mikroserwis√≥w w ≈õrodowisku chmurowym oraz rozw√≥j us≈Çug AI w bliskiej wsp√≥≈Çpracy z product ownerami, innymi in≈ºynierami i zespo≈Çami cloudowymi. Wa≈ºny jest tu nie tylko mocny background techniczny, ale tak≈ºe umiejƒôtno≈õƒá szukania rozwiƒÖza≈Ñ w otwartych, nieoczywistych problemach i dzielenia siƒô wiedzƒÖ wewnƒÖtrz zespo≈Çu. Wymagania: Bardzo dobra znajomo≈õƒá Python, SQL, Spark Do≈õwiadczenie w budowie i wdra≈ºaniu modeli ML (klasyfikacja, klasteryzacja, prognozowanie) Znajomo≈õƒá TensorFlow i/lub PyTorch Praktyka w pracy z GCP lub innƒÖ du≈ºƒÖ chmurƒÖ (AWS, Azure) Do≈õwiadczenie z MLOps, pipelines, kontrolƒÖ wersji (Git) Wykszta≈Çcenie wy≈ºsze kierunkowe (Informatyka, Statystyka, Data Science) Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego ‚Äì komunikacja wewnƒôtrzna w firmie odbywa siƒô w tym jƒôzyku Mile widziane: Do≈õwiadczenie z GCP (BigQuery, Vertex AI) Znajomo≈õƒá specyfiki us≈Çug finansowych lub rozwiƒÖza≈Ñ dla ksiƒôgowo≈õci Wiedza z zakresu generative AI vs AI agents Informacje organizacyjne: üìç Lokalizacja: Krak√≥w ‚Äì model hybrydowy (2 dni w tygodniu z biura) üí¨ Wymagana bardzo dobra znajomo≈õƒá jƒôzyka angielskiego üìë Forma wsp√≥≈Çpracy: Umowa o pracƒô na czas nieokre≈õlony lub B2B (w zale≈ºno≈õci od preferencji) üóì Start: mo≈ºliwie jak najszybciej üíº Proces rekrutacyjny: rozmowy techniczne oraz spotkanie z przedstawicielem firmy","[{""min"": 21000, ""max"": 25000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Science,21000,25000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,124,Senior Data Engineer (Azure/Fabric),Onwelo,"üü† Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, a tak≈ºe otworzy≈Ça biura w siedmiu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. üöÄ O projekcie: Do naszego zespo≈Çu Data & Analytics poszukujemy do≈õwiadczonego Azure/Fabric Data Engineera, kt√≥ry bƒôdzie wspiera≈Ç naszych klient√≥w w planowaniu, budowie i wdra≈ºaniu nowoczesnych rozwiƒÖza≈Ñ danych w ≈õrodowisku Microsoft Azure i Fabric. Bƒôdziesz pracowaƒá w zespole z ekspertami od analizy danych, chmury i architektury, w ≈õrodowisku miƒôdzynarodowym i projektach o du≈ºej skali. . üéØ Z nami bƒôdziesz: Projektowaƒá i wdra≈ºaƒá rozwiƒÖzania oparte na Microsoft Fabric ‚Äì nowoczesnej platformie analitycznej ≈ÇƒÖczƒÖcej dane, raportowanie i orkiestracjƒô w jednym ≈õrodowisku Planowaƒá i przeprowadzaƒá orkiestracjƒô danych w ≈õrodowisku Microsoft Azure oraz Fabric Budowaƒá, rozwijaƒá i wdra≈ºaƒá nowoczesnƒÖ hurtowniƒô danych w oparciu o Databricks, Data Vault 2.0, Python i PySpark Tworzyƒá i optymalizowaƒá potoki danych oraz procesy ETL/ELT zasilajƒÖce hurtownie danych Projektowaƒá modele danych wspierajƒÖce analitykƒô biznesowƒÖ i raportowanie w Power BI Wdra≈ºaƒá rozwiƒÖzania z wykorzystaniem Microsoft Fabric, Azure Data Factory, Synapse, Data Lake, Azure SQL Przeprowadzaƒá analizƒô danych i projektowaƒá modele danych wspierajƒÖce cele biznesowe Wspieraƒá innych cz≈Çonk√≥w zespo≈Çu ‚Äì technicznie i merytorycznie Monitorowaƒá jako≈õƒá i efektywno≈õƒá przep≈Çyw√≥w danych oraz optymalizowaƒá je pod kƒÖtem koszt√≥w i wydajno≈õci üòé Czekamy na Ciebie, je≈õli: Masz minimum 5-letnie do≈õwiadczenie jako Data Engineer ‚Äì w projektach zwiƒÖzanych z integracjƒÖ danych, modelowaniem i budowƒÖ hurtowni Masz praktyczne do≈õwiadczenie z Microsoft Fabric lub chcesz rozwijaƒá siƒô w tym obszarze i szybko siƒô uczysz Pracujesz z us≈Çugami chmurowymi Azure, w tym: Azure Data Factory, Azure Databricks, Azure SQL, Data Lake Znasz SQL na poziomie eksperckim Biegle pos≈Çugujesz siƒô jƒôzykami Python i/lub PySpark Rozumiesz architekturƒô nowoczesnych hurtowni danych (np. Data Vault 2.0 ) Masz wy≈ºsze wykszta≈Çcenie techniczne (np. informatyka, matematyka, in≈ºynieria danych) Komunikujesz siƒô po angielsku na poziomie min. B2 (czƒô≈õƒá projekt√≥w i zespo≈Ç√≥w jest miƒôdzynarodowa) ü§ù Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15000,18000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,136,Backend Engineer (Data & AI),Appliscale,"About the role Our client is an early-stage, venture-backed startup transforming the $800B franchising industry. Their innovative AI platform helps leading franchise brands automate operations, leverage data insights, and scale faster and smarter. We're hiring a product-focused Backend Software Engineer eager to build impactful solutions, work closely with AI technology, and thrive in a fast-paced startup environment. This is an opportunity to contribute directly to product development, seeing your code and ideas quickly in users' hands. Responsibilities Please note, availability to attend daily afternoon/evening meetings is a specific requirement for this role as most of the team is located in the US Develop scalable backend systems, data pipelines, and APIs using Python, TypeScript, and AWS infrastructure Collaborate cross-functionally with product, ML, and infrastructure teams to integrate and deploy AI features in a multi-tenant SaaS environment Required qualifications Minimum of 2 years full-time commercial backend software development experience, ideally with Python and TypeScript Bachelor's or higher degree in Computer Science, Software Engineering, or a related field Comfortable building and managing services in AWS environments (EC2, Lambda, ECS, Airflow) Experienced using AutoML frameworks, time-series DBs Product-minded engineer who enjoys collaborating closely with product and business teams Startup-oriented: thrives in ambiguity, eager to learn quickly, iterate fast, and build impactful solutions Excellent communication skills and high fluency in English, it‚Äôs our daily business language Nice to have AI-curious: experience deploying ML models into production is a strong plus but not required","[{""min"": 14000, ""max"": 20000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 16000, ""type"": ""Gross per month - Permanent""}]",Data Science,10000,16000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,138,Senior Data Science/AI Engineer,N-iX,"Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management.Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact.Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company‚Äôs R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 21700, ""max"": 29000, ""type"": ""Net per month - B2B""}, {""min"": 17500, ""max"": 23950, ""type"": ""Gross per month - Permanent""}]",Data Science,17500,23950,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,144,Senior Data Engineer with Databricks,Crestt,"Cze≈õƒá! Poszukujemy osoby na stanowisko Senior Data Engineer , kt√≥ra do≈ÇƒÖczy do zespo≈Çu naszego Klienta zajmujƒÖcego siƒô projektowaniem i rozwijaniem nowoczesnych rozwiƒÖza≈Ñ w obszarze Data Lakehouse, Business Intelligence oraz zaawansowanej analityki w chmurze. Wsp√≥≈ÇpracujƒÖc w miƒôdzynarodowym ≈õrodowisku, bƒôdziesz mia≈Ç(a) okazjƒô pracowaƒá z najnowszymi technologiami w tej dziedzinie. Lokalizacja : praca g≈Ç√≥wnie zdalna (1x w miesiƒÖcu spotkanie w biurze w Warszawie) Wide≈Çki: B2B 160-200 pln netto+vat/h UoP 26-30 tys. brutto/mies. Wymagania: Bieg≈Ço≈õƒá w SQL oraz Pythonie (min. 5 lat do≈õwiadczenia) Co najmniej 2-letnie do≈õwiadczenie w pracy z Databricks Do≈õwiadczenie w pracy w ≈õrodowisku chmurowym (preferowany Azure) Minimum 5-letnie do≈õwiadczenie w projektowaniu oraz implementacji rozwiƒÖza≈Ñ klasy BI, ETL/ELT, Data Warehouse, Data Lake, Big Data oraz OLAP Praktyczna znajomo≈õƒá zar√≥wno relacyjnych, jak i nierelacyjnych Do≈õwiadczenie z narzƒôdziami typu Apache Airflow, dbt, Apache Kafka, Flink, Azure Data Factory, Hadoop/CDP Znajomo≈õƒá zagadnie≈Ñ zwiƒÖzanych z zarzƒÖdzaniem danymi, jako≈õciƒÖ danych oraz przetwarzaniem wsadowym i strumieniowym Umiejƒôtno≈õƒá stosowania wzorc√≥w architektonicznych w obszarze danych (Data Mesh, Data Vault, Modelowanie wymiarowe, Medallion Architecture, Lambda/Kappa Architectures) Praktyczna znajomo≈õƒá system√≥w kontroli wersji (Bitbucket, GitHub, GitLab) Wysoko rozwiniƒôte umiejƒôtno≈õci komunikacyjne, otwarto≈õƒá na bezpo≈õredni kontakt z Klientem ko≈Ñcowym Certyfikaty z Databricks lub Azure bƒôdƒÖ dodatkowym atutem Zakres obowiƒÖzk√≥w: Projektowanie i wdra≈ºanie nowych rozwiƒÖza≈Ñ oraz wprowadzanie usprawnie≈Ñ w istniejƒÖcych platformach danych Udzia≈Ç w rozwoju platform danych i proces√≥w ETL/ELT, optymalizacja przetwarzania du≈ºych zbior√≥w danych zgodnie z najlepszymi praktykami in≈ºynierii danych Standaryzacja i usprawnianie proces√≥w technicznych ‚Äì implementacja standard√≥w kodowania, testowania i zarzƒÖdzania dokumentacjƒÖ Dbanie o jako≈õƒá kodu i zgodno≈õƒá z przyjƒôtymi standardami ‚Äì przeprowadzanie regularnych code review Aktywna wsp√≥≈Çpraca z innymi ekspertami technologicznymi, w celu doskonalenia proces√≥w oraz identyfikacji nowych wyzwa≈Ñ technologicznych Mentoring i wsparcie zespo≈Çu w zakresie projektowania rozwiƒÖza≈Ñ, optymalizacji proces√≥w i wdra≈ºania najlepszych praktyk Klient oferuje: Udzia≈Ç w miƒôdzynarodowych projektach opartych na najnowocze≈õniejszych technologiach chmurowych Pokrycie koszt√≥w certyfikacji (Microsoft, AWS, Databricks) 60 p≈Çatnych godzin rocznie na naukƒô i rozw√≥j Mo≈ºliwo≈õƒá wyboru miƒôdzy pracƒÖ zdalnƒÖ a spotkaniami w biurze Indywidualnie dopasowane benefity: prywatna opieka medyczna, dofinansowanie karty sportowej, kursy jƒôzykowe, premie roczne i medialne oraz bonus za polecenie nowego pracownika (do 15 000 PLN)","[{""min"": 25600, ""max"": 32000, ""type"": ""Net per month - B2B""}, {""min"": 26000, ""max"": 30000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,26000,30000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,165,üëâ Data Platform Architect,Xebia sp. z o.o.,"üü£You will be: üü£Your profile: extensive experience working with Azure cloud provider, üü£Recruitment Process: CVreview ‚ÄìHRcall ‚ÄìTechnical Interview‚ÄìClientInterview (with Live-coding) ‚ÄìHiring ManagerInterview ‚ÄìDecision üéÅBenefits üéÅ ‚úçDevelopment: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏èWe are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 29800, ""max"": 36800, ""type"": ""Net per month - B2B""}, {""min"": 23900, ""max"": 29700, ""type"": ""Gross per month - Permanent""}]",Data Architecture,23900,29700,Gross per month - Permanent
Full-time,Manager / C-level,Permanent or B2B,Remote,179,Data Engineering Team Leader,Profitroom,"We are currently looking for an experiencedData Engineering Team Leaderto join our Data Team and help us make our company even more data-driven. The results of your work will directly impact product development, the way we support our customers, and influence our high-level business strategy. If you are ready to take initiative and believe in data-driven decision-making ‚Äì this role is perfect for you! Serve as a technical and team leader in the Data Engineering team: Lead team development and foster soft people management Support career paths and mentor team members Act as a Delivery Manager for data-related projects: Define, prioritize, and ensure timely delivery of engineering tasks Take ownership of major technical decisions and engineering excellence: Establish and enforce standards for testing, code review, CI/CD, documentation, and monitoring Oversee the architecture of our data platform: Maintain and evolve our Lakehouse infrastructure (preferably Databricks-based) Introduce new tools and technologies aligned with business and technical goals Supervise the logical structure and modeling of data: Ensure semantic and architectural consistency of datasets Leverage approaches such as the Kimball model or Medallion architecture Contribute to coding: Develop and optimize data pipelines using Dagster, Python, and PySpark Collaborate cross-functionally with data analysts, PMs, and other business stakeholders Drive and mature data governance practices, including cataloguing and lineage Minimum of 3+ years of experience as a Data Engineer or in a similar role related to data Experience in leading technical teams or managing data projects = at least 1 year (a big advantage) or willingness to grow into it Strong knowledge of Python, PySpark (nice to have), orchestration tools like Dagster or Airflow and SQL Experience working with cloud data platforms (Databricks experience is a plus) and Lake/Lakehouse architectures Ability to define and uphold high engineering standards and processes Proficiency in data modeling (Kimball, Medallion) Excellent communication skills (English at B2+ level) Strong ownership and self-organization Nice to have: Experience with Databricks and Delta Lake Familiarity with tools such as DataHub, Terraform, Docker Background in implementing data governance, lineage, and quality frameworks Python, PySpark, SQL, Databricks, GCP (BigQuery), Dagster, Airflow, Delta Lake, Docker, Terraform, DataHub Enjoy Work-Life Balance: Embrace a fully remote and flexible work environment. Explore the World: Avail annual 'Work with Us, Travel with Us' vouchers. Grow Your Skills: Access to English language classes along with a dedicated team development fund. Stay Healthy: Benefit from co-financed life and medical insurance, access sports facilities and receive professional mental health support whenever needed. Take Time Off: Get 26 days off with a Contract of Employment and 24 days off break with B2B contracts. Share hospitality: Take 2 extra days off (annually) for CSR activities. Join Celebrations: Participate in company retreats, events, and wedding & baby packs, benefit from our employee referral program. Transparent Culture: Experience a flat hierarchy and open communication channels for transparency. Contract Enhancements: earn between 25 500 PLN to 30 000 PLN on a B2B contract or between 21 000 to 25 000 PLN gross for Contract of Employment. About Us: We're a global leader in hospitality software, founded in Pozna≈Ñ, Poland in 2008. We‚Äôve grown to serve over 3,500 customers across five continents, helping hotels and resorts maximize their revenue and guest satisfaction.","[{""min"": 25500, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,21000,25000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,194,üëâ Senior AWS Data Engineer (Future Opening),Xebia sp. z o.o.,"üü£ You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. üü£ Your profile: 3+ years‚Äô experience with AWS (Glue, Lambda, Redshift, RDS, S3), 5+ years‚Äô experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools ‚Äì Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, working knowledge of Git, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ üü£ Nice to have: experience with Amazon EMR and Apache Hadoop, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification, üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16600,25900,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,259,Senior Data Engineer (Microsoft),Onwelo,"üü† Poznaj Onwelo: Onwelo to nowoczesna polska sp√≥≈Çka technologiczna, kt√≥ra specjalizuje siƒô w budowaniu innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z szeregu sektor√≥w na ca≈Çym ≈õwiecie. G≈Ç√≥wne obszary dzia≈Çalno≈õci Onwelo to: tworzenie oprogramowania, jego rozw√≥j oraz utrzymanie, a tak≈ºe mocne wsparcie kompetencyjne. W kr√≥tkim czasie firma wdro≈ºy≈Ça ponad 300 projekt√≥w w Europie i w USA, a tak≈ºe otworzy≈Ça biura w siedmiu miastach Polski oraz oddzia≈Çy w Stanach Zjednoczonych, Niemczech i w Szwajcarii. üöÄ O projekcie: Szukamy do≈õwiadczonego Microsoft Data Engineera , kt√≥ry poprowadzi techniczne aspekty projekt√≥w i bƒôdzie kluczowym ≈ÇƒÖcznikiem miƒôdzy IT a biznesem. PracujƒÖc z nami, bƒôdziesz mieƒá realny wp≈Çyw na spos√≥b, w jaki przetwarzane sƒÖ dane kluczowe dla biznesu i klient√≥w. Je≈õli masz do≈õwiadczenie w hurtowniach danych i lubisz wyzwania zwiƒÖzane z presales oraz definiowaniem wymaga≈Ñ , do≈ÇƒÖcz do nas i wsp√≥≈Çtw√≥rz rozwiƒÖzania. üéØ Z nami bƒôdziesz: Projektowaƒá, rozwijaƒá i optymalizowaƒá hurtownie danych opartych na technologii Microsoft SQL Server Implementowaƒá i utrzymywaƒá procesy ETL przy u≈ºyciu SSIS Tworzyƒá raporty i analizy z wykorzystaniem SSRS oraz modele wielowymiarowe i tabelaryczne w SSAS Integrowaƒá dane z r√≥≈ºnych ≈∫r√≥de≈Ç, w tym Oracle Optymalizowaƒá wydajno≈õci zapyta≈Ñ SQL oraz proces√≥w przetwarzania danych Aktywnie wsp√≥≈Çpracowaƒá z klientami w celu zbierania i definiowania wymaga≈Ñ biznesowych oraz ich przek≈Çadania na rozwiƒÖzania techniczne Prowadziƒá dzia≈Çania presales ‚Äì przygotowywanie ofert, udzia≈Ç w spotkaniach z klientami, doradztwo w zakresie architektury danych Koordynowaƒá i nadzorowaƒá pracƒô zespo≈Çu developerskiego, pe≈Çniƒá rolƒô lidera technicznego üòé Czekamy na Ciebie, je≈õli: Masz m in. 5 lat do≈õwiadczenia w pracy z hurtowniami danych oraz rozwiƒÖzaniami Microsoft BI Bardzo dobra znasz SQL Server , w tym mechanizm√≥w przechowywania i przetwarzania danych Posiadasz Do≈õwiadczenie w pracy z SSIS, SSRS, SSAS oraz umiejƒôtno≈õƒá efektywnego wykorzystywania tych narzƒôdzi. Pracujesz r√≥wnie≈º z innymi ≈∫r√≥d≈Çami danych, w szczeg√≥lno≈õci Azure , Oracle Potrafisz prowadziƒá projekty i wsp√≥≈Çpracowaƒá z biznesem ‚Äì umiejƒôtno≈õƒá definiowania wymaga≈Ñ, rekomendowania rozwiƒÖza≈Ñ oraz prezentowania wynik√≥w. Masz do≈õwiadczenie w dzia≈Çaniach presales , tworzeniu ofert i doradztwie technologicznym. Mo≈ºesz pochwaliƒá siƒô umiejƒôtno≈õciƒÖ zarzƒÖdzania zespo≈Çem oraz mentoringu m≈Çodszych cz≈Çonk√≥w zespo≈Çu. Znasz jƒôzyka angielskiego na poziomie min. B2 ü§ù Dowiedz siƒô, jak skorzystasz, bƒôdƒÖc w Onwelo: Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Zaoszczƒôdzisz czas na dojazdach ‚Äì pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15000,18000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,262,Tableau/BI Developer (in the EU),Andersen,"Andersen is seeking aTableau/BI Developerin the EUfor a major logistics project with a global UK-based transport leader. The role involves supporting data-related tasks for a mobile app focused on trip planning, ticket booking, and real-time transport information.","[{""min"": 10300, ""max"": 19400, ""type"": ""Gross per month - Permanent""}, {""min"": 10300, ""max"": 19400, ""type"": ""Net per month - B2B""}]",Data Engineering,10300,19400,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,285,üëâ Senior GenAI Engineer (Future Opening),Xebia sp. z o.o.,"üü£About project: This role focuses on developing and deploying AI applications using tools such as Azure AI Studio, vector databases, and Retrieval-Augmented Generation (RAG) frameworks. You will collaborate closely with senior team members to deliver robust, high-quality solutions that drive innovation. üü£You will be: designing, developing, and implementing AI solutions using Python, with a focus on LLMs, vision models, and generative AI technologies, building, testing, and optimizing RAG applications, leveraging effective prompt engineering techniques to improve model performance, integrating AI models with Azure AI Studio, SharePoint, and Azure Blob Storage for efficient deployment and data handling, utilizing vector databases and Agentic frameworks (e.g., LlamaIndex) to enhance the functionality and intelligence of AI systems, implementing event-driven architectures using tools like Event Hub and Kafka for real-time data processing and scalability, collaborating with the AI Lead and team members to troubleshoot issues, test models, and ensure successful deployment, writing clean, efficient, and well-documented code adhering to best practices and version control standards, staying current with emerging AI tools, frameworks, and technologies to continuously improve development processes and outcomes. üü£Your profile: Bachelor‚Äôs degree in computer science, Data Science, Engineering, or a related field, 4+ years of hands-on experience in AI/ML development, with an emphasis on generative AI and related technologies, strong experience with Python, REST APIs, Git proven expertise in developing and deploying LLMs, vision models, vector databases, and RAG applications, strong proficiency in Azure AI Studio, Azure Blob Storage, Event Hub, Kafka, familiarity with Agentic frameworks and tools like LlamaIndex for advanced AI development, ability to thrive in a collaborative team environment while managing multiple tasks effectively, good verbal and written communication skills in English (min. B2). üü£Nice to have: exposure to cloud fundamentals (Azure preferred) and containerization tools like Docker, experience with CI/CD pipelines for AI model deployment, understanding RESTful services and API integration. üü£Recruitment Process: CVreview ‚ÄìHRcall ‚ÄìInterview(with Live-coding) ‚ÄìClientInterview (with Live-coding) ‚ÄìHiring ManagerInterview ‚ÄìDecision üéÅBenefits üéÅ ‚úçDevelopment: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏èWe are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 23500, ""max"": 33500, ""type"": ""Net per month - B2B""}, {""min"": 18000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,27000,Gross per month - Permanent
Full-time,Mid,Mandate,Remote,314,IT Support & Systems Administrator (Salesforce CRM),BookingHost Sp. z o.o.,"Do zespo≈Çu IT poszukujemy osoby skrupulatnej, bystrej, komunikatywnej i samodzielnej, kt√≥ra wesprze nas w codziennej obs≈Çudze i rozwoju system√≥w informatycznych, w tym rozwiƒÖzywaniu zg≈Çosze≈Ñ naszych pracownik√≥w (ticket√≥w). Administracji system√≥w informatycznych - Salesforce CRM i szeregiem innych aplikacji - zarzƒÖdzaniem u≈ºytkownikami, konfiguracjƒÖ kont, zakresem ich uprawnie≈Ñ itp.); Udziale w planowaniu proces√≥w biznesowych w firmie, przygotowaniem pod nie wymaga≈Ñ technicznych oraz ich realizacji; Wdra≈ºaniu nowych funkcjonalno≈õci m.in. automatyzacji z u≈ºyciem Salesforce Flows, Make; Integracji nowych narzƒôdzi; Sprawowaniu pieczy nad porzƒÖdkiem w modelu danych; Generowaniu raport√≥w i analizie danych; Bie≈ºƒÖcym wsparciu u≈ºytkownik√≥w. Dobra znajomo≈õƒá administracji CRM Salesforce (ewentualnie innego systemu CRM / ERP); Do≈õwiadczenie w administracji innych system√≥w i/lub wsparciu technicznym; Sprawne pos≈Çugiwanie siƒô arkuszami Google Sheets / Excel; Samodzielno≈õƒá i chƒôƒá uczenia siƒô, tak≈ºe z wykorzystaniem dostƒôpnych w sieci materia≈Ç√≥w; Umiejƒôtno≈õƒá pracy w dynamicznym ≈õrodowisku, wielozadaniowo≈õƒá, szybko≈õƒá adaptacji do zmian; Dobra organizacja pracy, umiejƒôtno≈õƒá trafnej oceny priorytet√≥w; Swoboda w komunikacji, wysoka kultura osobista; Dobra znajomo≈õƒá jƒôzyka angielskiego (czytanie dokumentacji, aktywne uczestnictwo w spotkaniach). architekta rozwiƒÖza≈Ñ Salesforce i/lub w programowaniu w APEX lub Java; z narzƒôdziami takimi jak Front, Calendly, Make, Zapier, Google Workspace, Slack, wirtualna centralka, systemy ticketowe (np. Jira, ServiceNow); w nadzorowaniu projekt√≥w prowadzonych z zewnƒôtrznymi podmiotami jak agencje deweloperskie; W integracji Salesforce z innymi systemami / aplikacjami. W pe≈Çni zdalnƒÖ pracƒô, swobodnƒÖ atmosfera w zespole, elastyczne godziny; P≈Çatny urlop przy umowie B2B; Wsparcie w rozwoju umiejƒôtno≈õci administrowania system√≥w; Realny wp≈Çyw na codziennƒÖ pracƒô firmy i rozw√≥j struktury IT; Pracƒô w szybko rosnƒÖcej i dynamicznej firmie, aspirujƒÖcej do pozycji lidera na rynku wynajmu mieszka≈Ñ; Dostƒôpne opcje Karty MultiSport i MultiLife. üìùProces rekrutacji Proces rekrutacji sk≈Çada siƒô z dw√≥ch etap√≥w ‚Äì oba odbywajƒÖ siƒô zdalnie: Rozmowa telefoniczna(ok. 15‚Äì30 minut) ‚Äì kr√≥tkie poznanie siƒô, om√≥wienie do≈õwiadczenia i oczekiwa≈Ñ. Spotkanie online( ok. 60 minut) ‚Äì rozmowa techniczna z cz≈Çonkiem zespo≈Çu IT, podczas kt√≥rej sprawdzimy wiedzƒô praktycznƒÖ i dopasowanie do roli.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 9000, ""type"": ""Gross per month - Mandate""}]",Database Administration,7000,9000,Gross per month - Mandate
Full-time,Mid,Permanent or B2B,Remote,318,Middle Data Analyst,N-iX,"#3683 We are seeking afor aMiddle Data Analystin Polandwith strong expertise in Tableau and a passion for turning data into actionable insights. In this role, you will lead the development of advanced visual analytics, collaborate with stakeholders to understand business needs, and help shape data-driven decision-making across the organization. Responsibilities: Develop and maintain dashboards and reports using Tableau to support key business functions Translate complex business questions into analytical solutions Work closely with stakeholders to gather requirements, understand KPIs, and deliver meaningful insights Use SQL to extract, transform, and analyze data from various sources Present findings in a clear, concise, and impactful way to both technical and non-technical audiences Continuously improve reporting performance and usability through iteration and feedback Requirements: 4+ years of experience in analytics, business intelligence, or data visualization roles Python skills for data processing (at least 1 year of experience) Advanced Tableau skills (dashboard development, calculated fields, LOD expressions, performance optimization) Strong SQL proficiency for data querying and preparation Proven ability to derive insights from data and explain them effectively Solid understanding of data modeling, joins, and ETL principles Strong analytical thinking and attention to detail Experience working with cross-functional teams and translating business requirements into data deliverables Must-Have: Expert-level Tableau development experience (at least 2+ years of experience) Hands-on experience creating scalable, interactive dashboards for enterprise use Advanced SQL skills applied to analytical/reporting contexts Ability to work independently and proactively in a data-driven environment Nice-to-Have: Experience with additional BI tools (Power BI, Looker, etc.) Knowledge of modern data warehouses (Snowflake, BigQuery) Familiarity with dbt or Python for data manipulation Exposure to A/B testing, experimentation frameworks, or product analytics Understanding of data visualization best practices and UX principles We offer: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits","[{""min"": 12560, ""max"": 16254, ""type"": ""Net per month - B2B""}, {""min"": 9974, ""max"": 12929, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,9974,12929,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,322,Data Analyst,Remodevs,"Please note - hybrid from Warsaw (3 days from the office, 2 remotely). About us: We help businesses use AI and digital tools to work better and grow faster, especially in private capital markets. Our Core Platform improves workflows and gives useful insights with AI. Olympus Software is a fast, smart cloud system that grows with your needs. The Pantheon Suite offers flexible tools to manage and improve business performance. With over 10 years of experience, we know how to turn technology into real business value. About the Role We are seeking a Data Analyst to join our PaaS (Platform-as-a-Service) Customer Delivery team in Warsaw. In this role, you will be responsible for transforming data into actionable insights to support decision-making across product, operations, and strategy for customer-facing platform implementations. You will work closely with stakeholders across implementation projects to develop dashboards, conduct ad-hoc analyses, and ensure the integrity of platform usage and performance metrics. This role is ideal for someone who is curious, business-minded, and eager to make an impact through data. Key Responsibilities Partner with cross-functional teams to define key metrics and build dashboards and reports that provide visibility into business performance. Conduct deep-dive analyses to answer business questions, uncover trends, and identify opportunities for growth and optimization. Design and maintain scalable data models and SQL queries to support reporting and analytical needs. Collaborate with data engineers to ensure data availability, quality, and consistency across systems. Communicate findings and recommendations clearly to technical and non-technical audiences. Develop documentation and contribute to data literacy across the organization. Qualifications Required 2‚Äì4 years of experience in a data analyst or business intelligence role. Strong SQL skills and experience working with large datasets in a cloud data warehouse environment. Proficiency with BI tools such as Looker, Tableau, Power BI, or similar. Strong analytical thinking and attention to detail. Excellent communication and data storytelling skills. Preferred Experience working with dbt or similar modeling tools. Familiarity with A/B testing design and analysis. Some experience with Python, R, or another scripting language for data analysis. Exposure to product analytics platforms (e.g., Mixpanel, Amplitude).","[{""min"": 16623, ""max"": 18470, ""type"": ""Net per month - B2B""}, {""min"": 16623, ""max"": 18470, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,16623,18470,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,369,Data Engineer,Tooploox,"We areTooploox üíé,an AI software development companyoffering custom AI solutions and services. We help innovative companies and startups design and build digital products with generative AI, mobile, and web technologies. Our team, consisting of nearly 200 experts including our R&D team of over 40 engineers, many with PhDs, has pioneered AI solutions across industries like healthcare, fashion, and e-commerce. We‚Äôve published over 15 research papers in top conferences like NeurIPS and ICML. We're on the lookout for aData Engineerüìä to take on a pivotal role in our team.You'll be at the heart of working with data, focusing on scalable batch and streaming data pipelines. If you're someone who loves to merge traditional software development with innovative AI technologies, this role is tailor-made for you. Design, develop, and maintain scalable batch and streaming data pipelines. Work withPythonto transform, process, and integrate data. Handle a mix of structured and unstructured data, including work withNoSQL and vector databases. Optimize performance acrossbig data workflows, including tuningHive and Sparkjobs. BS/BA in Data Engineering/Computer Science+ 2 years of experience or related field or 5 years of relevant experience. Extensive expertise withApache Spark (especially PySpark), Hadoop, and Apache Hivewith a proven track record of optimizing large-scale data systems. Strong programming skills inPython. Comprehensive understanding of database concepts, including experience withNoSQL databases (e.g., MongoDB, Redis)and ideally vector databases. Proven hands-on experience with stream processing, preferably usingApache Flink. In-depth knowledge of distributed computing, data warehousing, and performance optimization techniques. Exceptional problem-solving and communication skills, with experience working in cross-functional teams. Fluency in Polish and English. Experience with LLMs, prompt engineering, or machine learning workflows (we use this in conjunction with vector DBs). Experience in Java or Scala - useful for deeper Spark optimization or contributing to broader engineering projects. Familiarity with Spring Boot for building and deploying data applications. üèñÔ∏è26 days of annual service break. ü§íAn additional pool of 14 days per yearpaid at80% of your standard rate. üá¨üáßEnglish lessonsonce a week or more frequently, depending on your needs. üìöAccess to a curated libraryof books and e-books, regularly updated based on employee suggestions, plus recurringknowledge-sharing initiatives. üè°Flexible hoursand the option to work100% remotelyor from one of our offices inWroc≈Çaw or Warsaw. üíªTop-quality equipment‚Äì we provide MacBooks, new monitors, noise-cancelling headphones, and any additional gear you may need to work comfortably. üè•Group insurancewith Warta andprivate medical carewith Enel-Med for just 1 PLN. üß†Mental health support‚Äì we offer access to a psychologist with fully anonymous consultations if needed. üèãÔ∏è‚Äç‚ôÇÔ∏èMultisport card(we cover most of the cost ‚Äì your contribution is currently no more than 45 PLN, or less depending on the selected package), access togyms in our Wroc≈Çaw and Warsaw offices, andsports initiativeslike the annualBike 2 Work Challenge. üçïüéÆüï∫üèª We hostteam lunches, webinars, game nights, and social events. We enjoy the occasional barbecue, dance party, time on the terrace, foosball, or PlayStation session.","[{""min"": 18000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16000,20000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,373,Snowflake Data Engineer,Onwelo Sp. z o.o.,"Jeste≈õmy nowoczesnƒÖ polskƒÖ firmƒÖ technologicznƒÖ, kt√≥ra dostarcza wsparcie eksperckie organizacjom na ca≈Çym ≈õwiecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiƒÖzania IT, oferujƒÖc przy tym solidne zaplecze kompetencyjne. W ciƒÖgu kilku lat zrealizowali≈õmy ponad 300 projekt√≥w w Europie i USA, dynamicznie rozbudowujƒÖc zesp√≥≈Ç do kilkuset specjalist√≥w i otwierajƒÖc sze≈õƒá biur w Polsce oraz oddzia≈Çy w USA, Niemczech i Szwajcarii. Do≈ÇƒÖczysz do zespo≈Çu realizujƒÖcego projekt dla globalnej organizacji z sektoralife science i healthcare, specjalizujƒÖcej siƒô w rozwiƒÖzaniach laboratoryjnych i biotechnologicznych. Klient prowadzi dzia≈Çalno≈õƒá na wielu rynkach i obs≈Çuguje tysiƒÖce jednostek operacyjnych na ca≈Çym ≈õwiecie.Celem projektu jest budowa i rozw√≥j skalowalnej platformy danych w ≈õrodowisku chmurowym, kt√≥ra wspiera analitykƒô biznesowƒÖ, planowanie operacyjne oraz zaawansowane modele predykcyjne.Zesp√≥≈Ç Onwelo wspiera klientam.in. w rozwoju hurtowni danych, projektowaniu potok√≥w ETL, modelowaniu danych i zapewnieniu jako≈õci danych w ≈õrodowisku enterprise.Szukamy osoby, kt√≥ra wniesie swoje do≈õwiadczenie i wesprze zesp√≥≈Ç w rozwoju architektury danych, zapewniajƒÖc wydajno≈õƒá, jako≈õƒá i bezpiecze≈Ñstwo danych. Projektowaƒá, budowaƒá i rozwijaƒá hurtownie danych oparte na platformieSnowflake Tworzyƒá oraz optymalizowaƒápotoki danych (ETL/ELT)w ≈õrodowiskach chmurowych Wdra≈ºaƒá i zarzƒÖdzaƒá komponentami Snowflake: Snowpipe, Streams, Tasks, Secure Views Projektowaƒá i rozwijaƒámodele danychwspierajƒÖce analitykƒô biznesowƒÖ Wsp√≥≈Çpracowaƒá z zespo≈Çami Data Science i BI w zakresie zasilania modeli i dashboard√≥w Wspieraƒá automatyzacjƒô proces√≥w danych poprzez integracjƒô z narzƒôdziami CI/CD (GitLab, Jenkins) Posiadasz minimum 3-letnie do≈õwiadczenie jakoData Engineer‚Äì z naciskiem na Snowflake Znasz platformƒôSnowflake: strukturƒô danych, architekturƒô, optymalizacjƒô zapyta≈Ñ, zarzƒÖdzanie schematami i dostƒôpem Biegle pos≈Çugujesz siƒôSQL(w tym: CTE, window functions, UDF, optymalizacja zapyta≈Ñ) Masz do≈õwiadczenie z procesamiETL/ELT, r√≥wnie≈º z wykorzystaniem danych p√≥≈Çstrukturalnych (JSON, XML, Parquet) Znasz zasady projektowania nowoczesnych modeli danych (np. Kimball, Data Vault) Maszwy≈ºsze wykszta≈Çcenie techniczne(np. informatyka, matematyka, in≈ºynieria danych) Komunikujesz siƒôpo angielsku na poziomie min. B2 Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Otrzymasz mo≈ºliwo≈õƒá korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Integracje firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu Zabezpieczysz swojƒÖ przysz≈Ço≈õƒá, korzystajƒÖc z dodatkowego ubezpieczenia na ≈ºycie Z prywatnƒÖ opiekƒÖ medycznƒÖ zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umo≈ºliwi Ci prowadzenie aktywnego trybu ≈ºycia","[{""min"": 16800, ""max"": 23100, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 17000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14000,17000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,375,Data Scientist,Onwelo Sp. z o.o.,"Onweloto nowoczesna polska sp√≥≈Çka technologiczna, specjalizujƒÖca siƒô w budowie innowacyjnych rozwiƒÖza≈Ñ IT dla organizacji z r√≥≈ºnych sektor√≥w na ca≈Çym ≈õwiecie. Firma oferuje kompleksowe us≈Çugi z zakresu tworzenia, rozwoju i utrzymania oprogramowania, oraz silne wsparcie kompetencyjne. Do naszego zespo≈ÇuData & Analyticsposzukujemy Data Scientista, kt√≥ry bƒôdzie pracowa≈Ç nad budowƒÖ i rozwojem modeli analitycznych oraz uczenia maszynowego dla klient√≥w z r√≥≈ºnych bran≈º ‚Äì zar√≥wno z Polski, jak i z rynk√≥w zagranicznych. Bƒôdziesz wsp√≥≈Çpracowaƒá z zespo≈Çami analitycznymi i biznesowymi, wspieraƒá podejmowanie decyzji na podstawie danych i tworzyƒá rozwiƒÖzania, kt√≥re realnie wp≈ÇywajƒÖ na dzia≈Çalno≈õƒá naszych klient√≥w. Przeprowadzaƒá eksploracyjnƒÖanalizƒô danych (EDA) Poszukiwaƒázale≈ºno≈õci, wzorc√≥w i insight√≥w w danych biznesowych Budowaƒámodele klasyfikacyjne, regresyjne i klasteryzacyjne Przeprowadzaƒáfeature engineering i przygotowywaƒá dane do modelowania Wsp√≥≈Çpracowaƒá z zespo≈Çami analitycznymi, technologicznymi i biznesowymi Wizualizowaƒá wyniki analiz i przygotowywaƒá raporty oraz prezentacje Maszminimum 2-letnie do≈õwiadczeniew pracy jakoData Scientistlub na podobnym stanowisku Bardzo dobrze znaszPythona i pracowa≈Çe≈õ z bibliotekami: pandas, scikit-learn, numpy, matplotlib, seaborn Swobodnie pracujesz zdanymi tabelarycznymii znasztechniki EDA Potrafisz wyciƒÖgaƒá trafne wnioski z danych i prezentowaƒá je w przystƒôpny spos√≥b ZnaszSQLi masz do≈õwiadczenie z du≈ºymi zbiorami danych Dodatkowo docenimy, je≈õli: Korzysta≈Çe≈õ znarzƒôdzi BI i znasz metody interpretacji modeli Masz do≈õwiadczenie w automatyzacji proces√≥w analitycznych Znasz system kontroli wersjiGITi technologie konteneryzacji (Docker) Pracowa≈Çe≈õ z rozwiƒÖzaniami w chmurze obliczeniowej(Azure, AWS lub GCP) Wybierzesz wygodnƒÖ dla Ciebie formƒô zatrudnienia Potrzebujesz pracowaƒá zdalnie? Jeste≈õmy otwarci! Rozwiniesz swoje umiejƒôtno≈õci, wsp√≥≈ÇpracujƒÖc z do≈õwiadczonymi ekspertami Bƒôdziesz pracowaƒá z wykorzystaniem nowych technologii Uzyskasz dostƒôp do szkole≈Ñ wewnƒôtrznych i zewnƒôtrznych We≈∫miesz udzia≈Ç w ciekawych projektach dla polskich i miƒôdzynarodowych klient√≥w Bƒôdzie na Ciebie czekaƒá przyjazne i komfortowe ≈õrodowisko pracy Wydarzenia firmowe pozwolƒÖ Ci na bli≈ºsze poznanie zespo≈Çu","[{""min"": 15750, ""max"": 21000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 15000, ""type"": ""Gross per month - Permanent""}]",Data Science,12000,15000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Hybrid,384,DATA Architect,Power Media,"Nasz klient to firma, kt√≥ra odwa≈ºnie podchodzi do wyzwa≈Ñ technologicznych i nie boi siƒô szukaƒá nieszablonowych rozwiƒÖza≈Ñ. Innowacyjno≈õƒá ≈ÇƒÖczy tu siƒô z solidnƒÖ wiedzƒÖ technicznƒÖ oraz doskona≈ÇƒÖ znajomo≈õciƒÖ reali√≥w bran≈ºy. SpecjalizujƒÖ siƒô w projektach z pogranicza eCommerce i Business Intelligence, oferujƒÖc kompleksowe rozwiƒÖzania oparte na sprawdzonych technologiach. Co wiƒôcej ‚Äì umiejƒôtnie ≈ÇƒÖczƒÖ te dwa ≈õwiaty, tworzƒÖc narzƒôdzia, kt√≥re zapewniajƒÖ pe≈Çen wglƒÖd w dane i realne wsparcie dla biznesu. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozw√≥j rozwiƒÖza≈Ñ Data Platform w ≈õrodowisku chmurowym. Kluczowym zadaniem bƒôdzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejs√≥w w z≈Ço≈ºonym krajobrazie systemowym klienta. Projekt obejmuje budowƒô hurtowni danych oraz platform danych od podstaw. Zesp√≥≈Çsk≈Çada siƒô z 22 os√≥b: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiƒÖzk√≥w: Rozw√≥j i utrzymanie Data Warehouse oraz Data Platform przy u≈ºyciu narzƒôdzi ETL i SQL, Analiza wymaga≈Ñ biznesowych i proponowanie rozwiƒÖza≈Ñ technicznych, Wsp√≥≈Çpraca z zespo≈Çem w celu zapewnienia dostƒôpno≈õci rozwiƒÖza≈Ñ biznesowych, Wykorzystanie swojej wiedzy i do≈õwiadczenia do tworzenia innowacyjnych rozwiƒÖza≈Ñ. G≈Ç√≥wne wymagania: Bardzo dobra znajomo≈õƒá metod i technik projektowania oprogramowania, Ponad 10-letnie do≈õwiadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych, Znajomo≈õƒá ekosystem√≥w Big Data, Znajomo≈õƒározwiƒÖza≈Ñ chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejƒôtno≈õci analityczne (analiza i dokumentowanie wymaga≈Ñ biznesowych oraz specyfikacji technicznych) Do≈õwiadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomo≈õƒá SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomo≈õƒá rozwiƒÖza≈Ñ ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (min. B2+/C1)‚Äì codzienna praca w miƒôdzynarodowym ≈õrodowisku, Gotowo≈õƒá do podr√≥≈ºy s≈Çu≈ºbowych. Mile widziane: Znajomo≈õƒánarzƒôdzi Informatica, Znajomo≈õƒá Machine Learning oraz framework√≥w ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomo≈õƒá jƒôzyk√≥w programowania (Java, Scala, C++, Python). Znajomo≈õƒá jƒôzyka niemieckiego. Firma oferuje: Stabilne, d≈Çugofalowe zatrudnienie w oparciu o B2B lub UoP, Mo≈ºliwo≈õƒá pracy w wiƒôkszo≈õci zdalnej(praca z biura 1 raz w tygodniu, we wtorek) P≈Çaska struktura, anty korporacyjne podej≈õcie do pracy i zespo≈Çu, Ciekawe, miƒôdzynarodowe projekty, Zgrany zesp√≥≈Ç chƒôtnie uczestniczƒÖcy w aktywno≈õciach sportowo ‚Äì rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team ‚Äì building), Dodatkowe zajƒôcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajƒôƒá sportowych, Nowoczesne biuro ze strefƒÖ relaksu, Co roczny 5 dniowy wyjazd firmowy (ca≈Ça firma), warsztaty kulinarne, wyj≈õcia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywno≈õciami ≈öwietna atmosfera, partnerskie podej≈õcie, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa ‚Äûmiƒôkko-techniczna‚Äù. GorƒÖca pro≈õba o CV w j. angielskim : )","[{""min"": 18000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,14000,18000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,402,Data Engineer,dotLinkers,"Type of contract: contract of employment (UoP) / B2B Salary ranges: up to 24 000 PLN a month Working model: 100% remote Join our client, one of the leading logistics and transport solutions providers. About the role: Looking for a skilled Lead Data Engineer to drive the design and implementation of robust data systems, while effectively connecting technical teams with business goals. This role involves hands-on development, strategic planning, and cross-team collaboration. Responsibilities: Designing scalable data systems and tools to support analytics, modeling, and decision-making Building advanced data pipelines and platforms on Azure Collaborating closely with technical and business teams Establishing best practices and reusable standards in data engineering Engaging with international projects and external partners Requirements: 5+ years in IT, data engineering, or information systems Expertise in Azure, Spark (Scala/PySpark), Databricks, Kafka, Event Hubs, Python, Java, SQL/NoSQL Experience with DevOps tools and practices (CI/CD, Terraform, Kubernetes) Skilled in streaming data and big data environments Strong leadership and communication skills (English and Polish) Experience working with distributed teams The offer: Flexible remote work with occasional office visits Benefits include private healthcare, insurance, and a sports card High-end equipment Development budget for learning and growth","[{""min"": 20000, ""max"": 24000, ""type"": ""Net per month - B2B""}, {""min"": 20000, ""max"": 24000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,24000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,415,Tester Data&BI,summ-it,"Uprzejmie informujemy, ≈ºe z uwagi na sezon urlopowy czas odpowiedzi w procesie rekrutacji mo≈ºe siƒô wyd≈Çu≈ºyƒá. Je≈õli chcesz nauczyƒá siƒô nowych technologii, a nastƒôpnie pracowaƒá w projektach dla marek znanych na ca≈Çym ≈õwiecie, a jednak wciƒÖ≈º w ramach niekorporacyjnej struktury, to idealne miejsce dla Ciebie! W summ-it stwarzamy przestrze≈Ñ na zg≈Çaszanie swoich pomys≈Ç√≥w i dbamy o Tw√≥j rozw√≥j. Nasi pracownicy biorƒÖ udzia≈Ç w konferencjach bran≈ºowych i wydarzeniach dotyczƒÖcych szeroko pojƒôtej tematyki IT, a tak≈ºe udoskonalajƒÖ swoje umiejƒôtno≈õci dziƒôki r√≥≈ºnego rodzaju szkoleniom i warsztatom. Pracujemy z bazami danych o ≈ÇƒÖcznej wielko≈õci liczonej w PB, i optymalizujemy systemy walczƒÖc o ka≈ºdƒÖ ms. Obecnie zarzƒÖdzamy ponad 10 000 system√≥w baz dla naszych klient√≥w. Zatrudnienie na podstawie: umowy o pracƒô, umowy B2B, umowy zlecenie Pracƒô w atmosferze kole≈ºe≈Ñstwa i zaufania ‚Äì w ankiecie satysfakcji ponad 92% pracownik√≥w zgadza siƒô z tym stwierdzeniem Odpowiednie wsparcie i wsp√≥≈Çpracƒô w swoim zespole ‚Äì w ankiecie satysfakcji 100% pracownik√≥w zgadza siƒô z tym stwierdzeniem Elastyczne godziny pracy oraz pracƒô w modelu hybrydowym Pracƒô z biura w centrum Poznania Dostƒôp do najnowszych technologii IT Mo≈ºliwo≈õƒá rozwoju w miƒôdzynarodowej firmie Szkolenia zewnƒôtrzne i wewnƒôtrzne: Miƒôkka ≈õroda, Bezpieczne czwartki, Science Friday Spotkania firmowe: Summer Party, Winter Party, AllHands, Talk to Your Boss, Lokalne ≈õrody, summ‚Äëitowe ≈õniadania Programy doceniania pracownik√≥w: summ-it heores i nagrody za przyznane kudosy Program polece≈Ñ pracowniczych Mo≈ºliwo≈õƒá do≈ÇƒÖczenia do benefit√≥w (opieka medyczna, karta Multisport, ubezpieczenie grupowe) Pracƒô w zr√≥wnowa≈ºonym zespole ‚Äì 3 generacji: X, Y, Z Przeprowadzanie test√≥w w obszarze danych (Azure Databricks, Azure Data Factory, Azure Synapse, Azure Analysis Services, Power BI, MDS) zgodnie z opisem w User stories Rejestrowanie wynik√≥w test√≥w w Azure DevOps zgodnie z procesem Analiza wyniku test√≥w i informacja zwrotna dla developer‚Äô√≥w i SME Opracowywanie scenariuszy testowych (unit, integration, regression) Analiza test√≥w, identyfikacja i wdra≈ºanie automatyzacji test√≥w oraz usprawnie≈Ñ w procesach QA Proponowanie usprawnie≈Ñ w zakresie test√≥w Identyfikacja wzorc√≥w na powtarzajƒÖce siƒô b≈Çƒôdy Min. 3 lata do≈õwiadczenia w testowaniu danych Znajomo≈õƒá Azure Databricks (notebook, job, cluster Spark) Znajomo≈õƒá Azure Data Factory (pipeline‚Äôs datasets, linked services, monitor) Znajmo≈õƒá Azure Synapse (Dedicated pool) Znajomo≈õƒá modeli tabularnych (Analysis Services) i serwisu Power BI (data lineage, model semantyczny, po≈ÇƒÖczenia) Bieg≈Ço≈õƒá w SQL (weryfikacja jako≈õci danych, przygotowywanie zapyta≈Ñ testowych) Zrozumienie proces√≥w ETL Znajomo≈õƒá narzƒôdzi do automatyzacji test√≥w i CI/CD (w szczeg√≥lno≈õci Azure DevOps, TestPlans) Znajomo≈õƒá jƒôzyka angielskiego na poziomie min. B2 Dziƒôkujemy za zainteresowanie naszƒÖ ofertƒÖ i nades≈Çanie aplikacji. Uprzejmie informujemy, ≈ºe skontaktujemy siƒô z wybranymi kandydatami.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 10000, ""type"": ""Gross per month - Permanent""}]",Unclassified,7000,10000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,426,Data Engineer,Tesco Technology,"Tesco Technology is multi-functional and specialist team that drives operational excellence of services improves scale for our systems and processes globally and creates business leading capabilities. We are an agile team of an industry-leading team of engineers. We create the future continuous integration and delivery tools for Colleague and Customer & Loyalty areas, solving problems, and developing new features through quality, scalable, performant, and maintainable technical solutions. The solutions that we are responsible for will have a global reach, impacting hundreds of thousands of Tesco colleagues worldwide. We operate in a DevOps philosophy. We take responsibility for the software through its entire lifecycle. We practice continuous integration, delivery, and support of our code through to production and beyond. As Tech Hub we cooperate within the group of Tesco Technology Hubs located in the UK, Poland, Hungary, and India. We always welcome conversations about flexible working, so feel free to talk to us during your application about how we can support you.We value connecting, collaborating, and innovating with our colleagues in person. At Tesco Technology, we work in a hybrid model. This role requires you to be based in or near Krak√≥w, as we currently meet in the office three days a week. The Data Engineering department at Tesco Technology is at the forefront of data processing within the retail and technology industry. This vital department handles a range of responsibilities, including: Analyzing order and delivery data to optimize logistics processes and enhance delivery efficiency. Managing critical data related to customer orders, suppliers, and products to ensure the seamless flow of our fulfillment operations. Upholding data integrity and security during the processing of order and delivery-related information. As we continue to expand, we are actively seeking a skilled Data Engineer to join our team of analytics experts. In this role, you will take charge of expanding and refining our data and data pipeline architecture. Additionally, you will be instrumental in optimizing data flow and collection to cater to the needs of cross-functional teams. Our ideal candidate is an experienced data pipeline builder and data enthusiast who relishes the opportunity to optimize data systems and construct them from the ground up. As a Data Engineer, you will collaborate closely with software developers, database architects, data analysts, and data scientists on various data-driven initiatives. You will play a crucial role in ensuring that the optimal data delivery architecture remains consistent across all ongoing projects. This role calls for a high level of self-direction and the ability to effectively support the data requirements of multiple teams, systems, and products. If you are enthusiastic about the prospect of optimizing, and possibly even redesigning, our company's data architecture to support our next generation of products and data initiatives, we encourage you to apply and be part of our dynamic team shaping the future of our data operations! Responsibilities Create and maintain optimal data pipeline architecture Assemble large complex data sets that meet functional / non-functional business requirements. Identify design and implement internal process improvements: automating manual processes optimising data delivery re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction transformation and loading of data from a wide variety of data sources Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition operational efficiency and other key business performance metrics. Work with stakeholders including the Executive Product Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure Create data tools for analytics and data scientist team members that assist them in building and optimising our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Mandatory skills: Data Processing: Apache Spark - Scala or Python Data Storage: Apache HDFS or respective cloud alternative Resource Manager: Apache Yarn or respective cloud alternative Lakehouse: Apache Hive/Kyuubi or alternative Workflow Scheduler: Airflow or alternative Nice to have skills: Functional programming Apache Kafka Kubernetes Stream processing CI/CD Unsure if you fit all the criteria? Apply and give us the chance to evaluate your potential ‚Äì you could be the perfect fit! We value flexibility at Tesco; therefore, this position is also available for candidates who are interested in working part time ‚Äì about 120 hours a month or more. Please let us know what would work for you. Hybrid work We know life looks a little different for each of us. That‚Äôs why at Tesco, we always welcome chats about different flexible working options. Some people are at the start of their careers, some want the freedom to do the things they love. Others are going through life-changing moments like becoming a carer, adapting to parenthood, or something else. So, talk to us throughout your application about how we can support.This role requires you to be based in or near Krak√≥w, as you will spend 60% (3 days) of your week collaborating with colleagues at our office locations or local sites and the rest remotely. Benefits Tesco is a diverse and exciting employer, dedicated to being #aplacetogeton, providing career-defining opportunities to all of our colleagues. If you choose to join our business, we will provide you with (for all): MacBook as your tool for work Learning opportunities - certified technical training and learning platforms like Udemy, Pluralsight and O‚Äôreily Referral Bonus Sports activities with a personal trainer in the office Benefits for colleagues on employment of contract only: Additional 4 days of paid leave to support your well-being and family life Up to 20% yearly salary bonus ‚Äì based on both individual and business performance Private healthcare (LuxMed) Cafeteria & Multisport Supporting those, who are not yet eligible for full holiday entitlement, by expanding their pool from 20 to 25 days Relocation Help IP Tax Deductible Costs If that sounds exciting, then we'd love to hear from you. Tesco is committed to celebrating diversity and everyone is welcome at Tesco. As a Disability Confident Employer, we‚Äôre committed to providing a fully inclusive and accessible recruitment process, allowing candidates the opportunity to thrive and inform us of any reasonable adjustments they may require.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 18500, ""max"": 26000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18500,26000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,432,Senior Data Science/AI Engineer,N-iX,"(3767) About client: Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management.Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact. Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company‚Äôs R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 22164, ""max"": 25859, ""type"": ""Net per month - B2B""}, {""min"": 18470, ""max"": 21056, ""type"": ""Gross per month - Permanent""}]",Data Science,18470,21056,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,436,Middle/Senior Data Engineer,N-iX,"#3682 We are seeking aMiddle/Senior Data Engineerwith proven expertise inAWS, Snowflake, and dbtto design and build scalable data pipelines and modern data infrastructure. You'll play a key role in shaping the data ecosystem, ensuring data availability, quality, and performance across business units. 4+ years of experience in Data Engineering roles. Experience with theAWScloud platform. Proven experience withSnowflakein production environments. Hands-on experience building data pipelines usingdbt. Pythonskills for data processing and orchestration. Deep understanding of data modeling and ELT best practices. Experience with CI/CD and version control systems (e.g., Git). Strong communication and collaboration skills. Strong experience withSnowflake(e.g., performance tuning, storage layers, cost management) Production-level proficiency withdbt(modular development, testing, deployment).. Experience developingPythondata pipelines. Proficiency in SQL (analytical queries, performance optimization). Experience with orchestration tools like Airflow, Prefect, or Dagster. Familiarity with cloud platforms (e.g., GCP, or Azure). Knowledge of data governance, lineage, and catalog tools. Experience in working in Agile teams and CI/CD deployment pipelines. Exposure to BI tools like Tableau or Power BI. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 110, ""max"": 195, ""type"": ""Net per hour - B2B""}, {""min"": 81, ""max"": 162, ""type"": ""Gross per hour - Permanent""}]",Data Engineering,81,162,Gross per hour - Permanent
Full-time,Senior,Permanent or B2B,Remote,437,üëâ Data & AI Enterprise Architect,Xebia sp. z o.o.,"üü£You will be: leading and inspiring cross-functional architecture efforts across our client‚Äôs organization, filling an internal strategic role designed to bring added value to the client by aligning data architecture with business innovation and long-term growth, acting as a thought leader and enabler, supporting dedicated architects and teams across multiple domains, including: AI & ML Projects (40‚Äì60%), B2B & B2C E-commerce Platforms (20%), DataOps & Data Engineering (20%). üü£Your profile: proven experience as an Enterprise or Lead Data Architect in complex, enterprise-scale environments, deep expertise in Microsoft Azure and Databricks, strong understanding of data architecture principles, data governance, and modern data platforms, ability to work across business and technical domains, with a focus on value creation and business impact, willingness to occasionally work on site in Amsterdam, very good command of English (min. C1). üü£Nice to have: familiarity with technologies used across the client‚Äôs ecosystem, such as: Salesforce, Event Hubs / Kafka, Contentful or other headless CMS platforms, experience in customer-facing roles, pre-sales, or innovation consulting.in. Work from the European Union region and a work permit are required. üü£Recruitment Process: CVreview ‚ÄìHRcall ‚ÄìTechnical Interview‚ÄìClientInterview (with Live-coding) ‚ÄìHiring ManagerInterview ‚ÄìDecision üéÅBenefits üéÅ ‚úçDevelopment: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏èWe are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 43500, ""type"": ""Net per month - B2B""}, {""min"": 26850, ""max"": 35500, ""type"": ""Gross per month - Permanent""}]",Data Architecture,26850,35500,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,467,üëâ GenAI Lead,Xebia sp. z o.o.,"üü£About project: GenAI Lead will take ownership of designing, developing, and deploying advanced AI solutions, specializing in machine learning, deep learning, and generative AI technologies. This role demands a strategic leader with deep technical expertise to architect scalable GenAI solutions, guide a team of AI professionals, and deliver transformative business outcomes through innovative AI applications. üü£You will be: designing and implementing scalable Generative AI (GenAI) systems using Large Language Models (LLMs), vision models, and vector databases, leading the design, development, and deployment of AI-driven solutions, including machine learning models, deep learning frameworks, and generative AI applications, overseeing seamless integration of AI solutions with Azure AI Studio, SharePoint, and Power BI for deployment, reporting, and visualization, collaborating with cross-functional teams to shape AI strategies and deliver high-impact solutions, providing technical leadership in implemention of Agentic frameworks and tools like LlamaIndex to advance AI workflows, mentoring and empowering a team of AI developers, fostering a culture of innovation, collaboration, and technical excellence, staying at the forefront of AI advancements, proposing creative and practical solutions to address complex challenges, enforcing best practices in code quality, model optimization, and solution scalability to ensure robust production-ready systems. üü£Your profile: Bachelor‚Äôs or Master‚Äôs degree in computer science, Data Science, Engineering, or a related field (PhD preferred), 8+ years of hands-on experience in AI/ML development, including at least 3 years in leadership or solution architect capacity, demonstrated success in delivering machine learning, deep learning, and generative AI projects in production environments, exceptional programming proficiency in Python and experience with frameworks such as TensorFlow, PyTorch, or equivalent. in-depth expertise in LLMs, vision models, vector databases, and RAG applications, strong command of Azure AI Studio, SharePoint, and basic Power BI for integration and reporting purposes, proven experience with Agentic frameworks and tools like LlamaIndex for intelligent system development, outstanding problem-solving abilities, with a knack for translating business needs into technical solutions, superior communication and leadership skills to manage teams, align stakeholders, and drive project success, excellent verbal and written communication skills in English (min. C1). üü£Nice to have: experience with cloud platforms beyond Azure (e.g., AWS, GCP), knowledge of DevOps practices (e.g., CI/CD pipelines, Kubernetes), familiarity with AI ethics, governance, and compliance standards, exposure to advanced visualization tools or BI platforms. üü£Recruitment Process: CVreview ‚ÄìHRcall ‚ÄìInterview(with Live-coding) ‚ÄìClientInterview (with Live-coding) ‚ÄìHiring ManagerInterview ‚ÄìDecision üéÅBenefits üéÅ ‚úçDevelopment: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏èWe are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 40000, ""type"": ""Net per month - B2B""}, {""min"": 27000, ""max"": 32500, ""type"": ""Gross per month - Permanent""}]",Unclassified,27000,32500,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,476,Data Analyst,Vasco Electronics,"Miejsce pracy: Krak√≥w Tryb pracy: Hybrydowy, 3 dni z biura, 2 dni zdalnie Etat: Full time Rodzaj umowy: Umowa o Pracƒô, Umowa Zlecenie, B2B Wynagrodzenie: UoP (brutto): 9900 - 12000 PLN UZ (brutto): 59 - 71 PLN/h B2B (netto/h FV): 73 - 93 PLN/h Zakres obowiƒÖzk√≥w Kompleksowa analiza danych (marketingowych, sprzeda≈ºowych, finansowych, magazynowych) z wykorzystaniemGoogle BigQueryiSQL ≈ÅƒÖczenie, agregowanie, por√≥wnywanie i weryfikowanie danych z r√≥≈ºnych ≈∫r√≥de≈Ç Projektowanie, tworzenie i utrzymywanie interaktywnych dashboard√≥w oraz raport√≥w wPower BI T≈Çumaczenie wymaga≈Ñ biznesowych na specyfikacje techniczne, przygotowywanie analiz i prezentowanie rekomendacji dla interesariuszy ≈öcis≈Ça wsp√≥≈Çpraca zzespo≈Çem Data Engineer√≥woraz dba≈Ço≈õƒá o jako≈õƒá i sp√≥jno≈õƒá dokumentacji Wykorzystywanie LLM√≥w do automatyzacji swojej pracy Nasze oczekiwania 3 - 5 lat do≈õwiadczenia Zaawansowana, praktyczna znajomo≈õƒáSQLoraz ≈õrodowiska bazodanowegoGoogle BigQuerylub innych baz danych Praca z wersjonowaniem kodu (Dataform, dbt lub podobne) Bieg≈Ço≈õƒá w wizualizacji danych i budowaniu raport√≥w wPower BIlub podobne Do≈õwiadczenie w samodzielnym prowadzeniu analiz, od ekstrakcji danych po prezentacjƒô wniosk√≥w Wysoko rozwiniƒôte umiejƒôtno≈õci komunikacyjne i zdolno≈õƒá do efektywnej wsp√≥≈Çpracy z odbiorcami biznesowymi i technicznymi Proaktywne podej≈õcie, umiejƒôtno≈õƒá jasnego formu≈Çowania rekomendacji i otwarto≈õƒá na kulturƒô feedbacku Jƒôzyk angielski na poziomie B2 Mile widziane BigQuery Power BI Dataform Python Oferujemy ≈örodowisko oparte na warto≈õciach i przyjaznƒÖ, nieformalnƒÖ atmosferƒô ‚Äì bez nadƒôcia, z fajnymi lud≈∫mi i dobrƒÖ kawƒÖ Du≈ºy wp≈Çyw na kszta≈Çt pracy zespo≈Çu Bud≈ºet do wykorzystania na platformie Worksmile, kt√≥ra oferuje dostƒôp do takich benefit√≥w jak m.in. Multisport, Allianz, Luxmed, PZU oraz wiele innych Elastyczny czas pracy Inicjatywy rozwojowe Dofinansowanie okular√≥w korekcyjnych 800 z≈Ç Czƒôste integracje Atrakcje w biurze tj. PS5 + VR2, biuro przyjazne zwierzƒôtom, przekƒÖski i owoce w biurze Parking przy biurze","[{""min"": 9900, ""max"": 12000, ""type"": ""Gross per month - Permanent""}, {""min"": 73, ""max"": 93, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,73,93,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,487,üëâ Senior GCP Data Engineer,Xebia sp. z o.o.,"üü£ You will be: developing and maintaining data pipelines to ensure seamless data flow from the Loyalty system to the data lake and data warehouse, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, ensuring data integrity, consistency, and availability across all data systems, integrating data from various sources, including transactional databases, third-party APIs, and external data sources, into the data lake, implementing ETL processes to transform and load data into the data warehouse for analytics and reporting, working closely with cross-functional teams including Engineering, Business Analytics, Data Science and Product Management to understand data requirements and deliver solutions, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, optimizing data storage and retrieval to improve performance and scalability, monitoring and troubleshooting data pipelines to ensure high reliability and efficiency, implementing and enforcing data governance policies to ensure data security, privacy, and compliance, developing documentation and standards for data processes and procedures. üü£ Your profile: 7+ years in a data engineering role, with hands-on experience in building data processing pipelines, experience in leading the design and implementing of data pipelines and data products, proficiency with GCP services, for large-scale data processing and optimization, extensive experience with Apache Airflow, including DAG creation, triggers, and workflow optimization, knowledge of data partitioning, batch configuration, and performance tuning for terabyte-scale processing, strong Python proficiency, with expertise in modern data libraries and frameworks (e.g., Databricks, Snowflake, Spark, SQL), hands-on experience with ETL tools and processes, practical experience with dbt for data transformation, deep understanding of relational and NoSQL databases, data modelling, and data warehousing concepts, excellent command of oral and written English, Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Information Systems, or a related field. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ üü£ Nice to have: experience with ecommerce systems and their data integration, knowledge of data visualization tools (e.g., Tableau, Looker), understanding of machine learning and data analytics, certification in cloud platforms (AWS Certified Data Analytics, Google Professional Data Engineer, etc.). üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Interview (with Live-coding) ‚Äì Client Interview (with Live-coding) ‚Äì Hiring Manager Interview ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 21500, ""max"": 33000, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16600,25900,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,503,üëâ Senior Azure Data Engineer,Xebia sp. z o.o.,"üü£ You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. üü£ Your profile: ready to start immediately , 3+ years‚Äô experience with Azure (Data Factory, SQL, Data Lake, Power BI, Devops, Delta Lake, CosmosDB), 5+ years‚Äô experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools ‚Äì Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, experience with CI/CD tooling (GitHub, Azure DevOps, Harness etc), working knowledge of Git, Databricks will be benefit, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ Please note that we are currently looking to expand our talent pool for future opportunities within the IT industry. While we may not have an immediate project for you at the moment, we are proactively recruiting to ensure that we have the right expertise when new projects arise. We will contact you when a potential project matching your skills and experience becomes available. Thank you for your interest in joining our team. üü£ Nice to have: Ôªø experience with Azure Event Hubs, Azure Blob Storage, Azure Synapse, Spark Streaming, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification. üü£ Recruitment Process: CV review ‚Äì HR call ‚Äì Technical Interview (with live-coding elements) ‚Äì Client Interview (live-coding)‚Äì Hiring Manager call ‚Äì Decision üéÅ Benefits üéÅ ‚úç Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. ü©∫ We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. ü§∏‚Äç‚ôÇÔ∏è We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16600,25900,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,515,Middle Data Engineer (Databricks),N-iX,"#3821 Join our team to work on enhancing a robust data pipeline that powers ourSaaS product,ensuring seamless contextualization, validation, and ingestion of customer data. Collaborate withproduct teamsto unlock new user experiences by leveragingdata insights.Engage with domain experts to analyze real-world engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Clientwas established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client‚Äôs platform empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renownedScandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities: Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements: Programming: Minimum of3-4 yearsas data engineer, or in a relevant field. Python Proficiency: Advanced experience inPython, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights. Cloud: Familiarity with cloud platforms (preferablyAzure). Data Platforms: Experience withDatabricks, Snowflake, or similar data platforms. Database Skills: Knowledge of relational databases, with proficiency inSQL. Big Data: Experience using Apache Spark. Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability. Version Control: Experience withGitlabor equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have: Experience withDockerandKubernetes. Experience with document and graph databases. Ability to travel abroad twice a year for an on-site workshops.","[{""min"": 18101, ""max"": 21536, ""type"": ""Net per month - B2B""}, {""min"": 14591, ""max"": 17547, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14591,17547,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,517,Data Scientist,NeuroSYS,"Nasz klient toglobalna firma farmaceutyczna, kt√≥ra rozbudowuje swoje systemy zarzƒÖdzania produkcji opierajƒÖc siƒô na analityce danych z u≈ºyciem Data Science i Machine Learning. Obecnie poszukujemy do≈õwiadczonego Data Scientista, kt√≥ry pomo≈ºe namwykorzystaƒá zgromadzone dane do optymalizacji proces√≥w produkcyjnych. W ramach projektu klient pracuje nad przygotowaniem zestawu narzƒôdzi analitycznych wykorzystujƒÖcych Power BI jako interfejs do wizualizacji danych oraz bazujƒÖcy na algorytmach Machine Learning do analizy trend√≥w, odkrywania nieoczywistych wzorc√≥w w celu usprawnienia procesu produkcyjnego i wykrywania wczesnych oznak zu≈ºycia lub nieprawid≈Çowego dzia≈Çania element√≥w maszyn (Predictive Maintenance). Twoje zadania: Wykorzystywanie technologiiOCR do ekstrakcji danychz dokument√≥w papierowych, ≈öcis≈Ça wsp√≥≈Çpraca z ekspertami bran≈ºowymi w celu zrozumienia proces√≥w i danych produkcyjnych Analiza potrzeb i wymaga≈Ñ biznesowych w zakresie analizy danych i oczekiwanych rezultat√≥w, Ocena wykonalno≈õci i szacowanie czasoch≈Çonno≈õci dla zg≈Çaszanych potrzeb, Dob√≥r, implementacja i usprawnianiemodeli analitycznych, Przetwarzanie i eksploracja danych pochodzƒÖcych zsystemu typu historian, Regularne raportowanie postƒôp√≥w prac i prezentacja wynik√≥w interesariuszom. Wymagane umiejƒôtno≈õci: Do≈õwiadczenie wMachine Learning i Data Science: min.3 lata do≈õwiadczeniaw komercyjnych projektach wykorzystujƒÖcych ML, umiejƒôtno≈õƒá trenowania i oceny modeli ML, do≈õwiadczenie z technikami uczenia nadzorowanego i nienadzorowanego. Umiejƒôtno≈õci programistyczne: znajomo≈õƒá narzƒôdziOCR(AWS Textract, Tesseract), bieg≈Ça znajomo≈õƒá jƒôzykaPythonoraz popularnych bibliotekML(m.in. aiohttp, scikit-learn, TensorFlow, PyTorch, XGBoost), do≈õwiadczenie w przetwarzaniu i analizie du≈ºych zbior√≥w danych (Big Data), znajomo≈õƒá narzƒôdzi doprzetwarzania danych, takich jak Pandas, NumPy czy SQL, do≈õwiadczenie w pracyz bazami danychSQL i noSQL, praktyczna znajomo≈õƒá GIT oraz GitFlow, podstawowa znajomo≈õƒá Power BI bƒôdzie dodatkowym atutem. Umiejƒôtno≈õci analityczne i komunikacyjne: zdolno≈õƒá do analizy wymaga≈Ñ biznesowych i przek≈Çadania ich na techniczne rozwiƒÖzania, umiejƒôtno≈õƒá prezentacji wynik√≥w oraz klarownego wyja≈õniania z≈Ço≈ºonych koncepcji technicznych osobom nietechnicznym, swoboda komunikacji wjƒôzyku angielskimw mowie i pi≈õmie na poziomie C1. Mile widziane: Znajomo≈õƒá koncepcji i metod Predictive Maintenance i Przemys≈Çu 4.0. (np. prognozowanie stanu maszyn, analiza drga≈Ñ, wykrywanie awarii), zrozumienie pojƒôƒá takich jak IoT, SCADA, DCS, IIoT, Znajomo≈õƒá system√≥w typu historian (np. AVEVA Historian, OSIsoft PI) lub protoko≈Ç√≥w przemys≈Çowych (OPC, Modbus), Do≈õwiadczenie z technologiami chmurowymi (np. Azure, AWS, Google Cloud), zw≈Çaszcza w kontek≈õcie ML Ops, Znajomo≈õƒá technik DevOps i CI/CD (np. Docker, Kubernetes) u≈ºywanych w ≈õrodowisku ML, Do≈õwiadczenie w analizie danych procesowych i integracji z systemami przemys≈Çowymi, Do≈õwiadczenie w pracy z danymi produkcyjnymi i procesowymi (np. dane z PLC, SCADA, Historian), Do≈õwiadczenie w tworzeniu dashboard√≥w Power BI i ich integracji ze ≈∫r√≥d≈Çami danych. Oferujemy: AtrakcyjnƒÖ, pe≈ÇnƒÖ wyzwa≈Ñ pracƒô w zgranym zespole pasjonat√≥w IT i lu≈∫nej atmosferze, Udzia≈Ç w innowacyjnych projektach realizowanych dla globalnego klienta, DowolnƒÖ formƒô zatrudnienia, elastyczne godziny pracy, PrywatnƒÖ opiekƒô medycznƒÖ, Mo≈ºliwo≈õƒá pracy zdalnej, jak i w biurze; ze sporadycznymi, obowiƒÖzkowymi spotkaniami w biurze, w kt√≥rym czekajƒÖ ≈õwie≈ºe owoce, przekƒÖski i pyszna kawa non stop!","[{""min"": 15000, ""max"": 21800, ""type"": ""Net per month - B2B""}, {""min"": 11200, ""max"": 16200, ""type"": ""Gross per month - Permanent""}]",Data Science,11200,16200,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Hybrid,522,Senior Data Engineer,Remodevs,"Please note it's now remote role but later turns into hybrid - so only candidates from Warsaw and surroundings are required. About: We are seeking a highly motivated and self-driven data engineer for our growing data team -who is able to work and deliver independently and as a team. In this role, you will play a crucial part in designing, building and maintaining our ETL infrastructure and data pipelines. Major Responsibilities: ‚óè Design, develop, and deploy Python scripts and ETL processes with Prefect and Airflow to prepare data for analysis. ‚óè Model dimensional and denormalized schemas for optimal performance reporting and discovery. ‚óè Design AI-friendly DB schemas and ontologies. ‚óè Architect cloud ops solutions for data topologies. ‚óè Transform and migrate data with Python, DBT, and Pandas. ‚óè Work with event-based/streaming technologies for real-time ETL. ‚óè Ingest and transform structured, semi-structured, and unstructured data. ‚óè Optimize ETL jobs for performance and scalability to handle big data workloads. ‚óè Monitor and troubleshoot ETL jobs to identify and resolve issues or bottlenecks. ‚óè Implement best practices for data management, security, and governance with Prefect, DBT, and Pandas. ‚óè Write SQL queries, program stored procedures, and reverse engineer existing data pipelines. ‚óè Perform code reviews to ensure fit to requirements, optimal execution pattern,s and adherence to established standards. ‚óè Assist with automated release management and CI/CD processes. ‚óè Validate and cleanse data and handle error conditions gracefully. Skills ‚óè 3+ years of Python development experience, including Pandas ‚óè 5+ years writing complex SQL queries with RDBMSes. ‚óè 5+ years of Experience with developing and deploying ETL pipelines using Airflow, Prefect, or similar tools. ‚óè Experience with cloud-based data warehouses in environments such as RDS, Redshift, or Snowflake. ‚óè Experience with data warehouse design: OLTP, OLAP, Dimensions, and Facts. ‚óè Experience with Cloud-based data architectures, messaging, and analytics. Pluses: Experience with ‚óè Docker ‚óè Kubernetes ‚óè CI/CD automation ‚óè AWS lambdas/step functions ‚óè Data partitioning ‚óè Databricks ‚óè Pyspark ‚óè Cloud certifications","[{""min"": 24012, ""max"": 25859, ""type"": ""Net per month - B2B""}, {""min"": 24012, ""max"": 25859, ""type"": ""Gross per month - Permanent""}]",Data Engineering,24012,25859,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,545,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for aData Engineerwho can help us to provide solutions for customers for making informed decisions in volatile short-term markets. You will design, implement, and maintain data pipelines and storages, become a data manager of our model inputs, and create insightful and powerful analysis and visualization for day-ahead, intraday and balancing data. In our day-to-day work, we include pair programming, joint learning sessions, and recurring hacking days to explore new ideas. We have very much an agile and digital way of working with fast feedback loops and embracing a culture of learning and personal growth. What you will be doing to make a difference? Thrive in an empowered, self-driven team where you take ownership across the entire data lifecycle: Work together with in-house analysts to understand the domain and the data in question. Design, build and maintain flexible and scalable end-to-end data pipelines together with software engineers. Monitor quality and reliability of data and implement required tooling in coordination with data scientists. Visualize data in a meaningful way for in-house analysis and together with product manager and UX designer, for customer facing dashboards. What do you need to succeed in the role? A Bachelor‚Äôs or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. A good understanding of database systems. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Why will you love being part of our team? Supportive Onboarding: Begin your journey with a thorough introduction and a steep learning curve. Room to Grow: Shape and develop your role with a large degree of influence. Mission-Driven Culture: Join one of Europe‚Äôs most exciting green tech companies and contribute to building a more sustainable future. Inclusive Environment: Work in an innovative, international, and supportive atmosphere. Competitive Benefits: Enjoy salaries that reflect your professional experience, flexible working hours, and a hybrid work model that fits your lifestyle. Team Spirit: Collaborate with talented, inspiring colleagues who believe in succeeding together. Attractive Perks: Benefit from our referral program and other employee-focused initiatives. We are looking to hire for Volue office in Gda≈Ñsk but will be ready to consider other locations for the right candidate. In Volue, we cherish each employee‚Äôs competence, ideas and personality. Let your skills and talent be a part of our team ‚Äì and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,12000,20000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,549,Senior Data Engineer,N-iX,"#3071 We are seeking a proactive Senior Data Engineer to join our vibrant team. As a Senior Data Engineer, you will play a critical role in designing, developing, and maintaining sophisticated data pipelines, Ontology Objects, and Foundry Functions within Palantir Foundry. The ideal candidate will possess a robust background in cloud technologies, data architecture, and a passion for solving complex data challenges. Key Responsibilities: Collaborate with cross-functional teams to understand data requirements, and design, implement and maintain scalable data pipelines in Palantir Foundry, ensuring end-to-end data integrity and optimizing workflows. Gather and translate data requirements into robust and efficient solutions, leveraging your expertise in cloud-based data engineering. Create data models, schemas, and flow diagrams to guide development. Develop, implement, optimize and maintain efficient and reliable data pipelines and ETL/ELT processes to collect, process, and integrate data to ensure timely and accurate data delivery to various business applications, while implementing data governance and security best practices to safeguard sensitive information. Monitor data pipeline performance, identify bottlenecks, and implement improvements to optimize data processing speed and reduce latency. Troubleshoot and resolve issues related to data pipelines, ensuring continuous data availability and reliability to support data-driven decision-making processes. Stay current with emerging technologies and industry trends, incorporating innovative solutions into data engineering practices, and effectively document and communicate technical solutions and processes. Tools and skills you will use in this role: Palantir Foundry Python PySpark SQL TypeScript Required: 5+ years of experience in data engineering, preferably within the pharmaceutical or life sciences industry; Strong proficiency in Python and PySpark; Proficiency with big data technologies (e.g., Apache Hadoop, Spark, Kafka, BigQuery, etc.); Hands-on experience with cloud services (e.g., AWS Glue, Azure Data Factory, Google Cloud Dataflow); Expertise in data modeling, data warehousing, and ETL/ELT concepts; Hands-on experience with database systems (e.g., PostgreSQL, MySQL, NoSQL, etc.); Proficiency in containerization technologies (e.g., Docker, Kubernetes); Effective problem-solving and analytical skills, coupled with excellent communication and collaboration abilities; Strong communication and teamwork abilities; Understanding of data security and privacy best practices; Strong mathematical, statistical, and algorithmic skills. Nice to have: Certification in Cloud platforms, or related areas; Experience with search engine Apache Lucene, Webservice Rest API; Familiarity with Veeva CRM, Reltio, SAP, and/or Palantir Foundry; Knowledge of pharmaceutical industry regulations, such as data privacy laws, is advantageous; Previous experience working with JavaScript and TypeScript.","[{""min"": 18470, ""max"": 19579, ""type"": ""Net per month - B2B""}, {""min"": 14776, ""max"": 15515, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14776,15515,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,564,BI Developer / SQL Developer,Beesafe,"About Us: Join our trailblazing team as we expand our digital horizons. From our beginnings as visionary InsurTech to becoming a key player in the Polish digital insurance market, we are part of the esteemed Vienna Insurance Group. We're redefining the rules in the insurance industry with our innovative approach. Our hybrid working model supports both the collaborative energy of office work and the flexibility of remote work. About the Role: We're seeking passionate BI/SQL Developer to join our team. You'll be at the heart of developing and implementing high-quality application software, using state-of-the-art tools and technologies. This is your chance to make a significant impact on one of the most exciting and unique products in the Polish digital insurance market. Why it is worth to work with us? You‚Äôll be contributing to the reporting solution and data model design of our Data Platform with close cooperation with our Data Engineers You‚Äôll gather business requirements and work closely with business stakeholders You‚Äôll deliver end-to-end business intelligence/reporting solutions using SQL/PowerBI What you need to start the adventure with us: +1 year of commercial experience in data extraction, ETL, and report development Proficiency in SQL Understanding of Relational Database Management System and Business Intelligence concepts Business and collaboration skills, and responsive to service needs and operational demands Domain knowledge gained across the insurance or financial sector Nice to have: Experience with BI tools (Power BI would be a plus) Experience with cloud solutions (we use Azure) Familiarity with code version control systems such as GIT Understanding of the principles of Agile and Scrum (we work in Scrum) Enthusiastic approach to coffee breaks (we love informal discussions with a cup of favorite coffee or tea) Why Join Us? Be part of a dynamic team driving digital innovation in the insurance and eCommerce sectors Opportunity to work in a collaborative and forward-thinking environment Contract options: B2B cooperation Engage in meaningful work that directly impacts business success Join a company that values work-life balance and fosters a positive team culture Comprehensive onboarding, including a dedicated Buddy program Remote work flexibility with hybrid office visits Flexible working hours Access to the latest tools and cloud-native solutions A comprehensive benefits package, including health insurance and MultiSport card Employee discounts on insurance products Referral program and sports club memberships Sounds interesting? Join us and help shape the future! üöÄ","[{""min"": 9000, ""max"": 12000, ""type"": ""Net per month - B2B""}, {""min"": 9000, ""max"": 12000, ""type"": ""Gross per month - Permanent""}]",Database Administration,9000,12000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,569,Azure Data Engineer z kompetencjami Devopsowymi,UNIVIO,"Jeste≈õmy polskƒÖ firmƒÖ technologicznƒÖ z ponad 25-letnim do≈õwiadczeniem jako partner cyfrowej transformacji handlu. Realizujemy miƒôdzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocze≈õnie lu≈∫nƒÖ, niezobowiƒÖzujƒÖcƒÖ atmosferƒô. Nasza organizacja opiera siƒô na kulturze otwarto≈õci i dzielenia siƒô wiedzƒÖ. Dowiedz siƒô kogo szukamy i zaaplikuj, je≈õli spe≈Çniamy Twoje oczekiwania üòâ","[{""min"": 16800, ""max"": 23520, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 17300, ""type"": ""Gross per month - Permanent""}]",Data Engineering,13600,17300,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,581,Machine Learning Engineer (LLM),UNIVIO,"Jeste≈õmy polskƒÖ firmƒÖ technologicznƒÖ z ponad 25-letnim do≈õwiadczeniem jako partner cyfrowej transformacji handlu. Realizujemy miƒôdzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocze≈õnie lu≈∫nƒÖ, niezobowiƒÖzujƒÖcƒÖ atmosferƒô. Nasza organizacja opiera siƒô na kulturze otwarto≈õci i dzielenia siƒô wiedzƒÖ. Dowiedz siƒô kogo szukamy i zaaplikuj, je≈õli spe≈Çniamy Twoje oczekiwania üòâ","[{""min"": 16800, ""max"": 20160, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 16500, ""type"": ""Gross per month - Permanent""}]",Data Science,13600,16500,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,612,Senior Data Engineer,XTB,"Tworzymy XTB ‚Äì globalnƒÖ firmƒô inwestycyjnƒÖ, oferujƒÖcƒÖ innowacyjne rozwiƒÖzania technologiczne, kt√≥re pozwalajƒÖ naszym klientom skutecznie zarzƒÖdzaƒá swoimi finansami na wiele sposob√≥w. Wszystko to w jednej intuicyjnej aplikacji XTB, z kt√≥rej korzysta ju≈º ponad milion u≈ºytkownik√≥w na ca≈Çym ≈õwiecie! Jeste≈õmy certyfikowanƒÖ firmƒÖGreat Place to Work. Poszukujemy osoby, kt√≥ra do≈ÇƒÖczy do naszego zespo≈Çu Data Platform w roli Data Engineer. G≈Ç√≥wnym zadaniem zespo≈Çu jest utrzymanie i rozw√≥j hurtowni danych on premise i jej docelowa migracja na ≈õrodowisko chmurowe. Wsp√≥≈Çpracujemy z zespo≈Çami produktowymi w celu dostarczenia danych potrzebnych do podejmowania kluczowych decyzji biznesowych. Pracujemy w frameworku Scrum. Do Twoich codziennych obowiƒÖzk√≥w bƒôdzie nale≈ºa≈Ço: Projektowanie i utrzymywanie hurtowni danych oraz migracja danych i proces√≥w na ≈õrodowisko chmure, Tworzenie i utrzymywanie modeli danych do wspierania kluczowych decyzji biznesowych, Integracja danych w celu wytworzenia wymaganych modeli danych, Wdra≈ºanie kontroli jako≈õci danych i proces√≥w walidacji, aby zapewniƒá dok≈Çadno≈õƒá, sp√≥jno≈õƒá i kompletno≈õƒá, Wsp√≥≈Çpraca z innymi zespo≈Çami, aby tworzyƒá zestawy danych spe≈ÇniajƒÖce potrzeby raportowania i rozwiƒÖzujƒÖce problemy systemowe, Projektowanie i wykonywanie test√≥w wydajno≈õciowych i integracyjnych, Raportowanie kluczowych wska≈∫nik√≥w w firmie w narzƒôdziu BI Wymagania: Co najmniej 4-letnie do≈õwiadczenie w pracy na stanowisku SQL Developer / Data Engineer Znajomo≈õƒá Python, SQL w tym T-SQL, pisanie z≈Ço≈ºonych procedur sk≈Çadowanych, optymalizacja wydajno≈õci, Umiejƒôtno≈õƒá tworzenia proces√≥w ETL (SSIS, Airflow), Do≈õwiadczenie zawodowe w eksploracji danych, analizie i modelowaniu z≈Ço≈ºonych zbior√≥w danych na du≈ºƒÖ skalƒô; Do≈õwiadczenie w pracy z narzƒôdziami do zarzƒÖdzania kodem ≈∫r√≥d≈Çowym, takimi jak GIT Dobra znajomo≈õƒá zasad standard√≥w integracyjnych: REST, gRPC Umiejƒôtno≈õƒá tworzenia rozwiƒÖza≈Ñ w oparciu o serwisy w Snowflake Do≈õwiadczenie w tworzeniu, wdra≈ºaniu i rozwiƒÖzywaniu problem√≥w z aplikacjami danych na platformie Microsoft Azure Znajomo≈õƒá rozwiƒÖza≈Ñ chmurowych (Azure, GCP), Silne umiejƒôtno≈õci rozwiƒÖzywania problem√≥w i dba≈Ço≈õƒá o szczeg√≥≈Çy. Umiejƒôtno≈õƒá skutecznej komunikacji i wsp√≥≈Çpracy w zespole. Chƒôƒá uczenia siƒô i dostosowywania do nowych technologii i koncepcji. Rozumienie zasad Agile i Scrum (pracujemy w Scrumie) Dodatkowe atuty: Do≈õwiadczenie w budowaniu skalowalnych, dzia≈ÇajƒÖcych w czasie rzeczywistym rozwiƒÖza≈Ñ typu Data Lake, Do≈õwiadczenie ze strumieniowym przesy≈Çaniem danych (Kafka), Do≈õwiadczenie w pracy z Kubernetes Znajomo≈õƒá koncepcji Data Mesh. Znajomo≈õƒá podej≈õcia DevOps Oferujemy Realny wp≈Çyw na rozw√≥j firmy i produktu Pracƒô w do≈õwiadczonym zespole, kt√≥ry chƒôtnie dzieli siƒô wiedzƒÖ JasnƒÖ wizjƒô rozwoju dziƒôki regularnym feedbackom i klarownym ≈õcie≈ºkom karier Bud≈ºet szkoleniowy na interesujƒÖce Ciƒô kursy i konferencje Dodatkowy dzie≈Ñ wolny z okazji Twoich urodzin Dodatkowy dzie≈Ñ wolny dla rodzic√≥w Sprzƒôt dopasowany do Twoich potrzeb PrywatnƒÖ opiekƒô medycznƒÖ i ubezpieczenie grupowe Dostƒôp do platformy e-learningowej do nauki jƒôzyka angielskiego oraz platformy benefitowej Dostƒôp do platformy wellbeingowej i mo≈ºliwo≈õƒá skorzystania z warsztat√≥w oraz prywatnych sesji terapeutycznych Pracƒô zdalnƒÖ, z biura w Warszawie lub z coworku w Twoim mie≈õcie Regularne spotkania integracyjne","[{""min"": 15000, ""max"": 19000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14000,18000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,627,Data Architect / Data Warehouse Specialist,Power Media,"Nasz klient kompleksowo realizuje us≈Çugi w zakresie konsultingu, projektowania i zarzƒÖdzania projektami. Specjalizuje siƒô g≈Ç√≥wnie w obszarach e-commerce, data management (Agile Data Warehouse, Data Governance) oraz Content Management. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozw√≥j rozwiƒÖza≈Ñ Data Platform w ≈õrodowisku chmurowym. Kluczowym zadaniem bƒôdzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejs√≥w w z≈Ço≈ºonym krajobrazie systemowym klienta. Projekt obejmuje budowƒô hurtowni danych oraz platform danych od podstaw. Zesp√≥≈Çsk≈Çada siƒô z 22 os√≥b: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiƒÖzk√≥w: Rozw√≥j i utrzymanie Data Warehouse oraz Data Platform przy u≈ºyciu narzƒôdzi ETL i SQL, Analiza wymaga≈Ñ biznesowych i proponowanie rozwiƒÖza≈Ñ technicznych, Wsp√≥≈Çpraca z zespo≈Çem w celu zapewnienia dostƒôpno≈õci rozwiƒÖza≈Ñ biznesowych, Wykorzystanie swojej wiedzy i do≈õwiadczenia do tworzenia innowacyjnych rozwiƒÖza≈Ñ. G≈Ç√≥wne wymagania: Bardzo dobra znajomo≈õƒá metod i technik projektowania oprogramowania, Ponad 10-letnie do≈õwiadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych, Znajomo≈õƒá ekosystem√≥w Big Data, Znajomo≈õƒározwiƒÖza≈Ñ chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejƒôtno≈õci analityczne (analiza i dokumentowanie wymaga≈Ñ biznesowych oraz specyfikacji technicznych) Do≈õwiadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomo≈õƒá SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomo≈õƒá rozwiƒÖza≈Ñ ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomo≈õƒá jƒôzyka angielskiego (min. B2+/C1)‚Äì codzienna praca w miƒôdzynarodowym ≈õrodowisku, Gotowo≈õƒá do podr√≥≈ºy s≈Çu≈ºbowych. Mile widziane: Znajomo≈õƒánarzƒôdzi Informatica, Znajomo≈õƒá Machine Learning oraz framework√≥w ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomo≈õƒá jƒôzyk√≥w programowania (Java, Scala, C++, Python). Znajomo≈õƒá jƒôzyka niemieckiego. Firma oferuje: Stabilne, d≈Çugofalowe zatrudnienie w oparciu o B2B lub UoP, Mo≈ºliwo≈õƒá pracy w wiƒôkszo≈õci zdalnej(praca z biura 1 raz w tygodniu, we wtorek) P≈Çaska struktura, antykorporacyjne podej≈õcie do pracy i zespo≈Çu, Ciekawe, miƒôdzynarodowe projekty, Zgrany zesp√≥≈Ç chƒôtnie uczestniczƒÖcy w aktywno≈õciach sportowo ‚Äì rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team ‚Äì building), Dodatkowe zajƒôcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajƒôƒá sportowych, Nowoczesne biuro ze strefƒÖ relaksu, Co roczny 5 dniowy wyjazd firmowy (ca≈Ça firma), warsztaty kulinarne, wyj≈õcia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywno≈õciami ≈öwietna atmosfera, partnerskie podej≈õcie, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa ‚Äûmiƒôkko-techniczna‚Äù. CV w j. angielskim.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,16000,18000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,648,Senior Data Engineer with Snowflake (greenfield),N-iX,"#3359 Join an exciting journey to create a greenfield, cutting-edge Consumer Data Lake for a leading global organization based in Europe. This platform will unify, process, and leverage consumer data from various systems, unlocking advanced analytics, insights, and personalization opportunities. As a Senior Data Engineer, you will play a pivotal role in shaping and implementing the platform's architecture, focusing on hands-on technical execution and collaboration with cross-functional teams. Your work will transform consumer data into actionable insights and personalization on a global scale. Using advanced tools to tackle complex challenges, you‚Äôll innovate within a collaborative environment alongside skilled architects, engineers, and leaders. Key Responsibilities: Hands-On Development : Build, maintain, and optimize data pipelines for ingestion, transformation, and activation. Create and implement scalable solutions to handle diverse data sources and high volumes of information. Data Modeling & Warehousing : Design and maintain efficient data models and schemas for a cloud-based data platform. Develop pipelines to ensure data accuracy, integrity, and accessibility for downstream analytics. Collaboration : Partner with Solution Architects to translate high-level designs into detailed implementation plans. Work closely with Technical Product Owners to align data solutions with business needs. Collaborate with global teams to integrate data from diverse platforms, ensuring scalability, security, and accuracy. Platform Development : Enable data readiness for advanced analytics, reporting, and segmentation. Implement robust frameworks to monitor data quality, accuracy, and performance. Testing & Quality Assurance : Implement robust security measures to protect sensitive consumer data at every stage of the pipeline Ensure compliance with data privacy regulations (e.g., GDPR, CCPA ..) and internal policies. Monitor and address potential vulnerabilities, ensuring the platform adheres to security best practices. Requirements: Over 4+ years of experience showcasing technical expertise and critical thinking in data engineering. Hands-on experience with DBT and strong Python programming skills. Proficiency in Snowflake and expertise in data modeling are essential. Demonstrated experience in building consumer data lakes and developing consumer analytics capabilities is required. In-depth understanding of privacy and security engineering within Snowflake , including concepts like RBAC, dynamic/tag-based data masking, row-level security/access policies, and secure views. Ability to design, implement, and promote advanced solution patterns and standards for solving complex challenges. Familiarity with multiple cloud platforms ( Azure or GCP preferred, with a focus on Azure). Practical experience with Big Data batch and streaming tools. Competence in SQL, NoSQL, relational database design (SAP HANA experience is a bonus), and efficient methods for data retrieval and preparation at scale. Proven ability to collect and process raw data at scale, including scripting, web scraping, API integration, and SQL querying. Experience working in global environments and collaborating with virtual teams. A Bachelor‚Äôs or Master‚Äôs degree in Data Science, Computer Science, Economics, or a related discipline. We offer*: Flexible working format - remote, office-based or flexible. A competitive salary and good compensation package. Personalized career growth. Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more). Active tech communities with regular knowledge sharing. Education reimbursement Memorable anniversary presents. Corporate events and team building. Other location-specific benefits. *not applicable for freelancers.","[{""min"": 22164, ""max"": 29553, ""type"": ""Net per month - B2B""}, {""min"": 17731, ""max"": 24566, ""type"": ""Gross per month - Permanent""}]",Data Engineering,17731,24566,Gross per month - Permanent
