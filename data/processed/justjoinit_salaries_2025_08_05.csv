Type of work,Experience,Employment Type,Operating mode,Job ID,Job Title,Employer Name,Job Description,Salaries,Category,min,max,type
Full-time,Mid,B2B,Hybrid,0,System Engineer,Radpoint,"Poszukujemy Inżyniera ds. Wdrożeń, który będzie odpowiedzialny za wdrażanie złożonych i zintegrowanych systemów informatycznych w przedsiębiorstwach i instytucjach. Oferujemy pracę w dynamicznym środowisku, w którym Twoje umiejętności i doświadczenie będą doceniane. Jeśli spełniasz poniższe wymagania zachęcamy do aplikowania! Wymagania: Co najmniej rok pracy jako Inżynier odpowiedzialny za wdrożenia złożonych, zintegrowanych systemów informatycznych w przedsiębiorstwach lub instytucjach. Umiejętność prowadzenia szkoleń aplikacyjnych. Praktyczne doświadczenie w integracji systemów informatycznych w tym analiza i tworzenie specyfikacji technicznych Znajomość Excela, umiejętność analizy danych oraz podstawowa znajomość języka SQL Jako dodatkowy atut: Znajomość obszaru ochrony zdrowia oraz doświadczenie w radiologii i diagnostyce obrazowej Znajomość i doświadczenie w pracy z MIRTH, HL7, DICOM oraz standardów IHE Praktyczna znajomość podstawowych zagadnień związanych z wdrożeniami w sektorze publicznym- protokoły i planowanie Znajomość zagadnień związanych z rozliczeniami NFZ Doświadczenie w zarządzaniu danymi Zadania: Przeprowadzanie szkoleń aplikacyjnych. Wykonywanie integracji i testów integracyjnych z udziałem wielu zewnętrznych dostawców systemów IT. Planowanie i prowadzenie testów integracyjnych. Dokonywanie wdrożeń w przedsiębiorstwach i instytucjach na terenie Polski. Oferujemy: Stabilne zatrudnienie w nowoczesnym środowisku pracy Możliwość rozwoju zawodowego i zdobycia doświadczenia w projektach różnego rodzaju core hours 9: 00-15: 00 Możliwość zdalnego świadczenia usług, jednak chętnie zobaczymy Cię w biurze 😉 Wynagrodzenie adekwatne do umiejętności i doświadczenia Jak rekrutujemy? Pierwszy etap to techniczna rozmowa online trwająca około 1-1,5h Etap drugi to spotkanie w biurze/online składające się ze swobodnej rozmowy i części negocjacyjnej, gdy już wiemy, że chcemy współpracować","[{""min"": 6800, ""max"": 9000, ""type"": ""Net per month - B2B""}]",Data Engineering,6800,9000,Net per month - B2B
Full-time,Senior,B2B,Remote,1,Senior Database Administrator / Lead,Experis Manpower Group,"We’re seeking a highly skilled and experienced Senior Database Administrator (DBA) to join our dynamic IT team. This position requires a deep understanding of database technologies, strong problem-solving skills, and the ability to lead a team effectively. Responsibilities: Database Management: • Design, implement, and maintain database systems to ensure high availability, performance, and security • Monitor database performance and troubleshoot issues to optimise system efficiency • Perform regular database backups, recovery, and disaster recovery planning Leadership and Team Management: • Lead and mentor a team of database administrators, providing guidance and support in their professional development • Collaborate with cross-functional teams to define database requirements and ensure alignment with organisational goals • Foster a culture of continuous improvement and innovation within the database team Strategic Planning: • Develop and implement database strategies that align with overall IT strategy • Stay current with emerging database technologies and trends, making recommendations for upgrades and enhancements Documentation and Compliance: • Maintain comprehensive documentation of database configurations, procedures, and policies • Ensure compliance with data governance and security policies User Support: • Provide technical support and training to end-users and other IT staff as needed • Collaborate with application developers to optimize database performance for applications Requirements: • Bachelor's degree in Computer Science, Information Technology, or a related field • Minimum of 5 years of experience as a Database Administrator, with a focus on leadership roles • Strong knowledge of database management systems (e.g., Oracle, SQL Server, MySQL) • Experience with database design, performance tuning, and optimisation • Proven leadership and team management skills • Excellent problem-solving and analytical abilities • Strong communication and interpersonal skills • Experience with cloud-based database solutions (e.g., AWS, Azure) • Knowledge of data warehousing and business intelligence concepts • Familiarity with database security best practices Our offer: • B2B via Experis • 100% remote work • MultiSport Plus • Group insurance • Medicover Premium • e-learning platform","[{""min"": 144, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Database Administration,144,160,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,7,Programista PL/SQL ze znajomością integracji,4IT Solutions,"Poszukujemy Programisty PL/SQL ze znajomością integracji dla naszego klienta – globalnego lidera w dziedzinie baz danych, rozwiązań chmurowych i oprogramowania biznesowego. To wyjątkowa okazja do pracy dla jednej z wiodących firm tworzących kompleksowe oprogramowanie bazodanowe oraz nowoczesne rozwiązania chmurowe oparte na sztucznej inteligencji. Dzięki ciągłemu rozwojowi technologii oraz inwestycjom w nowoczesne rozwiązania, firma pozostaje kluczowym graczem w sektorze IT, dostarczając innowacyjne produkty dla firm na całym świecie. Dynamiczny rozwój oraz globalna społeczność ekspertów IT sprawiają, że jest to idealne miejsce dla ambitnych specjalistów. Na początek konkrety: Możliwa forma współpracy: B2B Stawka: 130-150 PLN/h Tryb pracy: hybryda Warszawa (1-2x w tygodniu w biurze) Wymiar pracy: 1FTE O projekcie: Projekt dotyczy rozwoju kodu w PL/SQL oraz integracji systemów w architekturze SOA dla klienta z sektora publicznego. Zakres obowiązków: Modelowanie danych oraz tworzenie struktur danych na potrzeby integracji systemów w architekturze SOA Rozwój i optymalizacja kodu PL/SQL, w tym implementacja zapytań, procedur składowych i interfejsów usług z naciskiem na wydajność i skalowalność Analiza i optymalizacja zapytań SQL na dużych zbiorach danych Praca zgodnie z wymaganiami SLA (Service Level Agreement) Wymagania: Minimum 5-letnie doświadczenie na podobnym stanowisku Bardzo dobra znajomość SQL i PL/SQL Doświadczenie w pracy z dużymi zbiorami danych Umiejętność optymalizacji zapytań w bazie danych Oracle Kompetencje w projektowaniu oraz implementacji interfejsów usług w architekturze SOA Praktyczne doświadczenie w wykorzystywaniu SOAP, REST, XQuery, XSLT w projektach integracyjnych Mile widziane: Certyfikaty Oracle PL/SQL Znajomość Oracle Service Bus, Oracle SOA Suite, Oracle Weblogic ServerZa Oferujemy: Atrakcyjna lokalizacja biura Brak dress code’u Możliwość pracy zdalnej Przyjazna niekorporacyjna atmosfera Spotkania integracyjne Opis procesu rekrutacji: Wstępna rozmowa telefoniczna z naszym przedstawicielem - 4IT Solutions (ok 20min) Zdalna rozmowa techniczna z osobami z zespołu naszego Klienta Spotkanie zapoznawcze z Managerem w biurze ... i finalizujemy rozmowy","[{""min"": 130, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,130,150,Net per hour - B2B
Full-time,Mid,B2B,Remote,8,Data Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients’ systems, departments and sites. We provide an open technology platform that’s shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience.Want to be part of it? Job Title: Data Platform Engineer Job Summary: We are seeking a highly skilled and experienced Data Engineer to join our team. The successful candidate will be responsible for developing and maintaining our Data Lake and existing Data Pipelines, as well as continually exploring, analyzing, and proposing improvements to existing processes and tooling. They will also be responsible for ensuring best practices are being adopted and staying up to date with the latest research and trends in Data Engineering. Skills Required: Strong background in Data Engineering, with experience in developing and maintaining Data Pipelines and Data Lakes Proven record of accomplishment of staying up to date with the latest research and best practices in Data Engineering Excellent technical skills in Data Engineering tools and technologies Advanced proficiency in Python and SQL Understanding of AWS cloud technologies, including infrastructure as code (CDK preferred) Effective communication and interpersonal skills, with the ability to work effectively with stakeholders at all levels Strong understanding of information security and data protection principles Experience in driving technical and career development, creating appropriate goals and seeking learning opportunities within the company and the wider software community Good understanding and prior experience of the Agile process (Scrum or Kanban) Fluency with software design patterns Experience working with automotive retail technology would be a distinct advantage Key Responsibilities: Maintain and develop the Data Lake and existing Data Pipelines to support the product and data teams’ requirements Continuously explore, analyze, and propose improvements to existing processes and tooling Stay up to date with the latest research, trends and best practices in Data Engineering Support the Business Intelligence team and wider Company in querying centralized data stores, including the Data Lake Work within department to maintain an ongoing understanding of the company’s data strategy and roadmap Proactively report on issues and problems Work independently, manage day-to-day workload and priorities, and take accountability for direction and output Drive your own technical and career development, create appropriate goals, and seek learning opportunities within the company and the wider software community Support colleagues on calls or in meetings with clients, partners, and suppliers as required Maintain systems under the team’s control, including user and access management Support colleagues and HR with onboarding as well as offboarding processes Ensure information security, data protection and support the business in complying with any legal obligations imposed upon it through positive actions Technologies: Python SQL: Trino, Spark-SQL, Hive, TSQL AWS Cloud services (including: s3, step functions, glue, CDK) Terraform Linux Windows Why join us? We’re on a journey to become market leaders in our space – and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We’re committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles – not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn’t require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply.","[{""min"": 18000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Engineering,18000,23000,Net per month - B2B
Full-time,Mid,Permanent or B2B,Hybrid,9,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for aData Engineerto join us to provide value to our customers in line with the Volue mission. In Volue Insight we enable our customers to make data driven decisions – spanning from creating pricing models for their energy products, when to buy energy, to invest in renewable power plants or power-consuming industry. In the fuels team, we provide actuals and forecasts for prices, production levels, flows for the gas markets and conventional power plant operators. We build and maintain data pipelines, collect and process data from external sources, craft mathematical models, analyze time series, train machine learning models, build web applications, and enjoy working together. What you will be doing to make a difference: Design, build and maintain flexible and scalable end-to-end data pipelines for forecasting and prediction models. Contribute to our continuous push towards highly scalable and automated data pipelines and forecasting models where performance is monitored, quality is continuously evaluated, and experimentation is easy. Take part in the entire lifecycle of our models, from initial concept to deployment and ongoing maintenance, ensuring reliability and performance. Implement automated tests, participate in peer code reviews, and embrace continuous integration practices to ensure robust, maintainable code. What you need to succeed: A Bachelor’s or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Reasons to join Volue team and what we offer: Large degree of influence in shaping and developing the role further Great colleagues in one of Europe’s most exciting green tech companies with innovative and international work environment Flexible working hours and competitive compensation package, which includes a Multisport card, group life insurance, private healthcare, English classes, memorable offsite events, outstanding referral programme and access to various sports groups. In Volue, we cherish each employee’s competence, ideas and personality. Let your skills and talent be a part of our team – and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14000,22000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,10,Senior Data Engineer,Winged IT,"Do you want to play a key role in revolutionizing the future of finance? Join our Client - a global leader in deferred payments, with over 85 million active users and 2.5 million transactions processed daily! As a pioneer in modern payment solutions, our Client is developing innovative methods that streamline the shopping experience, enhance transaction security, and expand the availability of purchasing options. We're seeking individuals eager to achieve remarkable results and share their bold vision to redefine the future of payments and fintech. Send us your CV - we can’t wait to meet you! Your role is: -> To design and implement robust, scalable data pipelines to collect, integrate, and analyze large volumes of data from various sources including Jira, AWS S3, and external websites via APIs; -> To develop and maintain databases and data tables that are optimized for performance and scalability within Klarna’s cloud environment, leveraging AWS services; -> To collaborate with different teams to understand data needs and deliver solutions that support business objectives; -> To ensure data integrity and compliance with data governance and security policies; -> To provide technical leadership and mentorship to data analysts in the team; Stay current with industry trends and evaluate new technologies for continuous improvements in data architecture and processing; -> To work closely with other teams to leverage Klarna’s internal systems for optimized data management and operations; -> To assist DevOps or Full Stack engineers, when necessary, to achieve team objectives. Your skills and experiences: -> 5+ years of experience in data engineering, particularly in designing and developing data pipelines; -> Strong programming skills in Python and experience with API integrations; -> Extensive experience with AWS Cloud Services (e.g., S3, EC2, RDS, Lambda, Glue Jobs, EMR) and understanding of best practices in cloud security; -> Experience with Terraform and infrastructure as a code; -> Proficiency in SQL and experience with relational and NoSQL databases; -> Experience with graph and vector databases; -> Capability to handle multiple high-priority tasks simultaneously and meet tight deadlines without sacrificing detail or accuracy; -> Excellent interpersonal skills to engage with colleagues across various teams, contributing to fast-paced projects; -> Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field; -> Language proficiency: Advanced English (minimum B2 level). Nice to have: + DevOps experience and skills will be viewed as a huge plus; + FullStack or Development experience and skill will be viewed as a huge plus; + Familiarity with Klarna’s internal systems, such as C2C and Data Platform services, is highly preferred. Our client offers: + Great opportunity for personal development in a stable and friendly large multinational company; + Start-up mentality, small agile teams; + Global Reach: Impact millions with seamless shopping and payments; + Career growth and additional education. ﻿","[{""min"": 170, ""max"": 195, ""type"": ""Net per day - B2B""}]",Data Engineering,170,195,Net per day - B2B
Full-time,Mid,B2B,Remote,12,Data Engineer,Britenet,"Naszym klientem jest znana, globalna firma z branży retail. Zostaliśmy zaangażowani przez klienta jako zespół Ekspertów ds. danych do kluczowego projektu wdrożeniowego. Oczekiwania: Minimum 3 lata doświadczenia w IT (preferowane w zespole Data/Software) Bardzo dobra znajomość Pythona, wykorzystywanego do tworzenia aplikacji opartych na mikroserwisach i usługach REST API Doświadczenie w projektowaniu i budowaniu aplikacji w architekturze mikroserwisowej Praktyczne doświadczenie z Google Cloud Platform (GCP) Znajomość narzędzi do orkiestracji i automatyzacji procesów danych (np. Apache Airflow) Dobra znajomość SQL oraz relacyjnych baz danych (np. MS SQL, PostgreSQL) Doświadczenie w pracy z Google BigQuery Znajomość i umiejętność pracy z Dockerem Doświadczenie w analizie i integracji danych z różnych źródeł, w tym niestandardowych (API, pliki, systemy zewnętrzne) Umiejętność samodzielnego rozwijania i utrzymywania aplikacji o istotnym znaczeniu biznesowym Znajomość języka angielskiego na poziomie komunikatywnym (minimum B2) Mile widziane: Umiejętność uruchamiania i konfigurowania infrastruktury chmurowej oraz CI/CD (np. z użyciem Azure Pipelines) Znajomość Microsoft Azure Zadania: Tworzenie i rozwój aplikacji pośredniczących (middleware) między systemami e-commerce (np. Ocado Smart Platform) a wewnętrznymi systemami firmy (np. kasy, fakturowanie, logistyka) Budowa i rozwój mikroserwisów backendowych w Pythonie Integracja danych z systemów magazynowych i e-commerce do ekosystemu firmy Projektowanie i wdrażanie przepływów danych w chmurze (BigQuery, GCP, Azure) Rozwój ekosystemu danych – wsparcie dla Data Science, analityków i raportowania Utrzymanie i rozwój istniejących aplikacji oraz wdrażanie nowych funkcjonalności Współpraca z analitykami, kierownikami projektów i zespołem Data (4–5 osób), w tym przejmowanie wiedzy domenowej Praca z dokumentacją techniczną i architekturą danych","[{""min"": 100, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,100,150,Net per hour - B2B
Full-time,Senior,B2B,Remote,13,Remote Data Engineer/ Analytics Engineer,Ework Group,"Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. 🔹 For our Client we are looking forSenior Analytics Engineer🔹 ✔️The Position Join our Global Commercial IT team as aSenior Analytics Engineerand take part in transforming our Common Data Platform (CDP), which integrates and delivers business-critical data across 43 European and Canadian affiliates. You’ll be joining a product team responsible for building and maintaining our AWS-based Data Lake, DBT Cloud transformation pipelines, and Snowflake data warehouse, enabling high-quality commercial and engagement insights across channels. This role goes beyond coding – we’re looking for someone with strong technical expertise, a proactive mindset, and the ability to build foundations where there are none, improve data quality and reliability, and collaborate closely with business-facing product owner. ✔️ Key Responsibilities: Design, build, and maintain robust ETL/ELT pipelines in DBT Cloud, ingesting data from sources like OCE IQVIA, Adobe Analytics, Qualtrics, Salesforce CIAM, and others Contribute to the migration of legacy Snowflake and Qlik environments into the unified EUCAN CDP instance Implement data quality validation frameworks (e.g., DBT tests, freshness checks, anomaly detection) Support and improve our RUN Management process: monitoring, reruns, root-cause analysis and preventive actions Maintain and document model ownership, schema logic and transformations within our CDW_CORE > CDW_STAGE > CDW_PROD architecture Collaborate closely with Product Owner, Data Architect, and affiliate stakeholders to ensure delivery of trusted, scalable and governed data products Mentor junior engineers and act as a point of escalation for complex data issues ✔️ Tech Stack & Environment: DBT Cloud (core ELT engine) Snowflake (Data Warehouse) AWS S3 (Data Lake, ingestion zone) GitHub, Azure DevOps Data sources: IQVIA OCE, Salesforce Marketing Cloud, Adobe AEM/Analytics, Shopify, Qualtrics, Accutics, Facebook/LinkedIn/Instagram ✔️ Must-Have Qualifications: 5+ years of experience as a Data Engineer working with cloud-based ELT pipelines Expert-level SQL and DBT modeling skills Deep understanding of data warehousing concepts (dimensional modeling, incremental loading, performance optimization) Experience with Snowflake and cloud data architecture Strong communication and documentation skills – must work well in distributed teams Proactive attitude: ownership over data quality, incident handling and delivery discipline ✔️ Nice-to-Haves: Experience in the pharmaceutical or commercial and Sales data domains (CRM, HCP, field force) Exposure to GDPR and data privacy requirements in commercial data ✔️ We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 170, ""max"": 196, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,170,196,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,15,Data Engineer,Hays Poland,"Data Engineer - SQL Developer Refactor and re-architect solutions to get rid of legacy Handle incidents and support daily operations Help design and build data pipelines and integrations Work on data integration, transformation, and storage Collaborate with business users to improve existing systems Suggest process improvements (aligned with GxP standards) Document your work clearly and consistently Support cloud-native data services and APIs Join project planning and execution for new and existing apps Provide on-call support for key data services when needed Skills in SQL, T-SQL, C#, SSIS, and VBA Understanding of ETL/ELT, data modeling, and API integration Familiarity with DevOps, version control, and CI/CD pipelines Awareness of data governance and compliance (GxP, GDPR) A sharp eye for detail and a problem-solving mindset Great communication skills and a team-first attitude Career in an Organization with Scandinavian Culture and Values: Experience a work environment that emphasizes equality, work-life balance, and sustainability. Work in a hybrid model (3 days in the office per week). Potential for a permanent contract after an initial 3-month period. Comprehensive medical coverage through Medicover. Sports Card: Access to various sports facilities and activities. Life insurance coverage for added security. Opportunity to be involved in the transition of processes. Attractive and competitive salary.","[{""min"": 11000, ""max"": 17000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,11000,17000,Gross per month - Permanent
Full-time,Mid,B2B,Remote,16,Data Vault Architect with Snowflake,Link Group,"Opis stanowiska: Poszukujemy doświadczonegoData Vault Architecta, który wesprze naszego klienta w strategicznym przeglądzie, planowaniu i transformacji istniejącej platformy danych. Osoba na tym stanowisku będzie odpowiedzialna nie tylko za ocenę obecnej architektury, ale również za opracowanie wizji docelowej, zarządzanie interesariuszami oraz nadzór nad realizacją techniczną i zespołem wdrożeniowym. Projekt ma kluczowe znaczenie dla poprawy efektywności i skalowalności systemów danych opartych oSnowflake, DBT oraz AWS. Przegląd i ocena istniejącej platformy danych klienta oraz zaproponowanie optymalnej ścieżki rozwoju. Tworzenie strategii i wizji docelowej dla nowoczesnej platformy danych z wykorzystaniem Data Vault 2.0. Koordynacja dostarczania rozwiązań, planowanie działań oraz nadzór nad ich realizacją. Zarządzanie zmianą oraz skuteczna współpraca z interesariuszami biznesowymi i technicznymi. Możliwość objęcia roli menedżerskiej oraz budowania zespołu technicznego. Współpraca z zespołami inżynieryjnymi, analitycznymi oraz DevOps. Must-have: Doświadczenie w projektowaniu i wdrażaniu architektury danych w modeluData Vault 2.0. Bardzo dobra znajomość platformySnowflake. Praktyczne doświadczenie z narzędziemDBT (Data Build Tool). Znajomość i doświadczenie z infrastrukturąAWS. Umiejętność zarządzania interesariuszami oraz prowadzenia inicjatyw transformacyjnych. Zdolność do definiowania strategii technicznej i architektonicznej.","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Architecture,160,170,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,21,Data Engineer,Vaillant Group Business Services,"What we achieve together We are looking for an experienced Data Engineer to join our innovative team at Vaillant. In this role, you will be instrumental in advancing our data infrastructure, driving analytics excellence, and leveraging cutting-edge technologies to transform data into actionable insights. If you are passionate about making a meaningful impact and eager to collaborate with a team that appreciates your expertise, we encourage you to apply. You will design, build, and maintain robust infrastructure and programs for efficient data extraction, transformation, loading, and serving. You will leverage Azure Cloud services, Spark technologies, and DataOps practices to handle large data volumes from diverse sources such as IoT, CRM, ERP, and PLM. You will play a pivotal role in supporting the design, development, maintenance, and automation of data products on our data platform, contributing significantly to data governance and security compliance. Ensuring data quality and accuracy is your priority, and you will implement validation and cleansing processes to achieve this. You will collaborate with a dynamic team of DevOps engineers, data scientists, and data engineers, and you will engage in advanced analytics of machine data, business process-related data, and customer data, enhancing Vaillant Group’s digital service portfolio. Your effective communication skills will shine as you work with cross-functional colleagues, sharing innovative ideas and fostering a collaborative environment. Agile methodologies are your preferred approach, allowing you to work seamlessly with the team and stakeholders. What makes us successful together Qualification: You hold a university degree, preferably in Computer Science or a related field, with a focus on Data Science, Data Mining, or Data Analytics. Experience: You bring at least 3 years of relevant experience in data analytics, particularly within a Big Data context. Know-how and skills: Your proficiency in Python (PySpark) and SQL is exceptional, and you are adept at working with cloud services and related components, ideally within the Microsoft Azure ecosystem and Databricks. Nice to have: Experience with additional data platforms and tools will be a plus. Personality: You thrive in a team setting, showcasing a high degree of initiative and motivation to excel in an agile and interdisciplinary environment. Language skills: Fluency in English is essential, enabling effective communication and collaboration across our global teams. What you can count on Hybrid work and environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee. Package of additional benefits: private medical care, multi-sport card. Onboarding: our clearly structured onboarding process, including an Onboarding App, enables us to integrate new employees into Vaillant Group quickly and in a targeted manner. Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings.","[{""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16000,20000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,22,DWH Architect (public/healthcare),Britenet,"Projekt dla instytucji odpowiedzialnej za rozwój i utrzymanie systemów informatycznych wspierających funkcjonowanie ochrony zdrowia w Polsce. Oczekiwania: Minimum 5 lat doświadczenia zawodowego na stanowisku Architekta IT; Doświadczenie zawodowe w zakresie projektowania architektur systemów zorientowanych na usługi, systemów w architekturze wielowarstwowej, systemów o wysokiej wydajności i niezawodności; Doświadczenie projektowe w szacowaniu pracochłonności prac programistycznych i architektonicznych; Doświadczenie projektowe w szacowaniu złożoności aplikacji/ rozwiązania (ilość komponentów, wielkość komponentów), skalowanie aplikacji horyzontalne i wertykalne; Doświadczenie projektowe w zakresie badania i oceny bezpieczeństwa informacji w systemach teleinformatycznych; Doświadczenie w realizacji architektury rozwiązań zawierających elementy hurtowni danych i narzędzi analitycznych; Znajomość zagadnień związanych z architekturą, projektowaniem i integracją systemów IT; Znajomość zagadnień związanych z architekturą zorientowaną na usługi (SOA) oraz mikroserwisy; Znajomość wzorców projektowych i architektonicznych, relacyjnych baz danych, serwerów aplikacyjnych oraz integracji systemów IT; Znajomość SQL oraz procesów ETL; Doświadczenie w programowaniu w języku Python; Znajomość baz danych PostgreSQL/EDB/MySQL/ MongoDB/Oracle; Znajomość Enterprise Architect; Dobra organizacja pracy własnej, orientacja na realizacje celów; Umiejętności interpersonalne, w szczególności umiejętność planowania, definiowania, realizacji, oraz monitorowania i rozliczania celów; Efektywna komunikacja, kreatywność, samodzielność, kultura osobista i odporność na stres, proaktywność; Zdolność adaptacji i elastyczność, otwartość na stały rozwój i gotowość uczenia się. Mile widziane: Doświadczenie projektowe w obszarze ochrony zdrowia; Certyfikat z obszaru zarządzania projektem metodą zwinną (np.. Agile PM lub równoważny); Certyfikat potwierdzający umiejętności z obszaru projektowania architektury rozwiązań IT (np.. TOGAF® EA Foundation lub równoważny); Certyfikat potwierdzający wiedzę z zakresu administrowania EDB (np. EDB Certification - PostgreSQL Essentials 15 lub równoważny); Certyfikat z obszaru administrowania środowiskiem Hadoop (np. Cloudera Certified Administrator for Hadoop (CCAH), Hortonworks Certified Apache Hadoop Administrator (HCAHA) lub równoważny) Kluczowe zadania: Tworzenie koncepcji i projektów architektury systemów DWH zgodnych z wymaganiami biznesowymi i technologicznymi. Wybór odpowiednich technologii oraz rozwiązań integracyjnych (ETL, bazy danych, narzędzia analityczne). Projektowanie modeli danych (w tym modeli logicznych i fizycznych). Określanie podziału komponentów systemu, ich zależności oraz rozmiaru (szacowanie złożoności rozwiązania). Projektowanie przepływów danych oraz procesów ekstrakcji, transformacji i ładowania. Zapewnienie wydajności i niezawodności procesów przetwarzania danych. Określanie zakresu i kosztów prac architektonicznych i programistycznych. Ocena skali rozwiązania oraz rekomendowanie sposobów skalowania (wertykalne/horyzontalne). Identyfikowanie ryzyk związanych z bezpieczeństwem informacji i rekomendowanie środków zaradczych. Współpraca z zespołami ds. bezpieczeństwa w celu wdrażania mechanizmów zabezpieczających. Projektowanie integracji z innymi systemami IT (systemy źródłowe, API, hurtownie danych). Współpraca z zespołami integracyjnymi przy wdrażaniu rozwiązań. Konsultowanie rozwiązań z analitykami, programistami, testerami i interesariuszami biznesowymi. Udział w planowaniu i przeglądach technicznych. Tworzenie i aktualizacja dokumentacji architektonicznej w narzędziach takich jak Enterprise Architect. Utrzymywanie zgodności z wewnętrznymi i zewnętrznymi standardami architektonicznymi. Ocena efektywności wdrożonych rozwiązań i rekomendowanie zmian. Śledzenie trendów technologicznych i proponowanie innowacji. Tworzenie lub przegląd skryptów i komponentów automatyzujących przetwarzanie danych. Zapewnienie jakości kodu i jego zgodności z architekturą systemu.","[{""min"": 25000, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Architecture,25000,32000,Net per month - B2B
Full-time,Senior,B2B,Remote,23,Lead Business Intelligence (Power BI) Engineer,CLOUDFIDE,"You are Lead Power BI Developer who excels in transforming complex data into clear, actionable insights. As a motivated and detail-oriented professional, you will lead a team of developers, drive business growth, and maintain strong communication with our customers. Your creativity and curiosity will help fuel smart decisions and innovative solutions. Opportunity overview This global project is centered around empowering a vast number of business analysts to navigate large-scale data hubs effectively. As the Lead Power BI Developer, you will not only work on translating business logic into DAX, optimizing user-written measures, and training users for enhanced self-sufficiency but also manage and mentor a team of developers. Additionally, you will play a crucial role in business development by identifying new project opportunities and building strong relationships with clients. In this role, you will enhance your problem-solving skills, gain substantial experience in data modeling, and drive the strategic direction of our BI initiatives. Your leadership will be key in ensuring the delivery of high-quality solutions while fostering a culture of innovation and continuous improvement within the team. Your impact zone Translating business requirements into technical solutions. Working closely with stakeholders and business users. Design and implement modern cloud-based solutions. Build and launch new data models. Optimize existing data models and reports, elevating the efficiency and impact of our data intelligence. Implement best practices in data engineering to maintain data integrity, quality, and documentation – your work will increase data discoverability and understanding. Take part in sharing your knowledge and engage in training activities to promote a learning culture Key responsibilities 5+ years of experience in delivering complex BI solutions – it’s your time to excel! Proven track record of leading and managing BI development teams. Strong business acumen and ability to develop and present compelling business cases. Excellent communication and interpersonal skills, with the ability to build and maintain relationships with customers and stakeholders. Hands-on experience (3+ years) with Microsoft BI stack (Power BI, SSAS/AAS). Mastery of DAX, M, SQL, Microsoft SQL Server, and PostgreSQL and query performance tuning – you make data dance! Proficiency in Microsoft Fabric and data integration techniques. Practical know-how in implementing row-level security (RLS) and Data Lake/Warehouse architectures. Python coding experience – you’re our code whisperer! Familiarity with public cloud architecture, security, networking concepts (MS Azure preferred) – we like our clouds secure and efficient. Strong conceptual and analytical skills – you’re a whiz at defining and documenting complex requirements. Fluent English communication – you can articulate tech in plain language. Qualifications & tech toolbox 5+ years of experience in delivering complex BI solutions – it’s your time to excel! Proven track record of leading and managing BI development teams. Strong business acumen and ability to develop and present compelling business cases. Excellent communication and interpersonal skills, with the ability to build and maintain relationships with customers and stakeholders. Hands-on experience (3+ years) with Microsoft BI stack (Power BI, SSAS/AAS). Mastery of DAX, M, SQL, Microsoft SQL Server, and PostgreSQL and query performance tuning – you make data dance! Proficiency in Microsoft Fabric and data integration techniques. Practical know-how in implementing row-level security (RLS) and Data Lake/Warehouse architectures. Python coding experience – you’re our code whisperer! Familiarity with public cloud architecture, security, networking concepts (MS Azure preferred) – we like our clouds secure and efficient. Strong conceptual and analytical skills – you’re a whiz at defining and documenting complex requirements. Fluent English communication – you can articulate tech in plain language. Extra stardust for Experience with Databricks, Azure Synapse, Azure Data Factory, and Azure DevOps – these are your secret weapons! Here's why you'll love Cloudfide BENEFITS: Regardless of the form of employment - Budget for your professional development - training and certification. MyBenefit cafeteria (with Multisport). Medicover medical care. Team-building meetings and trips. FLEXIBILITY: Enjoy the freedom of working from anywhere, and have a genuine say on our tools, tech, and solutions. STABILITY: Stable and long-term employment (employment contract, B2B). START-UP CULTURE: Open communication, creative problem solving and a flat hierarchy. GROWTH: Skyrocket your career by exploring new territories – you can work on various projects related to Big Data and Cloud. COLLABORATION: Be part of our diverse, passionate team, where every voice matters. Work in a company full of well-coordinated people who do their work with passion and commitment. Equal opportunities CLOUDFIDE is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.","[{""min"": 20000, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20000,30240,Net per month - B2B
Full-time,Senior,B2B,Hybrid,24,Senior Big Data Engineer,Grid Dynamics Poland,"We are seeking a highly skilled Big Data Engineer with a strong background in data engineering, real-time streaming, and large-scale distributed data processing. The engineer will be a part of the team responsible for building critical risk and fraud detection applications. The ideal candidate will have expertise in big data technologies, real-time data pipelines, and experience developing solutions that enhance fraud prevention and risk mitigation in the payments domain. Responsibilities: Design, develop, and implement scalable batch and real-time data processing systems for fraud detection and risk mitigation. Build and optimize real-time streaming data pipelines using Kafka, Spark Streaming, ELK, and related technologies. Develop ETL/ELT pipelines, including data quality checks, anomaly detection, and notification systems. Work with Hadoop, MapReduce, Hive, Spark, NoSQL, and relational databases (e.g., MySQL) to create data-driven solutions. Architect high-availability, low-latency, and strongly consistent distributed data processing systems. Collaborate with cross-functional teams to create fraud prevention and risk detection solutions that add business value. Follow Agile development methodologies, incorporating CI/CD best practices for continuous integration and delivery. Present complex data-driven insights in a clear and concise manner. Requirements: Proven experience in big data engineering and data-driven business solutions. Strong background in Hadoop, Spark, MapReduce, Hive, and NoSQL databases. Hands-on experience in real-time data streaming using Kafka, Spark Streaming, and ELK. Expertise in ETL/ELT pipeline development, data quality checks, and anomaly detection. Experience building distributed data processing systems with high availability and low latency. Proficiency in Java or Python for data engineering and application development. Familiarity with Agile methodologies, CI/CD, and best development practices. Strong communication skills and ability to present complex ideas clearly. We offer: Opportunity to work on bleeding-edge projects Work with a highly motivated and dedicated team Competitive salary Flexible schedule Benefits package - medical insurance, sports Corporate social events Professional development opportunities Well-equipped office About us: Grid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India. Collapse","[{""min"": 22000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,30000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,25,Big Data Engineer,Grid Dynamics Poland,Description: ,"[{""min"": 17000, ""max"": 18300, ""type"": ""Net per month - B2B""}]",Data Engineering,17000,18300,Net per month - B2B
Full-time,Senior,B2B,Remote,27,Senior Data Engineer (Python i Django),Lumicode Sp. z o.o. (Pentacomp Group),"Lumicode Sp. z o.o. jest częścią Grupy Pentacomp, która jest producentem rozwiązań informatycznych i dostawcą profesjonalnych usług IT dla dużych przedsiębiorstw i sektora publicznego. Jako Pentacomp tworzymy rozwiązania informatyczne, które łączą innowacyjność z wieloletnim doświadczeniem - a mamy go całkiem sporo. Działamy na rynku od prawie 30 lat i możemy pochwalić się wieloma zrealizowanymi projektami. Aktualnie poszukujemy Data Engineera ze znajomością Pythona i Django do klienta z branży ubezpieczeniowej. Oferujemy: Pracę w 100% zdalną Pracę w pełnym wymiarze godzin; Forma współpracy: B2B Stawka do 170 pln/h netto + VAT B2B w zależności od doświadczenia; Możliwość korzystania z prywatnej opieki medycznej i karty sportowej; Wymagania na stanowisko: Min. 5 lat doświadczenia jako Data Engineer/Python Developer Znajomość Python & Django Bardzo dobra znajomość SQL Znajomość Snowflake'a Mile widziana znajomość Flyway Mile widziana znajomość RDS SQL Database Mile widziana znajomość AWS CDK Mile widziana znajomość TypeScript, React oraz Dash Język angielski na poziomie min. B2 Proces rekrutacyjny: Rozmowa z rekruterem z Lumicode Rozmowa wstępna Rozmowa techniczna Decyzja, oferta oraz doprecyzowanie warunków kontraktu Zapraszamy do aplikowania!","[{""min"": 140, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,140,170,Net per hour - B2B
Full-time,Mid,Mandate,Hybrid,30,Celonis Process Mining Specialist (Maternity Cover),Vaillant Group Business Services,"What we achieve together In this role you lead and drive the Digital Twin implementation process while delivering exceptionally high levels of service to ensure the optimal solution for internal customers with the Celonis technology. You design, optimize and monitor data models and data connections (ETL) to build the best possible and real-time capable data architecture. In addition, you support the implementation of process mining solutions across enterprise end-to-end processes (Design-to-Operate, Procure-to-Pay, Order-to-Cash). You translate business requirements into technical requirements, assess the feasibility, plan the technical implementation and ensure the overall quality of implementation. You also design and implement innovative analyses and execution apps and enrich them with Machine Learning algorithms (Python) or Task Mining to make the customer's processes transparent and automated. Within your responsibilities is also the analysis of the data to identify process inefficiencies, while monitoring compliance and data security measures. What makes us successful together Experience : You have at least 3 years of commercial experience in IT-Consulting, Management Consulting, Process Improvement/Excellence or a similar area. You also bring a solid track record in successfully developing and shipping data driven solutions in Celonis. Know-how and skills: You have proficiency in SQL, other programming languages (Python, R, Matlab) as a plus and a strong interest in Big Data, Data Analytics, Data Mining, Process Mining and Digital Twin. Personality : With your positive attitude and trustworthy personality, you can build strong stakeholder relationships. You understand and interpret business processes and communicate proactively and clearly. In addition, you have Excellent analytical skills, well organized and known for being a quick learner. Language skills : You speak English fluently; German language skills would be a plus. What makes us special Environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee. Package of additional benefits: private medical care, multi-sport card. A fast growing, agile and very dynamic team that challenges established routines and helps transforming the Vaillant Group to a data informed business. Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings.","[{""min"": 15000, ""max"": 20000, ""type"": ""Gross per month - Mandate""}]",Data Analysis & BI,15000,20000,Gross per month - Mandate
Full-time,Senior,B2B,Remote,31,Data Engineer (Databricks),Remodevs,"Role We are looking for an enthusiastic Senior Data Engineer to join our team. In this role, you will collaborate with skilled professionals dedicated to providing insights into user behavior. Your work will help the organization understand when, where, and how users engage with our digital platforms. As a Senior Data Engineer, you will take charge of developing and improving advanced data-tracking solutions, ensuring seamless platform integration and transforming data into valuable insights that drive key business decisions. We are a global manufacturing company based in Scandinavia, with offices and operations around the world. 4+ years of experience with Python Knowledge of Databricks Experience with Snowplow or other tracking solutions Skills in building and maintaining data pipelines with dbt Proficiency with cloud platforms (preferably Azure) Strong SQL skills for data processing and transformation Fluent English Collaborate with developers and product managers Design, build, and manage data pipelines using dbt to process bronze, silver, and gold data layers in Databricks Work with cloud infrastructure, particularly Azure Use SQL and Python to create reliable data solutions Ensure data quality, security, and compliance throughout its lifecycle","[{""min"": 20000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,20000,28000,Net per month - B2B
Full-time,Senior,B2B,Remote,34,Senior/ Lead Data Science Engineer,N-iX,"We are looking for a Data Scientist with a strong analytical mindset and a passion for solving real-world supply chain problems. You will join a cross-functional team focused on optimizing the flow of units from warehouses to stores. This is a high-impact role where your insights and models will directly influence key business operations. Responsibilities: Prepare and clean datasets to enable reliable experimentation Develop and fine-tune machine learning models and algorithms Test models, analyze outcomes, and generate actionable insights Communicate findings to stakeholders through reports and visualizations Propose data-driven solutions and optimization strategies Requirements: Proficiency in Python and/or R Experience with SQL Data Platforms & Tools Hands-on experience with Azure Databricks or Snowflake Familiarity with building and deploying machine learning pipelines is a strong advantage Java experience is a plus Education - Bachelor's or Master's degree in Computer Science, Mathematics, Engineering, or a related field Mathematics & Statistics: Solid understanding of multivariable calculus and linear algebra Applied knowledge of statistical distributions and hypothesis testing Machine Learning Practical knowledge of ML techniques: kNN, decision forests, regressions, MLE, time series Understanding of model evaluation metrics and performance tuning Data Handling & Visualization Skilled in managing missing or inconsistent data Experience with tools like matplotlib, seaborn, or ggplot2 Nice to Have: Experience with deploying ML models into production environments Previous work in supply chain or logistics domains","[{""min"": 23642, ""max"": 28445, ""type"": ""Net per month - B2B""}]",Data Science,23642,28445,Net per month - B2B
Full-time,Senior,B2B,Hybrid,36,Product Owner – Data & Analytics,ITDS,"Product Owner Join us, and lead innovation through data-driven decision making! Kraków - based opportunity with hybrid work model (2 days/week in the office). As aProduct Owner, you will be working for our client, a global financial institution focused on transforming its data and analytics landscape. You will be leading the development of data-driven products that support critical business decisions across credit, lending, and operational processes. This project aims to optimize investigation outcomes by leveraging advanced analytics and technology solutions, driving measurable value through effective stakeholder collaboration and Agile product development. You will be working in a complex, global environment that requires a strong strategic mindset and hands-on delivery expertise. Your main responsibilities: Defining and evolving the product vision in alignment with business goals Collaborating with stakeholders to prioritize features based on value and business impact Leading Agile ceremonies and maintaining a clear and refined product backlog Supporting the team in decomposing epics into actionable user stories Communicating with cross-functional teams to ensure product alignment and clarity Resolving conflicting stakeholder priorities and negotiating trade-offs Showcasing product iterations and improvements to business stakeholders Promoting and advocating for the product across the broader organization Shaping and validating business outcomes through data analytics and metrics Ensuring consistent delivery of business value through iterative releases You're ideal for this role if you have: Proven experience as aProduct Ownerin alarge,complex organization Strong leadership and communication skills across all seniority levels Expertise in Agile frameworks and tools such asJIRAandConfluence Experience managing product strategy, roadmaps, and value delivery Strong stakeholder management and conflict resolution capabilities Demonstrated ability to deliver business and technology change Understanding of data management and analytics within a corporate context Experience working with global, cross-functional teams Familiarity with credit and lending processes in the financial industry Ability to translate business outcomes into technical requirements It is a strong plus if you have: Hands-on experience withSQL, BigQuery, or other data tools Background in data science, analytics, or business analysis Experience with big data technologies or cloud-based platforms Proficiency inPythonor PySpark for data processing Knowledge of data visualization tools likeTableauorQlikSense","[{""min"": 17850, ""max"": 24150, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,17850,24150,Net per month - B2B
Full-time,Senior,B2B,Remote,38,Senior Data Engineer (Databricks),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition byForbesas one of the top 10 AI consulting companies. As aSenior Data Engineer, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of a universal data platform for global aerospace companies.This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Data Platform Transformation for energy management association body.This project addressed critical data management challenges, boosting user adoption, performance, and data integrity. The team is implementing a comprehensive data catalog, leveraging Databricks and Apache Spark/PySpark, for simplified data access and governance. Secure integration solutions and enhanced data quality monitoring, utilizing Delta Live Table tests, established trust in the platform. The intermediate result is a user-friendly, secure, and data-driven platform, serving as a basis for further development of ML components. Design of the data transformation and following data ops pipelines for global car manufacturer.This project aims to build a data processing system for both real-time streaming and batch data. We’ll handle data for business uses like process monitoring, analysis, and reporting, while also exploring LLMs for chatbots and data analysis. Key tasks include data cleaning, normalization, and optimizing the data model for performance and accuracy. 🚀 Your main responsibilities: Design and optimize scalable data processing pipelines for both streaming and batch workloads using Big Data technologies such as Databricks, Apache Airflow, and Dagster. Architect and implement end-to-end data platforms, ensuring high availability, performance, and reliability. Lead the development of CI/CD and MLOps processes to automate deployments, monitoring, and model lifecycle management. Develop and maintain applications for aggregating, processing, and analyzing data from diverse sources, ensuring efficiency and scalability. Collaborate with Data Science teams on Machine Learning projects, including text/image analysis, feature engineering, and predictive model deployment. Design and manage complex data transformations using Databricks, DBT, and Apache Airflow, ensuring data integrity and consistency. Translate business requirements into scalable and efficient technical solutions while ensuring optimal performance and data quality. Ensure data security, compliance, and governance best practices are followed across all data pipelines. 🎯 What you’ll need to succeed in this role: At least 5 years of commercial experienceimplementing, developing, or maintaining Big Data systems. Strong programming skills inPython: writing a clean code, OOP design. StrongSQLskills, including performance tuning, query optimization, and experience withdata warehousing solutions. Experience in designing and implementing data governance and data management processes. Deep expertise in Big Data technologies, includingApache Airflow, Dagster, Databricks, and other modern data orchestration and transformation tools. Experience implementing and deploying solutions in cloud environments (with a preference forAzure). Knowledge of how to build and deployPower BI reports and dashboards for data visualization. Excellent understanding of dimensional data and data modeling techniques. Consulting experience and the ability to guide clients through architectural decisions, technology selection, and best practices. Ability to work independently and take ownership of project deliverables. Familiarity withSpark, Azure Event HuborKafka. Master’s or Ph.D. in Computer Science, Big Data, Mathematics, Physics, or a related field. 🎁 Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage withtop-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you towork remotelyor from modern offices and coworking spaces. Accelerate your professional growth throughcareer paths,knowledge-sharinginitiatives,languageclasses, and sponsoredtrainingorconferences, including a partnership withDatabricks, which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days offavailable for B2B contractors and individuals under contracts of mandate. Participate inteam-building eventsand utilize theintegration budget. Celebratework anniversaries, birthdays,andmilestones. Accessmedicalandsports packages, eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you canboostyourpersonal brandby speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website (career page) and social media (Facebook,LinkedIn,Instagram).","[{""min"": 21000, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,28560,Net per month - B2B
Full-time,Mid,B2B,Remote,40,Revalize CPQ Product Support Specialist,Link Group,"Codzienne wsparcie użytkowników i obsługa zgłoszeń w ramach CPQ (BAU) Diagnozowanie i rozwiązywanie incydentów technicznych Wprowadzanie usprawnień i modyfikacji funkcjonalności systemu Konfiguracja i utrzymanie procesów ofertowania w CPQ Współpraca z zespołami technicznymi i biznesowymi po stronie klienta Dokumentowanie zmian i konfiguracji Minimum 2 lata doświadczenia w pracy z platformami CPQ Praktyczna znajomość przynajmniej jednej z platform: Revalize CPQ Oracle CPQ (BigMachines) Infor CPQ Tacton CPQ Doświadczenie w obsłudze zgłoszeń BAU, incydentów oraz konfiguracji i optymalizacji CPQ Umiejętność pracy w środowisku międzynarodowym (projekt prowadzony po angielsku)","[{""min"": 85, ""max"": 100, ""type"": ""Net per hour - B2B""}]",Unclassified,85,100,Net per hour - B2B
Full-time,Senior,B2B,Remote,43,Data Engineer (Snowflake),SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: udział w międzynarodowym projekcie realizowanym dla branży logistycznej. Wykorzystywany stos technologiczny w projekcie: Snowflake (Cortex AI), AWS, DataLake, Data Warehouse, ETL, Tableau, PowerBI, Python, Docker, Kubernetes, projektowanie i rozwijanie skalowalnych pipeline'ów danych, hurtowni danych oraz data lake'ów w oparciu o Snowflake, integracja i automatyzacja procesów przetwarzania danych (ETL/ELT) z różnych źródeł, wykorzystywanie usług AWS oraz Cortex AI do budowy inteligentnych rozwiązań opartych na chmurze, rozwijanie aplikacji analitycznych i prototypów AI z użyciem Pythona i frameworka Streamlit, konteneryzacja aplikacji i ich uruchamianie przy pomocy Docker oraz Kubernetes, współpraca z zespołami analitycznymi i interesariuszami biznesowymi w celu dostarczania odpowiednich rozwiązań danych, monitorowanie oraz optymalizacja pipeline'ów danych i procesów przetwarzania, udział w projektach z obszaru AI i uczenia maszynowego, w tym tworzenie rozwiązań konwersacyjnych (np. chatboty), praca 100% zdalna, stawka do 150 zł/h przy B2B w zależności od doświadczenia. Ta oferta jest dla Ciebie, jeśli: masz min. 5-letnie doświadczenie na stanowisku Data Engineer, posiadasz doświadczenie w projektowaniu i utrzymywaniu hurtowni danych oraz pipeline’ów w Snowflake, biegle posługujesz się Cortex AI i narzędziami AWS, programujesz swobodnie w Pythonie, znasz framework Streamlit, pracowałeś/aś z technologiami konteneryzacji, takimi jak Docker i Kubernetes, masz doświadczenie w tworzeniu i optymalizacji workflowów ETL/ELT, dobrze rozumiesz procesy analizy danych i potrafisz rozwiązywać problemy w złożonym środowisku, potrafisz efektywnie współpracować w międzynarodowym zespole, posługujesz się językiem angielskim na poziomie płynnym - min. B2+, mile widziane doświadczenie w projektach opartych na conversAI oraz machine learning. Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 19200, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Engineering,19200,24000,Net per month - B2B
Full-time,Senior,B2B,Remote,44,Senior MS Data Engineer (Azure SQL DB / ETL / PySpark),1dea,"Poszukujemy doświadczonegoSenior MS Data Engineer’az pasją do tworzenia innowacyjnych rozwiązań. Idealny kandydat powinien posiadać doświadczenie zAzure SQL, silne umiejętności woptymalizacji zapytańoraz zdolność proponowanianowych implementacjiw odpowiedzi na zgłaszane problemy. Dodatkowo, kluczowe jestsolidne zrozumienie architektury, powinieneś być w stanie sugerować zmiany, takie jak wdrożeniewymiany danych niemal w czasie rzeczywistym. Informacje organizacyjne: Branża: IT Consulting Wakaty: 2 Lokalizacja: praca w 100% zdalnie Start: ASAP (max 2msc okresu wypowiedzenia) Stawka: do ustalenia w zakresie 155 - 170 PLN netto + VAT / h Warunki zaangażowania: B2B (outsourcing przez 1dea), full-time, long-term Proces rekrutacyjny (w pełni zdalny): Krótka rozmowa telefoniczna z rekruterem 1dea - o projekcie i warunkach zaangażowania: (~10 minut) Przedstawienie Twojej kandydatury Klientowi Rozmowa techniczno-projektowa z Klientem (wideo) (~1-1.5 h) (Opcjonalnie) rozmowa podsumowująca proces z Managerem Technicznym ze strony naszego Klienta (~30 minut) Podjęcie decyzji o współpracy Projektowanie i wdrażanie zaawansowanych rozwiązań inżynierii danych na platformie Azure Optymalizacja zapytań Proponowanie nowych implementacji w odpowiedzi na zgłaszane problemy Sugerowanie zmian, takich jak wdrożeniewymiany danych niemal w czasie rzeczywistym Optymalizacja i utrzymanie istniejących procesów ETL Budowa skalowalnych i wydajnych pipeline’ów danych Współpraca z zespołami biznesowymi w celu zrozumienia wymagań i przełożenia ich na rozwiązania techniczne Zapewnienie wysokiej jakości, bezpieczeństwa i zgodności danych z obowiązującymi przepisami Udział w projektach związanych z rozwojem platformy danych Minimum 3 lata doświadczenia w inżynierii danych, z naciskiem na platformę Azure Solidna znajomość Azure SQL Databases, Databricks oraz innych kluczowych usług Azure Praktyczna umiejętność wykorzystania Power Query, Python i PySpark do transformacji i manipulacji danymi Umiejętność modelowania danych, projektowania i optymalizacji procesów ETL Silne umiejętności analityczne i zdolność do rozwiązywania problemów Umiejętność pracy w metodyce Agile Znajomość języka angielskiego pozwalająca na swobodną komunikację w mowie i piśmie (B2+) Mile widziane: Certyfikaty Microsoft Data Engineering (DP 600, DP 203) Doświadczenie w pracy z narzędziami BI, takimi jak Power BI Znajomość praktyk DevOps Długotrwały kontrakt B2B (od razu podpisujemy umowę na czas nieokreślony / bezterminowo - Klient nastawia się tylko i wyłącznie na długofalową współpracę) Zostań częścią firmy o silnej pozycji na rynku Nowoczesny sprzęt i oprogramowanie (zapewnia nasz Klient) Elastyczne godziny pracy: ciesz się swobodą efektywnego zarządzania swoim czasem Pracę w 100% zdalnie Kultura współpracy: Cenimy pracę zespołową, otwartość, szacunek i wzajemne wsparcie w rozwoju umiejętności. Kreatywność mile widziana: Twoje pomysły i sugestie w realizacji projektu będą uwzględniane i brane pod uwagę : -)","[{""min"": 155, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Database Administration,155,170,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,46,Data Privacy and Compliance Specialist,OChK,"Data Privacy and Compliance Specialist Miejsce pracy: Warszawa / hybrydowo Poziom stanowiska: Intermediate 10.000 - 13.000 brutto UoP lub umowa B2B Twój zakres obowiązków: udział w negocjacjach i opiniowaniu umów z klientami oraz dostawcami, w zakresie zapisów dotyczących ochrony danych i bezpieczeństwa informacji, wsparcie w ocenie i monitorowaniu bezpieczeństwa łańcucha dostaw, ze szczególnym uwzględnieniem aspektów związanych z przetwarzaniem danych osobowych, udział w kontrolach zgodności, audytach wewnętrznych i zewnętrznych oraz wdrażanie działań korygujących i zapobiegawczych, przygotowywanie, aktualizacja i rozwój dokumentacji z zakresu bezpieczeństwa informacji (m.in. polityki, procedury, klauzule, rejestry), wsparcie zespołów biznesowych w analizie ryzyka oraz ocenie i zapewnianiu zgodności planowanych działań z obowiązującymi regulacjami, weryfikacja zgodności przetwarzania danych osobowych w procesach biznesowych i systemach IT, prowadzenie szkoleń i działań edukacyjnych dla pracowników w obszarze bezpieczeństwa informacji, w tym ochrony danych, monitorowanie zmian w przepisach prawa i regulacjach oraz inicjowanie i wdrażanie niezbędnych działań dostosowawczych. Nasze wymagania: wykształcenie wyższe (preferowane: prawnicze, cyberbezpieczeństwo), doskonała znajomość przepisów oraz norm i standardów dotyczących bezpieczeństwa informacji oraz ochrony danych osobowych, z uwzględnieniem zagadnień dotyczących chmury obliczeniowej oraz sztucznej inteligencji, znajomość języka angielskiego na poziomie minimum B2, co najmniej 3-letnie doświadczenie na stanowisku związanym z bezpieczeństwem informacji/ochroną danych osobowych, certyfikaty poświadczające wiedzę z zakresu bezpieczeństwa informacji oraz ochrony danych osobowych – mile widziane. W OChK: pracujemy zadaniowo w trybie hybrydowym (nowoczesne biuro przy ul. Grzybowskiej), działamy w zwinnym środowisku, z wykorzystaniem aplikacji zwiększających efektywność (m. in. Google Workspace, Slack, GitHub, Jira), inwestujemy w Twój rozwój poprzez finansowanie szkoleń i certów, a od pierwszego dnia pracy udostępniamy platformy edukacyjne Google i Microsoft, oferujemy prywatne ubezpieczenie medyczne, preferencyjne warunki ubezpieczenia grupowego oraz kartę Multisport organizujemy i współfinansujemy naukę języka angielskiego, udostępniamy program poleceń, dzięki któremu pracownicy zyskują dodatkowe bonusy za skuteczną rekomendację kandydatów do pracy, cenimy proaktywność i inicjatywę własną, dlatego wspieramy autonomię w podejmowaniu decyzji, budujemy kulturę organizacyjną na wartościach takich jak profesjonalizm, współodpowiedzialność i wzajemny szacunek, przykładamy dużą wagę do efektywnego onboardingu, podczas którego w luźnej atmosferze i ze wsparciem Twojego CloudBuddiego poznajesz zespół, firmę i swoje obowiązki, stawiamy na integrację zespołów podczas różnorodnych inicjatyw, zarówno firmowych jak i oddolnych, które pomagają nam lepiej się poznawać oraz budować i utrzymywać dobrą atmosferę współpracy.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 13000, ""type"": ""Gross per month - Permanent""}]",Unclassified,10000,13000,Net per month - B2B
Full-time,Mid,B2B,Remote,50,Data Analyst,Haddad Brands USA,"Along with dynamic company development, we are looking for talented and experienced Data Analyst. Self-motivated and team player. Long term contract only. As a Data Analyst, you will be responsible for developing and maintaining Power BI dashboards, analyzing complex data sets, and collaborating with cross-functional teams to provide actionable insights. The ideal candidate should have a strong analytical mindset, advanced proficiency in Power BI, and a deep understanding of data modeling, visualization, and business intelligence reporting. Key Responsibilities: Develop, maintain, and enhance interactive Power BI dashboards and reports. Transform raw data into meaningful insights using Power BI to support business decisions. Work closely with business stakeholders to understand their requirements and translate them into analytical solutions. Collaborate with data engineering and IT teams to ensure data accuracy, integrity, and consistency. Perform data analysis using DAX, Power Query, and other advanced Power BI features. Optimize dashboard performance and recommend best practices for data visualization. Support ad-hoc reporting and analytical requests from various departments. Identify trends, patterns, and key insights through complex data analysis. Ensure the security and governance of Power BI reports according to company standards. Train and support users in the effective use of Power BI reports and analytics tools. Required Qualifications: Bachelor’s degree in Statistics, data science, Computer Science, or a related field. 3+ years of hands-on experience in data analysis and business intelligence using Power BI. Proficiency in Power BI Desktop, Power BI Service, DAX, and Power Query. Strong experience with data modeling, ETL processes, and SQL. Knowledge of database management systems (e.g., SQL Server, Azure, etc.). Ability to work with large datasets and perform data cleaning, transformation, and analysis. Excellent communication skills with the ability to present complex data in a simple, actionable format. Strong problem-solving skills and attention to detail. Ability to work in a fast-paced environment and manage multiple projects simultaneously. Preferred Qualifications: Knowledge of Python or R for data analysis. Power Automate experience Web scraping experience Familiarity with other BI tools (Tableau, QlikView, etc.). Experience with cloud platforms (Azure, AWS, Google Cloud) and Power BI integrations. Power BI certification is a plus. Job Profile: 40% New features/reports, 40% Maintenance / Bug Fixing, 20% Meetings (developers / business) Hire process: Get to know meeting in Polish followed by a second one, technical oriented within 2 weeks period. Brief English conversation is possible to estimate the skills. Benefits: private medical insurance, 21 paid vacations a year in B2B, hardware/software, training and career-oriented development path, freedom in tools, friendly environment based on trust.","[{""min"": 10000, ""max"": 14000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,10000,14000,Net per month - B2B
Full-time,Mid,B2B,Remote,56,Data Scientist,in4ge sp. z o.o.,"Zakres obowiązków: Opracowywanie innowacyjnych rozwiązań z wykorzystaniem zaawansowanych technologii uczenia maszynowego i/lub sztucznej inteligencji. Analiza i przetwarzanie dużych zbiorów danych. Poszukiwanie nowych źródeł danych oraz eksploracja zależności w danych. Współpraca z zespołami programistów i analityków biznesowych. Przekazywanie złożonych wyników analizy w sposób zrozumiały dla odbiorców technicznych i nietechnicznych. Śledzenie nowych trendów i technologii w obszarze sztucznej inteligencji, machine learning i analizy danych oraz proponowanie ich zastosowania w projektach. Automatyzacja procesów przetwarzania i analizy danych przy użyciu nowoczesnych narzędzi i bibliotek. Wdrażanie oraz zarządzanie modelami w środowiskach produkcyjnych z uwzględnieniem praktyk MLOps. Wymagania: Minimum 3 lata doświadczenia zawodowego na podobnym stanowisku. Doświadczenie w pracy z danymi i tworzeniem modeli uczenia maszynowego. Znajomość języka Python i bibliotek związanych z analizą danych (np. Pandas, NumPy) oraz podstawowe umiejętności w zakresie SQL. Znajomość narzędzi chmurowych (AWS, Azure, GCP) będzie dodatkowym atutem. Umiejętność pracy z danymi tekstowymi (NLP) lub danymi obrazowymi (CV) mile widziana. Znajomość podstaw MLOps i narzędzi do wdrażania modeli (Docker, MLflow, CI/CD). Wiedza z zakresu statystyki i podstawowych algorytmów uczenia maszynowego. Umiejętność analitycznego myślenia i rozwiązywania problemów. Znajomość języka angielskiego na poziomie pozwalającym na swobodną komunikację. Mile widziane: Doświadczenie w pracy z dużymi modelami językowymi (LLM) i koncepcjami takimi jak Retrieval Augmented Generation, bazy wektorowe czy inżynieria promptów. Znajomość frameworków LLM, takich jak Langchain, LLamaindex czy agentowych frameworków. Wiedza z zakresu przetwarzania języka naturalnego (NLP). Znajomość dodatkowych języków programowania (np. Java, C#, Go). Znajomość narzędzi i bibliotek związanych z agentami GenAI (np. Taskweave, Autogen). Co oferujemy? Rozwój kariery w międzynarodowych projektach, z wykorzystaniem nowoczesnych narzędzi i technologii. Elastyczny model pracy: możliwość 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejętności i doświadczenia. Współpracę w zgranym zespole, który ceni wymianę wiedzy oraz otwartą komunikację. Realny wpływ na projekty oraz wdrażane rozwiązania.","[{""min"": 13000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Science,13000,25000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,57,Data Scientist – Investment Management,ITDS,"Data Scientist – Investment Management Join a world of innovation and data-driven excellence! This is a Wrocław -based hybrid opportunity – 3 days in the office per week (Relocation package available, including move and acocomodation) As a Data Scientist, you will be working for our client – a global leader in quantitative and systematic investment management. You will be responsible for leveraging data to drive investment strategies and enhance trading decisions. Your main responsibilities: Collaborate with Quantitative Researchers and Traders to design impactful datasets. Prototype and design code for data extraction, cleaning, and aggregation. Work with Engineers to automate and optimize data processes. Manage the onboarding of new datasets from start to finish. Solve data-related challenges to expedite production timelines. Innovate with novel data extraction methods to enhance capabilities. You're ideal for the role if you have: 3+ years of experience as a Data Scientist; buy-side quantitative finance experience is a plus. A postgraduate degree in Mathematics, Physics, or Engineering. Advanced Python programming skills, with proficiency in Pandas and NumPy. A keen interest in financial markets and data analysis. Experience with traditional and alternative financial datasets. Excellent communication skills for effective stakeholder collaboration. Ability to thrive in a high-performance, fast-paced environment. We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7131 You can report violations in accordance with ITDS’s Whistleblower Procedure available here .","[{""min"": 25000, ""max"": 30000, ""type"": ""Gross per month - Permanent""}]",Data Science,25000,30000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,58,Data Scientist,Ework Group,"Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. 🔹 For our Client we are looking forSenior Data Scientist – Medical Image Analysis - remote work🔹 ✔️The Position This position focuses on exploring new insights and developing innovative technologies in the field of medical image analysis. Key responsibilities include the collection, curation, and processing of medical image data, as well as applying software engineering principles within the context of medical imaging and computer vision. You will also collaborate with team members, as well as clinical scientists from other disciplines to design, code, train, test, deploy, and iterate on machine learning systems. ✔️Main responsibilities include: Research, develop and implement medical image processing pipelines utilizing state of the art computer-vision-based approaches in the medical image analysis space. Implement and maintain local and cloud-based data and computational environments and platforms to enable the work. Build data pipelines (data curation, preparation, cleaning, and consolidation) as an integral part of data science activity. Utilize AI/ML (Artificial Intelligence/Machine Learning) to develop complex methodologies and analyses, all around various medical imaging modalities. Perform informed semi-automated quality assessments of the acquired imaging data and of the derived measures. Align with clinical experts on the requirements, constraints and deliverables of a project. ✔️Qualifications: In this role, it is necessary to have either a PhD degree with 3+ years’ relevant direct non-academic professional experience or PhD degree with a strong post-doc experience in the field of medical image analysis. Degree within computer science, mathematics, (medical-)engineering, physics, statistics, or a related quantitative discipline is preferred. ✔️Furthermore, you must have: Hands-on experience with deep learning for computer vision using deep/machine learning frameworks, mainly PyTorch, Sklearn, and with conventional image processing techniques. Strong practical knowledge in medical image analysis in multiple medical imaging modalities (X-ray, US, MRI, CT, Digital Pathology) across different therapies (cardiology, obesity, diabetes etc.) with a proven high impact academic publication record Solid understanding of deep learning theory: CNNs, Transformers, RNN, GenAI, etc., and its various applications in semantic segmentation, classification, object detection and more. The ability to develop and validate algorithms against clinical data, ensuring appropriateness of fit for data science processes The ability to do quick prototyping/proofs of concept up to production ready models following best practices. The ability to perform in-depth data analysis and present results and conclusions to engineering and leadership teams. Appetite for research work Ideally, you have hands on knowledge in imaging biomarkers in pharmaceutical industry, healthcare industry, medical device development or in another regulated field. Experience with modern software development toolset (CI/CD, AWS, Azure Machine Learning, git, JIRA) which includes writing high quality code processing vision/imaging data using Python is highly preferred and so are excellent written and oral communication skills. ✔️Department Overview The Imaging Analytics department part of AI and Analytics Department within Data Science division, where we apply state of the art algorithms and machine learning techniques to some of the hardest problems in the discovery and development of new healthcare solutions, focusing on medical imaging in the clinical trials space. By leveraging a blend of scientific, problem-solving, and quantitative skills, we provide superior data insights that empower us to further develop and deliver life-changing treatments. We work in multidisciplinary teams with strong collaboration across all areas of the organization and engage in external collaborations to ensure access to cutting edge research and technology. ✔️Join the team The organisation values flexibility in ways of working to support various life situations. Employees are recognised for their unique qualities and skills, and the environment fosters development and collaboration. The broader mission includes improving the lives of millions of patients globally through innovation and dedication to chronic disease care. There is a commitment to becoming not just the best company in the world, but the best company for the world. This vision can only be achieved through the contributions of talented employees with diverse backgrounds and perspectives. An inclusive culture is fostered that celebrates diversity across employees, patients, and communities. ✔️ We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 162, ""max"": 179, ""type"": ""Net per hour - B2B""}]",Data Science,162,179,Net per hour - B2B
Full-time,Mid,B2B,Remote,59,ETL Developer,EndySoft,"Position Overview: We are seeking an experienced ETL Developer to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust ETL processes to ensure the efficient flow of data from various sources to data warehouses or data lakes. This role involves collaborating with business and technical teams to support data-driven decision-making and analytics initiatives. MD rate: 16600 - 20000PLN Roles and Responsibilities: Design, develop, and optimize ETL pipelines to extract, transform, and load data from multiple data sources. Collaborate with data architects and business analysts to gather and understand data requirements. Implement and maintain data integration workflows using ETL tools such as Informatica , Talend , SSIS , or Apache NiFi . Perform data validation and quality checks to ensure data integrity and accuracy. Troubleshoot and resolve issues related to ETL processes and data flows. Monitor and enhance the performance of ETL jobs to meet business SLA requirements. Maintain and document technical solutions, including data mappings, workflows, and procedures. Work closely with other data team members to support data warehouse and data lake initiatives. Required Skills and Experience: Proficiency in SQL for querying and transforming data. Hands-on experience with ETL tools such as Informatica , Talend , SSIS , or similar. Strong knowledge of data modeling techniques, including star schema and snowflake schema . Experience in data integration and data warehousing concepts. Familiarity with cloud platforms (e.g., AWS Glue , Azure Data Factory , Google Dataflow ) for ETL processes. Strong problem-solving skills and the ability to troubleshoot complex data issues. Experience with scripting languages such as Python , Shell , or Bash for automation. Excellent communication and collaboration skills to work effectively with cross-functional teams. Nice to Have: Experience with big data tools such as Spark , Kafka , or Hadoop . Knowledge of NoSQL databases like MongoDB or Cassandra . Familiarity with DataOps practices and CI/CD pipelines for ETL workflows. Exposure to data governance and metadata management tools. Understanding of data security and compliance requirements. Experience with version control systems like Git . Exposure to Agile/Scrum methodologies. Additional Information: This role provides an opportunity to work on complex data integration projects and contribute to the development of scalable data solutions. If you are passionate about transforming raw data into actionable insights and thrive in a fast-paced environment, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,16600,20000,Net per month - B2B
Full-time,Senior,B2B,Remote,62,Senior Cloud Data Engineer (Azure and Databricks),Future Processing,"Do naszej linii biznesowej Data Solutions poszukujemy osoby na stanowisko Senior Cloud Data Engineer ze znajomością Azure. Szukamy Ciebie, jeśli: masz min. 5 lat doświadczenia w IT, w tym min. 3,5 roku w pracy z danymi w chmurze Azure (potwierdzone projektami komercyjnymi wdrożonymi na produkcje), masz komercyjne doświadczenie w przetwarzaniu sporych danych przy użyciu Databricks, korzystasz z SQL na poziomie zaawansowanym i wykorzystujesz go na rozwiązaniach technologicznych MS i nie tylko, znasz metodyki i stosujesz biegle Git oraz CI/CD , masz doświadczenie w pracy z platformą Microsoft Fabric , tworzysz i optymalizujesz rozwiązania przetwarzające dane (ETL, ELT, itp.) poprzedzone projektem technicznym oraz alternatywami rozwiązań, monitoring, diagnostyka oraz rozwiązywanie problemów w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanować infrastrukturę oraz obliczyć jej koszt, koncepcje Delta Lake i Data Lakehouse są Ci znane, znasz architekturę SMP oraz MPP wraz z przykładami rozwiązań opartych o te architektury, masz wiedzę na temat migracji rozwiązań on-premise do chmury oraz znasz podstawowe typy migracji, masz wiedzę na temat stosowania mechanizmów związanych z bezpiecznym przechowywaniem i przetwarzaniem danych w chmurze, bardzo dobrze znasz usługi związanie z przechowywaniem i przetwarzaniem danych, oferowanych przez dostawcę chmury Azure, masz doświadczenie w bezpośredniej współpracy z klientem, posługujesz się j. angielskim na poziomie średniozaawansowanym (min. B2). Praca na tym stanowisku w naszej firmie oznacza: odpowiedzialność za całość rozwiązań współtworzonych wraz z zespołem, tworzenie lub modyfikowanie rozwiązań do przetwarzania danych w chmurze, tworzenie i modyfikowanie dokumentacji, analizowanie i optymalizowanie rozwiązań w zakresie działającego lub projektowanego systemu, analizowanie wymagań klienta pod kątem dostarczenia optymalnego rozwiązania jego potrzeby biznesowej, analizowanie potencjalnych zagrożeń, dostosowywanie rozwiązań względem wymagań biznesowych.","[{""min"": 135, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,135,200,Net per hour - B2B
Full-time,Senior,B2B,Remote,63,Programista Baz Danych,TSS,"W TSS tworzymy najwyższej jakości rozwiązania z zakresuSoftware Development, FinTech, AI Solution. Tworzymy systemy płatnicze, bramki płatnicze online oraz rozwiązania umożliwiające innowacyjne procesowanie płatności. Nasze zespoły uczestniczą również w projektach wykonywanych dla klientów z wielu różnych branż i specjalizacji. Jeśli chcesz dołączyć do zespołu entuzjastów, dla których praca jest jednocześnie pasją, przygodą i możliwością rozwoju zawodowego dołącz do team’u TSS już teraz! Doświadczenie: 5 lat w pracy z relacyjnymi bazami danychnp. PostgreSQL, MySQL, MSSQL, ORACLE 4 lata doświadczenia w zakresie wykorzystania jednego z proceduralnych języków programowania np.PL/SQL, PL/PqSQL Bardzo dobra znajomość SQL orazPostgreSQL Znajomość zasad zarządzania, konfiguracji i optymalizacji bazy danych PostgresSQL Doświadczenie w migracji danych z systemówklasy enterprise Doświadczenia w analizie i transformacji danych Projektowanie i eksploatacja baz danych systemu Optymalizacja obecnie eksploatowanych baz danych systemu Bieżąca współpraca z zespołem wytwórczym Co oferujemy? Możliwość pracy w pełni zdalnej lub w biurze w Warszawie; Stabilną współpracę na podstawie B2B; Dofinansowanie do prywatnej opieki medycznej w PZU; Wsparcie w rozwoju zawodowym - wewnętrzne szkolenia z zakresu cyberbezpieczeństwa;","[{""min"": 14000, ""max"": 18000, ""type"": ""Net per month - B2B""}]",Database Administration,14000,18000,Net per month - B2B
Full-time,Mid,Permanent,Remote,64,Data Engineer,INFOPLUS TECHNOLOGIES,"Job Description Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources data sources using SQL and AWS ‘big data’ technologies. Build analytics tools (Tableu) that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Work with system integration and middlewares (MuleSoft, Talend, Solace and etc) Perform proof of concept with customer in data integration and data load Managing structure and unstructured data set Managing and design data privacy, integrity and security solution Managing data with high confidentiality, integrity and privacy Skills Experience with working in agile methodologies (Scrum or SAFe) Good teamwork and communication skill (Higher Performance Team) Able to be self organized and focus in delivering values to the business Always work with integrity, passion and courage Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) and Nosql unstructured database Familiarity with a variety of databases (Microsoft SQL is mandatory). Familiarity reporting tools and dashboards like Tableu. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Preferable middleware knowledge, example: MuleSoft, Solace. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Experience with big data tools: Hadoop, Spark, Kafka, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, RDS, Redshift Experience with stream-processing systems: Storm, Spark-Streaming, etc. Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.","[{""min"": 220000, ""max"": 250000, ""type"": ""Gross per year - Permanent""}]",Data Engineering,220000,250000,Gross per year - Permanent
Full-time,Senior,B2B,Remote,65,Analityk Systemowy,Detable,"Dołącz doDetable Sp. Z o.o.- prężnie rozwijającej się firmy, która stawia na długofalową współpracę z doświadczonymi profesjonalistami. Od ponad 3 lat jesteśmy partnerem dla instytucji z sektora publicznego, współpracujemym.in. zCentrum e-zdrowia, Aplikacjami Krytycznymi Ministerstwa Finansów, Głównym Urzędem Nadzoru Budowlanego, Urzędem Do Spraw Cudzoziemców,Narodowym Funduszem Zdrowiai wieloma innymi. Nasi konsultanci pracują nad rozwojem aplikacji i systemów, z których korzystają miliony Polaków! Posiadasz minimum 3 lata doświadczenia w pracy na stanowisku Analityka w projektach dla klienta masowego; Na co dzień pracujesz z SQL, Python, PostgresSQL, Oracle; Składnice danych, Mapowania, Raportowanie (BI) nie mają przed Tobą tajemnic; Wykorzystujesz w codziennej procesy ETL/ELT; Miałeś okazję tworzyć modele logiczne i fizyczne z wykorzystniem narzędzia Enterprise Architect; Możesz pochwalić się znajomością i doświadczeniem w obszarze ochrony zdrowia; Posiadasz doświadczenie w obszarze Hurtowni Danych; Dodatkowym atutem będzie jeśli możesz pochwalić się jednym z certyfikatów: AgilePM, certyfikat potwierdzający znajomość SQL/Python. Pozyskiwaniu wymagań systemowo- biznesowych dla projektów IT; Modelowaniem otoczenia i procesów systemowo-biznesowych oraz wytwarzaniem modelu danych; Określanie przypadków użycia systemów informatycznych; Modelowanie danych na poziomie logicznym; Projektowanie i dokumentowanie przepływów danych (ETL) oraz struktur raportowych; Określanie architektury dla projektowanych systemów; Definiowanie interfejsów oraz przepływu komunikacji między modułami; Modelowanie diagramów aktywności, sekwencji oraz stanów dla modułów projektowanych systemów; Proponowanie i konsultowanie rozwiązań systemowych ze zleceniodawcami oraz realizatorami zdefiniowanych wymagań; Wsparcie analityczne na etapach projektowania, wytwarzania i testowania systemów informatycznych. Konkurencyjne wynagrodzenie w oparciu o kontrakt B2B ( do 140PLN netto/h); Długofalową współpracę opartą o wzajemny szacunek i partnerstwo; Dedykowanego opiekuna kontraktu po stronie Detable; Możliwość 100% pracy zdalnej lub z naszego biura w Białymstoku; Możliwość podnoszenia swoich kwalifikacji poprzez skorzystanie z budżetu szkoleniowego; Realny wpływ na rozwój projektu; Atrakcyjny program poleceń pracowniczych; Zdalny proces rekrutacji. Rozmowa HR z naszą IT Rekruterką; Weryfikacja umiejętności technicznych przez naszego Lidera technicznego; Spotkanie z klientem; Decyzja i rozpoczęcie współpracy","[{""min"": 20160, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20160,23520,Net per month - B2B
Full-time,Mid,B2B,Hybrid,66,Data Engineer (Scala),SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: rozwijanie platformy DataHub do przechowywania i przetwarzania danych. Wykorzystywany stos technologiczny: Spark, Scala, TDD, Hadoop, Hive, DataBricks, CI/CD, Git, GitHub, Jenkins, Sonar, Nexus, Jira, SQL, PostgreSQL, rozwój, testowanie i wdrażanie specyfikacji technicznych i funkcjonalnych przygotowanych przez Solution Designerów, Architektów Biznesowych oraz Analityków Biznesowych, zapewnienie poprawnego działania opracowanych rozwiązań, dbanie o zgodność z wewnętrznymi standardami jakości, praca w modelu hybrydowym: 1-2 razy w tygodniu praca z biura w Warszawie, stawka do 170 zł/h przy B2B w zależności od doświadczenia. Ta oferta jest dla Ciebie, jeśli: masz minimum 2 lata doświadczenia z Apache Spark i Scalą, znasz wzorce projektowe i pracujesz w podejściu Test-Driven Development (TDD), masz doświadczenie z technologiami Big Data – Spark, Hadoop, Hive (znajomość Azure Databricks będzie dodatkowym atutem), masz doświadczenie w pracy w metodyce SCRUM/Agile, masz doświadczenie w integracji i zarządzaniu dużymi wolumenami danych, dobrze znasz narzędzia CI/CD i DevOps: Git, GitHub, Jenkins, Sonar, Nexus, Jira, posiadasz znajomość struktur baz danych (PostgreSQL, SQL, Hive), biegle komunikujesz się w języku angielskim (min. B2), mile widziane doświadczenie z Bash, Control-M, Docker, Kubernetes, OS3, Azure, AWS. Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 25200, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,28560,Net per month - B2B
Full-time,Mid,B2B,Hybrid,67,Big Data Analyst,SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: udział w projekcie gromadzącym dane dotyczące użytkowania urządzeń domowych. Wykorzystywany stos technologiczny w projekcie: SQL, Python, R, Scala, Hadoop, Spark, Hive, Kafka, Power BI, DAX, Power Query, przetwarzanie, modelowanie i transformacja danych, tworzenie i wdrażanie modeli danych reprezentujących struktury i relacje danych, programowanie skryptów w Pythonie, R lub Scala na potrzeby analizy i wizualizacji danych, automatyzacja procesów przetwarzania danych, wdrażanie efektywnych metod przechowywania i pozyskiwania danych, integracja technologii Big Data z przepływami danych, projektowanie i tworzenie interaktywnych, profesjonalnych dashboardów w Power BI, zapewnienie intuicyjności i dopasowania dashboardów do potrzeb biznesowych, tworzenie trafnych wizualizacji skutecznie komunikujących wnioski z danych, dostosowywanie wizualizacji do różnych odbiorców biznesowych, optymalizacja przepływów pracy związanych z przetwarzaniem i wizualizacją danych, praca w modelu hybrydowym: min. 1x na miesiąc w biurze w Warszawie, stawka do 120 zł/h przy B2B w zależności od doświadczenia. Ta oferta jest dla Ciebie, jeśli: masz min. 3 lata doświadczenia na stanowisku Data Analyst, masz bardzo dobrą znajomość SQL i technik zapytań do baz danych, masz praktyczne doświadczenie w przetwarzaniu, modelowaniu i transformacji danych, znasz przynajmniej jeden język programowania wykorzystywany w analizie danych (np. Python, R, Scala), masz doświadczenie w pracy z dużymi, złożonymi zbiorami danych, potrafisz tworzyć profesjonalne, interaktywne dashboardy w Power BI, masz doświadczenie w pracy z DAX i Power Query przy modelowaniu i transformacji danych w Power BI, potrafisz przekładać potrzeby biznesowe na rozwiązania techniczne oraz prezentować rekomendacje oparte na danych, znasz język angielski na poziomie min. C1, mile widziane doświadczenie z technologiami Big Data (np. Hadoop, Spark, Hive, Kafka). Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 16000, ""max"": 19200, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,16000,19200,Net per month - B2B
Full-time,Mid,B2B,Remote,68,Data Architect with AWS,Link Group,"About the Role: We are looking for a skilled and experiencedData Architectwith strong knowledge ofAWS cloud servicesto join our team. This role involves designing scalable, secure, and high-performance data solutions in a cloud-native environment. You'll work closely with data engineers, analysts, and business stakeholders to create modern data platforms that support analytics, reporting, and AI/ML initiatives. Design and implement cloud-native data architecture using AWS services Define data models, architecture standards, and best practices for data pipelines, storage, and security Collaborate with stakeholders to understand business and technical requirements Guide development of data lakes, data warehouses, and real-time data streaming systems Ensure compliance with data governance, security, and privacy standards Evaluate and recommend appropriate AWS services and tools Support the development and review of ETL/ELT processes Provide technical leadership to data engineering teams Proven experience as aData Architect,Cloud Architect, orSenior Data Engineerwith architectural responsibilities Expertise inAWS cloud services(e.g., S3, Redshift, Glue, Lambda, EMR, Athena, Kinesis, Lake Formation) Strong knowledge ofdata modeling,ETL/ELT, anddata warehousing concepts Experience designing and implementingdata lakesandmodern data platforms Familiarity withinfrastructure-as-code(e.g., Terraform, CloudFormation) Experience withSQLand at least one scripting/programming language (e.g., Python) Understanding ofdata governance,compliance, andsecurity standards Excellent communication skills in English Experience withSnowflake,Databricks, or similar platforms Knowledge ofmachine learning workflowsand MLOps Familiarity withdata meshorevent-driven architectures","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Architecture,150,180,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,70,Senior Data Engineer,N-iX,"We are seeking a Senior Data Engineer specializing in Databricks to join our global team. You will be instrumental in setting up and maintaining our Databricks platform, building robust data pipelines, and collaborating closely with our solution architects and data scientists. Your expertise will directly support our mission to leverage data and AI effectively within a cutting-edge automotive claims management environment. Key Responsibilities: Design, build, and maintain robust data pipelines within Databricks. Collaborate closely with international teams, including data scientists and architects, to develop scalable data solutions. Debug complex issues in data pipelines and proactively enhance system performance and reliability. Set up Databricks environments on cloud platforms (Azure/AWS). Automate processes using CI/CD practices and infrastructure tools such as Terraform. Create and maintain detailed documentation, including workflows and operational checklists. Develop integration and unit tests to ensure data quality and reliability. Migrate legacy data systems to Databricks, ensuring minimal disruption. Participate actively in defining data governance and management strategies. What We Expect from You (Requirements): 5+ years of proven experience as a Data Engineer. Advanced proficiency in Python for developing production-grade data pipelines. Extensive hands-on experience with Databricks platform. Strong knowledge of Apache Spark for big data processing. Familiarity with cloud environments, specifically Azure or AWS. Proficiency with SQL and experience managing relational databases (MS SQL preferred). Practical experience with Airflow or similar data orchestration tools. Strong understanding of CI/CD pipelines and experience with tools like GitLab. Solid skills in debugging complex data pipeline issues. Proficiency in structured documentation practices. B2 level or higher proficiency in English. Strong collaboration skills, ability to adapt, and eagerness to learn in an international team environment. Nice to have: Experience with Docker and Kubernetes. Familiarity with Elasticsearch or other vector databases. Understanding of DBT (data build tool). Ability to travel abroad twice a year for on-site workshops. Why Join Us Work on impactful projects with cross-functional teams. Opportunity to grow your BI and analytics career in a data-driven organization. Flexible working hours and remote work options. Competitive compensation and benefits. Opportunity to work on presales We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 29553, ""max"": 30661, ""type"": ""Net per month - B2B""}, {""min"": 24381, ""max"": 25489, ""type"": ""Gross per month - Permanent""}]",Data Engineering,29553,30661,Net per month - B2B
Full-time,Senior,B2B,Remote,71,"Senior BI Developer (Power BI, Power Automate)",Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! 😊 We are looking for an experienced Business Intelligence Developer (Power BI)to join our team at a leadingSwedish manufacturing company. In this role, you will take ownership of the frontend development of a proprietary Business Intelligence (BI) and analytics platform used across distribution and customer management services. You’ll become part of across-functional team currently leading the migration of reporting systems from on-premise to the cloud, using modern Microsoft technologies. This is a great opportunity to contribute your expertise to a high-impact transformation initiative. The project environment is dynamic and international, and success in this role will requirestrong technical proficiency,proactive communication, anda solutions-driven mindset. While expectations are high, you’ll be working in a company shaped byScandinavian work culture, wheretrust, autonomy, and transparencyare central values. Responsibilities: Lead frontend development in Power BI Premium (Cloud) for BI and analytics tools. Design, build, and maintain dashboards, reports, and data visualizations that serve business-critical needs. Develop and optimize solutions using Power BI and Power Automate, ensuring high usability and performance. Collaborate with backend engineers, frontend engineers, analysts, and business stakeholders to translate complex requirements into clear, actionable reports. Contribute to the continuous improvement of reporting frameworks and analytical tools across the organization. We offer a B2B Contract: 120-150 PLN net/hour + VAT (depending on experience) You might be the perfect match if you are/have: 4+ years of experience withPower BI(including Premium features) andPower Automate. Proficiency inSQLand a strong understanding of data modeling, cubes, and reporting tools. Experience working oncomplex BI projects(preferablyin the finance sector), involving multiple dashboards and data sources. Familiarity with large-scale BI projects or migration efforts. Strong problem-solving skills and a passion for building well-designed, user-friendly reporting solutions. Clear and proactive communication skills, especially in cross-functional, international teams. Aself-starter mindset, able to work independently, prioritize tasks, and drive projects to completion. Fluent Englishfor smooth collaboration. Located in Polandto support formalities and occasional team meetings. Moreover, we appreciate skills in these areas: Hands-on experience withPython and/or Snowflake. Experience in cross-functional development teams within global organizations. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad – so far we've been in Cape Town, Are, and Barcelona). Perks and benefits: Fully remotework or in our office in Wrocław; Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories. If you apply for this position and match our expectations, then: 1) You will be invited to an HR Screening with our IT Recruiter.2) You will have an interview with client. Submit your application online in one easy step! Apply now!","[{""min"": 120, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,120,150,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,73,Data Warehouse Developer,ITDS,"Shape the Future of Banking Data – Become a DWH Developer Warsaw/ Wrocław based opportunity with hybrid work model (1 day in the office/week) As aData Warehouse Developer, you will be working for our client, a leading financial institution dedicated to digital transformation and data excellence. You will join the Data Warehouse team supporting both strategic initiatives and day-to-day development efforts. The project involves maintaining and enhancing a complex enterprise data platform used for business analysis, regulatory compliance, and operational reporting. You will play a key role in building efficient ETL processes and supporting scalable data solutions, contributing to long-term data architecture goals. Your main responsibilities: Develop and implement ETL processes based on business and technical requirements Optimize existing data pipelines to improve performance and reliability Test and validate technical solutions against data quality standards Collaborate with analysts and developers to understand data flow and dependencies Maintain technical documentation related to ETL jobs and data architecture Support Business-as-Usual tasks and participate in ongoing project work Ensure compliance with internal coding and security standards Monitor and troubleshoot ETL jobs and provide issue resolution Contribute to the continuous improvement of the Data Warehouse platform Participate in team planning and development lifecycle activities You're ideal for this role if you have: Proven experience in developing ETL processes using any technology Strong knowledge of SQL in a Data Warehouse context Hands-on experience working in a Data Warehouse or enterprise data environment Understanding of data pipeline design and data integration concepts Familiarity with Microsoft Azure and Databricks Ability to analyze and troubleshoot performance issues in ETL jobs Good communication skills and the ability to work in a cross-functional team Experience working in an Agile or iterative development environment Strong attention to detail and quality mindset Ability to document technical solutions clearly and accurately We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere. Ref. number 7467","[{""min"": 16800, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Engineering,16800,23100,Net per month - B2B
Full-time,Senior,B2B,Remote,74,Data Engineer with Snowflake & DBT,Holisticon Insight,"Holisticon Insightis a division ofhttp: //nexergroup.comfocused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! 👇https: //holisticon.pl/holisticon-insight/ 🚀 We are looking for aSenior Data Engineerwho is skilled inSnowflake and DBTto work on a project in a team of our client, a Swedish-based leading provider of transport solutions. In the role of Data Engineer, you willoversee and drive the backend development of our client's BI and analytics tool in the Finance Department. You might be the perfect match if you possess the following competencies: 5+ yearsof previous commercial experience in similar role - a strong backgroundin data engineering, data modeling and database management Proven experience withSnowflake and DBT Experience in settingCI/CD pipelinesfor seamless code integration Experience working with complex projects withFinance reports. Proactive communication skills and ability to communicate with different stakeholders Excellent English communication skills Location in Poland or EU 🙌 Nice to have: Experience Migrating from Onprem to Cloud. Experience working with AWS or other cloud By joining Holisticon Insight you will get: Life insurance Multisport card Fully remote job Private medical care Flexible working hours B2B or contract of employment Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories)","[{""min"": 26500, ""max"": 31900, ""type"": ""Net per month - B2B""}]",Data Engineering,26500,31900,Net per month - B2B
Full-time,Senior,B2B,Hybrid,75,DevOps Engineer (AI Team),Jit Team,"Salary: 1000 - 1200 PLN/day on B2B Work model: elastic hybrid from Gdynia / Gdańsk / Warszawa (at least 2 days per week from the office) Why choose this offer? You can expect a flexible work organization The international work environment will give you the opportunity to interact with the English language on a daily basis Scandinavian organizational culture will provide you with work-life balance, you will gain time for additional training (financed by Jit) The Jit community will bring you a nice time during regular integration meetings Project You will be involved in a financial project focused on building and deploying generative and predictive AI models to enhance operations in Financial Crime Prevention. We are currently seeking experienced DevOps Engineer to support the infrastructure, orchestration, and deployment of our AI-powered applications. You’ll be part of a fast-moving team, experimenting with new methods and tools to deliver scalable, production-ready AI solutions. Responsibilities you'll have Design, implement, and manage cloud-based infrastructure for generative AI solutions Develop and maintain orchestration workflows using tools like AWS Step Functions, EventBridge, and Lambda Support data processing pipelines on Glue, EMR, and EKS Collaborate with Data Scientists and Engineers on data transformation using PySpark, Python, and Hadoop ecosystem tools Build and optimize CI/CD pipelines using Jenkins and Terraform Expected competences and knowledge Solid experience with orchestration services such as AWS Step Functions, EventBridge, Managed Workflows for Apache Airflow (MWAA), and AWS Lambda Good knowledge of data processing frameworks and platforms like AWS Glue, EMR, and EKS Hands-on experience working with AWS S3 for data storage and Athena for querying and analysis Proven experience developing Big Data ETL pipelines using PySpark with Python; experience with Spark and Scala is a plus Strong skills in data manipulation and transformation using Python and Pandas Experience working with the Hadoop ecosystem , including tools like Hive, Impala, Sqoop, HDFS, and Oozie Familiarity with CI/CD practices and tools, particularly Jenkins English min. B2 Nice to have: Experience with MLflow and AWS SageMaker for machine learning lifecycle management Familiarity with AWS Bedrock and other generative AI services AWS Certified Cloud Practitioner or higher certification Experience with MLflow and AWS SageMaker for machine learning lifecycle management Familiarity with AWS Bedrock and other generative AI services AWS Certified Cloud Practitioner or higher certification Technologies you'll work with AWS Python Hadoop Jenkins, Terraform Apache Airflow AWS Lambda AWS Glue, EMR, EKS S3, Athena PySpark, Pandas Client – why choose this particular client from the Jit portfolio? Jit Team has had an over-decade-long relationship with the leading financial group in the Nordic countries, and we are privileged to be our client's premier partner in Poland. At present, over 200 Jit personnel are engaged in the completion of more than 60 projects for this Norwegian major provider of financial services with a global presence and a strong focus on modern technology. Our customer's work atmosphere is epitomized by the Scandinavian culture , which is conducive to people who place emphasis on work-life balance and feedback culture . Furthermore, all projects are executed in international teams, giving constant exposure to the English language. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 21000, ""max"": 25200, ""type"": ""Net per month - B2B""}]",Data Science,21000,25200,Net per month - B2B
Full-time,Mid,B2B,Remote,78,Programista PowerBI,Eyzee S.A.,"Poszukujemy Programisty BI z dogłębną znajomością Power BI, który dołączy do naszego zespołu i będzie odpowiedzialny za rozwijanie, utrzymanie oraz optymalizację zaawansowanych rozwiązań Business Intelligence. Jeśli posiadasz doświadczenie w pracy z dużymi zbiorami danych, tworzeniu raportów i dashboardów zarządczych, a także cenisz sobie samodzielność i ciągły rozwój, to ta oferta jest dla Ciebie! Tworzymy przyjazne miejsce pracy i rozwoju dla specjalistów w branży IT, zapewniamy ciekawe wyzwania, dbając o dobrą komunikację i atmosferę w zespole. Z nami przyspieszysz rozwój swojej kariery! Praca zdalna, pełen etat. Zadania dla Ciebie: rozwój i wdrażanie rozwiązań w oparciu o Power BI projektowanie i budowanie procesów ETL tworzenie zaawansowanych raportów i dashboardów zarządczych konfiguracja, rozwiązywanie problemów i wdrażanie hurtowni danych z wykorzystaniem Power BI praca z relacyjnymi bazami danych i programowanie w SQL oraz Python zapewnienie bezpieczeństwa danych i implementacja row-level-security optymalizacja wydajności istniejących rozwiązań Power BI Wymagania: min. 5 lat doświadczenia na stanowisku programisty systemów raportowych w obszarze wizualizacji danych doświadczenie w rozwijaniu rozwiązań Power BI praktyczna znajomość architektury środowiska Power BI umiejętność budowania raportów w Power BI oraz projektowania dashboardów zarządczych znajomość języka DAX oraz technik optymalizacji w Power BI doświadczenie w budowaniu procesów ETL umiejętność pracy z relacyjnymi bazami danych znajomość zagadnień bezpieczeństwa danych w Power BI, w tym umiejętność budowania funkcjonalności row-level-security. znajomość technik agregacji i przetwarzania danych dobra znajomość języka Python Mile widziane: doświadczenie w branży medycznej z systemami przetwarzającymi duże wolumeny danych certyfikat z obszaru znajomości Power BI Co oferujemy? stabilne zatrudnienie w oparciu o kontrakt B2B służbowy laptop i monitor dofinansowanie prywatnej opieki medycznej sportową kartę Multisport nauka języka angielskiego omawianie postępów i rozwoju co pół roku transparentna komunikacja z pracownikami możliwość zaangażowania się w rozwój organizacji chętnie dzielimy się wiedzą - dołącz do Akademii Eyzee mocny kompetencyjnie zespół składający się w większości z seniorów praca z narzędziami JIRA, Confluence, BitBucket dbamy o integracje i chętnie wspólnie spędzamy czas Kim jesteśmy? Jesteśmy polską firmą specjalizującą się w realizacji złożonych projektów informatycznych oraz doradczych dla firm z sektora finansowego, telekomunikacyjnego i publicznego. Stanowimy zgrany zespół konsultantów z wiedzą i wieloletnim doświadczeniem w tworzeniu i utrzymywaniu rozwiązań. Ważne dla nas są: doprecyzowanie wymagań przed napisaniem kodu, jakość tworzonego kodu, testowanie oraz CI/CD. Nasze projekty to głównie tworzenie nowych mikroserwisów lub nowych funkcjonalności do istniejących rozwiązań. Dodatkowo rozwijamy własne aplikacje i plugin’y, które nie tylko usprawniają pracę, ale też pozwalają rozwinąć nasze doświadczenie. Jeden z nich możesz pobrać tutaj (eZee Worklog). Jesteśmy partnerem Atlassian.","[{""min"": 15000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Unclassified,15000,22000,Net per month - B2B
Full-time,Senior,B2B,Remote,81,Analityk Danych,Detable,"Dołącz doDetable Sp. Z o.o.- prężnie rozwijającej się firmy, która stawia na długofalową współpracę z doświadczonymi profesjonalistami.Od ponad 3 lat jesteśmy partnerem dla instytucji z sektora publicznego, współpracujemym.in. zCentrum e-zdrowia, Aplikacjami Krytycznymi Ministerstwa Finansów, Głównym Urzędem Nadzoru Budowlanego, Urzędem Do Spraw Cudzoziemców,Narodowym Funduszem Zdrowiai wieloma innymi. Nasi konsultanci pracują nad rozwojem aplikacji i systemów, z których korzystają miliony Polaków! Posiadasz minimum 5 lata doświadczenia w pracy na stanowisku Analityka Danych / lub na stanowisku związanym z analizą danych lub analizą biznesową w projektach dla klienta masowego; Nie jest Ci obce przetwarzanie i analiza dużych zbiorów danych (Big Data); Na co dzień pracujesz z SQL, Python, PySpark; Wykorzystujesz w codziennej procesy ETL/ELT; Miałeś okazję pracy w Data Quality; Biegle posługujesz się relacyjnymi bazami danych; Możesz pochwalić się znajomością i doświadczeniem w obszarze ochrony zdrowia; Posiadasz doświadczenie w obszarze Hurtowni Danych; Dodatkowym atutem będzie jeśli możesz pochwalić się jednym z certyfikatów: certyfikatAgilePM, certyfikat potwierdzający znajomość Apache Airflow/tworzenie DAGów Airflow/Apache Spark, certyfikat potwierdzający znajomość SQL. Analiza danych i źródeł danych dla projektów IT; Analiza danych pod kątem ich jakości i spójności; Określanie przypadków użycia systemów informatycznych; Zbieranie i specyfikacja wymagań analityczno-raportowych; Projektowanie i dokumentowanie przepływów danych (ETL) oraz struktur raportowych; Zbieranie i specyfikacja wymagań analityczno-raportowych; Utrzymywanie modelu procesów biznesowych w zakresie usług analityczno-raportowych; Zapewnienie spójności procesów analizy i projektowania; Utrzymanie i rozwój kanonicznego modelu danych; Ocenianie i analiza zbiorów danych; Przygotowywanie analiz i dokumentacji projektowej; Współpraca z zespołem projektowym, wytwórczym i interesariuszami w celu zapewnienia zgodności działań z oczekiwaniami i wymaganiami projektowymi w zakresie praktyki Zarządzania Danymi; Proponowanie i konsultowanie rozwiązań systemowych ze zleceniodawcami oraz realizatorami zdefiniowanych wymagań; Wsparcie analityczne na etapach projektowania, wytwarzania i testowania systemów informatycznych. Konkurencyjne wynagrodzenie w oparciu o kontrakt B2B ( do 140PLN netto/h); Długofalową współpracę opartą o wzajemny szacunek i partnerstwo; Dedykowanego opiekuna kontraktu po stronie Detable; Możliwość 100% pracy zdalnej lub z naszego biura w Białymstoku; Możliwość podnoszenia swoich kwalifikacji poprzez skorzystanie z budżetu szkoleniowego; Realny wpływ na rozwój projektu; Atrakcyjny program poleceń pracowniczych; Zdalny proces rekrutacji. Rozmowa HR z naszą IT Rekruterką; Weryfikacja umiejętności technicznych przez naszego Lidera technicznego; Spotkanie z klientem; Decyzja i rozpoczęcie współpracy","[{""min"": 20160, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20160,23520,Net per month - B2B
Full-time,Mid,B2B,Remote,82,Data Engineer,best HR and PM solutions,"For our client we are looking forData Engineer. Client is the emerging leader in the $100B+ cloud communications platform market. Customers like Airbnb, Viber, Whatsapp, Snapchat, and many others depend on client's APIs and SDKs to connect with their customers all over the world. As businesses continue to shift to a real-time, customer-centric communications model, we are experiencing a time of impressive growth. They're looking for a Data Analyst or Data Engineer to join the Engineering Productivity team and help them make smarter, data-driven decisions that improve developer productivity and engineering outcomes. You'll work closely with development, platform, and product teams to analyze, structure, and optimize data flows related to engineering metrics, DevOps performance, and platform usage. This role is ideal for someone who thrives in transforming ambiguous data into actionable insight and is excited about the potential of AI tools in developer platforms. Key Responsibilities: Analyze engineering, operational, and productivity data to uncover trends, risks, and opportunities Design and implement data models that improve accessibility, structure, and long-term maintainability of engineering metrics. Build or enhance ETL pipelines to collect, transform, and export data from various systems (e.g., GitHub, Jira, Security Scans, Costs tools). Partner with stakeholders to define meaningful KPIs across engineering domains (e.g., reliability, security, velocity). Explore and implement GenAI tooling to support automation, summarization, and pattern detection in engineering workflows. Maintain data hygiene and enforce best practices in data governance and lineage within the API Engineering environment. What You’ll Gain: A unique opportunity to shape how engineering data is used across a large and evolving platform organization. The chance to make a difference using GenAI tools in a real-world engineering context. Collaboration with a team driving developer experience, reliability, and engineering consistency at scale. Required Skills and Experience: Proven experience as a Data Analyst or Data Engineer, preferably in a software engineering or DevOps context. Strong SQL skills and experience with Python or another scripting language for data transformation and analysis. Hands-on experience working with APIs and integrating data across SaaS tools (e.g., Jira, GitHub, Datadog). Familiarity with dashboarding/visualization platforms like Looker, Grafana or Tableau. Demonstrated experience structuring unorganized or siloed data into actionable reporting models. Desirable: Experience designing and building ETL pipelines and data lakes or warehouses (e.g. Snowflake). Exposure to GenAI tooling and experience applying AI to engineering or operational workflows. Knowledge of modern data orchestration tools (e.g., Airflow, dbt). Understanding of software development lifecycle and metrics used in engineering productivity and platform health. What we offer: Contract: B2B directly with US company Salary: up to 160 pln/h 100% remote Polish time zone Polish public holidays Long term cooperation Recruitment process: 1-2 technical calls","[{""min"": 120, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,85,Remote Data Engineer,Ework Group,"Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. 🔹 For our Client we are looking forData Engineer with AWS experience - remote work🔹 At our Digital Data & IT, we’re on a mission to unlock the full potential of data towards helping more patients worldwide. It's an ambitious mission that requires many bright minds to succeed. So, we warmly welcome your data engineering skills to help us unfold smarter solutions that better support the business needs and, in the end, save more lives. Do you want to join a purpose-driven ride? Then read on. ✔️What you will be doing: As a Data Engineer, you will be an integral member of the Data Harmonisation Layer (DHL) Agile team. DHL is a global platform designed to collect and harmonize of data from various Operational Technology (OT) data sources across company production facilities. DHL makes data easily accessible to develop several front-end Manufacturing Intelligence (MI) products, focusing on improving our production processes. Key technologies used in DHL are Kafka, Ignition, and AWS services. ✔️ Your main responsibilities include: Software development activities i.e., writing, reviewing, refactoring, testing, and documenting the code base Build production ready stable and scalable data pipelines Act as a trusted advisor to provide technical expertise on one or more projects for our business partners Work with the Agile teams to understand and handling enabler work and work towards technical agility Contribute in Agile events such as Program Increment (PI) planning, system demos, and Inspect and Adapt (I&A) We are looking forAn ambitious and proactive colleague who can contribute to the team both professionally and personally. ✔️ On a professional level, you have: A master’s degree in computer science, engineering, chemistry, biology, or other relevant fields Experienced with data engineering principles such as data warehousing, batch processing, data streaming, data lakes, databases, and data modelling Experience with building CI/CD pipelines Coding/scripting skills (Bash, Python, or similar) Hands-on experience in AWS services such as Lambda, Glue, IAM, Kinesis, MKS, Step Functions, DMS, RDS, Managed Grafana, SNS, S3, CloudWatch, etc Hands-on experience with IaC for managing the cloud Strong mathematical, statistical, and problem-solving skills ✔️ On a personal level, you are: Strategic mindset Innovative mindset Strong communicator Systematic ✔️The team waiting for you... You will be part of the Manufacturing Intelligence (MI) department. Our vision is to enable data using cutting-edge technologies and empower our colleagues in Product Supply to make data-driven decisions to continuously improve manufacturing processes, in short #EnableDataEmpowerDecisions. ✔️ We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 162, ""max"": 179, ""type"": ""Net per hour - B2B""}]",Data Engineering,162,179,Net per hour - B2B
Full-time,Senior,Permanent,Hybrid,86,Senior Data Engineer,Sparta Global,"Senior Data Analytics Specialist – Financial Services (Kraków) Join one of our leading financial clients in Kraków and play a key role in delivering accurate, timely, and insightful data and management information. You’ll be supporting Risk, Compliance, and Finance functions within a major global financial institution. Role Overview As a Data Analytics Specialist, you will be responsible for ingesting data from internal and external sources, performing quality assessments, and creating visualisations. The role also includes peer benchmarking, platform security, capacity management, and active contribution to global risk data projects. Key Responsibilities Perform data analysis, ensuring data integrity and suitability for intended use. Collaborate with model development and monitoring teams to understand data requirements and governance standards. Contribute to planning the data and business process architecture roadmap. Prepare and present insights to senior stakeholders, fostering strong cross-functional relationships. Identify opportunities for process improvement and drive efficiency initiatives. About You – Qualifications & Experience A bachelor’s degree in IT, Computer Science, or a numerate discipline. At least 5 years’ experience in data analytics or a related role. Proficiency in programming and data tools such as SAS, Python, PySpark, and SQL. Solid understanding of business analysis, data architecture, and project management principles. Strong organisational and analytical skills with the ability to manage multiple tasks effectively. Familiarity with JIRA, Confluence, and an aptitude for working under pressure. Fluency in English (spoken and written).","[{""min"": 20000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,27000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,87,Senior MLOps Engineer (Azure/ AWS),Scalo,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania tom.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! budowa nowoczesnej Platformy Analitycznej opartej o Azure Databricks w domenie finansowej, implementacja narzędzi monitorujących działanie modeli ML w środowisku produkcyjnym, bliska współpraca z zespołami Data Science w zakresie MLOps i platformy zapewnienie wysokiej dostępności, bezpieczeństwa i skalowalności platformy ML, implementacja procesów trenowania, wdrażania, wersjonowania i monitorowania modeli w środowisku chmurowym, integracja rozwiązań ML z innymi systemami biznesowymi w organizacji, udział w długoterminowej rozbudowie i doskonaleniu Platformy Analitycznej, praca 100% zdalna, a dla chętnych możliwość pracy z biura we Wrocławiu, stawka do 200 zł/h przy B2B, w zależności od doświadczenia. masz minimum 3-4 lat doświadczenia w MLOps oraz budowie platform ML w chmurze (Azure preferowane, AWS także akceptowalne), znasz bardzo dobrze Azure Databricks (konfiguracja, utrzymanie, optymalizacja kosztów), posiadasz umiejętności programistyczne w Python i SQL (R będzie dodatkowym atutem), pracowałeś z MLFlow, Spark, Rest API oraz narzędziami CI/CD i automatyzacją pipeline’ów, masz doświadczenie z Docker, Kubernetes; Terraform będzie plusem, znasz cykl życia modeli ML od trenowania po wdrożenie i monitoring, posługujesz się językiem angielskim na poziomie B1/B2 (dokumentacja, okazjonalne spotkania). długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 26880, ""max"": 33600, ""type"": ""Net per month - B2B""}]",Data Science,26880,33600,Net per month - B2B
Full-time,Mid,B2B,Remote,88,BI Consultant (Power BI),Onwelo Sp. z o.o.,"Poznaj Onwelo: Jesteśmy nowoczesną polską firmą technologiczną, która dostarcza wsparcie eksperckie organizacjom na całym świecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiązania IT, oferując przy tym solidne zaplecze kompetencyjne. W ciągu kilku lat zrealizowaliśmy ponad 300 projektów w Europie i USA, dynamicznie rozbudowując zespół do kilkuset specjalistów i otwierając sześć biur w Polsce oraz oddziały w USA, Niemczech i Szwajcarii. Będziesz pełnić zarówno rolę konsultanta, jak i developera, pomagając organizacjom w optymalizacji procesów raportowania i podejmowaniu świadomych decyzji opartych na danych. Dołącz do zespołu Data & Analytics w Onwelo, który realizuje projekty dla różnych klientów – zarówno polskich, jak i zagranicznych. W zależności od Twoich kompetencji i dostępności będziesz mieć możliwość pracy nad wdrożeniami Power BI do różnych organizacji, migracją narzędzi BI (np.Tableau) do Power BI, wsparciem biznesu w definiowaniu potrzeb analitycznych oraz budowę nowoczesnych hurtowni danych. Tworzyć i rozwijaćraporty oraz dashboardy w Power BIzgodnie z wymaganiami biznesowymi. Modelować dane, tworzyćzapytania DAXoraz optymalizować wydajność raportów. Projektować i wdrażaćprocesy ETLdo przekształcania oraz ładowania danych. Integrować danez różnych źródeł – baz danych SQL, plików Excel, usług chmurowych i systemów ERP/CRM stanie się częścią Twojej codziennej pracy. Współpracować z zespołami biznesowymi i technicznymi, aby zapewnićefektywne raportowanie i analizę danych. Utrzymywać i optymalizowaćistniejące rozwiązania BI, dbając o ich wydajność oraz skalowalność. Pracować nadmigracją danych i systemów BI, np. przenoszeniem raportów zTableau lub Qlik do Power BI. Brać udział wbudowie i rozwoju hurtowni danych np. Snowflake, modelując i integrując dane. Doradzać w zakresienajlepszych praktyk BIoraz wspierać klientów w podejmowaniuświadomych decyzji opartych na danych. Maszmin. 3 lata doświadczeniaw pracy zPower BI– tworzysz raporty, dashboardy i modelujesz dane. ZnaszDAX oraz Power Queryi potrafisz je wykorzystać do analizy oraz transformacji danych. Swobodniemodelujesz danei projektujeszoptymalne struktury raportowe. Posiadasz doświadczenie w pracy zbazami danych SQL– tworzysz zapytania, optymalizujesz je i integrujesz dane. Znasz procesyETLi masz praktyczne doświadczenie wprzekształcaniu oraz łączeniu danychz różnych źródeł. Rozumiesz potrzeby biznesowe i potrafisz przekładać je nakonkretne rozwiązania analityczne. Posługujesz się językiemangielskim na poziomie min. B2, co pozwala Ci pracować w międzynarodowym środowisku. Dodatkowym atutem będzie, jeśli: Masz doświadczenie wmigracji narzędzi BI(np.Tableau, Qlik) do Power BI. ZnaszDynamic 365 Praca zeSnowflakenie jest Ci obca Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczędzisz czas na dojazdach – pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 800, ""max"": 950, ""type"": ""Net per day - B2B""}]",Data Analysis & BI,800,950,Net per day - B2B
Full-time,Senior,B2B,Hybrid,89,Data Engineer,Unit8,"Who We Are Founded in 2017, Unit8 is a fast-growing Swiss AI and data analytics consulting and services company dedicated to solving complex problems of traditional industries like automotive, chemical, financial services, manufacturing and pharma. We work with some of the biggest organisations in Europe to solve the challenges that directly affect their business - be it operations, finance, manufacturing or R&D. Since our foundation, we have successfully delivered more than 180 projects and have grown to 140+ talented individuals across 6 office locations. Unit8's mission is to drive the adoption of AI and Data Science in the non-digital industries and to help accelerate their digital transformation. Among its activities, Unit8 dedicates a part of its resources and time on projects that deeply matter to us - that includes collaboration on pro-bono and ""engineering for good"" causes. You can learn more about what we are passionate about at Unit8 Talks and on our website you can find more useful information about our business and recruiting process. These are some examples of projects that we have been working on Building a real-time production line monitoring system for the pharmaceutical industry in order to improve the throughput of the production processes for the Pharmaceutical Industry. Technologies: AWS, EKS, CloudFormation, Docker, Python Building system for calculating and delivering global climate scores i.e. estimating the impact of climate change on the risk of natural disasters (i.e. floods, wildfires & droughts) for the Insurance Industry. Technologies: PySpark, Python, Proprietary Data Processing Platform Building a modern data science platform including a data lake for Chemical Industry. Technologies: Azure, ML Studio, Data Factory, Databricks, Spark About You As a member of agile project teams, your mission will be to build solutions and infrastructure aiming at solving the business problems of our clients. You are a proficient software engineer who knows the fundamentals of computer science and you master at least one widely adopted programming language (Python, Java, C#, C++). You know how to write distributed services and work with high-volume heterogeneous data, preferably with distributed systems such as Spark. You are knowledgeable about data governance, data access, and data storage techniques. You have strong client-facing skills: comfortable interacting with clients (business & technical audience), delivering presentations, problem-solving mindset. You are willing to travel to meet with our clients and the team (mainly in Europe - up to 10% of your time). You are eligible to register as a sole trader (self-employment) in Poland. Don't worry if you don't know how to do the registration, we can help with that. What You'll Do Design, build, maintain, and troubleshoot data pipelines and processing systems that are relied on for both production and analytics applications, using a variety of open-source and closed-source technologies. Help drive optimization, testing, and tooling to improve data quality. Collaborate with other software engineers, ML experts, and stakeholders, taking learning and leadership opportunities that will arise every single day. Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives. What we offer Opportunity to shape the expansion of one of the leading Swiss data & AI consultancies Compensation package including base salary, yearly bonus based on the both individual + company performance Above the norm flexibility regarding when and from where you work as long as both client and internal commitments are met Work on cutting-edge data, AI & analytics topics (e.g., Generative AI) that have real impact across industries Dedicated time and budget for training and pro-bono projects 30 paid days off Private health care and Multisport Cross offices/company-wide frequent events (off-site or online) as well as quarterly budget to spend with the team on after-work activities","[{""min"": 20000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,20000,26000,Net per month - B2B
Full-time,Mid,B2B,Remote,90,Oracle SQL/APEX Developer,Pretius,"WPretiusposzukujemyOracle SQL/APEX Developeraw projekcie platformy w obszarze Healthcare na rynku UK. Lokalizacja: zdalnie lub Warszawa Wynagrodzenie: 90-150 pln netto/h O projekcie: Współpraca z zespołem projektowym, szacowanie zmian, definiowanie API, wsparcie testów Tworzenie formatek APEX, zapytań SQL i pakietów PL/SQL Wykorzystanie technologii frontendowych do budowy UI Udział w R&D i wprowadzaniu nowych rozwiązań technicznych Stack: SQL, PL/SQL, Oracle APEX (wer. 18+), Oracle Cloud Oczekiwania: Dobra znajomość relacyjnych baz danych (struktury, SQL, PL/SQL - funkcje, procedury, pakiety) Doświadczenie w budowaniu aplikacji web komunikujących się z bazą danych Znajomość standardów w obszarach testów, CI/CD, jakości kodu Podstawowa znajomość HTML/CSS/JS Język angielski - B2+ Co oferujemy w Pretius? Stawiamy na długofalowe relacje oparte na uczciwych zasadach i rzetelności Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Możliwość pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnętrzne, konferencje, certyfikacje","[{""min"": 90, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,90,150,Net per hour - B2B
Full-time,Senior,B2B,Remote,91,Senior Administrator Baz Danych Oracle / Senior Oracle DBA,Simora,"Dołącz do naszego zespołu jako Senior Administrator Baz Danych Oracle! Poszukujemy osoby, która zajmie się zarządzaniem bazami danych naszych klientów. Jesteśmy firmą, która rozwija nowoczesne rozwiązania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzędzi wspierających naszych klientów. Cenimy pozytywną atmosferę, wzajemny szacunek i zaangażowanie – to fundamenty naszego zespołu. Twój zakres obowiązków Tworzenie baz danych oraz środowisk bazodanowych na środowisku produkcyjnym i testowym Migrowanie baz danych na nowe środowiska Zarządzania i aktualizacja baz danych i środowisk bazodanowych Konfigurowanie i optymalizacja środowiska bazodanowego Automatyzacja zadań za pomocą języków skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jakości i bezpieczeństwa dla tworzonych rozwiązań Wdrażanie rozwiązań opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Stałe doskonalenie sposobu Twojej pracy Nasze wymagania Min 5 letnie doświadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomość języka SQL i PLSQL Umiejętność dbania o szczegóły i jakość rozwiązań Komunikatywność Znajomość języka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykształcenie wyższe (preferowany kierunek: informatyka) Znajomość języków programowania: Python, Java itp. Znajomość systemów operacyjnych Linux / Windows Znajomość systemów wirtualizacyjnych np.: OLVM Oferujemy Ciekawą pracę w firmie o wysokiej dynamice rozwoju Możliwość podniesienia kwalifikacji w obszarach związanych z bazami danych, bezpieczeństwem danych oraz sztuczną inteligencją Benefity Karta Multisport Prywatna opieka zdrowotna – Medicover Praca zdalna Brak dress code’u Dofinansowanie szkoleń i kursów Elastyczny czas pracy","[{""min"": 10000, ""max"": 16000, ""type"": ""Net per month - B2B""}]",Database Administration,10000,16000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,92,Data Visualization Specialist (Tableau),ITDS,"Data Visualization Specialist (Tableau) Join us, and shape how data tells its story! Kraków - based opportunity with hybrid work model (6 days/month in the office). As a Data Visualization Specialist, you will be working for our client, a leading global financial institution focused on optimizing financial resource management through innovative IT solutions. You will be joining a high-performing team to design and deliver advanced data visualizations that turn complex datasets into meaningful insights. Your work will directly support strategic decision-making by developing and maintaining dashboards, ensuring data integrity, and promoting best practices in data sharing. This is an exciting opportunity to shape visualization standards and influence how data is consumed across the organization. Your main responsibilities: Developing advanced dashboards and reports using Tableau Managing data sources to ensure accuracy and consistency Collaborating with stakeholders to gather analytical requirements Maintaining and overseeing project sites on Tableau Server Documenting dashboards, data sources, and development processes Promoting best practices for data sharing and user access management Ensuring optimal performance of Tableau workbooks for large datasets Supporting infrastructure and architectural requirements for Tableau deployment Following internal control standards and compliance procedures Working with cross-functional teams to deliver effective visual solutions You're ideal for this role if you have: 3+ years of professional experience in data visualization or analysis Proven proficiency in Tableau development and dashboard optimization Ability to handle large data sets while ensuring performance efficiency Experience managing Tableau Server project sites Strong analytical mindset with a high level of mathematical competence Familiarity with data governance and compliance standards Ability to collaborate effectively with cross-functional stakeholders Excellent communication and documentation skills Strong attention to detail and problem-solving abilities Demonstrated ability to work in fast-paced, high-stakes environments It is a strong plus if you have: Working knowledge of Tableau administration and architecture Experience developing dashboards in QlikSense Understanding of infrastructure best practices for enterprise data tools Familiarity with regulatory environments in financial institutions Knowledge of offshoring practices and documentation workflows We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7098 You can report violations in accordance with ITDS’s Whistleblower Procedure available here .","[{""min"": 20800, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,20800,28000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,93,Senior Data Engineer,N-iX,"#3204 Join our team to work on enhancing a robust data pipeline that powers our SaaS product, ensuring seamless contextualization, validation, and ingestion of customer data. Collabd engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Client was established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client’s platform rates with product teams to unlock new user experiences by leveraging data insights. Engage with domain experts to analyze real-world empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renowned Scandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities : Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements : Programming: Minimum of 3-4 years as a data engineer, or in a relevant field Python Proficiency: Advanced experience in Python, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights Cloud: Familiarity with cloud platforms (preferably Azure) Data Platforms: Experience with Databricks, Snowflake, or similar data platforms Database Skills: Knowledge of relational databases, with proficiency in SQL. Big Data: Experience using Apache Spark Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability Version Control: Experience with Gitlab or equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have : Experience with Docker and Kubernetes Experience with document and graph databases Ability to travel abroad twice a year for an on-site workshops","[{""min"": 18359, ""max"": 30993, ""type"": ""Net per month - B2B""}, {""min"": 14776, ""max"": 26228, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18359,30993,Net per month - B2B
Full-time,Senior,B2B,Remote,94,Senior Data Engineer,emagine Polska,"Project information: Industry: Insurance and IT services Rate: up to 175 zł/h netto + VAT Location: remote work with occasional meetings (Warsaw) Project language: English Summary: Join a newly forming team responsible for building the next Data Hub in a large-scale enterprise environment. You’ll have a direct impact on the architecture and development of a modern data platform based on a proven internal framework (Databricks + Azure). The platform supports a variety of use cases, from analytics and reporting to operational systems and Generative AI. We’re looking for an experienced Senior Data Engineer to shape the foundation of this new initiative and bring deep expertise in scalable data engineering solutions. Your Responsibilities: Data Hub Development: Implement a scalable Data Hub platform based on a company-wide framework using Azure and Databricks. Data Engineering: Build and optimize both batch and streaming data pipelines using Python and PySpark. Architectural Collaboration: Work closely with the Data & Solution Architect to implement enterprise-grade architecture. Technical Standards & Mentorship: Help define best practices, participate in code reviews, and support knowledge sharing within the team. Automation & CI/CD: Automate deployment and data operations using tools like Azure DevOps, Terraform, Docker, and Kubernetes. Data Quality & Monitoring: Ensure data validation, anomaly detection, and pipeline monitoring. Cross-Team Collaboration: Collaborate with multidisciplinary teams to align technical solutions with business goals. Documentation: Produce high-quality technical documentation in line with corporate standards. Must-Have Qualifications At least6 years of experienceas a Data Engineer in enterprise-scale environments. Proficiency inPythonandPySpark. Solid hands-on experience withMicrosoft Azure. Familiarity withAzure DevOpsand automation workflows. Practical knowledge ofDatabricks. Comfortable working incross-functional teams(e.g., architects, DevOps, analysts) within complex organizational structures. Fluent inEnglish(minimumB2 level). Nice to Have Hands-on experience withdbt(Data Build Tool) andDLT pipelinesin Databricks. Understanding ofmedallion architecturein a production context. Background inlarge enterprisesorconsulting firms, ideally with exposure to complex data ecosystems. Experience working inAgile/Scrumdevelopment environments. Experience with infrastructure as code (Terraform), containerization (Docker), orchestration (Kubernetes).","[{""min"": 160, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,175,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,95,Database Administrator MS SQL,emagine Polska,"INFORMACJE O PROJEKCIE: Branża : Bankowość Lokalizacja : hybryda (1 x w tygodniu w biurze Kraków lub Warszawa) Stawka: do 120 PLN/h netto + VAT (B2B) Szukamy doświadczonego administratora baz danych MS SQL, który posiada 3-5 letnie doświadczenie w tej roli. Idealny kandydat powinien wykazać się solidnymi umiejętnościami w administracji baz danych oraz dostosowywaniu systemów operacyjnych i silnika baz danych do złożonych wymagań. Administrator baz danych MS SQL będzie odpowiedzialny za zapewnienie ciągłości działania baz danych oraz wsparcie podczas incydentów i awarii. OBOWIĄZKI: Wdrażanie i utrzymywanie systemów baz danych MS SQL Server. Wykonywanie zadań administracyjnych, takich jak instalacja, konfiguracja i uaktualnienia. Monitorowanie wydajności baz danych oraz identyfikowanie problemów. Wdrażanie i utrzymywanie bezpieczeństwa baz danych. Współpraca z zespołami rozwoju i operacji. Planowanie i wykonywanie procedur tworzenia kopii zapasowych. Opracowywanie planów odzyskiwania po awarii. Automatyzowanie zadań przy użyciu języków skryptowych. Tworzenie dokumentacji operacyjnej i technicznej. WYMAGANIA: 3+ lat doświadczenia jako administrator bazy danych. Solidne doświadczenie z bazami danych MS SQL Server. Udokumentowane doświadczenie w implementacji i utrzymywaniu baz danych . Doświadczenie w dostrajaniu wydajności baz danych. Znajomość najlepszych praktyk bezpieczeństwa baz danych. Doświadczenie w procedurach tworzenia kopii zapasowych. Znajomość języków skryptowych, takich jak PowerShell. Silne umiejętności analityczne i rozwiązania problemów. Doskonałe umiejętności komunikacyjne. Umiejętność pracy w zespole. Doświadczenie z s ystemem operacyjnym Windows Server. Znajomość bankowości będzie atutem, ale nie jest obowiązkowa. Komunikatywna znajomość jęz. angielskiego. MILE WIDZIANE: Doświadczenie z usługami baz danych w chmurze. Doświadczenie z bazami danych PostgreSQL. Doświadczenie z systemem operacyjnym Linux. Doświadczenie z IBM InfoSphere Data Replication.","[{""min"": 100, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Database Administration,100,120,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,96,Senior Looker Specialist (LookML),Spyrosoft,"Requirements: Minimum 2 years of experience as a Data Engineer working with Google Cloud Platform (GCP) and cloud-based infrastructure. Hands-on experience with LookML and Looker modelling language. Proven experience in transforming and migrating data models from SSAS and Power BI Semantic Models to LookML. Ability to translate DAX expressions into LookML formulas. Deep understanding of GCP services and cloud computing architecture. Strong background in designing, building, and deploying cloud-based data pipelines, including ingestion from various data sources (e.g., relational databases). Proficiency in data modelling and database optimisation, including query tuning, indexing, and performance optimisation for efficient data processing and retrieval. Nice to Have: Experience with at least one orchestration and scheduling tool – Airflow is strongly preferred. Familiarity with ETL/ELT processes and the ability to integrate data from multiple sources into a usable analytical format. Working knowledge of modern data transformation tools such as DBT and Dataform. Strong communication skills to collaborate effectively with cross-functional teams (data scientists, analysts, business stakeholders). Ability to translate technical concepts into business-friendly language and present findings. Experience leading or actively contributing to discussions with stakeholders to identify business needs and improvement opportunities. Relevant certifications in big data technologies and/or cloud platforms (GCP, Azure). Main responsibilities: Design and develop data models in LookML, adhering to best practices. Support business users in building Looker dashboards. Migrate existing data structures and models to LookML. Support and mentor team members in designing and managing LookML-based data models. Build efficient and optimised aggregations and calculations for analytics purposes. Optimise and continuously improve existing data models to enhance performance and usability.","[{""min"": 120, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Unclassified,120,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,99,PowerBI Developer,Spyrosoft,"Project description: Our Partner is a prestigious Saudi Arabian conglomerate that stands today as one of the Middle East’s most influential family businesses, blending commercial expansion, global partnerships, and impactful philanthropy. Over eight decades, it has diversified into seven major sectors, including automotive, energy and financial services, operating in over 30 countries. Together, we are looking for a skilled and proactivePower BI Data Engineersto join a cross-functional analytics team supporting a client from the automotive and mobility industry. This is a delivery-focused role requiring fast turnaround, strong attention to UX and quality, and close collaboration with both technical teams and non-technical stakeholders across various countries and cultures. Main responsibilities: Designing and delivering intuitive dashboards and reports that consolidate data from multiple business systems to provide critical insights for company leadership, Play a key part in helping decision-makers track performance across sales, revenue, margins, and regional brand performance. Must-Have Skills and Experience: AdvancedPower BIskills, including data modeling, DAX, and dashboard/report creation Proficient inSQLand experience working with relational databases Experience integrating data from APIs into reporting solutions Ability to design clean, user-friendly dashboards for both desktop and mobile formats Strong understanding of KPIs, metrics, and performance reporting in a business context Excellent communication skills – able to present insights clearly to non-technical stakeholders Strong problem-solving skills and attention to detail Proactive, self-motivated, and able to manage work independently Comfortable working in fast-paced, delivery-driven environments with tight deadlines Culturally aware and respectful – capable of working effectively in an international, multicultural team English proficiency at B2 level or higher (spoken and written) Familiarity with Microsoft Dynamics CRM and Business Central Experience working with external data sources like Google Analytics Background in the automotive industry or dealership operations Previous experience in Agile, cross-functional, or distributed teams Strong collaboration skills – open to feedback and able to contribute constructively in a team setting Adaptable mindset – comfortable with change and shifting priorities Interest in the automotive and mobility domain Our Partner is a prestigious Saudi Arabian conglomerate that stands today as one of the Middle East’s most influential family businesses, blending commercial expansion, global partnerships, and impactful philanthropy. Over eight decades, it has diversified into seven major sectors, including automotive, energy and financial services, operating in over 30 countries. Together, we are looking for a skilled and proactivePower BI Data Engineersto join a cross-functional analytics team supporting a client from the automotive and mobility industry. This is a delivery-focused role requiring fast turnaround, strong attention to UX and quality, and close collaboration with both technical teams and non-technical stakeholders across various countries and cultures.","[{""min"": 90, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,90,160,Net per hour - B2B
Part-time,Mid,B2B,Remote,100,Regular Data Scientist,P&P Solutions,"🚀 We are looking for aData Scientistfor our client from the insurance sector to carry out apilot project in the area of predictive scoring. The project aims to assess the potential of predictive models within the client's environment. If the results of the pilot are positively evaluated, the client plans to extend the project to a production phase, which will includeoperationalizing machine learning models on the Microsoft Azure platformandintegrating them with Microsoft Dynamics. 🕐 Start no later than thebeginning of September, initially for25 man-days, with the option to extend for an additional 3 months. 2+ years of experiencein Data Science and/or Machine Learning Proficiency inPythonorR(knowledge of one is sufficient) Familiarity with theMicrosoft Azureplatform Strong analytical mindset and practical experience with predictive modeling Good communication skills and ability to work independently as well as in a team English language proficiency at B1/B2 level Key Technologies: Azure, Machine Learning, Python Performexploratory data analysis (EDA)on customer and insurance scoring data Build and evaluate 2–3 competitive machine learning models, including training and testing phases Prepare a summary of final results, including model performance metrics and key insights Validate the results with the clientto support a go/no-go decision for production deployment Rate: up to 135 PLN/hour net(B2B contract) Flexible payment method Short 14-day invoice payment term Opportunity to work oninteresting and growth-oriented projects","[{""min"": 100, ""max"": 135, ""type"": ""Net per hour - B2B""}]",Data Science,100,135,Net per hour - B2B
Full-time,Mid,B2B,Remote,101,Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Development and maintenance of a large platform for processing automotive data. A significant amount of data is processed in both streaming and batch modes. The technology stack includes Spark, Cloudera, Airflow, Iceberg, Python, and AWS. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Centralized reporting platform for a growing US telecommunications company. This project involves implementing BigQuery and Looker as the central platform for data reporting. It focuses on centralizing data, integrating various CRMs, and building executive reporting solutions to support decision-making and business growth. 🎁 Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. 🚀 Your main responsibilities: Develop and maintain a high-performance data processing platform for automotive data, ensuring scalability and reliability. Design and implement data pipelines that process large volumes of data in both streaming and batch modes. Optimize data workflows to ensure efficient data ingestion, processing, and storage using technologies such as Spark, Cloudera, and Airflow. Work with data lake technologies (e.g., Iceberg) to manage structured and unstructured data efficiently. Collaborate with cross-functional teams to understand data requirements and ensure seamless integration of data sources. Monitor and troubleshoot the platform, ensuring high availability, performance, and accuracy of data processing. Leverage cloud services (AWS) for infrastructure management and scaling of processing workloads. Write and maintain high-quality Python (or Java/Scala) code for data processing tasks and automation. 🎯 What you'll need to succeed in this role: At least 3 years of commercial experience implementing, developing, or maintaining Big Data systems, data governance and data management processes. Strong programming skills in Python (or Java/Scala): writing a clean code, OOP design. Hands-on with Big Data technologies like Spark , Cloudera, Data Platform, Airflow, NiFi, Docker, Kubernetes, Iceberg, Hive, Trino or Hudi. Excellent understanding of dimensional data and data modeling techniques. Experience implementing and deploying solutions in cloud environments. Consulting experience with excellent communication and client management skills, including prior experience directly interacting with clients as a consultant. Ability to work independently and take ownership of project deliverables. Fluent in English (at least C1 level). Bachelor’s degree in technical or mathematical studies. ➕ Nice to have: Experience with an MLOps framework such as Kubeflow or MLFlow. Familiarity with Databricks, dbt or Kafka. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn ).","[{""min"": 15120, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Data Engineering,15120,21000,Net per month - B2B
Full-time,Senior,B2B,Remote,104,Senior Data Modeller,GS Services,Poszukujemy superbohaterów – Senior Data Modeller’ów! 🦸‍♀️🦸‍♂️,"[{""min"": 180, ""max"": 210, ""type"": ""Net per hour - B2B""}]",Data Science,180,210,Net per hour - B2B
Full-time,Senior,B2B,Remote,105,Senior Data Engineer,Link Group,"Senior Data Engineer We are looking for a seasoned Senior Data Engineer to join a dynamic team that’s building a cutting-edge platform designed to enable automated decision-making and intelligent automation across the entire business lifecycle. This role offers a unique opportunity to help shape a transformative solution from the ground up. We're seeking individuals who are mission-driven, proactive, and passionate about data engineering and innovation. Key Responsibilities Partner with business stakeholders and product owners to gather data requirements and design scalable technical solutions. Build and maintain robust data models and schema structures to support analytics, business intelligence, and machine learning initiatives. Optimize data processing pipelines for speed, scalability, and cost-efficiency across cloud and on-premise environments. Ensure high data quality and consistency through validation frameworks, automated monitoring, and comprehensive error-handling processes. Collaborate closely with data analysts and data scientists to deliver reliable, well-structured, and easily accessible datasets. Stay informed of emerging trends, tools, and best practices in data engineering to help drive innovation and technical excellence. Maintain operational stability and system performance across data pipelines and platforms. Provide Level 3 production support when necessary, resolving critical data-related issues swiftly and effectively. Required Experience and Skills Minimum 8+ years of experience in data engineering, data architecture, or a similar technical role. Strong programming skills in SQL , Python , Java , or equivalent languages for data processing and pipeline development. Experience with both relational (e.g., PostgreSQL, SQL Server, Oracle) and NoSQL (e.g., MongoDB) databases, OLAP tools like Clickhouse , and vector databases (e.g., PGVector , FAISS , Chroma ). Expertise in distributed data processing frameworks such as Apache Spark , Flink , or Storm . Experience with cloud data solutions (e.g., Azure , AWS Redshift , BigQuery , Snowflake ) is highly desirable. Solid understanding of ETL/ELT pipelines , data transformation , and metadata management using tools such as Airflow , Kafka , NiFi , Airbyte , and Informatica . Proficiency in query performance tuning, profiling, and data pipeline optimization. Hands-on experience with data visualization platforms like Power BI , Tableau , Looker , or Apache Superset . Familiarity with DevOps principles, version control systems ( Git ), and CI/CD pipelines . Strong problem-solving skills, attention to detail, and the ability to work under pressure. Effective communicator with the ability to collaborate across multidisciplinary teams.","[{""min"": 200, ""max"": 250, ""type"": ""Net per hour - B2B""}]",Data Engineering,200,250,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,107,Senior Oracle Database Administrator,emagine Polska,"PROJECT DESCRIPTION: Work model: Hybrid (2 days/week form office Warsaw) Assignment type: B2B Start: ASAP Project length: long term Rate: 175 pln/h net + vat Project language: English Industry: Banking The Senior DBA will be responsible for managing Oracle-based applications, ensuring continuous service and integrity through implementation, testing, troubleshooting, and support for critical databases. RESPONSIBILITIES: Prepare the prePROD environment for enabling FSFO. Sync the DataGuard in case of issues or errors. Rebuild the Standby as needed. Create DB service and validate PDB-level services. Create a new database and migrate old PDB to a new container. Activate FSFO and troubleshoot related issues. Perform tests on prePROD, including switchover and failover. Apply DB-level patching. Execute changes related to OS (Unix processes, load management, daemon) as needed. Handle RAC issues with disaster recovery. Set up OGG Microservices and OEM. Implement changes in production. MUST HAVE: 10+ years of managing Oracle Database versions 12c to 19c (RAC, Data Guard). Operating system experience in Linux. Proficiency with Oracle management tools (Data Guard, RMAN, Data Pump). Understanding of architecture design principles. Strong problem-solving skills and ability to work independently and in a team. Experience with creating PDB services and handling Data Guard issues. Strong practical experience in a production RAC (ASM) environment (19c). Self-starter with attention to detail. Knowledge of Oracle GoldenGate. Understanding of storage systems. Knowledge on FSFO problem-solving, observers, and change/incident management processes. Experience with Oracle SR handling and OEM architecture.","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Database Administration,150,175,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,108,BI Backend Developer,emagine Polska,"PROJECT INFORMATION: Industry: Construction Assignment type: B2B Start: 1st April 2025 Work model: Hybrid model (3 days/week in office - Warsaw) Project length: 12 months + extensions Project language: English We are looking for a dedicated BI Developer to join our Danish client's BI scrum team of six members, who develop and maintain our BI back-end solutions. The team you will join works in close collaboration with our highly innovative departments and companies, where you identify and build the foundation for their new BI reports. Reports covering everything from working environment, diversity, IOT machine data to the more financial. Our team is at the forefront of digitization and automation, focused on streamlining the way we work by processing and presenting data through Business Intelligence tools. The role involves collaborating with various departments to develop insightful BI reports that enhance decision-making and operational efficiency. Main Responsibilities Develop and maintain BI back-end solutions. Collaborate with various teams to identify and implement new BI reporting frameworks. Analyze data trends and provide actionable insights to colleagues. Facilitate the transition to cloud environments and integrate new data sources. Contribute to the development and execution of our digital strategies. Identify and implement new data sources and improve existing data frameworks. Work closely with cross-functional teams to define requirements for BI initiatives. Analyze complex data sets and translate findings into actionable insights. Contribute to cloud integration and Data Lakehouse projects. Key Requirements +3 years of experience in BI area Experience in Business Intelligence and data analysis. Strong understanding of Python and SQL at an advanced level. Strong Experience with Databricks Basic experience with DevOps practices Knowledge of cloud-based solutions, particularly Azure. Capacity to perform dimensional data modeling. Experience with structured and unstructured data sources (API, SQL). Adept at communicating complex ideas effectively. Good problem-solving skills and a proactive attitude toward technological advancements. Nice to Have Familiarity Data Factory. Knowledge of MS Dynamics and process automation. Familiarity with Power BI and data transformation tools.","[{""min"": 170, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,180,Net per hour - B2B
Full-time,Manager / C-level,B2B,Remote,109,Lead AI Data Engineer,dotLinkers,"Position: Lead AI Data Engineer Employment: B2B Working model: Remote Our client is a rapidly growing, innovation-driven real estate company with a strong focus on customer experience and sustainable asset design. Operating across the UK, Spain, Portugal, and expanding into Germany and the Netherlands, the company is committed to leveraging technology and data to enhance business decisions, asset performance, and customer service. Their dedicated data and innovation unit is building a cloud-based, AI-enabled data platform to support smarter investment strategies and scalable data infrastructure. Role Overview We are currently seeking aLead AI Data Engineerto join the team on a B2B basis. This is a hands-on, full-stack data engineering role that combines leadership, architecture, and AI/ML deployment to scale a dynamic data platform. The successful candidate will lead a small, talented team and play a critical role in shaping the technological future of real estate investment. Responsibilities: Lead and mentor a growing team of data engineers and analysts. Design and implement scalable, cloud-native infrastructure (GCP) to support AI/ML-powered data solutions. Develop and optimize end-to-end data pipelines to extract real-time, actionable insights. Collaborate with domain experts to align data architecture with business needs. Maintain data SLAs and ensure accessibility and usability for investment analysts and decision-makers. Drive innovation through the deployment of GenAI and ML models to solve real business problems. Requirements: 5+ years of hands-on experience as a Senior or Lead Data Engineer with a focus on AI/ML solutions. Proven track record in designing, coding, and deploying scalable data architectures. Proficiency in Python (with experience in key libraries), JavaScript, and SQL. Strong understanding of CI/CD, cloud-native infrastructure, and back-end systems. Master’s degree in a STEM field, preferably Data Science, Data Engineering, or Computer Science. Demonstrated ability to coach and lead a team (3+ years of mentoring experience). GCP certification or experience with Google Cloud Platform and its AI/ML tools is a strong plus. Full-stack mindset with experience building robust, production-grade data pipelines. The offer: Full-time engagement on a B2B basis. Competitive compensation: £6,500 – £7,500 per month. Work remotely with minimal travel (2 days/month in London). Work in a fast-paced, innovation-first environment with a collaborative team. Opportunity to shape a cutting-edge data platform at the intersection of AI and real estate. Direct collaboration with company founders and strategic stakeholders for high-impact outcomes.","[{""min"": 31843, ""max"": 36741, ""type"": ""Net per month - B2B""}]",Data Science,31843,36741,Net per month - B2B
Full-time,Mid,B2B,Hybrid,110,ML Platform / Data Engineer,Beesafe,"About Us: Join our trailblazing team as we expand our digital horizons. From our beginnings as visionary InsurTech to becoming a key player in the Polish digital insurance market, we are part of the esteemed Vienna Insurance Group. We're redefining the rules in the insurance industry with our innovative approach. Our hybrid working model supports both the collaborative energy of office work and the flexibility of remote work. About the Role: We're seeking passionate Data Engineer, both at mid and senior levels, to join our team. You'll be at the heart of developing and implementing high-quality application software, using state-of-the-art tools and technologies. This is your chance to make a significant impact on one of the most exciting and unique products in the Polish digital insurance market. You’ll be leadingcutting-edge tech initiativesin the Azure Cloud, using state-of-the-art tools and methodologies. You’llown your work, influencing the project’sarchitecture, strategy, and direction. You’ll play a key role inshaping MLOps/LLMOps pipelines, improving data availability, and ensuring seamless collaboration between analytics and IT teams. You’ll be joining a team ofcustomer-focusedData Analysts, Data Engineers, and Machine Learning Engineers to drive one of the most exciting products on the Polish insurance market. Proven experience as a Data Engineeror a strong aspiration to take on aleadership rolein cloud-based data and AI projects. Commercial experience in building Data, Analytics, or MLOps/LLMOps Platforms in the Cloud(Azure preferred). Hands-on expertise inmachine learning model lifecycle management, deployment, and monitoring in a cloud environment. Practical knowledge ofopen-source Big Data tools(e.g., Kafka, Airflow, Presto, Spark). +3 years of programming experience(Python preferred, but Java/Scala experience with a willingness to learn Python is also welcome). Experience indatabase development, data model design, and distributed systems. Strongbusiness acumen and collaboration skills, with a proactive approach to addressing service needs and operational demands. Understanding ofAgile and Scrum principles(we work in Scrum, by the way). Hands-on experience inMLOps/LLMOps, including model versioning, CI/CD pipelines for ML, and monitoring of deployed AI models. Familiarity withDatabricks, BI Tools, and Azure Machine Learning Services. Experience withDevOps, Kubernetes, and microservices architectures. Strongmentoring skills, supporting other engineers in their professional growth. Passion forinformal discussions over coffee or tea—we believe great ideas start in a relaxed atmosphere! Why Join Us? Be part of a dynamic team driving digital innovation in the insurance and eCommerce sectors Opportunity to work in a collaborative and forward-thinking environment Contract options: B2B cooperation Engage in meaningful work that directly impacts business success Join a company that values work-life balance and fosters a positive team culture Comprehensive onboarding, including a dedicated Buddy program Remote work flexibility with hybrid office visits Flexible working hours Access to the latest tools and cloud-native solutions A comprehensive benefits package, including health insurance and MultiSport card Employee discounts on insurance products Referral program and sports club memberships Join us and help shape the future ofAI-driven insurance solutions! 🚀","[{""min"": 18000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Science,18000,23000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,111,Data Visualization Specialist,ITDS,"Data Visualization Specialist (Qlik) Join us, and bring data to life through smart visuals Kraków - based opportunity with hybrid work model (6 days/month in the office). As a Data Visualization Specialist , you will be working for our client, a globally recognized financial institution that is transforming how finance teams leverage data for strategic resource management. You will be contributing to the development of best-in-class visualization solutions by building advanced dashboards and reports in Qlik. Working closely with analysts and engineers, you will translate complex data into intuitive visual tools that support high-impact decision-making. This is a great opportunity to work in a dynamic, fast-paced environment where innovation, precision, and collaboration are key. Your main responsibilities: Developing advanced dashboards and reports using Qlik and QlikSense Managing data sources to ensure accuracy and integrity Collaborating with IT analysts and engineers to define visualization requirements Maintaining and overseeing project sites on Qlik Server Documenting processes, dashboards, and data sources clearly and thoroughly Promoting data sharing best practices and managing user access controls Ensuring optimal performance of Qlik workbooks with large datasets Supporting infrastructure and deployment requirements for Qlik tools Following internal control standards and compliance procedures Engaging with stakeholders to gather feedback and implement improvements You're ideal for this role if you have: 3+ years of experience in data analysis or data visualization Proficiency in Qlik and hands-on experience with QlikSense Ability to work with and visualize large, complex datasets efficiently Experience managing Qlik Server environments and deployments Strong analytical skills and a high level of mathematical competence Excellent collaboration and communication skills Ability to document technical and process information effectively Understanding of compliance and internal control frameworks Experience working in fast-paced environments with tight deadlines Proactive attitude and ability to solve problems independently It is a strong plus if you have: Working knowledge of Qlik architecture and administration Experience with data governance and regulatory compliance in finance Familiarity with offshoring workflows and documentation standards Background in financial services or resource management analytics Ability to influence stakeholders and drive data visualization best practices We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7099 You can report violations in accordance with ITDS’s Whistleblower Procedure available here .","[{""min"": 20800, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,20800,28000,Net per month - B2B
Full-time,Mid,B2B,Remote,112,Hadoop/Big Data Devops,Detable,"Posiadasz minimum 5 lat doświadczenia na stanowisku DevOps Engineer, w projektach dla klienta masowego, związanych z przetwarzaniem dużych wolumentów danych; Realizowałeś projekty związane z klastrami High Availability i HDP ( tu integracja z Karberos/ AD); Brałeś udział w wdrożeniu standardami CI/CD; Biegle posługujesz się oprogramowaniem ekosystemu Apache Hadoop Bigtop (hdfs, spark, yarn, mapreduce, hive, ranger) w zakresie instalacji i konfiguracji; Swobodnie poruszasz się używając Ansible; Posiadasz praktyczne umiejętności z Kubernetes, Docker, Grafana/Prometheus, PostrgreSQL (EDB), Airflow, JupyterLab, Zeppelin –(instalacja, administracja i zarządzanie); Masz udokumentowane doświadczenie w zakresie pisania skryptów shellowych; Miałeś okazje pracować w obszarze Big Data, Hurtowni Danych i Zarządzania Danymi minimum 3 lata; Nie jest Ci obca platforma programistyczna Hadoop Cloudera/Hortonworks; Z łatwością odnajdujesz się w programowaniu procesów Apache Spark w Python(lub Scala); Pracowałeś z różnymi formatami danych (np. JSON, ORC, PARQUET); Biegle posługujesz się językiem polskim. Praca w środowisku bazodanowym Big Data, Spark, Apache Hadoop; Projektowanie, implementacja i optymalizacja procesów przetwarzania danych w środowiskach Big Data; Instalacja, konfiguracja, zarządzanie klastrem Hadoop; Instalacja bibliotek w klastrze HDP a także monitorowanie wydajności i aktywne zarządzanie oraz strojenie klastra HDP; Projektowanie i zarządzanie hurtowniami danych oraz zapewnienie jakości danych; Zarządzanie klastrem Kubernetes; Integracja danych z wielu źródeł o różnych formatach (m.in. JSON, PARQUET, ORC, AVRO); Tworzenie zaawansowanych zapytań SQL oraz optymalizacja dostępu do danych; Wykorzystywanie odpowiednich technologii bazodanowych w zależności od scenariusza użycia; Udział w projektowaniu architektury danych oraz wdrażaniu najlepszych praktyk zarządzania danymi; Rozwiązywanie problemów i awarii klastra HDP; Monitorowanie i zapewnianie wysokiej wydajności systemów Big Data. Konkurencyjne wynagrodzenie w oparciu o kontrakt B2B ( 140-170pln netto/h); Długofalową współpracę opartą o wzajemny szacunek i partnerstwo; Dedykowanego opiekuna kontraktu po stronie Detable; Możliwość 100% pracy zdalnej lub z naszego biura w Białymstoku; Możliwość podnoszenia swoich kwalifikacji poprzez skorzystanie z budżetu szkoleniowego; Realny wpływ na rozwój projektu; Atrakcyjny program poleceń pracowniczych; Zdalny proces rekrutacji. Rozmowa HR z naszą IT Rekruterką; Weryfikacja umiejętności technicznych przez naszego Lidera technicznego; Spotkanie z klientem; Decyzja i rozpoczęcie współpracy","[{""min"": 23520, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Unclassified,23520,28560,Net per month - B2B
Full-time,Mid,B2B,Hybrid,113,FinOps Specialist (GCP),Ness Solution,"Poszukiwany FinOps Specialist (GCP)! 📅Start: wrzesień 🌍Lokalizacja: Warszawa + praca zdalna (min. 1 dzień/tydz. w biurze) 📃Umowa B2Bdo 180 PLN/h 🔍Projekt Dołącz do zespołu wspierającego wdrożenie środowiska chmurowego wGoogle Cloud Platform– obejmującego obszaryzarządzania kosztami, struktury projektowej (Landing Zone) i budżetowania. Twoją rolą będzie zapewnienie wsparcia FinOps, optymalizacja kosztów oraz współpraca z zespołami technicznymi i finansowymi. 🛠Zakres zadań Wsparcie wdrożenia środowiska GCP z perspektywy FinOps Tworzenie i zarządzanie budżetami chmurowymi (alerty, limity, tagowanie) Monitorowanie i analiza kosztów (Google Cloud Billing, Budgets & Alerts, BigQuery) Współpraca z zespołami IT i finansowymi Identyfikowanie możliwości optymalizacji kosztów Wdrażanie dobrych praktyk FinOps i prowadzenie szkoleń ✅Wymagania Doświadczenie w FinOps w środowisku GCP Znajomość narzędzi: Google Cloud Billing, Budgets & Alerts, BigQuery, Cloud Monitoring Umiejętność pracy z budżetami chmurowymi i analiza kosztów Znajomość GCP Landing Zone Accelerator Znajomość Terraform i/lub Ansible Umiejętność współpracy z działami technicznymi i finansowymi Certyfikat FinOps Certified Practitioner(mile widziany również CCFM) 🎯Mile widziane Doświadczenie w szkoleniu zespołów z zakresu FinOps Praktyczna znajomość optymalizacji kosztów w dużych środowiskach chmurowych Co oferujemy: Elastyczny model pracy: hybrydowy – 1 dzień w biurze, pozostałe dni zdalnie Benefity: karta Multisport, prywatna opieka medyczna Jasny i sprawny proces rekrutacyjny: Rozmowa z rekruterem Spotkanie techniczne z klientem Jeśli oferta jest dla Ciebie interesująca, prześlij swoje CV!","[{""min"": 23520, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Unclassified,23520,30240,Net per month - B2B
Full-time,Senior,Permanent or B2B,Hybrid,117,Data Scientist,Link Group,"Poszukujemy doświadczonego Data Scientista do zespołu inżynieryjnego międzynarodowej firmy technologicznej specjalizującej się w rozwiązaniach automatyzujących procesy finansowe i księgowe. Firma od ponad 20 lat rozwija własną platformę SaaS wykorzystywaną globalnie przez działy finansowe dużych organizacji. Projekt skupia się na budowaniu nowoczesnych, skalowalnych rozwiązań AI/ML wspierających procesy księgowe i finansowe, z wykorzystaniem Python, TensorFlow lub PyTorch, Google Cloud Platform oraz MLOps. Na co dzień Data Scientist w tym zespole odpowiada za projektowanie, trenowanie i wdrażanie modeli uczenia maszynowego, budowanie pipelines danych, integrację mikroserwisów w środowisku chmurowym oraz rozwój usług AI w bliskiej współpracy z product ownerami, innymi inżynierami i zespołami cloudowymi. Ważny jest tu nie tylko mocny background techniczny, ale także umiejętność szukania rozwiązań w otwartych, nieoczywistych problemach i dzielenia się wiedzą wewnątrz zespołu. Wymagania: Bardzo dobra znajomość Python, SQL, Spark Doświadczenie w budowie i wdrażaniu modeli ML (klasyfikacja, klasteryzacja, prognozowanie) Znajomość TensorFlow i/lub PyTorch Praktyka w pracy z GCP lub inną dużą chmurą (AWS, Azure) Doświadczenie z MLOps, pipelines, kontrolą wersji (Git) Wykształcenie wyższe kierunkowe (Informatyka, Statystyka, Data Science) Bardzo dobra znajomość języka angielskiego – komunikacja wewnętrzna w firmie odbywa się w tym języku Mile widziane: Doświadczenie z GCP (BigQuery, Vertex AI) Znajomość specyfiki usług finansowych lub rozwiązań dla księgowości Wiedza z zakresu generative AI vs AI agents Informacje organizacyjne: 📍 Lokalizacja: Kraków – model hybrydowy (2 dni w tygodniu z biura) 💬 Wymagana bardzo dobra znajomość języka angielskiego 📑 Forma współpracy: Umowa o pracę na czas nieokreślony lub B2B (w zależności od preferencji) 🗓 Start: możliwie jak najszybciej 💼 Proces rekrutacyjny: rozmowy techniczne oraz spotkanie z przedstawicielem firmy","[{""min"": 21000, ""max"": 25000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Science,21000,25000,Net per month - B2B
Full-time,Senior,B2B,Remote,118,DataBricks Architect,CRODU,"🌴 Forma pracy: długoterminowo, fulltime, 100% zdalnie👈 ⏰ Start: ASAP👈 Cześć! 👋 Dla naszego klienta z USA poszukujemy DataBrick Architectów. Prace dotyczą działań w obszarachm.in. migracji, zbierania danych i optymalizacji rozwiązań opartych na DataBricks. Klient posiada stałe zapotrzebowanie na specjalistów. Projekty które prowadzą przeważnie są krótkoterminowe (spore prawdopodobieństwo na przedłużenia projektów) i ze względu na stałość zapotrzebowania klient jest w stanie zaproponować nowy temat po zakończeniu danego projektu. Obecnie poszukiwani są specjaliści do projektu AI/ML z obszarów healthcare. Projekt dotyczy analizy danych tekstowych i analizy obrazów generowanych przez urządzenia medyczne (rentgen, rezonans magnetyczny itp.). Zebrane dane będą migrowane do chmurowej bazy opartej na DataBricks. Platforma ma obsługiwać cały cykl życia danych w zgodzie z wbudowanymi funkcjonalnościami zapewniającymi zgodność z przepisami, możliwość przeprowadzania audytów, tworzenia kohort czy wtórnego wykorzystania modeli. Celem jest rozwiązanie problemów związanych z istniejącymi systemami zarządzania danych (rozproszone źródła, ręczne procesy, niewystarczające bezpieczeństwo). Poszukujemy osób, które biegle znają Pythona. Dla klienta kluczowe jest obycie w środowiskach chmurowych oraz znajomość DataBricks i Apache Spark. Projekty prowadzone przede wszystkim dla firm z USA - w większości przypadków wymagana jest praca jedynie z niewielką zakładką godzinową (np. od 10: 00 do 18: 00) natomiast jesteśmy w stanie dogadać się jeśli chodzi o godziny pracy. Ogólny zakres obowiązków: 📍 Stworzenie środowiska i architektury platformy na DataBricks 📍 Kontakt z biznesem pod kontem ustaleń projektowych 📍 Zapewnienie bezpieczeństwa przechowywanie danych 📍 Przetwarzanie i indeksowanie danych DICOM 📍 Walidacja danych, tworzenie pipeline'ów przetwarzania danych, tworzenie i udostępnianie kohort 📍 Zaplanowanie i przeprowadzenie migracji baz danych 📍 Ścisła współpraca z zespołem (m.in. data engineers, data scientists, informatycy kliniczni, zespół wsparcia) Wymagania: ⚡️ Solidne doświadczenie w pracy w roli data engineera lub pokrewnych rolach (8+ lat) ⚡️ Bardzo dobra znajomość platformy DataBricks oraz Apache Spark ⚡️ Bardzo dobra znajomość Python ⚡️ Doświadczenie w przeprowadzaniu migracji chmurowych ⚡️ Doświadczenie w pracy w środowisku AWS (Amazon s3) ⚡️Doświadczenie w prowadzeniu projektów związanych z AI/ ML ⚡️ Umiejętności interpersonalne i zespołowe ⚡️ Umiejętność podejmowania inicjatywy i samodzielność ⚡️ Angielski na poziomie umożliwiającym swobodną komunikację w zespole Mile widziane: ⚡️ Doświadczenie w pracy w środowisku innych środowiskach chmurowych (np. Azure - Data Factory, Synapse, Logic Apps, Data Lake) ⚡️ Doświadczenie w projektowaniu i optymalizacji przepływów danych za pomocą, DBT, SSIS, TimeXtender lub podobnych rozwiązań (ETL, ELT) ⚡️ Doświadczenie z dowolnymi platformami big data lub noSQL (Redshift, Hadoop, EMR, Google Data itp.) Jak działamy i co oferujemy? 🎯 Stawiamy na otwartą komunikację zarówno w procesie rekrutacji jak i po zatrudnieniu - zależy nam na klarowności informacji dotyczących procesu i zatrudnienia 🎯 Do rekrutacji podchodzimy po ludzku, dlatego upraszczamy nasze procesy rekrutacyjne, żeby były możliwie jak najprostsze i przyjazne kandydatowi 🎯 Pracujemy w imię zasady ""remote first"", więc praca zdalna to u nas norma, a wyjazdy służbowe ograniczamy do minimum 🎯 Oferujemy prywatną opiekę medyczną (Medicover) oraz kartę Multisport dla kontraktorów","[{""min"": 200, ""max"": 250, ""type"": ""Net per hour - B2B""}]",Data Architecture,200,250,Net per hour - B2B
Freelance,Senior,B2B,Hybrid,120,Data Architect – (Azure & MS Fabric),INFOPLUS TECHNOLOGIES,"Dutch is a preferred but not necessary Integration of Internal and External Data Responsible for onboarding and integrating internal and external data onto the data platform. Ensureinteroperabilitywith System Integrations. Data Modeling & Preparation According to Standards Structure data into standardizeddata modelsanddatamartsto enable dashboard development. Ensure consistency, quality, and alignment with enterprise data standards. Access Management Standardization Define and implement atechnical standard for access management, aligned with the organization’sIAM (Identity & Access Management)framework. Facilitate secure and controlled access to data assets. Reduction of Technical Impediments Proactively identify and resolve technical bottlenecks to enable TDA dashboard development. Aim to meet a95% service levelfor business operations dashboards. DP-700: Microsoft Fabric Data Engineer certification DP-600: Microsoft Fabric Analytics Engineer certification At least2 years of experienceintegrating diverse data sources for consolidated reporting/dashboarding At least2 years of hands-on experiencewithAzure and Microsoft Fabric Knowledge of and experience withAzure DevOps, includingCI/CD pipelines Familiarity withIAMandRBAC(Role-Based Access Control) Proven experience working inAgile teams","[{""min"": 1707, ""max"": 2348, ""type"": ""Net per day - B2B""}]",Data Architecture,1707,2348,Net per day - B2B
Full-time,Senior,B2B,Remote,121,Senior Azure Data Engineer with Databricks,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: 🌍100% remote from Poland 📝long-term 💻working hours - 7: 00-15: 00 🕰️ASAP project start Responsibilities: Being responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems Building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies Evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards Driving creation of re-usable artifacts Establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation Working closely with analysts/data scientists to understand impact to the downstream data models Writing efficient and well-organized software to ship products in an iterative, continual release environment Contributing and promoting good software engineering practices across the team Communicating clearly and effectively to technical and non-technical audiences Defining data retention policies Monitoring performance and advising any necessary infrastructure changes Requirements: 3+ years’ experience with Azure Data Factory and Databricks 5+ years’ experience with data engineering or backend/fullstack software development Strong SQL skills Python scripting proficiency Experience with data transformation tools - Databricks and Spark Experience in structuring and modelling data in both relational and non-relational forms Experience with CI/CD tooling Working knowledge of Git English level: B2-C2 Nice to have: Experience with Azure Event Hubs, CosmosDB, Spark Streaming, Airflow Experience with Airflow Experience in Aviation Industry and Copilot Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,170,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,122,Data Engineer - Delivery Experience,Allegro,"About the team The salary range for this position is (contract of employment): 14 200 - 19 690 PLN in gross terms A hybrid work model requires 1 day a week in the office In the area of Delivery Experience, we are building technology that makes Allegro's deliveries easy, cost-effective, fast and predictable. Our team takes care of critical services along the Allegro shopping journey, responsible for predicting delivery times using statistical algorithms and machine learning, selecting the best delivery methods tailored to customers, and integrating with carrier companies. Delivery Experience is also one of the fastest-growing areas where we undertake new, complex projects to enhance logistics and warehousing processes. We are looking for a Mid/Senior Data Engineer with a focus on the data processing and preparation, deployment and maintenance of our data projects. Join our team to enhance your skills related to deploying data-based processes, data ops approaches and share the skills within the team. Your main responsibilities: - You will be actively responsible for developing and maintaining processes for handling large volumes of data - You will be streamlining and developing the data architecture that powers analytical products and work along a team of experienced analysts - You will be monitoring and enhancing quality and integrity of the data - You will manage and optimize costs related to our data infrastructure and data processing on GCP This is the right job for you if: - You have at least 3 years of experience as Data Engineer and working with large datasets. - You have experience with cloud providers (GCP preferred). - You are highly proficient in SQL. - You have strong understanding of data modeling and cloud DWH architecture. - You have experience in designing and maintaining ETL/ELT processes. - You are capable of optimizing cost and efficiency of data processing. - You are proficient in Python for working with large data sets (using PySpark or Airflow). - You use good practices (clean code, code review, CI/CD). - You have a high degree of autonomy and take responsibility for developed solutions. - You have English proficiency on at least B2 level. - You like to share knowledge with other team members. Nice to have: - Experience with Azure and cross-cloud data transfers and multi-cloud architecture What we offer: - Big Data is not an empty slogan for us, but a reality - you will be working on really big datasets (petabytes of data). - You will have a real impact on the direction of product development and technology choices. We utilize the latest and best available technologies, as we select them according to our own needs. - Our tech stack includes: GCP, BigQuery, (Py)Spark, Airflow. - We are a close -knit team where we work well together. - You will have the opportunity to work within a team of experienced engineers and big data specialists who are eager to share their knowledge, including publicly through allegro.tech Apply to Allegro and see why it is #dobrzetubyć (#goodtobehere)","[{""min"": 14200, ""max"": 19690, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14200,19690,Gross per month - Permanent
Full-time,Senior,Permanent,Hybrid,123,"Senior Business Intelligence Analyst, Cost Controlling (m/f/x)",HelloFresh,"HelloFresh Group, the world’s leading integrated food solutions provider, is expanding with a new R&D Tech office in Poland. With brands offering meal kits, ready-to-eat meals, and specialty products such as meat, seafood, and pet food, we are seeking individuals who are ready to make an impact from day one. Joining us in Wrocław means shaping the culture, working on meaningful R&D Tech projects, and contributing to a global company changing how people eat. At HelloFresh Group, we are driven by a high-performance culture that values speed, agility, and continuous learning. We believe in hands-on contribution and fostering a truly collaborative, egoless environment where every team member contributes to our mission ofchanging the way people eat, foreverand welcome team players who thrive in a dynamic environment, lead with ownership, and bring diverse perspectives to the table. Our teams thrive on in-person collaboration, with an expectation to work from the office four days a week. This approach creates a dynamic space where ideas flourish, decisions are made efficiently, and our collective impact is accelerated. It's how we stay closely connected to our shared goals and drive swift execution. As a Senior Cost Controlling BI Analyst, you will be an integral part of HelloFresh's mission to empower our business with financial insights, data-driven strategies, and processes that elevate the effectiveness of how we invest our resources. You will support the business in developing global solutions that harmonize our reporting across our geographies and analyze our spend in key areas. You will help produce the data assets that provide insights to help our leaders make key decisions. Develop and implement complex calculations globally that help support local and global teams report and manage costs. Collaborate with cross-functional teams, including FP&A, Accounting, Finance Tech and key stakeholders in each area to enhance and automate reporting in relation to costs to meet the needs of senior stakeholders. Work closely with local stakeholders to ensure that local requirements and nuances are included in any central calculations. Support the cost controlling function in tracking their initiatives and efforts to reduce spend in key areas. 5+ years of experience in data analysis, financial controlling, FP&A, or a similar role, ideally in a fast-paced, data-driven environment as well as a university degree. Strong analytical and problem-solving skills, with the ability to translate complex data into actionable insights. Proficiency in SQL, and advanced Excel skill. Python is a plus. Excellent communication skills, with the ability to effectively collaborate with cross-functional teams and present financial insights to senior stakeholders. Ability to work independently, prioritize effectively, and manage multiple projects in a fast-paced environment. Experience with BI tools such as Tableau, Power BI, or similar is a plus. Health- You’re covered from your first day with private health insurance Hybrid Working Schedule- We work in-office 4 days a week to align on goals, with flexible hours to support work-life balance and personal needs Holidays- You receive 26 days of paid vacation each year, providing you time to rest and recharge Learning and Development- An annual Learning & Development budget and a Mentoring Program to support your ongoing professional growth Employee Referral Program- Our team members can participate in our internal employee referral program and receive a bonus for recommending successful candidates to open roles Daily Comforts- Free coffee, drinks, and fresh fruit are available to keep you refreshed throughout the day If you are passionate about making a tangible impact and thrive in a fast-paced environment where your work directly contributes to a global purpose, we encourage you to apply – even if your experience doesn't tick every single box, we believe there are many ways to develop skills and grow with us.","[{""min"": 178000, ""max"": 267000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,178000,267000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,124,Senior Data Engineer (Azure/Fabric),Onwelo,"🟠 Poznaj Onwelo: Onwelo to nowoczesna polska spółka technologiczna, która specjalizuje się w budowaniu innowacyjnych rozwiązań IT dla organizacji z szeregu sektorów na całym świecie. Główne obszary działalności Onwelo to: tworzenie oprogramowania, jego rozwój oraz utrzymanie, a także mocne wsparcie kompetencyjne. W krótkim czasie firma wdrożyła ponad 300 projektów w Europie i w USA, a także otworzyła biura w siedmiu miastach Polski oraz oddziały w Stanach Zjednoczonych, Niemczech i w Szwajcarii. 🚀 O projekcie: Do naszego zespołu Data & Analytics poszukujemy doświadczonego Azure/Fabric Data Engineera, który będzie wspierał naszych klientów w planowaniu, budowie i wdrażaniu nowoczesnych rozwiązań danych w środowisku Microsoft Azure i Fabric. Będziesz pracować w zespole z ekspertami od analizy danych, chmury i architektury, w środowisku międzynarodowym i projektach o dużej skali. . 🎯 Z nami będziesz: Projektować i wdrażać rozwiązania oparte na Microsoft Fabric – nowoczesnej platformie analitycznej łączącej dane, raportowanie i orkiestrację w jednym środowisku Planować i przeprowadzać orkiestrację danych w środowisku Microsoft Azure oraz Fabric Budować, rozwijać i wdrażać nowoczesną hurtownię danych w oparciu o Databricks, Data Vault 2.0, Python i PySpark Tworzyć i optymalizować potoki danych oraz procesy ETL/ELT zasilające hurtownie danych Projektować modele danych wspierające analitykę biznesową i raportowanie w Power BI Wdrażać rozwiązania z wykorzystaniem Microsoft Fabric, Azure Data Factory, Synapse, Data Lake, Azure SQL Przeprowadzać analizę danych i projektować modele danych wspierające cele biznesowe Wspierać innych członków zespołu – technicznie i merytorycznie Monitorować jakość i efektywność przepływów danych oraz optymalizować je pod kątem kosztów i wydajności 😎 Czekamy na Ciebie, jeśli: Masz minimum 5-letnie doświadczenie jako Data Engineer – w projektach związanych z integracją danych, modelowaniem i budową hurtowni Masz praktyczne doświadczenie z Microsoft Fabric lub chcesz rozwijać się w tym obszarze i szybko się uczysz Pracujesz z usługami chmurowymi Azure, w tym: Azure Data Factory, Azure Databricks, Azure SQL, Data Lake Znasz SQL na poziomie eksperckim Biegle posługujesz się językami Python i/lub PySpark Rozumiesz architekturę nowoczesnych hurtowni danych (np. Data Vault 2.0 ) Masz wyższe wykształcenie techniczne (np. informatyka, matematyka, inżynieria danych) Komunikujesz się po angielsku na poziomie min. B2 (część projektów i zespołów jest międzynarodowa) 🤝 Dowiedz się, jak skorzystasz, będąc w Onwelo: Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Potrzebujesz pracować zdalnie? Jesteśmy otwarci! Zaoszczędzisz czas na dojazdach – pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych i zewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,27000,Net per month - B2B
Full-time,Mid,B2B,Remote,126,Data Engineer (Azure & Databricks),in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmą rekrutacyjną, w której wierzymy, że wszystko jest możliwe dzięki odpowiednim ludziom. Naszym celem jest połączenie najbardziej utalentowanych pracowników z odpowiednimi firmami, tworząc synergiczne relacje, które przyczyniają się do wzrostu i sukcesu każdej ze stron. Uważamy, że prawdziwą wartość stanowią ludzie pracujący wspólnie w atmosferze wzajemnego szacunku i zaufania. Dla naszego Klienta poszukujemy doświadczonych Data Engineerów, którzy chcą pracować w środowisku opartym o Azure i Databricks. Szukamy osób, które potrafią projektować wydajne modele danych i mają doświadczenie w przetwarzaniu dużych zbiorów danych. Zakres roli: Projektowanie, implementacja i utrzymanie skalowalnych procesów przetwarzania i integracji danych (ETL/ELT) w środowisku Azure. Praca z dużymi zbiorami danych z wykorzystaniem Apache Spark i PySpark w Azure Databricks. Tworzenie i rozwój warstw danych w oparciu o Delta Lake – z uwzględnieniem dobrych praktyk modelowania danych i kontroli jakości. Udział w projektowaniu struktur danych pod kątem wydajności, przejrzystości i użyteczności biznesowej. Integracja danych z różnych źródeł z wykorzystaniem Azure Data Factory i Azure Functions. Optymalizacja kodu oraz monitorowanie i rozwiązywanie problemów wydajnościowych. Praca z zespołami analitycznymi, data science i biznesowymi w celu zapewnienia dostępności danych w odpowiedniej formie i czasie. Oczekujemy: Minimum 3 lata doświadczenia w roli Data Engineera lub pokrewnej. Bardzo dobra znajomość Azure Databricks, Spark i PySpark. Doświadczenie z Delta Lake oraz modelowaniem danych i tuningiem wydajności. Znajomość ADF, Azure Functions. Umiejętność pracy z formatami danych: Parquet, Avro, JSON. Praktyczna znajomość Python i SQL. Doświadczenie w pracy z Git. Umiejętność pracy z dużymi wolumenami danych i doświadczenie w tworzeniu wydajnych rozwiązań w środowiskach chmurowych. Zrozumienie architektury przetwarzania danych i zasad projektowania systemów odpornych i łatwo skalowalnych. Mile widziane: Doświadczenie w pracy w oparciu o metodyki Agile (np. Scrum). Znajomość narzędzi do monitorowania i automatyzacji procesów (CI/CD) Praktyka w pracy z danymi wrażliwymi lub w środowiskach regulowanych (np. sektor finansowy, medyczny) Proponujemy: Rozwój kariery w międzynarodowych projektach, z wykorzystaniem nowoczesnych narzędzi i technologii. Elastyczny model pracy: możliwość 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejętności i doświadczenia. Współpracę w zgranym zespole, który ceni wymianę wiedzy oraz otwartą komunikację. Realny wpływ na projekty oraz wdrażane rozwiązania. Wynagrodzenie na poziomie 20 000 - 30 000 PLN/miesięcznie w zależności od doświadczenia. 💡Nie przegap dopasowanych ofert! Mamy wiele rekrutacji, a nowe projekty pojawiają się na bieżąco. Pamiętaj, że zaznaczając zgodę na przetwarzanie danych w celu przyszłych procesów , będziemy mogli zaprosić Cię do udziału w kolejnych procesach, dopasowanych do Twojego doświadczenia i oczekiwań! PS Zamierzamy kontaktować się z Tobą wyłącznie wtedy, kiedy będziemy dla Ciebie ciekawe projekty, bez tej zgody nie będzie to możliwe. Jak wygląda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klientów. Składając aplikację, możesz liczyć na nasz obiektywizm, szacunek i pełny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 17000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,17000,28000,Net per month - B2B
Full-time,Senior,B2B,Remote,127,Senior Data & Analytics Engineer,N-iX,"About the project: Our customer is the European online car market with over 30 million monthly users, with a market presence in 18 countries. The company is now merging with a similar company in Canada and needs support in this way. As a Data& Analytics Engineer, you will play a pivotal role in shaping the future of online car markets and enhancing the user experience for millions of car buyers and sellers. Requirements: 5+ years of experience in Data Engineering or Analytics Engineering roles Strong experience building and maintaining pipelines in BigQuery, Glue, Athena, and Airflow Solid Python skills, especially for data processing and workflow orchestration Advanced SQL skills and experience designing dimensional models (star/snowflake) Familiarity with data quality tools like Great Expectations Understanding of data governance, privacy, and security principles Experience working with large datasets and optimizing performance Proactive problem solver who enjoys building scalable, reliable solutions English - Upper-Intermediate+ Responsibilities: Build and maintain robust data pipelines that deliver clean and timely data Organize and transform raw data into well-structured, scalable models Ensure data quality and consistency through validation frameworks like Great Expectations Work with cloud-based tools like Athena and Glue to manage datasets across different domains Collaborate with analysts, engineers, and stakeholders to understand data needs and deliver solutions Help set and enforce data governance, security, and privacy standards Continuously improve the performance and reliability of data workflows Support the integration of modern cloud tools into the broader data platform","[{""min"": 18470, ""max"": 21795, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,18470,21795,Net per month - B2B
Full-time,Mid,B2B,Remote,128,Data Engineer / Scientist,Kyotu Technology,"🔍 Data Engineer / Scientist (Mid) Location: Wrocław / Warszawa / remote from anywhere in PolandContract: B2B / Employment contractCapacity: Full-time You know that before a fancy dashboard or predictive model happens, someone needs to build a solid pipeline, clean up messy datasets from three continents, and deal with a file calledfinal_final_REALLY_final.csv? That’s exactly the kind of challenge we’re talking about. Kyotu Technology is a boutique software house based inWrocław and Warsaw, working fully remotely or in hybrid mode from anywhere in Poland. We partner with companies fromGermany, Switzerland, Western Europe, the United States, and the Middle East, focusing on complex, production-grade systems — the kind where code moves real money, not just pixels. Our teams are highly experienced and independent. We build things from scratch, with strong ownership and zero hand-holding. We’re looking for someone who’s comfortable in the world of data: capable of building and maintaining data pipelines, integrating different sources, preparing clean and structured datasets — and occasionally diving into analysis or experimentation. You don’t need to know every buzzword. But you should be confident with: Building and maintainingETL/ELT pipelines UsingSQLandPythonto automate and process data Working with tools likedbt, Airflow, Pandas, Jupyter Understanding data warehouse/lake architectures (Snowflake, BigQuery, Redshift, etc.) Making sense of data and describing it clearly for others Experience withmachine learning(e.g., scikit-learn, feature engineering, prompt tuning) is a plus, but not required. Curious minds welcome. Build and optimize pipelines that pull data from APIs, SQL databases, files — and, yes, occasionally weird Excel sheets Validate and clean incoming data (because no one wants to base decisions on fairytales) Collaborate with AI/ML teams to provide high-quality, structured inputs Occasionally explore, visualize, or summarize data to support product or business goals You won’t be working in a vacuum. You’ll have a team, proper code reviews, QA, DevOps support, and PMs who understand tech. We work in agile-ish cycles. If something works — we keep it. If it doesn’t — we fix it. English matters — most projects are international and require written and spoken communication with clients. Hourly rate: 120–170 PLN net/hour (B2B)— negotiable depending on experience Flexible working hours and full remote freedom — or drop by our Wrocław or Warsaw offices if you like Engaging, end-to-end projects — not just ticket-pushing Autonomy and influence — if something doesn’t make sense, you’ll help change it","[{""min"": 20160, ""max"": 28560, ""type"": ""Net per month - B2B""}]",Data Science,20160,28560,Net per month - B2B
Full-time,Senior,B2B,Remote,130,Senior Oracle Developer,P&P Solutions,"Poszukujemy dwóchdoświadczonych Programistów Oracle (PL/SQL)do udziału w rozbudowanym projekcie dlafirmy z sektora energetycznegozlokalizowanej we Wrocławiu. Projekt zakłada wieloetapowe działania rozwojowe nad systemem informatycznym bazującym na technologii Oracle – z silnym naciskiem na jakość kodu, optymalizację wydajności oraz wsparcie architektoniczne. 📅Start projektu: wrzesień 2025 📍Tryb pracy: Zdalnie + sporadyczne wizyty w biurze we Wrocławiu Min.10 latkomercyjnego doświadczenia zPL/SQLiOracle DB Zaawansowana znajomość projektowania struktur, optymalizacji zapytań, administracji Znajomość transformacji XML, JSON na poziomie bazy danych Praca z GIT, testy jednostkowe (np. utPLSQL) Doświadczenie w środowisku SCRUM, znajomość JIRA 🎯Mile widziane: Znajomość Pythona Doświadczenie w systemach czasu rzeczywistego Projekty w branży energetycznej lub elektroenergetycznej Projektowanie i implementacja struktur bazodanowych w Oracle Tworzenie złożonych procedur i optymalizacja zapytań PL/SQL Wsparcie architekta rozwiązania i architekta biznesowego Udział w projektowaniu aplikacji i procesie testowania (w tym testy automatyczne) Nadzór technologiczny nad rozwiązaniami bazodanowymi Udział w projektowaniu standardów dokumentacyjnych i rozwoju kompetencji zespołu ✅ Stabilną i długoterminową współpracę – projekt przewidziany na 12 miesięcy z możliwością kontynuacji ✅ Elastyczny model pracy ✅ Atrakcyjne wynagrodzenie do 150 zł/h ✅ Możliwość wpływu na kluczowe decyzje technologiczne ✅ Współpraca przy projekcie o znaczeniu strategicznym ✅ Wspierające i profesjonalne środowisko ✅ Dostęp do nowoczesnych narzędzi i technologii","[{""min"": 120, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,150,Net per hour - B2B
Full-time,Senior,B2B,Remote,132,Data Architect ETRM,INFOPLUS TECHNOLOGIES,"Exciting Opportunity: ETRM Data Architect (Remote, Poland) | 10+ Years Experience | 2000 PLN/day About the Role: As an ETRM Data Architect, you will be the driving force behind creating robust data transformation architectures, working closely with trading systems like Allegro, RightAngle, Endur, and more. Your expertise in Python, Streamlit, and Azure data technologies will enable us to deliver seamless, scalable solutions in the dynamic energy trading domain. What You’ll Do: Lead the design and development of cutting-edge data transformation solutions using Python. Build and optimize interactive web APIs leveraging Streamlit to enhance user engagement. Collaborate with cross-functional teams to translate business needs into high-performance technical solutions. Work extensively with Azure Data Factory, Data Lake, Snowflake, and Databricks to manage complex data workflows. Ensure data quality, integrity, and security throughout all processes. Drive continuous improvements for performance, scalability, and efficiency. Provide technical mentorship to junior team members and promote best practices. Bring your expertise in ETRM systems, especially in Gas and Power markets, to the forefront. What We’re Looking For: 10+ years of hands-on experience in data engineering, transformation, and architecture Strong proficiency in Python, Kafka, and Streamlit Deep experience with Azure Data Factory, Data Lake, Snowflake, Databricks Well-versed in DevOps practices (CI/CD) Proven ability to design scalable, real-time data processing systems Strong analytical, problem-solving, and communication skills Familiarity with ETRM systems and energy trading concepts (Allegro, RightAngle, Endur) is a big plus Why Join Us? Remote work from anywhere in Poland/EU Lead innovative projects in the fast-evolving energy sector Collaborative environment with passionate professionals","[{""min"": 38000, ""max"": 40000, ""type"": ""Net per month - B2B""}]",Data Architecture,38000,40000,Net per month - B2B
Full-time,Senior,B2B,Remote,133,Senior Data Scientist,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmą rekrutacyjną, w której wierzymy, że wszystko jest możliwe dzięki odpowiednim ludziom. Naszym celem jest połączenie najbardziej utalentowanych pracowników z odpowiednimi firmami, tworząc synergiczne relacje, które przyczyniają się do wzrostu i sukcesu każdej ze stron. Uważamy, że prawdziwą wartość stanowią ludzie pracujący wspólnie w atmosferze wzajemnego szacunku i zaufania. Senior Data Scientist Dla naszego Klienta, międzynarodowej organizacji, poszukujemy doświadczonego Senior Data Scientist, który swobodnie porusza się w świecie ML/AI, zna dobre praktyki MLOps i potrafi projektować skalowalne rozwiązania. Jeśli masz mocne zaplecze w Pythonie, świetnie radzisz sobie z dużymi zbiorami danych i chcesz pracować nad realnymi wyzwaniami biznesowymi – aplikuj. Czego oczekujemy: 5+ lat doświadczenia jako Data Scientist oraz udokumentowane sukcesy w stosowaniu technik ML do rozwiązywania rzeczywistych problemów. Biegłości w Pythonie i popularnych frameworkach ML (scikit-learn, TensorFlow, PyTorch). Doświadczenie w projektowaniu i wdrażaniu kompleksowych rozwiązań ML, w tym wstępnego przetwarzania danych, inżynierii cech, budowy i oceny modeli. Znajomość platform chmurowych (AWS, Azure, GCP) oraz praktyk MLOps. Solidne podstawy statystyki i matematyki oraz ich zastosowania w modelowaniu i ocenie modeli. Umiejętność zarządzania dużymi, złożonymi zbiorami danych oraz przeprowadzania zadań takich jak przetwarzanie danych, inżynieria cech i optymalizacja modeli. Silne umiejętności rozwiązywania problemów i analitycznego myślenia, pozwalające na proponowanie innowacyjnych rozwiązań w zakresie analizy danych. Doskonałe umiejętności komunikacyjne, w tym zdolność do przedstawiania koncepcji technicznych i wyników osobom nietechnicznym. Biegłość w języku angielskim (w mowie i piśmie). Mile widziane: Doświadczenie w pracy z Generative AI i dużymi modelami językowymi (LLMs). Znajomość przetwarzania języka naturalnego (NLP) lub technik wizji komputerowej. Dodatkowe umiejętności programistyczne w językach takich jak R, SQL, Java itp. Doświadczenie w pracy z narzędziami Big Data (np. Hadoop, Spark, Kafka). Twoje obowiązki: Projektowanie, rozwijanie i wdrażanie kompleksowych rozwiązań ML end-to-end. Analiza i przetwarzanie dużych zbiorów danych. Budowa, testowanie i wdrażanie modeli ML z wykorzystaniem nowoczesnych algorytmów i technik uczenia maszynowego. Praca z platformami chmurowymi (AWS, Azure, GCP) oraz implementacja rozwiązań zgodnych z praktykami MLOps. Eksperymentowanie z nowymi podejściami w AI, w tym Generative AI i LLMs. Optymalizacja istniejących modeli ML pod kątem wydajności i dokładności. Współpraca z zespołami biznesowymi i technicznymi, w celu identyfikowania potrzeb i wdrażania rozwiązań opartych na danych. Tworzenie dokumentacji technicznej oraz prezentowanie wyników analiz osobom nietechnicznym. Co oferujemy? Rozwój kariery w międzynarodowych projektach, z wykorzystaniem nowoczesnych narzędzi i technologii. Elastyczny model pracy: możliwość 100% zdalnie lub hybrydowo z biura. Atrakcyjne wynagrodzenie dopasowane do Twoich umiejętności i doświadczenia. Współpracę w zgranym zespole, który ceni wymianę wiedzy oraz otwartą komunikację. Realny wpływ na projekty oraz wdrażane rozwiązania. Jak wygląda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klientów. Składając aplikację, możesz liczyć na nasz obiektywizm, szacunek i pełny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 20000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Data Science,20000,29000,Net per month - B2B
Full-time,Senior,B2B,Remote,135,"Senior Data Engineer (DBT, Snowflake)",Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! 😊 We are looking foran experiencedSenior Data Engineerwith strong DBT and Snowflake skills to join our team at a leadingSwedish manufacturing company. This role requires someone who is proactive, solution-oriented and not afraid to speak up, push for quality and champion their ideas in a dynamic, international setting involving close collaboration with external partners. You will thrive here if you combine technical expertise with a collaborative mindset and excellent communication skills. Although the project environment is fast-paced and engaging, it is firmly rooted in the Scandinavian work culture of trust, autonomy and transparency, where your initiative will be met with genuine support. Responsibilites: Drive backend development for a proprietary BI and analytics tool in theFinance stream, ensuring high-performance, scalable solutions. Take part in thetransition from on-premises to the cloud, designing and implementing Snowflake data models and DBT transformations for the new environment. Architect and maintainCI/CD pipelinesinGitLabfor seamless code integration and deployment. Partner with stakeholders to translate reporting requirements into robust data solutions. Monitor and optimize thedata warehousein Snowflake for query performance, storage efficiency, and cost control during and after migration. Implement secure, cloud-based data workflows onAWSorAzure, ensuring smooth cutover from legacy systems. Troubleshoot complex dataissues and provide timely feedback to both technical and non-technical stakeholders throughout the migration. We offer a B2B Contract: 140 – 190 PLN net/hour + VAT You might be the perfect match if you are/have: Bachelor’s or Master’s degreein Computer Science or a related field (or equivalent experience). 5+ yearsof professional experience indata engineering, with at least 2 yearsfocused onSnowflakeandDBT. Proven track record incomplex Finance reporting projects. Experience inmigration projects, specifically moving from on-prem to cloud environments. Experience drivingon-prem to cloud migrationinitiatives. Experience setting up and maintainingCI/CD pipelinesfor data code inGitLab. Stronganalytical thinkingand passion for building innovative data models. Excellentcommunication skills, able to engage with diverse stakeholders and provide clear feedback. Aself-starter mindset, able to work independently, prioritize tasks, and drive projects to completion. Fluent Englishfor smooth collaboration in an international environment. Location in Polandto facilitate collaboration and attendance at occasional team meetings. Moreover, we appreciate skills in these areas: Experience in working inlarge, diverse, multinational development teams. Hands-on experience withAWSorAzuredata services beyond basic usage. Demonstrated leadership abilities and teamwork in high-pressure projects. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad – so far we've been in Cape Town, Are, and Barcelona). Perks and benefits: Fully remotework or in our office in Wrocław; Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories. If you apply for this position and match our expectations, then: 1) You will be invited to an HR Screening with our IT Recruiter. 2) You will have a technical meeting with a Team Leader. 3) You will have a meeting with your team. Submit your application online in one easy step! Apply now!","[{""min"": 140, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,140,190,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,136,Backend Engineer (Data & AI),Appliscale,"About the role Our client is an early-stage, venture-backed startup transforming the $800B franchising industry. Their innovative AI platform helps leading franchise brands automate operations, leverage data insights, and scale faster and smarter. We're hiring a product-focused Backend Software Engineer eager to build impactful solutions, work closely with AI technology, and thrive in a fast-paced startup environment. This is an opportunity to contribute directly to product development, seeing your code and ideas quickly in users' hands. Responsibilities Please note, availability to attend daily afternoon/evening meetings is a specific requirement for this role as most of the team is located in the US Develop scalable backend systems, data pipelines, and APIs using Python, TypeScript, and AWS infrastructure Collaborate cross-functionally with product, ML, and infrastructure teams to integrate and deploy AI features in a multi-tenant SaaS environment Required qualifications Minimum of 2 years full-time commercial backend software development experience, ideally with Python and TypeScript Bachelor's or higher degree in Computer Science, Software Engineering, or a related field Comfortable building and managing services in AWS environments (EC2, Lambda, ECS, Airflow) Experienced using AutoML frameworks, time-series DBs Product-minded engineer who enjoys collaborating closely with product and business teams Startup-oriented: thrives in ambiguity, eager to learn quickly, iterate fast, and build impactful solutions Excellent communication skills and high fluency in English, it’s our daily business language Nice to have AI-curious: experience deploying ML models into production is a strong plus but not required","[{""min"": 14000, ""max"": 20000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 16000, ""type"": ""Gross per month - Permanent""}]",Data Science,14000,20000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,138,Senior Data Science/AI Engineer,N-iX,"Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management.Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact.Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company’s R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 21700, ""max"": 29000, ""type"": ""Net per month - B2B""}, {""min"": 17500, ""max"": 23950, ""type"": ""Gross per month - Permanent""}]",Data Science,21700,29000,Net per month - B2B
Full-time,Senior,B2B,Remote,143,Data Engineer ETRM,INFOPLUS TECHNOLOGIES,"Join Our Team as an ETRM Data Engineer – Poland (Remote) | B2B Contract ETRM Data Engineer to play a key role in transforming complex data into actionable insights. If you're eager to work on cutting-edge energy trading systems from the comfort of your home in Poland, this could be your next big opportunity! What You’ll Do: Design and Build robust, scalable data pipelines and manage high-performance ETRM systems. Drive data integration projects within the dynamic Energy Trading and Risk Management (ETRM) landscape. Collaborate with cross-functional teams to seamlessly integrate data from leading ETRM trading platforms like Allegro, RightAngle, and Endur. Optimize data storage solutions using Data Lake and Snowflake for faster, more reliable access. Develop and maintain ETL workflows leveraging Azure Data Factory and Databricks. Write clean, efficient Python code for data processing, analysis, and automation. Uphold the highest standards of data quality, accuracy, and integrity across diverse sources and platforms. Partner with traders, analysts, and IT professionals to understand data needs and deliver innovative solutions. Continuously enhance data architecture for performance and scalability to support business growth. What We’re Looking For: Proven expertise with Azure Data Factory (ADF) Experience working with Data Lake and Snowflake/SQL databases Strong Python/PySpark programming skills Familiarity with FastAPI for API development Hands-on experience with Databricks platform Why Join Us? Remote flexibility from anywhere in Poland Engage with innovative energy trading solutions Be part of a forward-thinking team shaping the future of energy markets Competitive B2B contract terms","[{""min"": 28000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,28000,30000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,144,Senior Data Engineer with Databricks,Crestt,"Cześć! Poszukujemy osoby na stanowisko Senior Data Engineer , która dołączy do zespołu naszego Klienta zajmującego się projektowaniem i rozwijaniem nowoczesnych rozwiązań w obszarze Data Lakehouse, Business Intelligence oraz zaawansowanej analityki w chmurze. Współpracując w międzynarodowym środowisku, będziesz miał(a) okazję pracować z najnowszymi technologiami w tej dziedzinie. Lokalizacja : praca głównie zdalna (1x w miesiącu spotkanie w biurze w Warszawie) Widełki: B2B 160-200 pln netto+vat/h UoP 26-30 tys. brutto/mies. Wymagania: Biegłość w SQL oraz Pythonie (min. 5 lat doświadczenia) Co najmniej 2-letnie doświadczenie w pracy z Databricks Doświadczenie w pracy w środowisku chmurowym (preferowany Azure) Minimum 5-letnie doświadczenie w projektowaniu oraz implementacji rozwiązań klasy BI, ETL/ELT, Data Warehouse, Data Lake, Big Data oraz OLAP Praktyczna znajomość zarówno relacyjnych, jak i nierelacyjnych Doświadczenie z narzędziami typu Apache Airflow, dbt, Apache Kafka, Flink, Azure Data Factory, Hadoop/CDP Znajomość zagadnień związanych z zarządzaniem danymi, jakością danych oraz przetwarzaniem wsadowym i strumieniowym Umiejętność stosowania wzorców architektonicznych w obszarze danych (Data Mesh, Data Vault, Modelowanie wymiarowe, Medallion Architecture, Lambda/Kappa Architectures) Praktyczna znajomość systemów kontroli wersji (Bitbucket, GitHub, GitLab) Wysoko rozwinięte umiejętności komunikacyjne, otwartość na bezpośredni kontakt z Klientem końcowym Certyfikaty z Databricks lub Azure będą dodatkowym atutem Zakres obowiązków: Projektowanie i wdrażanie nowych rozwiązań oraz wprowadzanie usprawnień w istniejących platformach danych Udział w rozwoju platform danych i procesów ETL/ELT, optymalizacja przetwarzania dużych zbiorów danych zgodnie z najlepszymi praktykami inżynierii danych Standaryzacja i usprawnianie procesów technicznych – implementacja standardów kodowania, testowania i zarządzania dokumentacją Dbanie o jakość kodu i zgodność z przyjętymi standardami – przeprowadzanie regularnych code review Aktywna współpraca z innymi ekspertami technologicznymi, w celu doskonalenia procesów oraz identyfikacji nowych wyzwań technologicznych Mentoring i wsparcie zespołu w zakresie projektowania rozwiązań, optymalizacji procesów i wdrażania najlepszych praktyk Klient oferuje: Udział w międzynarodowych projektach opartych na najnowocześniejszych technologiach chmurowych Pokrycie kosztów certyfikacji (Microsoft, AWS, Databricks) 60 płatnych godzin rocznie na naukę i rozwój Możliwość wyboru między pracą zdalną a spotkaniami w biurze Indywidualnie dopasowane benefity: prywatna opieka medyczna, dofinansowanie karty sportowej, kursy językowe, premie roczne i medialne oraz bonus za polecenie nowego pracownika (do 15 000 PLN)","[{""min"": 25600, ""max"": 32000, ""type"": ""Net per month - B2B""}, {""min"": 26000, ""max"": 30000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,25600,32000,Net per month - B2B
Full-time,Senior,Permanent,Remote,145,Staff Engineer,dotLinkers,"The role: Staff Engineer Salary: up to 37 500 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Staff Software Engineer on the Compute team, you will lead the design and development of a next-generation compute platform that powers scalable workloads. Your focus will be on creating robust, event-driven, and batch-capable compute infrastructure using Kubernetes, KEDA, Temporal, Apache Spark, and advanced stream processing technologies. You will be part of an Infrastructure Services organization, structured into specialized teams focused on core platform areas such as compute, networking, and storage. The Compute team enables developers to deploy scalable workloads—from microservices and scheduled jobs to real-time data pipelines—leveraging cloud-native patterns that prioritize performance, elasticity, and developer autonomy. Key Responsibilities: Architect and implement cloud-native compute solutions to orchestrate background jobs, long-running workflows, and streaming data pipelines. Develop and maintain compute abstractions integrating Kubernetes, KEDA, and Temporal to support scalable job and service orchestration. Lead development of elastic data processing approaches using Apache Spark for batch workloads and streaming frameworks for real-time analytics. Define and promote best practices for running event-driven and parallel workloads in production environments. Collaborate with engineering teams to support various compute use cases, including microservices, cron jobs, ETL, workflow engines, and machine learning workloads. Ensure reliable autoscaling, failure handling, and resource optimization across compute workloads. Partner with platform security and observability teams to ensure compliance, transparency, and monitoring of workload execution. Mentor engineers on distributed system design and modern compute orchestration techniques. Minimum Qualifications: 8+ years of experience in backend, infrastructure, or data platform engineering roles. Extensive production experience with Kubernetes (preferably AKS). Hands-on experience with orchestration/eventing frameworks like KEDA, Temporal, or similar. Skilled in developing batch (e.g., Spark) and streaming (e.g., Kafka, Flink, Azure Event Hubs) processing systems. Strong programming skills in Go, Python, or C#, focusing on distributed or event-driven systems. Familiarity with secure compute design, autoscaling, and high-availability architectures. Preferred Qualifications: Experience implementing KEDA, Temporal, or Argo Workflows in production environments. Knowledge of Azure Synapse, Azure Data Explorer, Azure Data Factory, or Databricks. Experience building compute platforms for streaming analytics, ETL pipelines, or AI/ML workloads. Understanding of event-driven and pub-sub architectures like Kafka or Azure Event Grid. Contributions to cloud-native or CNCF open-source projects. Leadership Expectations: Define the strategy and technical roadmap for a unified compute platform. Promote cloud-native, event-driven, and scalable architecture practices across teams. Lead architecture reviews and encourage adoption of modern orchestration technologies. Mentor engineers and lead teams toward resilient, observable, and developer-friendly compute solutions. Drive long-term initiatives to improve workload performance, efficiency, and maintainability. Core Competencies: Cloud-Native Orchestration: Expert knowledge of Kubernetes-based job orchestration and scalable compute architectures. Data Processing: Ability to design for hybrid streaming and batch workloads in modern applications. Technical Leadership: Influences platform strategy and architectural direction organization-wide. Scalable System Design: Proven track record of scaling compute across diverse workloads and tenants. Developer Experience: Passionate about empowering engineering teams with powerful yet easy-to-use platforms. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 32500, ""max"": 37500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,32500,37500,Gross per month - Permanent
Full-time,Junior,Any,Hybrid,147,Młodszy/a Programista/tka ERP,Unisoft,"Jesteśmy producentem oprogramowania wspomagającego zarządzaniem przedsiębiorstwem. Od prawie 40 latnie tylko tworzymy, ale także doradzamy, wdrażamy i szkolimy w zakresie zarządzania większością procesów zachodzących w przedsiębiorstwie. Poszukujemy trzech osób, które dołączą do zespołu i w ramach przydzielonego obszaru merytorycznego będą odpowiedzialne za rozwijanie naszego autorskiego systemu. programowanie według przygotowanych założeń projektowych, dbanie o rozwój aplikacji zgodnie z wewnętrznymi standardami programowania i projektowania, tworzenie dokumentacji technicznej umożliwiającej dalszy rozwój i eksploatację oprogramowania. ukończone (bądź w trakcie ostatniego roku) studia na kierunku związanym z informatyką, matematyką, elektroniką lub telekomunikacją, znajomość dowolnej relacyjnej bazy danych, znajomość języka SQL, znajomość programowania obiektowego, znajomość języka angielskiego na poziomie czytania dokumentacji technicznej, umiejętność pracy w zespole, chęć rozwoju zawodowego. Mile widziane: znajomość któregoś z języków programowania: Delphi, C#/C++ itp., umiejętność posługiwania się systemami kontroli wersji (np. Git), doświadczenie w realizacji projektów informatycznych. pracę w firmie o prawie 40-letniej tradycji w branży IT, zatrudnienie w oparciu o wybraną formę współpracy (UoP, B2B, zlecenie), atrakcyjne wynagrodzenie dopasowane do kompetencji, stabilizację i work-life balance (pracujemy w stałych godzinach i nie zabieramy pracy do domu) możliwość rozwoju zawodowego pod opieką doświadczonych programistów, wsparcie merytoryczne od liderów technologicznych, zgrany zespół chętny do pomocy i dzielenia się wiedzą, pracę w centrum Gdyni: 10 min. do Dworca PKP/SKM Gdynia Główna, 2 minuty pieszo z przystanku ZKM „Plac Kaszubski-Świętojańska 01”, 5 min pieszo do Skweru Kościuszki, 10 min. na plażę miejską, co najmniej 40 restauracji i kawiarni w promieniu 800m, parking rowerowy pod biurem i prysznice w biurze (świetna lokalizacja ma też swoje minusy w postaci trudno dostępnych miejsc parkingowych).","[{""min"": 6500, ""max"": 9000, ""type"": ""Gross per month - Any""}]",Unclassified,6500,9000,Gross per month - Any
Full-time,Mid,B2B,Remote,148,Data Engineer with Palantir,Link Group,"About the Role We are looking for aData Engineer experienced with Palantir Foundryto join a cross-functional team working on large-scale data integration, modeling, and analytics platforms. The ideal candidate is hands-on, proactive, and capable of navigating complex data ecosystems in an enterprise environment. Design and build data pipelines and models usingPalantir Foundry Integrate multiple data sources (structured and unstructured) into usable, high-quality data assets Collaborate with data scientists, analysts, and business stakeholders to support advanced analytics initiatives Apply data governance, lineage, and cataloging principles within Foundry Develop and maintain Foundry “Objects”, Code Workbooks, and other tooling Ensure quality, performance, and scalability of the implemented data solutions Support and document platform usage and development best practices 3+ years of experience in Data Engineering Hands-on experience withPalantir Foundryin a commercial or enterprise setting Proficiency inSQL,Python, and data transformation techniques Good understanding ofdata modeling(dimensional, relational, and graph-based) Familiarity withdata governanceandmetadata management Experience working incloud-based environments(AWS, GCP, or Azure) Excellent communication skills and ability to work with cross-functional teams Previous experience in highly regulated industries (finance, pharma, defense, etc.) Experience integrating Foundry with external tools and systems via APIs Knowledge ofCI/CD,Git, and software engineering best practices Exposure to tools likeAirflow,dbt,Databricks,Snowflake, etc. Experience withdata privacy regulations(GDPR, HIPAA, etc.)","[{""min"": 90, ""max"": 105, ""type"": ""Net per hour - B2B""}]",Data Engineering,90,105,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,150,Data Engineer with Hadoop,Antal Sp. z o.o.,"Develop automation tools and integrate existing solutions within a complex platform ecosystem Provide technical support and design for Hadoop Big Data platforms (Cloudera preferred) Manage user access and security (Kerberos, Ranger, Knox, TLS, etc.) Implement and maintain CI/CD pipelines using Jenkins and Ansible Perform capacity planning, performance tuning, and system monitoring Collaborate with architects and developers to design scalable and resilient solutions Deliver operational support and improve engineering tooling for platform management Analyze existing processes and design improvements to reduce complexity and manual work Building scalable automation in a diverse ecosystem of tools and frameworks Enhancing service resilience and reducing operational toil Supporting the adoption of AI agents and real-time data capabilities Integrating with corporate identity, CI/CD, and service management tools Collaborating with cross-functional teams in a global environment Minimum 5 years of experience in engineering Big Data environments (on-prem or cloud) Strong understanding of Hadoop ecosystem: Hive, Spark, HDFS, Kafka, YARN, Zookeeper Hands-on experience with Cloudera distribution setup, upgrades, and performance tuning Proven experience with scripting (Shell, Linux utilities) and Hadoop system management Knowledge of security protocols: Apache Ranger, Kerberos, Knox, TLS, encryption Experience in large-scale data processing and optimizing Apache Spark jobs Familiarity with CI/CD tools like Jenkins and Ansible for infrastructure automation Experience working in Agile or hybrid development environments (Agile, Kanban) Ability to work independently and collaboratively in globally distributed teams To learn more about Antal, please visitwww.antal.pl","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,180,220,Net per hour - B2B
Full-time,Senior,B2B,Remote,152,Senior Data Scientist,Sii,"Minimum 5 years of experience in machine learning, data modeling, and statistical analysis Proficient in Python and ML frameworks such as scikit-learn, TensorFlow, or PyTorch Experience with cloud-based ML development (preferably AWS) Familiarity with Docker, Kubernetes, and CI/CD tools like Jenkins or GitLab CI Strong SQL skills and experience working with large datasets and data preprocessing Fluency in English (spoken and written) Fluent Polish required Residing in Poland required We are looking for an experienced and forward-thinking Senior Data Scientist to join our medical client’s team. In this role, you will lead the development and implementation of machine learning algorithms, ensuring their seamless integration into scalable, high-performance systems. You’ll play a key role in driving data-driven decision-making and delivering robust AI solutions that align with strategic goals. Develop and apply machine learning algorithms Perform experiments, run tests, and assess results to enhance model precision and effectiveness Translate data into actionable insights through advanced analytics and modeling Collaborate with cross-functional teams to deploy and monitor ML solutions in production Provide mentorship to junior team members and contribute to data science best practices Support cloud engineering initiatives, particularly on AWS infrastructure Rekrutacja online Język rekrutacji: polski Start ASAP Praca w pełni zdalna Darmowe śniadanie Bez wymaganego dress code'u Nowoczesne biuro Darmowa kawa Szkolenia wewnętrzne Pakiet sportowy Budżet na szkolenia Małe zespoły Prywatna opieka medyczna Międzynarodowe projekty Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title – get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers – Power People. Learn more atsii.pl.","[{""min"": 22000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Science,22000,28000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,154,Qlik Developer,ITDS,"Qlik Developer Join us, and turn data into powerful business decisions! Kraków - based opportunity with hybrid work model (6 days/month in the office). As aQlik Developer,you will be working for our client, a leading global financial institution undergoing a major digital transformation to enhance its data analytics capabilities. You will be part of a dynamic team focused on developing and maintaining scalable Qlik dashboards and reports, aimed at improving decision-making across various business units. The project involves managing complex data sets, implementing infrastructure best practices, and ensuring compliance within a fast-paced, highly regulated environment. Your role plays a key part in optimizing how data is shared, visualized, and utilized to deliver impactful business insights. Developing advanced dashboards and reports using QlikSense Ensuring data integrity and managing updates to data sources Managing project sites and content on Qlik Server Documenting data sources, processes, and dashboards clearly Analyzing data sharing policies and promoting compliance best practices Collaborating with cross-functional teams and stakeholders at all levels Supporting Qlik deployment by following infrastructure best practices Enhancing dashboard performance for large data sets Influencing stakeholders through thoughtful data presentations Following established internal control standards and audit requirements Proficiency in Qlik with strong dashboard development experience 3+ years of experience in data analysis roles Practical knowledge of QlikSense and Qlik architecture Ability to work with large data sets while maintaining performance High level of mathematical and analytical skills Proven experience in stakeholder management and communication Strong collaboration skills within cross-functional teams Experience documenting technical processes and data sources Ability to adapt quickly in fast-changing environments Understanding of regulatory requirements for data sharing Experience with Qlik administration Familiarity with financial services or highly regulated industries Knowledge of best practices for offshore project coordination Prior involvement in change or transformation projects Awareness of risk management and internal control standards We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7326 You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere.","[{""min"": 25200, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,31500,Net per month - B2B
Full-time,Senior,B2B,Remote,156,Data & AI Copilot Senior Developer,Link Group,"💼 Senior Data & AI Copilot Developer 📍 Remote or Hybrid | International enterprise client 💰 B2B | Start: ASAP Join a project focused on building advanced Copilot solutions within the Microsoft ecosystem (D365, Power Platform, Azure). We’re looking for an experienced developer with a strong background in AI automation, data integration, and enterprise-grade solution design. 🔧 Key Responsibilities: Design and develop customized Microsoft Copilot solutions based on business needs Integrate Copilot with D365, Power Platform, and external systems Automate business processes using Power Automate, Logic Apps, and other tools Continuously improve performance based on user feedback and usage metrics 🧠 Requirements: Proven experience with Microsoft Copilot, AI, automation, and integration projects Strong knowledge of Power Platform, D365, Azure (especially Logic Apps, Power Automate) Hands-on experience with Python or similar tools for AI integration Excellent communication skills – able to work cross-functionally and explain tech to business Nice to have: Microsoft AI certifications (e.g., Azure AI Engineer Associate)","[{""min"": 110, ""max"": 145, ""type"": ""Net per hour - B2B""}]",Data Science,110,145,Net per hour - B2B
Full-time,Senior,B2B,Remote,158,Remote Senior Quantitative Developer,Montrose Software,"Client description: Montrose Software has an ongoing relationship with one of the largest North American banks to enhance and maintain its production libraries and applications. The bank serves 17 million clients worldwide and provides personal and commercial banking, wealth management, insurance, investor services, and capital markets products and services globally. We work mainly with their capital markets teams, focusing on derivatives and fixed income systems. Project description: We work on maintaining and enhancing a Risk management system for a big bank. The system is a core internal instrument for the data analyst and economist to make risk calculations. It is split into 3 layers: Front end - Microsoft Excel with a fully custom set of function and menus; standalone Python app Business Logic Library - C++ and Python modules on the network Calculation grid - 50k computing grid running the simulations in house, AWS and Azure Qualifications: 5+ years of experience in software development, ideally in the quantitative finance area Good knowledge of C++ and basic knowledge of Python In-depth knowledge of various (vanilla and exotic) derivative products Familiarity with mathematical models for the dynamics of the financial market FpML knowledge is a plus Numeracy to be able to understand quantitative finance Commercial experience in financial or capital markets Ability to use a version control system, ideally Git Finding yourself in a large codebase Good communication skills Being open to code in other programming languages Fluent in spoken and written English Nice to have: Familiarity with AWS, Docker Experience in SQL Please note that the interview process is divided into four parts: Technical phone screening~30 minutes Technical interview~2 hours Non-technical call with hiring manager~45 minutes Optional: call with client for which you will be working We offer: Premier equipment to work Flexible working hours Remote work possibility Interesting, challenging and exciting work with international teams English lessons with native speaker Training Budget Multisport card Food : Lunches from Kraków's restaurants that are delivered both to the office and homes( or a refund of the budget allocated for it) Kitchen full of food, drinks, fruit, and snacks Health Private medical insurance Air-conditioning Well-being: No dress code The chillout area incudes comfortable bean bags, therapy balls, PlayStation 4 or Nintendo Switch + games stretching area and pull-up bar Team events Shower Additional : Indoor parking place for bicycles","[{""min"": 22000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,30000,Net per month - B2B
Full-time,Senior,B2B,Remote,159,Senior Data Engineer (GCP + Python),emagine Polska,"Industry: Energy System, Startup Rate: 42-50 Euro/h on B2B Location: Poland (Remote) Contract length: 6-9 months with possible extension Introduction & Summary The primary objective of this role is to spearhead a data migration project focused on transferring data pipelines from a third-party orchestration tool to native Google Cloud Platform (GCP) services. The ideal candidate should possess 5+ years of experience as a Data Engineer with strong competencies in Google Cloud Platform, Python, and Terraform. Main Responsibilities Design and implement the data migration architecture. Establish infrastructure representation using Terraform. Select, configure, and manage an orchestrator. Implement reliable CI/CD practices for testing and deployments. Migrate data pipelines to the GCP environment and validate performance. Onboarding a new full-time employee. Key Requirements At least 5 years of experience as Data Engineer. Proven experience with Google Cloud Platform (GCP). Proficiency in Python and SQL for pipeline development. Hands-on expertise with Terraform as an infrastructure-as-code tool. Nice to Have Experience with DBT (Data Build Tool). Previous migration experience of data pipelines from tools like Twirl to Composer. Understanding of CI/CD best practices in data projects.","[{""min"": 28500, ""max"": 35280, ""type"": ""Net per month - B2B""}]",Data Engineering,28500,35280,Net per month - B2B
Full-time,Mid,B2B,Remote,161,Experienced Data Scientist,Future Mind,"Future Mind is a brilliant, inspiring team, one of the most awarded tech consulting companies in the region with a broad portfolio of clients, including Żabka, Jeronimo Martins (Hebe, Biedronka), LPP (Reserved, Sinsay, Mohito), eObuwie, Modivo, and other well-recognized brands. We have received several industry awards for delivering some of the best eCommerce applications in Poland, listed among the most popular across app stores. Our expert engineers, designers, project managers, and analysts work on projects ranging from top mobile commerce apps used daily by millions of customers to IoT, and telematics platforms that produce vast amounts of data. In 2023, we joined forces with Solita, a Finnish tech powerhouse with a vibrant community of over 2000 specialists across Europe, that combines data, business, and technology skills to build and improve digital services for leading organizations in manufacturing, medical, shipping, and other major industries, including such clients as NATO, Nokian Tires, Pfizer and many others. Together we are dedicated to delivering cutting-edge, data-driven solutions. We value proactive professionals who take ownership, enjoy solving problems, sharing knowledge, and collaboration. Together, we create high-quality software solutions that fulfil our clients' business needs and impact their customers' lives every day. As part of the Solita Group, Future Mind provides digital advisory & delivery services with the support of our international partners and upholds equally high cultural standards. Role description: As a Data Scientist, your role involves taking part in data insights initiatives, designing and implementing machine learning solutions, while collaborating closely with the business stakeholders. You have a strong statistics background and have worked with cloud technologies for several years. This job is all about: Designing data science solutions based on large, complex data sets that meet both functional and non-functional business requirements; Building, testing, and deploying machine learning and statistics models on cloud environments; Working with cross-functional teams on delivering business value; Collaborating within our project teams to meet client needs and deliver high-quality solutions. Here’s what we’re looking for: Ability to analyze large datasets, understand data patterns, and implement AI solutions that drive business impact. Strong background in statistics, technical proficiency in Python, SQL, and leading modern ML frameworks. Familiarity with cloud platforms (AWS, Google Cloud, Azure) and deploying machine learning models in production environments. Previous experience in Ad Tech or analysing large behavioral datasets would be an advantage. Effective communication skills, proficient in Polish and English. You also must have the legal right to work in the EU to apply for this position. …and here’s what we offer: A dynamic work environment where innovation and collaboration are valued. Access to cutting-edge projects and technologies in a variety of industries. A supportive community of experts to foster your professional growth and development. Competitive compensation, comprehensive benefits, and a focus on work-life balance. Opportunities for continuous learning and career advancement, including specialized training in big data technologies and Snowflake certifications. The ability to work fully remotely or check into one of our offices whenever you like, Fully paid private health insurance, subsidized sports membership, mental health support, and language courses.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Science,14000,22000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,162,Senior + / Staff Big Data Engineer,The Stepstone Group Polska,"At The Stepstone Group, we have a simple yet very important mission: The right job for everyone. Using our data, platform, and technology, we create opportunities for job seekers and companies around the world to find a perfect match, in fair and equitable way. With over 20 brands across 30+ countries, we strive for fair and unbiased hiring. At our Tech Hub, located near Wilanowska Metro, we are here as more than 300 ambitious specialists who work on the development of our IT products. We are proud to be part of The Stepstone Group, a global expert in job-tech platforms and e-recruiting. Join our team of 4,000+ employees and be part of reshaping the labour market and becoming the world’s leading job-tech platform. The job at a glance We’re looking for a Staff Big Data Engineer who is passionate about building meaningful, data-driven products that have real-world impact. In this hands-on and strategic role, you’ll design and lead the development of secure, scalable data solutions that power intelligent search, matching, and recommendations across our global platform. Working within the Marketplace Enablement portfolio in the Data Products and Search & Match function, you’ll collaborate with a diverse mix of engineers, data scientists, product managers, and stakeholders to drive innovation and build with purpose. This is a hybrid role based in Warsaw, with flexible remote working options. Your responsibilities Lead the architecture and development of big data solutions within a modern cloud environment (AWS) Design, build, and scale batch and real-time data pipelines that support inclusive, AI-driven features Partner with teams across the business to align data products with user needs and long-term goals Establish best practices in Big Data Engineering, fostering a culture of quality, learning, and collaboration Mentor engineers across teams, helping them grow while raising the overall standard of our work Actively contribute to our big data engineering community, sharing knowledge and supporting others Your skills and qualifications Solid experience designing and deploying scalable, secure big data systems Proficiency in Python, Apache Spark / PySpark, and AWS big data technologies Familiarity with tools like Apache Airflow, AWS Step Functions, or AWS Glue Workflows Understanding of batch and real-time processing; experience with Kafka is a plus Solid understanding of the software development lifecycle, CI/CD, containerization, observability, and security Strong collaboration, communication, and mentoring skills Comfortable working in a cross-functional environment and advocating for best practices A growth mindset, with a passion for inclusive technology and solving meaningful problems Fluency in English; experience in agile environments is a plus Benefits We’re a community here that cares as much about your life outside work as how you feel when you’re with us. Because your job shouldn’t take over your life, it should enrich it. Here are some of the benefits we offer: Medical and dental care Life insurance Benefit platform budget Employee Referral Programme Hackathons, Knowledge Sharing Hours In-house projects Events and integration parties Charity initiatives, 2 extra volunteer days English/German lessons Game room and chillout zone Our commitment Equal opportunities are important to us. We believe that diversity and inclusion at The Stepstone Group are critical to our success as a global company, so we want to recruit, develop, and keep the best talent. We encourage applications from everyone, regardless of background, gender identity, sexual orientation, disability status, ethnicity, belief, age, family or parental status, and any other characteristic.","[{""min"": 18000, ""max"": 30500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,30500,Gross per month - Permanent
Full-time,Senior,B2B,Remote,163,Data Engineer,TechTorch,"TechTorch is a pioneer among service companies in the way it conducts projects in the area of digital transformation. Through an implementation process supported by an AI-powered platform and a global team of world-class managers and technology experts, TechTorch has enabled dozens of private equity sector companies to accelerate the realization of business benefits. Job Description We are seeking a skilled and motivated Data Engineer to join our growing team. The ideal candidate will have a strong background in data engineering, with specific experience in working with Snowflake. You will be responsible for designing, building, and maintaining scalable data pipelines that can handle large volumes of data, ensuring the reliability, security, and performance of our data infrastructure. Key Responsibilities Design and Implement Data Pipelines: Develop, test, and maintain scalable data pipelines to support business analytics and reporting needs. Work with Snowflake: Design and optimize data architecture using Snowflake, including data modeling, data warehousing, and performance tuning. Data Integration: Integrate data from various sources, including APIs, databases, and third-party platforms, into our centralized data warehouse. Collaborate Across Teams: Work closely with data scientists, analysts, and other engineers to ensure data availability, quality, and reliability. Maintain Data Infrastructure: Monitor and maintain the performance and health of the data infrastructure, addressing any issues proactively. Documentation: Document data processes, pipeline architecture, and procedures for ongoing maintenance and future enhancements. Qualifications Experience with Snowflake: Minimum of 3 years of hands-on experience with Snowflake, including data warehousing, schema design, and query optimization. Programming Skills: Proficiency in programming languages such as Python, SQL. Data Pipeline Tools: Experience with ETL/ELT tools such as Apache Airflow, or similar. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or Google Cloud, particularly in data storage and processing. Database Management: Strong knowledge of relational and non-relational databases. Problem-Solving: Excellent problem-solving skills and ability to troubleshoot data issues quickly and efficiently. Communication: Strong communication skills, with the ability to collaborate effectively with cross-functional teams. What We Offer: Work in an international team Work with the largest capital groups in the world Development and rapid promotion opportunities Autonomy in action Participation in building a new global company Team-building activities","[{""min"": 21000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,30000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,165,👉 Data Platform Architect,Xebia sp. z o.o.,"🟣You will be: 🟣Your profile: extensive experience working with Azure cloud provider, 🟣Recruitment Process: CVreview –HRcall –Technical Interview–ClientInterview (with Live-coding) –Hiring ManagerInterview –Decision 🎁Benefits 🎁 ✍Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 29800, ""max"": 36800, ""type"": ""Net per month - B2B""}, {""min"": 23900, ""max"": 29700, ""type"": ""Gross per month - Permanent""}]",Data Architecture,29800,36800,Net per month - B2B
Full-time,Senior,B2B,Remote,166,MicroStrategy Developer,Calimala.ai,"Calimala.aiis seeking a highly skilledMicroStrategy Developerwith over 5 years of proven experience. In this role, you'll play a pivotal part in designing, developing, and optimizing advanced MicroStrategy reports, dashboards, and datasets. Your expertise will drive business insights for our clients, particularly within the Telecom industry, ensuring that our reporting solutions are both efficient and scalable. Responsibilities Develop and optimize a wide range of MicroStrategy reports, dashboards, and datasets. Collaborate with data engineers and analysts to translate business requirements into actionable insights. Design and implement robust schema structures to support high-performance reporting. Integrate ETL processes to ensure comprehensive and reliable data feeds. Manage performance tuning and remediation efforts to maintain system efficiency. Requirements Candidates must have a demonstrated background in business intelligence with specific expertise in MicroStrategy BI and related technologies. The ideal candidate will combine technical proficiency with strong problem-solving and communication skills. Experience in the Telecom industry is highly preferred, as is a commitment to continuous improvement and excellence in data-driven environments. Benefits Competitive salary package with performance-based incentives. The position is fully remote Opportunities for career advancement within a rapidly growing company. Continuous learning and professional development resources. A collaborative work culture that fosters innovation and teamwork. ﻿Why Apply? This is an excellent opportunity to contribute to a dynamic team in a company that values technical innovation and professional growth. If you thrive in a challenging environment and are excited to apply your skills to impactful projects, we invite you to apply today.","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,14000,25000,Net per month - B2B
Full-time,Junior,Permanent,Hybrid,171,Associate Data Analyst,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Associate Data Analyst Are you ready to kickstart your career in data analytics and make a meaningful impact in the world of business intelligence? We are seeking a motivated Associate Data Analyst to join our dynamic Data Assets, Analytics, and AI Platform at Bayer IT. In this entry-level role, you will support the analysis of critical data across market, finance, and product supply domains, contributing to our innovative initiatives. As part of our diverse, international team, you will help harness the power of data to drive informed decision-making and enhance product performance. You will work with advanced analytics and visualization tools, gaining valuable experience in a modern tech stack featuring Azure Databricks and Snowflake. If you’re passionate about data and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks and Responsibilities: Collaborate with product managers and technical teams to gather and clarify product objectives and data requirements. Assist in the exploration and analysis of diverse data sets to uncover insights that can drive product improvements. Support the development and maintenance of data structures, ensuring they are scalable and meet the needs of the team. Contribute to the monitoring of key performance indicators (KPIs) to identify trends and support data-driven recommendations. Help with the extraction, cleaning, and preparation of data from various sources to ensure accuracy and reliability in reporting. Document analytical processes, findings, and data models to facilitate knowledge sharing and team collaboration. Participate in team discussions and brainstorming sessions to propose ideas for enhancing data utilization and reporting. Qualifications and Competencies: Bachelor’s degree in Statistics, Computer Science, Data Science, or a related field. Some experience in data analysis, internships, or relevant coursework, preferably within the pharmaceutical or healthcare industry. Familiarity with data manipulation and analysis tools, such as Azure Databricks and Snowflake, is a plus. Knowledge of relational databases (PostgreSQL, MySQL) is desirable. Strong analytical and problem-solving skills with attention to detail. Good communication skills, with the ability to present data clearly and understandably. Ability to work collaboratively in a team-oriented environment. Fluent in English, both written and spoken. What do We offer: A flexible, hybrid work model. A great workplace in a new modern office in Warsaw. Career development, 360° feedback, and mentoring programs. Wide access to professional development tools, training, and conferences. Company bonus and reward structure. VIP medical care package (including dental and mental health). Holiday allowance (“Wczasy pod gruszą”). Life and travel insurance. Pension plan. Co-financed sport card - FitProfit. Meals subsidy in the office. Additional days off. Budget for home office setup and maintenance. Access to the company game room equipped with table tennis, soccer table, Sony PlayStation 5, Xbox Series X consoles, and massage chairs. Tailored support for relocation to Warsaw when needed. If you feel you do not meet all criteria we are looking for, that doesn’t mean you aren’t the right fit for the role. Apply with confidence; we value potential over perfection! WORK LOCATION: WARSAW, AL. JEROZOLIMSKIE 158","[{""min"": 10000, ""max"": 16000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,10000,16000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,173,Data Architect,TechTorch,"TechTorch is a pioneer among service companies in the way it conducts projects in the area of digital transformation. Through an implementation process supported by an AI-powered platform and a global team of world-class managers and technology experts, TechTorch has enabled dozens of private equity sector companies to accelerate the realization of business benefits. Position Overview: The Data Architect is responsible for designing, creating, deploying, and managing an organization’s data architecture. This role involves defining how the data will be stored, consumed, integrated, and managed by different data entities and IT systems, as well as any applications using or processing that data. The Data Architect works closely with business and IT stakeholders to ensure that the data architecture aligns with business objectives and supports the needs of the organization. Key Responsibilities: Data Architecture Design: Develop and maintain the overall data architecture, including data models, data flow diagrams, and data integration plans. Define and document the data architecture framework, standards, and principles. Design scalable and flexible data solutions that meet the business requirements and integrate with existing systems. Data Governance and Management: Establish and enforce data management and governance policies, procedures, and standards. Ensure data integrity, quality, and security across the organization. Define and manage data architecture principles and guidelines for data modeling, design, and implementation. Data Integration: Design and oversee data integration processes, ensuring seamless data flow across various systems and platforms. Implement data integration solutions using ETL (Extract, Transform, Load) tools and other data integration technologies. Collaborate with application architects and developers to design and implement data interfaces and APIs. Qualifications: Experience: Minimum of 7 years of experience in data architecture, data management, or related roles. Proven experience in designing and implementing data solutions in complex environments. Strong background in data modeling, database design, and data warehousing. Technical Skills: Proficiency in data modeling tools (e.g., ER/Studio, Erwin). Expertise in database technologies (e.g., SQL Server, Oracle, MySQL, NoSQL databases). Experience with data integration tools (e.g., Informatica, Snowflake, Talend, Apache Nifi). Knowledge of big data technologies (e.g., Hadoop, Spark, Kafka) and cloud data platforms (e.g., AWS, Azure, Google Cloud). Understanding of BI systems such as PowerBI and Tableaux Key Competencies: Strong analytical and problem-solving skills. Excellent communication and interpersonal skills. Ability to lead and influence cross-functional teams. Strategic thinking with a focus on delivering business value. Adaptability and continuous learning mindset What We Offer: Work in an international team Work with the largest capital groups in the world Development and rapid promotion opportunities Autonomy in action Participation in building a new global company Team-building activities","[{""min"": 30000, ""max"": 35000, ""type"": ""Net per month - B2B""}]",Data Architecture,30000,35000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,176,Senior Database Engineer,Navblue,"Job Summary: Aviation. It connects our world, brings people together, provides opportunities, accelerates economic growth, and is just so very cool! Come work for NAVBLUE, a leading services company owned by Airbus, dedicated to Flight Operations, Air Traffic Management solutions and services for airlines, airports, and Air Navigation Service Providers (ANSPs). We combine aircraft manufacturer expertise, flight operations know-how, and agile development to enhance operational efficiency, optimize resources, and increase productivity for a safe and sustainable aviation future. Our global teams deliver a reliable, optimum, and customized user experience to more than 500 customers worldwide. We are looking for a seasoned Software Engineer with extensive experience in designing, implementing, and optimizing database solutions in a microservices-based environment. As a member of our team, you will contribute to the full lifecycle of our data persistence layer, from schema design and performance tuning to ensuring robust replication, disaster recovery, and seamless integration within our cloud-native microservice ecosystem. Responsibilities: Design & Develop Database Solutions: Architect, design, and implement highly optimized relational (e.g., MySQL, PostgreSQL, AWS Aurora, SQL Server) and NoSQL (e.g., MongoDB, DynamoDB, Redis) database schemas, ensuring data integrity, performance, and scalability for microservices. Performance Optimization & Tuning: Proactively analyze and optimize complex queries, implement efficient indexing strategies, and manage partitioning/sharding to ensure peak database performance and handle high throughput. Reliability & Disaster Recovery: Design, implement, and maintain robust backup, disaster recovery, and high-availability solutions, including replication (master-slave/multi-master) and failover configurations, to ensure data durability and system uptime. Cloud Database Management: Deploy, configure, and manage database instances within cloud infrastructure (e.g., AWS RDS, Aurora), leveraging cloud-native features for scalability and operational efficiency. Focus on quality by promoting coding best practices, a test-first mindset and highest security standards. Contribute to building new and improving existing development processes. Work within a small agile teams delivering new features and fixing defects. Lead technical designs, taking a holistic view of the product, and collaborate with multiple stakeholders to define the best approach to address upcoming challenges and deliverables. Define and drive the team's technical direction, mentor junior engineers, and proactively identify, propose, and implement new processes or architectural improvements to enhance team efficiency, code quality, and timely delivery. Contribute to software architecture discussions, translate system-level designs and architectural blueprints into robust, maintainable, and high-quality code, applying the latest best practices in software engineering. Required Skills/Experience: 6+ years in roles directly responsible for the availability, performance, and security of critical databases. Expertise in Database Technologies: Strong command of MySQL, PostgreSQL, AWS Aurora, SQL Server and experience with MongoDB, DynamoDB, or Redis, including schema design and complex query writing. Database Performance & Scalability: Proven ability in query optimization (e.g., EXPLAIN plans), operating system optimization, advanced indexing strategies, and implementing partitioning/sharding and caching (Redis). Cloud Database Operations: Hands-on experience deploying, managing, and optimizing databases within AWS infrastructure (RDS, Aurora) and virtual machine infrastructure (SQL Server). Database Reliability & Security: Experience with high availability (replication, failover), backup automation, PITR, and data security (encryption, SQL injection prevention). Automation & DevOps: Proficiency in integrating database changes into CI/CD pipelines using schema migration tools (DbUp, EF migrations, Flyway, Liquibase) and Git for version control. Scripting & Troubleshooting: Strong scripting skills (Python/Bash) for automation and ability to analyze logs and monitor performance using tools like AWS Cloudwatch, Datadog, Prometheus, Grafana, or pgBadger. Solid understanding of DevOps practices, including CI/CD pipelines (e.g., GitLab CI, Cloudbees, Jenkins, GitHub Actions), containerization with Docker, and monitoring/logging tools. Demonstrated experience in leading software development teams, fostering a collaborative and high-performance culture, and effectively representing the team's technical vision and needs to stakeholders, including architects and senior management Strong capability in identifying technical challenges and bottlenecks, constructively proposing and implementing effective solutions (either individually or by guiding the team), while actively building team engagement, fostering a positive atmosphere, and championing team spirit Master of Science Degree in software engineering or a related field Proficiency in English spoken and written Nice-to-Haves: Experience with ETL/ELT pipeline design and tools (e.g., Apache Airflow). Familiarity with Change Data Capture (CDC) solutions. Knowledge of database services on other cloud platforms (e.g., Azure SQL Database, Google Cloud Spanner). Understanding of ORM frameworks (Entity Framework, Dapper, SQLAlchemy, Hibernate) from a database performance perspective. Experience with data governance or data lineage concepts and tools. Active contributions to open-source database projects or the broader database community. Proficiency in designing and implementing GraphQL backends or similar API patterns that interact heavily with databases. Understanding of airline operations, flight planning, or air navigation principles. Passion for the aviation industry. We offer: Stable employment based on a full-time job contract International working environment in a dynamic company Access to the latest knowledge and technologies enabling professional development Training and development possibilities Participating in international projects and international trips Competitive salary dependent on experience and qualifications Flexible working hours and work-from-home opportunities Private medical coverage for you and your family Sport card Life insurance for you and your family Co-funding for meals Employee stock ownership plan How to Apply: Candidates who are interested in joining the NAVBLUE team are invited to submit their resume and cover letter, highlighting their work experiences and skills via email totalent@navblue.aero. We thank all applicants for applying. Only selected applicants will be contacted. Navblue is committed to creating an environment and a culture where everyone feels like they belong no matter who they are or where they are from. We are committed to providing equal employment opportunities to all individuals based on job-related qualifications and ability to perform a job. We do not discriminate against any employee or applicant for employment because of race, colour, sex, age, national or ethnic origin, religion, sexual orientation, gender identity or expression, marital status, family status, genetic characteristics, record of offences, and basis of disability or any protected class. Accommodations will be available on request for candidates throughout the entire recruitment and selection process. NAVBLUE is operating within the Airbus Helicopters Polska Structure. About Us: NAVBLUE, an Airbus Company, is a leading global provider of flight operations solutions, including aeronautical charts, navigation data solutions, flight planning, aircraft performance software (take-off/landing, weight and balance), and crew planning solutions. You’ll be able to shape the future of the digital aviation industry by working on several of the best in the industry flagship products enabling pilots, dispatchers, flight engineers and other aviation personnel on a daily basis to deliver safe, efficient, and reliable flight operations all over the world. You’ll have the opportunity to support millions of flights each year and help NAVBLUE customers maximize efficiency, reduce costs, ensure compliance with complex national and international safety regulations, and effectively deliver their services. You’ll join a team with a focus on digital and collaborative innovation that is passionate and customer-focused. Over the last few years, Airbus has been supportive of various initiatives such as Going Digital, Performance Based Navigation Services, Air Traffic Management Modernization Programs, FlySmart on iOS and other digital projects related to new aircraft technologies; the launch of NAVBLUE was therefore a natural step to further develop its Flight Operations and Air Traffic Management Portfolio. NAVBLUE is a fully owned subsidiary of Services by Airbus, fueled by the agility of Airbus ProSky and Navtech (acquired in 2016), and the pioneering spirit of Airbus, NAVBLUE was created in July 2016 with one mission: lead aviation into the digital age. Airbus and all subsidiaries, including NAVBLUE are proud to have been recognized as aGlobal Top Employer for 2025. Based on eight criteria: physical workplace, work atmosphere and social, health financial and family benefits, vacation and time off, employee communication, performance manager, training and skills development and community involvement. It was determined that we offer some of the most progressive and forward-thinking programs within the area. This achievement reflects our commitment to nurturing and empowering our people to reach their full potential. We’re grateful to our people, whose dedication and collaboration make this possible. NAVBLUE is based in Hersham (UK), Cardiff (UK), Toulouse (France), Waterloo, ON (Canada), Bangkok (Thailand) and Gdańsk (Poland) with other offices all around the world. The Future is Yours for the Taking: https: //youtu.be/vdY6gYuceYY","[{""min"": 15800, ""max"": 24000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15800,24000,Gross per month - Permanent
Full-time,Mid,B2B,Hybrid,177,Data & Reporting Senior Developer - banking 💥,ITDS,"Join us, and revolutionize how data powers financial decision-making! Kraków-based opportunity with the possibility to work 60% remotely. As a Data & Reporting Senior Developer, you will be working for our client, a global financial institution focused on enhancing data architecture and analytics to improve business performance. You will play a key role in delivering high-quality data insights and repository solutions aligned with industry standards. The role involves developing performance KPIs, designing data models, integrating data from multiple sources, and ensuring data security. You will work closely with stakeholders to understand business needs and create efficient, scalable reporting solutions using cutting-edge technologies. Your main responsibilities: Deliver assigned projects end-to-end, including data discovery, cleaning, transformation, and validation Develop and implement data models based on business requirements Perform matching and linking across datasets to ensure data consistency Securely transfer data from sources to target systems Utilize ETL and data tools for seamless data integration across databases and platforms Provide complex reporting solutions within Google Cloud and Microsoft SQL Server environments Develop and optimize SQL queries and scripts for data extraction and transformation Collaborate with stakeholders to understand business needs and process logic Identify and implement internal process improvement opportunities Document processes by creating roadmaps, descriptions, and technical documentation You're ideal for this role if you have: At least 3 years of experience in a data-related role (SQL development, data analysis, data engineering, or data architecture) Expertise in SQL programming and relational database systems Hands-on experience with Google Cloud Platform, especially BigQuery and Data Studio Knowledge of ETL principles and data warehousing Experience in developing business reports and front-end dashboards Proficiency in Python for data processing and automation Fluent English Proactive problem-solving approach and goal-oriented mindset Ability to identify risks and process blockers and propose solutions Experience in working with stakeholders and business partners to drive project success It is a strong plus if you have: Experience with additional cloud platforms and data integration tools Familiarity with machine learning concepts and their application in data analysis Background in financial services or banking sector Knowledge of regulatory requirements related to data governance Hands-on experience with visualization tools such as Tableau or Power BI We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #6476 You can report violations in accordance with ITDS’s Whistleblower Procedure available here .","[{""min"": 1100, ""max"": 1250, ""type"": ""Net per day - B2B""}]",Data Analysis & BI,1100,1250,Net per day - B2B
Full-time,Manager / C-level,Permanent or B2B,Remote,179,Data Engineering Team Leader,Profitroom,"We are currently looking for an experiencedData Engineering Team Leaderto join our Data Team and help us make our company even more data-driven. The results of your work will directly impact product development, the way we support our customers, and influence our high-level business strategy. If you are ready to take initiative and believe in data-driven decision-making – this role is perfect for you! Serve as a technical and team leader in the Data Engineering team: Lead team development and foster soft people management Support career paths and mentor team members Act as a Delivery Manager for data-related projects: Define, prioritize, and ensure timely delivery of engineering tasks Take ownership of major technical decisions and engineering excellence: Establish and enforce standards for testing, code review, CI/CD, documentation, and monitoring Oversee the architecture of our data platform: Maintain and evolve our Lakehouse infrastructure (preferably Databricks-based) Introduce new tools and technologies aligned with business and technical goals Supervise the logical structure and modeling of data: Ensure semantic and architectural consistency of datasets Leverage approaches such as the Kimball model or Medallion architecture Contribute to coding: Develop and optimize data pipelines using Dagster, Python, and PySpark Collaborate cross-functionally with data analysts, PMs, and other business stakeholders Drive and mature data governance practices, including cataloguing and lineage Minimum of 3+ years of experience as a Data Engineer or in a similar role related to data Experience in leading technical teams or managing data projects = at least 1 year (a big advantage) or willingness to grow into it Strong knowledge of Python, PySpark (nice to have), orchestration tools like Dagster or Airflow and SQL Experience working with cloud data platforms (Databricks experience is a plus) and Lake/Lakehouse architectures Ability to define and uphold high engineering standards and processes Proficiency in data modeling (Kimball, Medallion) Excellent communication skills (English at B2+ level) Strong ownership and self-organization Nice to have: Experience with Databricks and Delta Lake Familiarity with tools such as DataHub, Terraform, Docker Background in implementing data governance, lineage, and quality frameworks Python, PySpark, SQL, Databricks, GCP (BigQuery), Dagster, Airflow, Delta Lake, Docker, Terraform, DataHub Enjoy Work-Life Balance: Embrace a fully remote and flexible work environment. Explore the World: Avail annual 'Work with Us, Travel with Us' vouchers. Grow Your Skills: Access to English language classes along with a dedicated team development fund. Stay Healthy: Benefit from co-financed life and medical insurance, access sports facilities and receive professional mental health support whenever needed. Take Time Off: Get 26 days off with a Contract of Employment and 24 days off break with B2B contracts. Share hospitality: Take 2 extra days off (annually) for CSR activities. Join Celebrations: Participate in company retreats, events, and wedding & baby packs, benefit from our employee referral program. Transparent Culture: Experience a flat hierarchy and open communication channels for transparency. Contract Enhancements: earn between 25 500 PLN to 30 000 PLN on a B2B contract or between 21 000 to 25 000 PLN gross for Contract of Employment. About Us: We're a global leader in hospitality software, founded in Poznań, Poland in 2008. We’ve grown to serve over 3,500 customers across five continents, helping hotels and resorts maximize their revenue and guest satisfaction.","[{""min"": 25500, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,25500,30000,Net per month - B2B
Full-time,Mid,Permanent,Remote,181,Power BI Developer,EndySoft,"Position Overview: We are seeking a talented Power BI Developer to join our team. The ideal candidate will have expertise in developing and implementing Power BI dashboards, reports, and data visualizations to support data-driven decision-making. This role involves collaborating with various stakeholders to gather requirements and deliver interactive, high-quality BI solutions. MD rate: 16600-20000 PLN Roles and Responsibilities: Design, develop, and deploy Power BI dashboards and reports based on business requirements. Connect to various data sources, including SQL databases , Excel , and cloud services, to create comprehensive BI solutions. Develop and maintain Power BI datasets , dataflows , and queries to ensure efficient and accurate data models. Implement DAX calculations and measures to enable advanced data analysis and insights. Optimize Power BI reports for performance and usability, ensuring quick load times and responsive user experiences. Collaborate with stakeholders to gather requirements and translate them into actionable dashboards and reports. Perform data analysis and create meaningful visualizations to support business strategies and operational decisions. Conduct user training and provide ongoing support for Power BI tools and solutions. Stay updated with the latest Power BI features and best practices to continuously enhance BI solutions. Required Skills and Experience: Proficiency in Power BI development, including report building, data modeling, and visualization. Strong knowledge of DAX (Data Analysis Expressions) for creating complex calculations and measures. Experience with SQL for querying and transforming data. Familiarity with data integration tools and methods for connecting Power BI to various data sources. Strong understanding of data modeling concepts , including star schema and snowflake schema . Experience with Power BI Service for publishing and managing dashboards. Knowledge of data governance and security in Power BI, including row-level security (RLS). Strong analytical and problem-solving skills. Excellent communication and collaboration abilities. Nice to Have: Experience with Power Query for data transformation. Familiarity with Azure Data Services such as Azure Synapse , Azure Data Factory , or Azure SQL Database . Knowledge of Power BI Paginated Reports . Experience with Python or R for advanced analytics within Power BI. Understanding of big data technologies such as Databricks or Spark . Exposure to Agile/Scrum methodologies. Experience with version control tools like Git for Power BI development. Additional Information: This role offers an exciting opportunity to work on impactful BI projects and transform data into actionable insights. If you are passionate about leveraging Power BI to drive business success, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,16600,20000,Gross per month - Permanent
Full-time,Senior,B2B,Hybrid,182,Big Data Engineer,Link Group,"🚀 Big Data Engineer 📍Remote or Hybrid (EMEA preferred)| Full-time | AdTech Platform | Python/Java/Scala We’re looking for an experiencedBig Data Engineerto join our high-impact team building the backbone of a globaladvertising platformdelivering personalized content to millions of media-enabled devices. Your work will directly influencedata-driven decision making, real-time targeting, and analytics pipelinesfor one of the most advanced AdTech ecosystems in the industry. Build and maintainrobust, scalable data pipelinesto support attribution, targeting, and analytics Collaborate closely withData Scientists, Engineers, and Product Managers Design and implement efficient storage and retrieval layers for massive datasets Optimize data infrastructure andstreaming processing systems(e.g. Flink, Apache Ignite) Drive quality throughunit tests, integration tests, and code reviews Develop and maintainAirflow DAGsand other orchestration pipelines Translate business needs into robust, technical data solutions Lead or supportA/B testing and data-driven model validation Contribute toR&D initiativesaround cloud services and architecture 5+ years of experience in backend/data engineering usingPython,Java, orScala Strong experience withBig Data frameworks(Hadoop, Spark, MapReduce) Solid knowledge ofSQL/NoSQLtechnologies (e.g. Snowflake, PostgreSQL, DynamoDB) Hands-on withKubernetes,Airflow, andAWS(or similar cloud platforms) Stream processing experience: Flink, Ignite Experience withlarge-scale dataset processing and performance optimization Familiarity with modern software practices: Git, CI/CD, clean code, Design Patterns Fluent in English (B2+) Degree in Computer Science, Telecommunications, or related technical field Experience withGoLangorGraphQL Hands-on withmicroservicesorserverlesssolutions Experience incontainer technologies(Docker, Kubernetes) Previous work inAdTech, streaming media, or real-time data systems","[{""min"": 100, ""max"": 130, ""type"": ""Net per hour - B2B""}]",Data Engineering,100,130,Net per hour - B2B
Full-time,Mid,B2B,Remote,183,Business Intelligence Developer,EndySoft,"Position Overview: We are seeking a skilled Business Intelligence Developer to join our team. The ideal candidate will have a strong background in designing, developing, and maintaining BI solutions that help organizations make data-driven decisions. This role involves working with large datasets, building data pipelines, and creating insightful reports and dashboards to support business operations and strategy. MD rate: 16000 - 2 0000 PLN Roles and Responsibilities: Design, develop, and maintain BI solutions , including dashboards, reports, and data visualizations. Collaborate with stakeholders to gather requirements and translate them into technical BI solutions. Build and optimize ETL pipelines to ensure efficient data flow from various sources to data warehouses. Develop and maintain data models to support reporting and analytics needs. Create interactive dashboards and reports using tools like Power BI , Tableau , or QlikView . Perform data analysis to provide actionable insights and support decision-making processes. Ensure data accuracy and consistency through data validation and quality checks. Monitor and improve the performance of BI systems and applications. Stay updated with the latest BI tools, technologies, and best practices. Required Skills and Experience: Proficiency in SQL for querying and managing data in relational databases. Experience with ETL tools such as Informatica , Talend , or SSIS . Hands-on experience with BI tools like Power BI , Tableau , QlikView , or similar. Strong understanding of data modeling concepts, including star schema and snowflake schema . Familiarity with data warehousing technologies such as Snowflake , Redshift , or Azure Synapse . Experience with scripting languages like Python or R for data analysis and automation. Strong problem-solving skills and attention to detail. Good communication and collaboration abilities. Nice to Have: Experience with cloud platforms like AWS , Azure , or Google Cloud for BI solutions. Familiarity with big data technologies such as Spark , Hadoop , or Databricks . Knowledge of machine learning and predictive analytics . Experience with version control systems like Git . Exposure to Agile/Scrum development methodologies. Knowledge of performance tuning for BI applications and databases. Additional Information: This is an exciting opportunity to work on innovative BI solutions and contribute to data-driven decision-making. If you are passionate about transforming data into actionable insights and thrive in a fast-paced environment, we encourage you to apply.","[{""min"": 16000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,16000,20000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,184,"Senior Data Engineer, Finance Tech, SCM Tech Fulfilment (m/f/x)",HelloFresh,"HelloFresh Group, the world’s leading integrated food solutions provider, is expanding with a new R&D Tech office in Poland. With brands offering meal kits, ready-to-eat meals, and specialty products such as meat, seafood, and pet food, we are seeking individuals who are ready to make an impact from day one. Joining us in Wrocław means shaping the culture, working on meaningful R&D Tech projects, and contributing to a global company changing how people eat. At HelloFresh Group, we are driven by a high-performance culture that values speed, agility, and continuous learning. We believe in hands-on contribution and fostering a truly collaborative, egoless environment where every team member contributes to our mission ofchanging the way people eat, foreverand welcome team players who thrive in a dynamic environment, lead with ownership, and bring diverse perspectives to the table. Our teams thrive on in-person collaboration, with an expectation to work from the office four days a week. This approach creates a dynamic space where ideas flourish, decisions are made efficiently, and our collective impact is accelerated. It's how we stay closely connected to our shared goals and drive swift execution. HelloFresh's Supply Chain Management (SCM) is fundamental to our operations, overseeing supplier contracts, order placement, goods receipt, inventory control, and production processes. Our mission is to enhance efficiency and minimize waste across the supply chain, aspiring to build the world's leading, scalable, and fully-integrated food supply chain management platform. We are seeking an experienced and highly motivated Senior Data Engineer to join our Finance Tech Squad within the Supply Chain Management organization. This role is instrumental in the design, development, and maintenance of robust data services built upon our critical finance core systems, including Oracle Fusion Cloud ERP, Workday, and various internal applications. If you are not familiar with SCM, you can find more informationhere. Assume comprehensive ownership of the end-to-end data pipeline lifecycle, including architecture, design, development, deployment, and operational support. This will involve the application of DevOps principles, pair programming, and other cutting-edge methodologies. Act as a proactive, solution-oriented member within autonomous, cross-functional agile teams, collaborating extensively with Business Stakeholders in our global and local finance teams, Product Owners, Back-end Engineers and Business Intelligence teams. Cultivate and demonstrate an in-depth understanding of HelloFresh’s core product offerings and architectural landscape. Serve as an ambassador for software solutions, providing expert support and mentorship to colleagues. Leverage and contribute to state-of-the-art data technologies, including AWS services (EMR, Glue, S3), Kafka, PySpark, Kubernetes, Airflow, Prefect, Tecton, Databricks, Snowflake, and our proprietary in-house Data Pipeline tools. Software Engineering Proficiency: Demonstrated strong software engineering experience with the capability to design, implement, and deliver maintainable, high-quality code primarily in Python. Cloud & Data Processing Experience: Hands-on experience with prominent cloud computing platforms and distributed data processing technologies, including PySpark, Kubernetes. Kafka is a plus. Data Modeling Expertise: Proven expertise in data modeling principles, relational databases (e.g. PostgreSQL), and object storage solutions (e.g. AWS S3). Data Pipeline Development: Extensive experience in building, optimizing, and maintaining efficient and robust data pipelines. Data Quality Advocacy: A strong commitment to ensuring data quality assurance and implementing comprehensive data monitoring strategies. End-to-End Development Lifecycle: Proficiency across the entire software development lifecycle, including unit, integration, and functional testing, application tuning and profiling, and continuous integration. Agile Methodologies: Extensive experience working within agile methodologies, with a strong emphasis on delivery, lean principles, and rapid iteration. Continuous Learning: A proactive approach to continuous professional development and a willingness to learn from and contribute to a peer-learning environment. Collaboration & Mentorship: Excellent collaborative skills with the ability to effectively mentor team members and share practical knowledge and industry trends. Preferred: Familiarity with e-commerce and/or subscription-based business models is a significant advantage. Desirable: Experience with Snowflake. This role requires strong communication skills due to frequent interactions with diverse global teams, including back-end developers, front-end developers, product analysts, product managers, and business stakeholders. We are seeking highly capable problem-solvers who can apply their engineering expertise across a wide range of platforms and environments, while also serving as a coach and mentor to team members and stakeholders. Health- You’re covered from your first day with private health insurance Hybrid Working Schedule- We work in-office 4 days a week to align on goals, with flexible hours to support work-life balance and personal needs Holidays- You receive 26 days of paid vacation each year, providing you time to rest and recharge Learning and Development- An annual Learning & Development budget and a Mentoring Program to support your ongoing professional growth Employee Referral Program- Our team members can participate in our internal employee referral program and receive a bonus for recommending successful candidates to open roles Daily Comforts- Free coffee, drinks, and fresh fruit are available to keep you refreshed throughout the day If you are passionate about making a tangible impact and thrive in a fast-paced environment where your work directly contributes to a global purpose, we encourage you to apply – even if your experience doesn't tick every single box, we believe there are many ways to develop skills and grow with us.#Engineering","[{""min"": 14700, ""max"": 22100, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14700,22100,Gross per month - Permanent
Full-time,Senior,B2B,Remote,187,Big Data Developer,Altimetrik Poland,"2-3 day per week you need to be available until 9: 00 PM for meetings with the US team. Altimetrik Polandis a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are seeking a skilledSenior Data EngineerforAirbnb-our customer, an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. Together with them, we are building a world-class payments platform that moves billions of dollars, in 191 countries, with 75 currencies, through a complex ecosystem of payments partners. They are also reinventing how to serve users to improve performance, scalability and extensibility. Responsibilities: Proactively and effectively manages communications, ensuring all stakeholders are kept informed about project progress, changes, and updates. Debugs complex issues without assistance. Navigates ambiguous product requirements to build eloquent solutions. Manages multiple projects at once. Collaborates effectively with cross-functional teams such as Data Science, Product Engineering, Advanced Analytics, and other stakeholders to design and implement data solutions. And if you possess.. Solid understanding of Spark and ability to write, debug and optimize Spark code with Python. Strong knowledge of Python, and expertise with data processing technologies and query authoring (SQL). Nice to have: Expertise in data modeling, warehousing, and working with columnar databases (e.g., Redshift, BigQuery). Extensive experience designing, building, and operating robust distributed data platforms (e.g., Spark, Kafka, Flink) and handling data at the petabyte scale. … then we are looking for you! We work 100% remotely or from our hub inKraków. 🔥We grow fast. 🤓We learn a lot. 🤹We prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 25000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Engineering,25000,33000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,188,Senior Data Engineer,emagine Polska,"Project information: Industry: Insurance and IT services Rate: up to 180 zł/h netto + VAT Location: Warsaw (first 2-3 months of office visits once a week, then occasionally) Project language: Polish, English Summary: As aSenior Data Engineer, you will play a pivotal role in developing scalableData Hubsto support various applications, includinganalytical, reporting, operational,andGenerative AI. This position emphasizes key skills such asPythonandSQLprogramming, expertise incloud environmentslikeAzure, and proficiency inDatabricks and Spark. Your responsibilities will include defining architecture standards, mentoring team members, and ensuring the performance and compliance of data solutions. Responsibilities: Data Hub Architecture: Design scalable Data Hub systems for integrating both structured and unstructured data. Technical Leadership: Define practices and patterns for data engineering implementations. Mentoring & Knowledge Sharing: Provide guidance and support to fellow engineers. Data Pipeline Optimization: Create and enhance batch and real-time data workflows. Quality & Monitoring: Implement measures for data validation and anomaly detection. Automation & CI/CD: Streamline deployment processes through automation and DevOps practices. Collaboration & Strategy: Work with diverse teams to align data solutions with business goals. Security & Compliance: Ensure data governance and adherence to regulations. Documentation & Knowledge Management: Produce quality technical documentation for reference. Key Requirements: Programming Skills: Proficient inPython and SQL. Cloud Experience: Familiarity withAzure Data Factory, ADLS, Azure SQL, Synapse. Databricks & Spark: Extensive knowledge ofDatabricks and Apache Spark. Data Architecture: Expertise in designing enterprise-scale platforms. Streaming Data Processing: Experience with real-time data analysis tools. Automation & DevOps: Skills inCI/CD, Terraform, Kubernetes, Docker. Data Governance: Knowledge of data compliance practices. Leadership: Capability to guide teams effectively. Communication: Proficient in technical documentation and fluent in English (minimum B2). Nice to Have: Stream Analytics Skills: Experience with Azure Stream Analytics. Event Streaming Tools: Familiarity with Azure Event Hubs. Agile Methodologies: Experience working in Agile/Scrum environments.","[{""min"": 160, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,180,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,189,Data Engineer,ITDS,"Data Engineer Join us, and create cutting-edge pipelines for seamless data transformation! Kraków - based opportunity with hybrid work model (2 days/week in the office). As a Data Engineer, you will be working for our client, a global financial institution that is driving DevOps transformation through data analytics and engineering. You will be part of a team that provides key metrics and analytical products to enhance software engineering practices across the organization. Your role will focus on developing data transformation pipelines, ensuring data quality, and supporting a cloud data platform to improve the overall DevOps experience. You will collaborate with diverse global teams to deliver enriched datasets, dashboards, and insights that enable strategic decision-making. Your main responsibilities: Designing, developing, testing, and deploying data ingest, quality, refinement, and presentation pipelines Operating and iterating on a cloud data platform to support internal goals Building and maintaining ETL processes and data transformation pipelines Ensuring data quality and implementing automated data validation solutions Developing data marts and optimizing schema designs for performance and usability Collaborating with business stakeholders to understand data needs and deliver actionable insights Working with cloud-based big data technologies, particularly Google Cloud Platform (GCP) and BigQuery Utilizing orchestration and scheduling tools such as Airflow and Cloud Composer Supporting continuous integration and continuous delivery (CI/CD) processes Following Agile methodologies and working within a product-oriented culture You're ideal for this role if you have: At least 7 years of professional experience in SQL development Strong experience in data engineering and ETL processes Expertise in GCP, BigQuery, and data build tools (DBT) Hands-on experience with Apache Airflow and Cloud Composer Proficiency in data modeling and designing optimized data schemas Experience with data streaming technologies such as Kafka Familiarity with BI tools, especially Looker Studio Understanding of DevOps principles and working in a DevOps environment Experience with Continuous Integration and Continuous Delivery (CI/CD) practices Strong communication skills and ability to work with global teams It is a strong plus if you have: Experience in building and operating a cloud data platform Knowledge of data architecture and data marts Proficiency in Git, Shell scripting, and Python Ability to quickly learn and adapt to new technologies Experience collaborating with technical staff and project managers for efficient delivery Proactive approach to identifying improvement opportunities and solving issues Comfort in working in fast-paced, changing, and ambiguous environments","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,26000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,192,Senior Database Developer,1dea,"Do warszawskiego oddziału firmy z UK - FinTech, praca nad wewnętrznym produktem - szukamy Senior Database Developer'a. Zakres obowiązków: Tworzenie i rozwijanie nowych funkcjonalności oraz modyfikacja istniejących systemów bazodanowych. Optymalizacja i utrzymanie aplikacji wykorzystywanych przez klientów firmy. Wprowadzanie ulepszeń technicznych oraz administracja bazami danych (instalacja poprawek, aktualizacje). Wsparcie zespołów biznesowych i współpraca z Project Managerami. Rozwiązywanie incydentów oraz problemów związanych z bazami danych. Wymagania: Minimum 5 lat doświadczenia w pracy jako Database Developer, w tym min. 2 lata z Oracle PL/SQL. Wymagane doświadczeniem w migracji z Oracle do PostgreSQL Bardzo dobra znajomość SQL i PL/SQL oraz architektury baz danych Oracle. Doświadczenie w pracy z systemami kontroli wersji (np. SVN, Git). Znajomość metod debugowania i testowania jednostkowego kodu PL/SQL. Umiejętność pracy w zespole i dobra organizacja pracy. Dobra znajomość języka angielskiego B2/C1 (praca w międzynarodowym środowisku). Lokalizacja biura: Warszawa Śródmieście, 1 dzień/ tydzień w biurze Umowa: B2B, długoterminowe, bezpośrednie zatrudnienie Rekrutacja: 2 etapy online","[{""min"": 24000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Engineering,24000,33000,Net per month - B2B
Full-time,Mid,B2B,Remote,193,Data Engineer,Link Group,"About the Role We're looking for aData Engineerto join a growing team working on modern data platforms. You will play a key role in designing, developing, and maintaining scalable data pipelines and infrastructure to support data analytics and reporting initiatives. This is a great opportunity to work with cutting-edge cloud and big data technologies. Design, build, and maintain scalableETL/ELT pipelines Work with structured and unstructured data from diverse sources Optimize data workflows for performance, reliability, and cost Implement data quality checks and monitoring Collaborate with analysts, architects, and other engineers to support data needs Build data integrations with internal and third-party APIs Support cloud data infrastructure and automation 3+ years of experience as a Data Engineer or similar role Strong knowledge ofSQLand data modeling principles Experience withPythonorScalafor data processing Hands-on experience with cloud platforms (ideallyAWS, but Azure/GCP also valuable) Familiarity with tools likeApache Spark,Airflow,Kafka, or similar Experience withdata lakes,data warehouses, orlakehouse architectures Git and CI/CD workflows Strong problem-solving skills and ability to work independently Experience withSnowflake,Redshift, orBigQuery Familiarity withdbt,Terraform, or other IAC tools Background indata governanceorsecurity Experience with real-time data processing (e.g., Flink, Kinesis) Exposure toML pipelinesorMLOps","[{""min"": 90, ""max"": 105, ""type"": ""Net per hour - B2B""}]",Data Engineering,90,105,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,194,👉 Senior AWS Data Engineer (Future Opening),Xebia sp. z o.o.,"🟣 You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. 🟣 Your profile: 3+ years’ experience with AWS (Glue, Lambda, Redshift, RDS, S3), 5+ years’ experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools – Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, working knowledge of Git, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ 🟣 Nice to have: experience with Amazon EMR and Apache Hadoop, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification, 🟣 Recruitment Process: CV review – HR call – Interview (with Live-coding) – Client Interview (with Live-coding) – Hiring Manager Interview – Decision 🎁 Benefits 🎁 ✍ Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺 We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️ We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,22300,33700,Net per month - B2B
Full-time,Senior,B2B,Remote,196,Senior Administrator Baz Danych Oracle / Senior Oracle DBA,Simora,"Dołącz do naszego zespołu jako Senior Administrator Baz Danych Oracle! Poszukujemy osoby, która zajmie się zarządzaniem bazami danych naszych klientów. Jesteśmy firmą, która rozwija nowoczesne rozwiązania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzędzi wspierających naszych klientów. Cenimy pozytywną atmosferę, wzajemny szacunek i zaangażowanie – to fundamenty naszego zespołu. Twój zakres obowiązków Tworzenie baz danych oraz środowisk bazodanowych na środowisku produkcyjnym i testowym Migrowanie baz danych na nowe środowiska Zarządzania i aktualizacja baz danych i środowisk bazodanowych Konfigurowanie i optymalizacja środowiska bazodanowego Automatyzacja zadań za pomocą języków skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jakości i bezpieczeństwa dla tworzonych rozwiązań Wdrażanie rozwiązań opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Stałe doskonalenie sposobu Twojej pracy Nasze wymagania Min 5 letnie doświadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomość języka SQL i PLSQL Umiejętność dbania o szczegóły i jakość rozwiązań Komunikatywność Znajomość języka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykształcenie wyższe (preferowany kierunek: informatyka) Znajomość języków programowania: Python, Java itp. Znajomość systemów operacyjnych Linux / Windows Znajomość systemów wirtualizacyjnych np.: OLVM Oferujemy Ciekawą pracę w firmie o wysokiej dynamice rozwoju Możliwość podniesienia kwalifikacji w obszarach związanych z bazami danych, bezpieczeństwem danych oraz sztuczną inteligencją Benefity Karta Multisport Prywatna opieka zdrowotna – Medicover Praca zdalna Brak dress code’u Dofinansowanie szkoleń i kursów Elastyczny czas pracy","[{""min"": 10000, ""max"": 16000, ""type"": ""Net per month - B2B""}]",Database Administration,10000,16000,Net per month - B2B
Full-time,Mid,B2B,Remote,198,Data Engineer,Ework Group,"💻 Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. 🔹 For our Client we are looking forData Engineer🔹✔️Ideally, candidates should be available to work from Wrocław office (hybrid) ; however, remote work is also possible. ✔️Your responsibilities include: Develop and enhance application features that support internal users and business processes. Diagnose and resolve issues related to data quality and system maintenance. Ensure efficient and reliable operation of the organization’s data processing systems. Design and implement solutions using cloud platforms – preferably Microsoft Azure. Work hands-on with Databricks (primarily PySpark) for big data processing and analysis. Manage and optimize databases such as SQL Server or Netezza. Implement and integrate solutions using services such as: Azure Data Factory, Databricks, PySpark, Python. Work in a diverse and rapidly changing landscape of data sources. Collaborate within an Agile/Scrum team environment, following modern software development practices. Take initiative and ownership of tasks while maintaining accuracy and accountability for results. Communicate and collaborate effectively in an international environment using English. Contribute to a knowledge-sharing and team-oriented culture. ✔️Your experience and background: You are graduated with a Master's degree in Computer Science, Data Engineering or any related field Data & Engineering has been your world for at least 3-5 years Cloud service platform expertise; preferably Azure Hands-on experience working with Databricks (PySpark) Databases (like SQL Server or Netezza) don’t have any secret for you Knowledge of at least three of these services: Azure Data Factory, Azure Synapse, Databricks, Azure SQL Database, Power BI, Spark, Python, Mongo DB, Azure Functions You are comfortable working in a diverse, complex and fast-changing landscape of data sources You care about agile software processes, data-driven development, reliability, and responsible experimentation. Capacity to work in an international environment using English Team player - having a sharing and collaborative mindset Autonomous, Rigorous, takes ownership at work Agile - flexibility to execute available tasks in a rapidly changing environment ✔️Soft Skills: Excellent communication skills Ability to understand business stakeholders and the business landscape Ability to challenge decisions made by the platform or other architects Strong analytical, organizational, problem-solving, and time-management skills Comfortable working in a diverse, complex, and fast-changing landscape of data sources Proactive problem solver with innovative thinking and a strong team player Fluent in English (C1) ✔️ We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events Contact person: karolina.rosikiewicz@eworkgroup.com(","[{""min"": 115, ""max"": 128, ""type"": ""Net per hour - B2B""}]",Data Engineering,115,128,Net per hour - B2B
Full-time,Mid,B2B,Remote,200,BI Developer,SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: projektowanie, budowa i utrzymanie kompleksowych przepływów danych oraz procesów wzbogacania danych, współpraca z Product Managerami, analitykami biznesowymi, Data Scientistami oraz interesariuszami na różnych poziomach organizacji w celu przekształcania wymagań biznesowych w konkretne rozwiązania BI, projektowanie i rozwój hurtowni danych oraz struktur analitycznych (data marts) wspierających raportowanie i analizę danych, tworzenie i utrzymywanie dashboardów w Power BI oraz rozwój modeli semantycznych, zarządzanie transformacjami danych w ramach procesów ELT przy użyciu DBT, utrzymanie wysokiej wydajności i skalowalności rozwiązań BI, wsparcie i szkolenie użytkowników końcowych w zakresie korzystania z systemów BI, praca zdalna w zespole międzynarodowym (+1h różnicy względem polskiej strefy czasowej), stawka do 130 zł/h przy B2B w zależności od doświadczenia. Aktualnie posiadamy dwa osobne wakaty, na które aktualnie rekrutujemy!1) Ta oferta jest dla Ciebie, jeśli: posiadasz minimum 3 lata doświadczenia na podobnym stanowisku, masz bardzo dobrą znajomość SQL posiadasz praktyczne doświadczenie z Lookerem, w tym rozwój LookML, tworzenie dashboardów oraz administracja Lookerem i zarządzanie uprawnieniami, posiadasz doświadczenie z DBT w kontekście transformacji danych w pipeline'ach ELT, znajomość angielskiego na poziomie min. B2, posiadasz znajomość Snowflake, lub innych nowoczesnych baz danych analitycznych, 2) Ta oferta jest dla Ciebie, jeśli: posiadasz minimum 4 lata doświadczenia na podobnym stanowisku, posiadasz bardzo dobrą znajomość SQL oraz modelowania danych, posiadasz doświadczenie w pracy z Power BI (dashboardy, modele danych, DAX), posiadasz praktyczną znajomość DBT i architektury ELT, posiadasz znajomość hurtowni danych - szczególnie Snowflake i/lub BigQuery, posiadasz doświadczenie w pracy z systemami. znajomość angielskiego na poziomie min. B2, mile widziane doświadczenie z Lookerem oraz znajomość platform chmurowych (Azure, AWS, GCP). Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 18480, ""max"": 21840, ""type"": ""Net per month - B2B""}]",Data Engineering,18480,21840,Net per month - B2B
Full-time,Manager / C-level,Permanent,Hybrid,206,"Engineering Manager, Data Infrastructure",Asana,"We are looking for an Engineering Manager who can provide technical leadership for our data platform and effectively support engineers with large-scope responsibilities. You will be responsible for the delivery and technical quality of our programs, taking on complex multi-year projects, and leading the work of several ICs. You’ll also partner with cross-functional stakeholders in Infrastructure, Data Science, and Business teams to drive initiatives forward, and mentor team members in technical design, project leadership, and running team processes. The Data Infrastructure team builds the infrastructure responsible for moving product and business data in a secure, efficient and timely manner to Data consumers. We operate several systems ranging from data ingestion pipelines, Spark processors, data/ML tooling, experimentation platform, and logging pipelines for Asana’s Data Lake. This role is based in our Warsaw office with an office-centric hybrid schedule - in-office days are Monday, Tuesday, and Thursday. We offer a Contract of Employment (UoP) for our employees in Poland. What you’ll achieve: Support and coach engineers in their technical and professional development. Partner with cross-functional stakeholders to guide the team’s roadmap and prioritization. Collaborate with data engineering and with other infrastructure teams on problems related to data infrastructure service models, operations, deployment and security. Evangelize good code and solid engineering and operability practices. Co-create and follow best practices of running data applications in the cloud (AWS) and all the state-of-the-art tooling around it. Bootstrap our Data Infra presence in Warsaw! Some upcoming technical challenges you may work on: Build cost tracking/observability systems for data use cases; Help improve the Databricks experience for Asana users; Improve deployment and release automation for data pipelines; and Implement best practices for security and privacy of our data infrastructure. About you: Prior experience building and operating cloud infrastructure (preferably involving analytical/real-time data or storage) and distributed systems at scale. Familiarity with distributed data processing systems (e.g. Spark, Databricks, Snowflake or similar systems). Expertise in AWS services (e.g. IAM, S3) and with infrastructure-as-code tooling (e.g. Terraform). 2-4 years building/leading engineering teams as a manager. Planning/execution as well as career and impact growth for your reports. Experience collaborating with other engineering teams, PMs, and cross-functional partners on alignment and project execution, and with teams in other timezones. Focus on maximizing impact, for yourself and your team. Ability to provide valuable input to any technical or product discussion. Oriented around the multi-year consequences of your decisions. What we offer: Generous, transparent and fair compensation system (base salary and generous Restricted Stock Unit for Asana Inc.). Contract of Employment (with 50% tax deductible costs for author’s rights usage for Engineers). Health insurance with dental and travel coverage (Lux Med). Lunch catering on the days that you work from the office. Career growth budget. Home office setup budget. Gym/Fitness card. Fertility healthcare and family-forming support with Carrot . Mental health support in Modern Health. Group life insurance. MacBooks with all necessary accessories. For this role, the estimated base salary range is between 39 500 - 50 400 PLN gross monthly on the contract of employment (UoP). The actual base salary will vary based on various factors and individual qualifications objectively assessed during the interview process. The listed range above is a guideline, and the base compensation range for this role may be modified. Our total compensation consists of base salary and equity (RSUs). About us Asana helps teams orchestrate their work, from small projects to strategic initiatives. Millions of teams around the world rely on Asana to achieve their most important goals, faster. Asana has been named a Top 10 Best Workplace for 5 years in a row, is Fortune's #1 Best Workplace in the Bay Area, and one of Glassdoor’s and Inc.’s Best Places to Work. After spending more than a year physically distanced, Team Asana is safely and mindfully returning to in-person collaboration, incorporating flexibility that adds hybrid elements to our office-centric culture . With 11+ offices all over the world, we are always looking for individuals who care about building technology that drives positive change in the world and a culture where everyone feels that they belong. We believe in supporting people to do their best work and thrive, and building a diverse, equitable, and inclusive company is core to our mission. Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We provide equal employment opportunities to all applicants without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by law. We also comply with the San Francisco Fair Chance Ordinance and similar laws in other locations. Our comprehensive compensation package plays a big part in how we recognize you for the impact you have on our path to achieving our mission. We believe that compensation should be reflective of the value you create relative to the market value of your role. To ensure pay is fair and not impacted by biases, we're committed to looking at market value which is why we check ourselves and conduct a yearly pay equity audit.","[{""min"": 35000, ""max"": 45000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,35000,45000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,209,Data Engineer with Data Vault,emagine Polska,"PROJECT INFORMATION: Industry: FinTech Client: company from Sweden Remote work: 100% remote Consultant’s location: Poland Project language: English Business trips: None Project length: 6 months contracts + prolongations (we’re looking for long-term) Start: ASAP / Flexible Assignment type: B2B Stack in the order of importance on the project: Data Vault, SQL, Python, dbt. Nice to have: AWS, Snowflake Summary: The Data Engineer role is central to enhancing data infrastructure as part of a team committed to innovative financial solutions. The position aims to drive data innovation and improve data management practices to support various business initiatives. Responsibilities: Develop and maintain scalable Data pipelines using tools like Matillion and SQL. Collaborate with data infrastructure and Data teams to design optimal solutions for data ingestion and reporting. Seamlessly ingest data into Snowflake to enable advanced analytics and reporting. Serve as a technical expert, supporting business users in leveraging data effectively. Ensure data integrity, security, and governance across systems and workflows. Document and communicate technical solutions to ensure clarity across the organization. Must Haves: Strong SQL skills and a solid understanding of cloud-based data platforms. Experience with dbt (ideally with Data Vault and Snowfake). Hands-on experience with Data Vault modeling. Knowledge of best practices in data warehousing and data modeling. Excellent communication and collaboration skills to work cross-functionally. Ability to work independently or as part of a team to deliver high-quality results. Experience with Python. Previous work experience in finance or fintech industries - payment cards, loans. Nice to have: Experience with Matillion. Experience with Snowflake (or with Databricks). Experience with AWS. We offer: Long-term cooperation. Transparently built relations based on trust and fair play. Co-financed benefits: Medicover card, Multisport card.","[{""min"": 213, ""max"": 256, ""type"": ""Net per day - B2B""}]",Data Architecture,213,256,Net per day - B2B
Full-time,Mid,B2B,Remote,210,Big Data Developer,B2Bnetwork,"KOMPETENCJE OBLIGATORYJNE: Doświadczenie zawodowe na stanowisku związanym z przetwarzaniem dużych zbiorów danych jako programista, minimum 2 lata Doświadczenie projektowe w przetwarzaniu dużych zbiorów danych, minimum 1 projekt Doświadczenie projektowe w programowaniu w języku Python, minimum 1 projekt Doświadczenie projektowe w środowisku obliczeniowym on-premise, minimum 1 projekt Doświadczenie w programowaniu w środowisku Apache Spark Doświadczenie w programowaniu w Python Doświadczenie w programowaniu w Apache Airflow Doświadczenie w programowaniu w SQL Znajomość zagadnień Hadoop Programowanie procesów ELT/ETL Znajomość zagadnień związanych z procesami CI/CD Umiejętność korzystania z systemu kontroli wersji (Git) KOMPETENCJE DODATKOWE: Wykształcenie wyższe Certyfikat z obszaru zarządzania projektem metodą zwinną (np. Agile PM lub równoważny) Certyfikat potwierdzający znajomość Apache Airflow (np. Airflow Fundamentals lub równoważny) Certyfikat potwierdzający umiejętność tworzenia DAGów Airflow (np. Dag Authoring lub równoważny) Certyfikat potwierdzający znajomość Apache Spark (np. Spark Developer Associate lub równoważny) Certyfikat potwierdzający znajomość SQL (np. W3Schools SQL Certificate lub równoważny) WYMAGANIA TECHNICZNE: Apache Spark Python Apache Airflow SQL Hadoop Git","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,110,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,211,Senior Data Engineer – Graph & Analytics Systems,Holisticon Insight,"Holisticon Insightis a division ofhttp: //nexergroup.comfocused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! 👇https: //holisticon.pl/holisticon-insight/ We offer a B2B Contract: 165 – 195 PLN net/hour + VAT We're seeking an experiencedSenior Data Engineerto lead work on our client's analytics platform. You'll assist in evaluating modern database technologies and designing migration strategies that enable advanced relationship analysis and improve client's ability to track metrics across complex interconnected systems. Responsibilities Design data models to represent events, entity relationships, and patterns Develop ETL pipelines to transform data structures Optimize query performance for complex relationship traversals Integrate new database solutions with existing analytics workflows Create proof-of-concept demonstrations for key use cases Help assessing current architecture and designing database migration strategy Evaluate and recommend appropriate database technologies, with focus on graph databases (Neo4j, Amazon Neptune, ArangoDB, etc.) Key Qualifications 5+ years hands-on experiencein similar role Strongunderstanding of data modeling and query optimization Experience migrating between different database paradigms (document, relational, graph) Proficiency with distributed data processing frameworks (Apache Spark preferred) Experience working with document databases (MongoDB) and JSON data structures Experience with large-scale analytics platforms Nice to have: Experience with graph databases and graph query languages(Cypher, Gremlin) Knowledge ofevent-driven data architecturesand batch processing pipelines Familiarity with visualization platforms(Grafana or similar) Experience with both batch and stream processing patterns Ability to evaluate trade-offs between different database technologies Strong communication skills to explain complex concepts to stakeholders Experience with hybrid architectures (combining multiple database types) Understanding of metrics-driven analysis and KPI tracking By joining Holisticon Insight you will get: Life insurance Multisport card Fully remote job Private medical care Flexible working hours B2B or contract of employment Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories)","[{""min"": 27720, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,27720,32000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,212,Technical Lead Erlang Engineer,Spyrosoft,"Join our team in Warsaw , where we’re collaborating on a cutting-edge fintech venture with a global industry leader. Together with our Partner – Klarna , we’re building an IT hub designed to drive innovation in digital payment solutions . We’re on the lookout for top-tier engineers who thrive in dynamic, forward-thinking environments. Spyrosoft is leading the recruitment process, facilitating a seamless experience for candidates who are ready to shape the future of online shopping and payments. This opportunity is ideal for engineers who value independence, proactiveness, and flexibility. Our engagement begins with a B2B contract through Spyrosoft , transitioning to a direct contract with our Partner . We offer a hybrid work model in Warsaw’s vibrant Wola district. English fluency and eligibility to work in Poland are essential, as is the successful completion of a background check to meet the rigorous standards of the financial domain. Our process: CV selection Initial recruitment screening Technical interview Online logic test Cultural fit interview Project Description: You will be part of a project focused on building a robust, scalable system with high availability, designed to support complex data flows and transaction processing. The system leverages Erlang/OTP for fault-tolerant, concurrent applications and operates within a modern AWS-based cloud environment. Key components include PostgreSQL for data storage and Kafka for message streaming. The system also integrates payment functionalities and aims for high performance and reliability in financial transaction processing. Tech Stack: Erlang + OTP AWS PostgreSQL Apache Kafka Elixir (nice to have) Python (nice to have) Requirements: Minimum 1 year of relevant experience. Hands-on experience with Erlang/OTP for system development. Strong understanding of concurrent programming and building fault-tolerant systems. Proven track record in working with AWS services, architecture, deployment, management, and scaling of cloud applications. Solid experience in managing Postgres databases and leveraging Kafka for real-time data processing and integration. Familiarity with Elixir is a plus, especially with a willingness to deepen Erlang expertise. Experience with the design and development of scalable, reliable payment and collection systems is a significant advantage. Deep understanding of cloud infrastructure, especially AWS tooling. Basic knowledge of Python is beneficial to broaden the tech stack and support versatile development tasks. Strong analytical and problem-solving skills. Ability to challenge the status quo and drive continuous improvement. Excellent verbal and written communication skills, enabling effective collaboration across teams and stakeholders. Ability to mentor and support peers while contributing to team success. A degree in Computer Science, Information Technology, or a related field. Fluency in English, both written and spoken. Main Responsibilities: Design, develop, and maintain backend systems using Erlang/OTP. Implement and optimize cloud infrastructure using AWS services. Integrate and manage PostgreSQL and Kafka in a production environment. Collaborate with cross-functional teams to understand requirements and deliver scalable solutions. Participate in architectural decisions and propose improvements to enhance system performance and reliability. Support the development of payment and collection modules with a focus on reliability and scalability. Mentor junior developers and support knowledge sharing within the team. Ensure best practices in code quality, testing, and documentation. Communicate effectively with internal teams and external stakeholders to align technical goals.","[{""min"": 220, ""max"": 255, ""type"": ""Net per hour - B2B""}]",Data Engineering,220,255,Net per hour - B2B
Full-time,Senior,B2B,Remote,214,Data Engineer ETRM domain,INFOPLUS TECHNOLOGIES,"Job Description We are seeking a skilled ETRM Data Engineer to join our data engineering team and support critical initiatives in the Energy Trading and Risk Management (ETRM) domain. This role involves building robust data pipelines, integrating trading systems, and ensuring data quality across platforms such as Azure Data Factory , Databricks , and Snowflake . You will collaborate closely with traders , analysts , and IT teams to design and implement scalable, high-performance data solutions that power decision-making in fast-paced trading environments. Key Responsibilities Design, develop, and maintain scalable data pipelines and ETRM systems Lead data integration projects within the energy trading ecosystem Integrate data from ETRM platforms such as Allegro , RightAngle , and Endur Build and optimize data storage solutions using Data Lake and Snowflake Develop and orchestrate ETL/ELT workflows using Azure Data Factory and Databricks Write efficient, production-grade Python / PySpark code for data processing and analytics Build and expose APIs using FastAPI for data services Ensure data quality, consistency, and reliability across complex systems Work closely with stakeholders to translate business requirements into technical data solutions Optimize and enhance data architecture for scalability and performance Mandatory Skills Strong experience with Azure Data Factory (ADF) Proficient in Data Lake architecture and best practices Hands-on expertise with Snowflake and SQL Solid experience in Python and PySpark Knowledge of FastAPI for building scalable APIs Proven work with Databricks in production environments Nice to Have Domain experience in ETRM / energy trading systems Familiarity with Streamlit for internal dashboards Experience integrating with Allegro , RightAngle , or Endur trading platforms","[{""min"": 32000, ""max"": 36800, ""type"": ""Net per month - B2B""}]",Data Engineering,32000,36800,Net per month - B2B
Full-time,Mid,B2B,Remote,215,Big Data Developer,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozwój i utrzymanie systemów informatycznych wspierających funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 2-letnie doświadczenie na stanowisku związanym z przetwarzaniem dużych zbiorów danych jako programista Doświadczenie projektowe w przetwarzaniu dużych zbiorów danych Doświadczenie projektowe w programowaniu w języku Python Doświadczenie projektowe w środowisku obliczeniowym on-premise Doświadczenie w programowaniu w środowisku Apache Spark Doświadczenie w programowaniu w Apache Airflow Doświadczenie w programowaniu w SQL Znajomość zagadnień Hadoop Doświadczenie w programowaniu procesów ELT/ETL Znajomość zagadnień związanych z procesami CI/CD Umiejętność korzystania z systemu kontroli wersji (Git) Dobra organizacja pracy własnej, orientacja na realizacje celów Umiejętności interpersonalne i organizacyjne, planowanie Komunikatywność, kreatywność, samodzielność, kultura osobista i dociekliwość Zdolność adaptacji i elastyczność, otwartość na stały rozwój i gotowość uczenia się Mile widziane Doświadczenie projektowe w obszarze ochrony zdrowia Doświadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarządzania projektem metodą zwinną (np.. Agile PM lub równoważny) Certyfikat potwierdzający znajomość Apache Airflow (np. Airflow Fundamentals lub równoważny) Certyfikat potwierdzający umiejętność tworzenia DAGów Airflow (np. Dag Authoring lub równoważny) Certyfikat potwierdzający znajomość Apache Spark (np. Spark Developer Associate lub równoważny) Certyfikat potwierdzający znajomość SQL (np.. W3Schools SQL Certificate lub równoważny) Kluczowe zadania Projektowanie, implementacja i utrzymanie rozwiązań do przetwarzania dużych zbiorów danych z wykorzystaniem języka Python oraz SQL Realizacja projektów w środowiskach obliczeniowych on-premise z wykorzystaniem Apache Spark i Apache Airflow Budowa i rozwój procesów integracji danych w modelu ETL/ELT Przetwarzanie danych w środowiskach opartych o technologię Hadoop Tworzenie i utrzymywanie wydajnych pipeline’ów danych oraz automatyzacja zadań przetwarzania danych Wdrażanie rozwiązań zgodnych z praktykami CI/CD oraz praca z systemem kontroli wersji Git Współpraca z zespołami projektowymi w celu realizacji celów biznesowych związanych z analizą i przetwarzaniem danych Planowanie i organizacja własnej pracy w sposób umożliwiający realizację zadań zgodnie z harmonogramem Aktywne rozwiązywanie problemów, analiza danych oraz usprawnianie istniejących procesów Ciągłe poszerzanie wiedzy technicznej i gotowość do nauki nowych technologii oraz narzędzi","[{""min"": 90, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,90,140,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,216,IPC Developer,ITDS,"Drive Innovation in Data Warehousing: IPC Developer wanted! Warsaw based opportunity with remote work model (2 days in the office/month). As anIPC Developer, you will be working for our client, a leading player in the online banking sector, on the development and optimization of data warehouse solutions. This includes designing and implementing ETL processes, enhancing data architecture, and supporting the business intelligence environment to ensure effective data reporting and analysis. The project involves using cutting-edge technologies like Informatica Power Center and Oracle-based systems to support complex financial systems. You’ll be part of a dynamic IT team that ensures data integrity, performance, and scalability of large-scale data systems. Your main responsibilities: Design and implement ETL processes using Informatica Power Center Develop and maintain data warehouses and reporting data marts Participate in the implementation and integration of Informatica tools Design, implement and develop BI-class analytical environments Create and maintain Oracle databases and applications Define and document technical specifications and administrative documentation Test and validate software solutions created by others Prepare software installation packages Support the software release and handover to production teams You're ideal for this role if you have: Strong knowledge of Informatica Power Center for ETL process development Solid understanding of RDBMS Oracle 9i/10g and database design Excellent command of SQL and good knowledge of PL/SQL Understanding of information systems engineering and development methodologies At least 6 months of experience designing and implementing ETL solutions At least 1 year of experience with Oracle-based systems in information environments Practical experience designing data warehouses for large-scale institutions Ability to read and write technical documentation in English Strong analytical thinking and problem-solving skills Team-oriented mindset with attention to quality and detail Nice to have: Theoretical knowledge and practical experience with Big Data, Python, and Spark Familiarity with Business Intelligence (BI) concepts Ability to work using Agile methodologies (Scrum, Kanban) Knowledge of tools such as SQL Developer, SVN, GitHub, JIRA, and Confluence We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere. Ref. number 7273","[{""min"": 16800, ""max"": 21500, ""type"": ""Net per month - B2B""}]",Data Engineering,16800,21500,Net per month - B2B
Full-time,Mid,B2B,Office,217,Data Analyst - Monopoly World,Reality Games,"Monopoly World is going global, and we’re looking for a talented Data Analyst to help us turn data into actionable insights that will shape the future of gaming. This is your chance to be part of an exciting, high-impact project that will reach players worldwide. This role will report to the Head o f Analytics. This is a full-time, in-office position based in the iconic railway station building in Krakow. If you have a knack for solving complex problems, love working with data, and want to be part of a team that’s pushing the boundaries of mobile gaming, this is the opportunity for you. Join us in creating something truly innovative and make your mark on one of the most iconic games of all time. Come build something incredible with us! Check out our trailer and see what’s in store! 🚀🎥 Key Responsibilities: Analyze and model data to provide actionable insights for game design and monetization strategies Develop and maintain data pipelines, and work with large datasets to derive meaningful patterns Collaborate with game designers and engineers to create data-driven solutions Perform web scraping and interact with REST APIs to collect and consume data Work with BigQuery SQL to query and manipulate game data for analysis Design and run simulations to support the development of Free-to-Play mobile games Continuously monitor key performance indicators and suggest improvements based on data analysis Job requirements Proficiency in at least one programming language, such as Python, JavaScript, Java, R, or Scala Experience with web scraping or consuming data from REST APIs Experience with BigQuery SQL Strong analytical thinking skills and the ability to interpret complex data At least 2 years of experience in data modeling and simulations for Free-to-Play mobile games Very good command of English Why Join Us? Leader’s support – Get mentorship, feedback, and career growth guidance Knowledge sharing – We invest in employee development Fast-paced career – Clear growth opportunities with performance reviews Top-notch equipment – Get the right tools for your job Beautiful office – Spacious, creative workspace in the city center Flexible hours – Whether you prefer early mornings or late evenings, we accommodate your schedule Open kitchen – Enjoy coffee, juices, fruits, a stocked fridge, and more Team gatherings – Connect with colleagues over pizza, games, and great company If you are passionate about data analytics and eager to apply your expertise to a groundbreaking project, we want to hear from you. Join us and contribute to an exciting and fast-growing industry. Apply now!","[{""min"": 90, ""max"": 110, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,90,110,Net per hour - B2B
Full-time,Mid,B2B,Remote,218,Data Platform Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients’ systems, departments and sites. We provide an open technology platform that’s shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience.Want to be part of it? Job Title: Data Platform Engineer Job Summary: We are seeking a highly skilled and experienced Data Engineer to join our team. The successful candidate will be responsible for developing and maintaining our Data Lake and existing Data Pipelines, as well as continually exploring, analyzing, and proposing improvements to existing processes and tooling. They will also be responsible for ensuring best practices are being adopted and staying up to date with the latest research and trends in Data Engineering. Skills Required: Strong background in Data Engineering, with experience in developing and maintaining Data Pipelines and Data Lakes Proven record of accomplishment of staying up to date with the latest research and best practices in Data Engineering Excellent technical skills in Data Engineering tools and technologies Advanced proficiency in Python and SQL Understanding of AWS cloud technologies, including infrastructure as code (CDK preferred) Effective communication and interpersonal skills, with the ability to work effectively with stakeholders at all levels Strong understanding of information security and data protection principles Experience in driving technical and career development, creating appropriate goals and seeking learning opportunities within the company and the wider software community Good understanding and prior experience of the Agile process (Scrum or Kanban) Fluency with software design patterns Experience working with automotive retail technology would be a distinct advantage Key Responsibilities: Maintain and develop the Data Lake and existing Data Pipelines to support the product and data teams’ requirements Continuously explore, analyze, and propose improvements to existing processes and tooling Stay up to date with the latest research, trends and best practices in Data Engineering Support the Business Intelligence team and wider Company in querying centralized data stores, including the Data Lake Work within department to maintain an ongoing understanding of the company’s data strategy and roadmap Proactively report on issues and problems Work independently, manage day-to-day workload and priorities, and take accountability for direction and output Drive your own technical and career development, create appropriate goals, and seek learning opportunities within the company and the wider software community Support colleagues on calls or in meetings with clients, partners, and suppliers as required Maintain systems under the team’s control, including user and access management Support colleagues and HR with onboarding as well as offboarding processes Ensure information security, data protection and support the business in complying with any legal obligations imposed upon it through positive actions Technologies: Python SQL: Trino, Spark-SQL, Hive, TSQL AWS Cloud services (including: s3, step functions, glue, CDK) Terraform Linux Windows Why join us? We’re on a journey to become market leaders in our space – and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We’re committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles – not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn’t require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply.","[{""min"": 18000, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Engineering,18000,23100,Net per month - B2B
Full-time,Senior,B2B,Remote,221,Data Modeller,SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: uczestnictwo w projekcie z branży finansowej/windykacyjnej, którego celem jest rozbudowa platformy danych i transformacja procesów danych w organizacji. Będziesz mieć realny wpływ na rozwój nowoczesnej platformy danych , wspierającej analitykę i zarządzanie danymi w całej organizacji, wykorzystywany stos technologiczny w projekcie: Azure (ADF, Databricks), SQL, dbdiagram.io, Azure Purview , modelowanie danych w podejściu Domain-Driven Design (DDD) – od warstwy logicznej po fizyczną, tworzenie i rozwój modeli danych hurtowni (DWH) w chmurze, opracowanie i dokumentowanie kontraktów danych – definiowanie wymagań danych wobec systemów źródłowych, praca z danymi z różnych źródeł: bazy danych (querying), API, event streaming, projektowanie i implementacja procesów ELT w środowisku Azure / Databricks (Bronze/Silver/Gold), współpraca z zespołami domenowymi, właścicielami procesów, architektami i źródłowymi zespołami IT, udział w budowie spójnego glosariusza danych oraz integracji z Data Governance i narzędziami klasy Purview, praca 100% remote, stawka do 220zł/h NET + VAT Ta oferta jest dla Ciebie, jeśli: posiadasz min. 4–5 lat doświadczenia w obszarze modelowania danych / DWH / Data Engineeringu, masz praktyczną znajomość podejścia DDD oraz umiejętność tworzenia logicznych i fizycznych modeli danych, znasz rozwiązania chmurowe – Azure (ADF, Databricks) lub pokrewne (GCP, AWS), bardzo dobrze znasz SQL i masz doświadczenie w procesach ELT/ETL, potrafisz tworzyć dokumentację: glosariusze danych, specyfikacje techniczne, wymagania kontraktowe, masz doświadczenie w pracy z danymi z systemów źródłowych i ich mapowaniu do struktur docelowych, znasz zasady Data Governance, Data Lineage i Data Quality, sprawnie komunikujesz się w j. angielskim (min. B2), dodatkowo mile widziane: znajomość Azure Purview, dbdiagram.io, Power BI, SSIS, Python/PySpark. Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 33600, ""max"": 36960, ""type"": ""Net per month - B2B""}]",Data Science,33600,36960,Net per month - B2B
Full-time,Junior,B2B,Remote,224,Junior Oracle SQL/APEX Developer,Pretius,"W Pretius poszukujemyJuniorOracle SQL/APEX Developerado projektunowegosystemu dla klienta z branży usług doradczych i inżynierskich. Lokalizacja: zdalnie lub Warszawa Wynagrodzenie: 40-60 pln netto/h O projekcie: System zarządzania aktywami (asset management) w przestrzeni publicznej oparty o rozwiązania goespatial Moduły planowania, budżetowania inspekcji i inwentaryzacji Wykorzystywany w ponad 120 miastach Rozwój i utrzymanie aktualnego systemu w celu ekspansji nowego rozwiązania Prace nad nowymi funkcjonalnościami Stack: Oracle APEX, Oracle Cloud, Azure Oczekiwania: Podstawowa znajomość Oracle APEX Język angielski na poziomie B2/C1 Znajomość baz danych i SQL Co oferujemy w Pretius? Stawiamy na długofalowe relacje oparte na uczciwych zasadach i rzetelności Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Możliwość pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnętrzne, konferencje, certyfikacje","[{""min"": 40, ""max"": 60, ""type"": ""Net per hour - B2B""}]",Database Administration,40,60,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,226,Lead Azure Data Engineer - Cybersecurity ☁️💥,ITDS,"Join us, and enhance data analytics capabilities for a secure digital future! Kraków - based opportunity with hybrid work model (6 days/month in the office) As a Lead Data Engineer , you will be working for our client, a global leader in cybersecurity and data analytics. The Client’s team focuses on advancing cybersecurity capabilities through innovative data engineering and analytics solutions. You will be contributing to the development and enhancement of their data lake and analytics platform, supporting cutting-edge technologies and cloud infrastructures. Your role will involve collaborating with data engineers and cybersecurity professionals to build scalable data pipelines and ensure the continuous growth of security analytics systems. Your main responsibilities: Designing and implementing data pipelines for cybersecurity use cases Ingesting and provisioning raw datasets and curated data assets Driving improvements in the reliability and frequency of data ingestion Supporting and enhancing data ingestion infrastructure and pipelines Automating and optimizing data engineering workflows Identifying and onboarding new data sources for cybersecurity use Conducting exploratory data analysis for new schemas Collaborating with platform engineers for cloud infrastructure and platform engineering Ensuring the operational efficiency of production data pipelines Monitoring and enhancing cloud-based data transport and data cleaning processes You're ideal for this role if you have: 5+ years of experience in data engineering or cloud infrastructure engineering Proficiency in programming languages like Python, Java, or C# Experience with cloud technologies, especially Azure (Azure Data Factory, Azure Databricks, etc.) Strong understanding of data engineering principles and data pipeline design Expertise in building and maintaining ETL workflows across disparate datasets Knowledge of data acquisition, cloud-based data pipelines, and data transport Experience with SQL, Kusto query language, or similar query languages Familiarity with cloud cost optimization and data asset curation Ability to work in a fast-paced, collaborative environment Knowledge of cybersecurity principles (preferred but not required) It is a strong plus if you have: Experience with Infrastructure-as-Code tools such as Terraform or Ansible Familiarity with big data technologies like Kafka, Spark, and streaming services Previous exposure to Security Information & Event Management (SIEM) systems Experience in real-time analytics deployment for large-scale datasets Understanding of cybersecurity frameworks such as NIST, ISO27001, or OWASP Knowledge of cloud security practices and network protocols Experience with cloud-based security orchestration, automation, and response (SOAR) technologies We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #6917 You can report violations in accordance with ITDS’s Whistleblower Procedure available here .","[{""min"": 1040, ""max"": 1450, ""type"": ""Net per day - B2B""}]",Data Engineering,1040,1450,Net per day - B2B
Full-time,Senior,B2B,Remote,229,Senior Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Senior Data Engineer , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Development and maintenance of a large platform for processing automotive data. A significant amount of data is processed in both streaming and batch modes. The technology stack includes Spark, Cloudera, Airflow, Iceberg, Python, and AWS. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Centralized reporting platform for a growing US telecommunications company. This project involves implementing BigQuery and Looker as the central platform for data reporting. It focuses on centralizing data, integrating various CRMs, and building executive reporting solutions to support decision-making and business growth. 🎁 Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. 🚀 Your main responsibilities: Develop and maintain a high-performance data processing platform for automotive data, ensuring scalability and reliability. Design and implement data pipelines that process large volumes of data in both streaming and batch modes. Optimize data workflows to ensure efficient data ingestion, processing, and storage using technologies such as Spark, Cloudera, and Airflow. Work with data lake technologies (e.g., Iceberg) to manage structured and unstructured data efficiently. Collaborate with cross-functional teams to understand data requirements and ensure seamless integration of data sources. Monitor and troubleshoot the platform, ensuring high availability, performance, and accuracy of data processing. Leverage cloud services (AWS) for infrastructure management and scaling of processing workloads. Write and maintain high-quality Python (or Java/Scala) code for data processing tasks and automation. 🎯 What you'll need to succeed in this role: At least 5 years of commercial experience implementing, developing, or maintaining Big Data systems, data governance and data management processes. Strong programming skills in Python (or Java/Scala): writing a clean code, OOP design. Hands-on with Big Data technologies like Spark , Cloudera, Data Platform, Airflow, NiFi, Docker, Kubernetes, Iceberg, Hive, Trino or Hudi. Excellent understanding of dimensional data and data modeling techniques. Experience implementing and deploying solutions in cloud environments. Consulting experience with excellent communication and client management skills, including prior experience directly interacting with clients as a consultant. Ability to work independently and take ownership of project deliverables. Fluent in English (at least C1 level). Bachelor’s degree in technical or mathematical studies. ➕ Nice to have: Experience with an MLOps framework such as Kubeflow or MLFlow. Familiarity with Databricks, dbt or Kafka. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn ).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,31920,Net per month - B2B
Full-time,Senior,B2B,Remote,230,Senior Data Engineer with Snowflake,N-iX,"#3668 Join an exciting journey to create a greenfield, cutting-edge Consumer Data Lake for a leading global organization based in Europe. This platform will unify, process, and leverage consumer data from various systems, unlocking advanced analytics, insights, and personalization opportunities. As a Senior Data Engineer, you will play a pivotal role in shaping and implementing the platform's architecture, focusing on hands-on technical execution and collaboration with cross-functional teams. Your work will transform consumer data into actionable insights and personalization on a global scale. Using advanced tools to tackle complex challenges, you’ll innovate within a collaborative environment alongside skilled architects, engineers, and leaders. Key Responsibilities: Hands-On Development: Build, maintain, and optimize data pipelines for ingestion, transformation, and activation. Create and implement scalable solutions to handle diverse data sources and high volumes of information. Data Modeling & Warehousing: Design and maintain efficient data models and schemas for a cloud-based data platform. Develop pipelines to ensure data accuracy, integrity, and accessibility for downstream analytics. Collaboration: Partner with Solution Architects to translate high-level designs into detailed implementation plans. Work closely with Technical Product Owners to align data solutions with business needs. Collaborate with global teams to integrate data from diverse platforms, ensuring scalability, security, and accuracy. Platform Development: Enable data readiness for advanced analytics, reporting, and segmentation. Implement robust frameworks to monitor data quality, accuracy, and performance. Testing & Quality Assurance: Implement robust security measures to protect sensitive consumer data at every stage of the pipeline Ensure compliance with data privacy regulations (e.g., GDPR, CCPA ..) and internal policies. Monitor and address potential vulnerabilities, ensuring the platform adheres to security best practices. Requirements: Over 4+years of experienceshowcasing technical expertise and critical thinking in data engineering. Hands-on experience withDBTand strongPythonprogramming skills. Proficiency inSnowflakeand expertise in data modeling are essential. Demonstrated experience in building consumer data lakes and developing consumer analytics capabilities is required. In-depth understanding of privacy and security engineering withinSnowflake, including concepts like RBAC, dynamic/tag-based data masking, row-level security/access policies, and secure views. Ability to design, implement, and promote advanced solution patterns and standards for solving complex challenges. Familiarity with multiple cloud platforms (Azureor GCP preferred, with a focus on Azure). Practical experience with Big Data batch and streaming tools. Competence in SQL, NoSQL, relational database design (SAP HANA experience is a bonus), and efficient methods for data retrieval and preparation at scale. Proven ability to collect and process raw data at scale, including scripting, web scraping, API integration, and SQL querying. Experience working in global environments and collaborating with virtual teams. A Bachelor’s or Master’s degree in Data Science, Computer Science, Economics, or a related discipline. We offer*: Flexible working format - remote, office-based, or flexible. A competitive salary and a good compensation package. Personalized career growth. Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more). Active tech communities with regular knowledge sharing. Education reimbursement. Memorable anniversary presents. Corporate events and team building. Other location-specific benefits. *not applicable for freelancers","[{""min"": 22164, ""max"": 30993, ""type"": ""Net per month - B2B""}]",Data Engineering,22164,30993,Net per month - B2B
Full-time,Senior,Permanent,Remote,235,Service Delivery Manager,INFOPLUS TECHNOLOGIES,"Summary – Service Delivery Manager with a technical background that is comfortable managing projects, and interacting with the business owners. Technical skills that are required is a rounded understanding of web frontend development, Enterprise reporting tools, data warehousing, and databases. Overall a firm understanding of how to build a solution well, and set it up to be a sustainable Enterprise service. Looking for about 10 years of experience. Pharma experience is an asset. Job Description: DSM - Global Portfolio Planning and Business Excellence Service Delivery Manager with a technical background that is comfortable managing projects, and interacting with the business owners. Technical skills that are required is a rounded understanding of web frontend development, Enterprise reporting tools, data warehousing, and databases. Overall a firm understanding of how to build a solution well, and set it up to be a sustainable Enterprise service. Looking for about 10 years of experience. Pharma experience is an asset. Knowledge/Competencies: Ability to influence and ensure efficient and effective delivery of operational plans Knowledge on customer service standards and ITSM processes Knowledge and experience defining and controlling SLAs, KPIs and contract management Significant experience working with virtual teams in a cross-functional organization. Demonstrates customer, quality, cost, and delivery focus Advanced analytical, problem-solving and decision making skills Interpersonal skills to interact and communicate effectively including Conflict Management Technical Skills: Experience in an audited, or validated sytems environment. Core ServiceNow knowledge (Incident, Service Requests, Problem and Change) Strong understanding of frontend and backend development. Data Warehousing, ETL, and Reporting background (Snowflake, TalenD, and Tableau preferred)","[{""min"": 240000, ""max"": 265000, ""type"": ""Gross per year - Permanent""}]",Unclassified,240000,265000,Gross per year - Permanent
Full-time,Senior,B2B,Remote,236,Data Engineer,Mirumee Software,"Hi! 👋 We are Mirumee , a Python and TypeScript software house specializing in developing unique products. Since 2009 , we’ve been helping businesses transform by delivering sustainable, API-first, efficient, and modern solutions. Our expertise empowers brands across Healthcare, Commerce, and Self-Publishing , from high-volume online retailers to disruptive innovators featured on the Forbes Next Billion Dollar Startups list. At Mirumee , culture matters. We thrive in open-source communities and believe in technical excellence, open communication, and mutual respect . We actively contribute to open-source projects - check out our work here: GitHub.com/mirumee . But first of all, we care about technical excellence, open communication, and treating each other with respect as equal human beings . We are seeking an experienced Senior Data Engineer to join an e-commerce project in the cannabis branch in the US, who combines strong technical execution with curiosity, ownership, and a collaborative mindset. You’ll help shape and scale our data platform while proactively engaging with cross-functional teams to drive clarity, resilience, and value in our data pipelines and tooling. The ideal candidate brings experience with modern data stacks (e.g., Airflow, DBT, cloud-native tooling), strong fundamentals in SQL and Python, and a willingness to engage beyond narrowly defined tasks. If you enjoy working through ambiguity, taking initiative, and helping set new standards of quality, we’d love to meet you! Your profile Strong SQL skills with experience writing performant, maintainable queries on large datasets Fluency in Python , especially with libraries like Pandas, and a strong testing discipline Experience with Airflow , DBT , or similar orchestration and transformation tools Familiarity with BI tools such as Sigma or Tableau Experience with AWS and IaaC best practices (especially serverless data services) Ability to break down complex projects into iterative steps and communicate progress effectively A growth-oriented mindset —you’re resourceful, self-directed, and open to feedback Someone curious and engaged , regularly asking thoughtful questions and offering insights beyond your immediate responsibilities A positive, collaborative teammate who brings energy and focus to the work and team Good command of English (both written and spoken) Responsibilities Design, build, and maintain scalable data pipelines using Airflow, DBT, and Python. Partner with stakeholders to clarify requirements, uncover edge cases, and ensure quality delivery - even when the problem space isn’t fully defined. Own and improve our data models and warehouse practices, helping ensure reliability, performance, and clarity Proactively monitor and respond to data issues (e.g., Sentry alerts, data quality checks), driving resolution and follow-through. Collaborate with others through well-scoped pull requests, clear documentation, and knowledge sharing Champion testing and observability, ensuring our pipelines Benefits 26 days of paid service disruption per is a standard for us 🌴 Personal development in the fast-growing industry based on your and our needs: boredom is something foreign to us Work wherever the way you want: we don't care if you start at 6 or 10 am and give you the choice to choose between a beautiful office, home office spot or combine both! We give you 5 extra days for health recovery (b2b) and the basics to take care of prophylactically - private healthcare that works (Signal Iduna) & sports card (Multisport Plus) for free Work-life balance (and we mean it!) High-end tools, physical (MacBook), and software ones And much more that we want to share during interviews! We can't wait to meet YOU . 🙌🏼","[{""min"": 17000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Engineering,17000,23000,Net per month - B2B
Full-time,Mid,Permanent,Hybrid,238,Data Engineer - Allegro Pay,Allegro,"About the team The salary range for this position is (contract of employment): 14 200 - 19 690 PLN in gross terms A hybrid work model requires 1 day a week in the office. Allegro Pay is the largest fintech in Central Europe – we are growing fast and need engineers who want to learn and develop, while at the same time solving problems related to serving thousands of RPSs. If, like us, you like flexing your mental muscles to solve complex problems and you would be happy to co-create the infrastructure which underpins our solutions, make sure you apply! In this role, you will be a contributor, helping us expand our modern cloud-based analytical solutions. We embrace challenging and interesting projects and take quality very seriously. Depending on your preference, your position may be more business-oriented or platform-oriented. Your main responsibilities: - You will be actively responsible for developing and maintaining processes for handling large volumes of data. - You will be streamlining and developing the data architecture that powers analytical products and work along a team of experienced analysts. - You will be monitoring and enhancing quality and integrity of the data. - You will manage and optimize costs related to our data infrastructure and data processing on GCP. This is the right job for you if: - You have at least 3 years of experience as Data Engineer and working with large datasets. - You have experience with cloud providers (GCP preferred). - You are highly proficient in SQL. - You have strong understanding of data modeling and cloud DWH architecture. - You have experience in designing and maintaining ETL/ELT processes. - You are capable of optimizing cost and efficiency of data processing. - You are proficient in Python for working with large data sets (using PySpark or Airflow). - You use good practices (clean code, code review, CI/CD). - You have a high degree of autonomy and take responsibility for developed solutions. - You have English proficiency on at least B2 level. - You like to share knowledge with other team members. What we offer: - Big Data is not an empty slogan for us, but a reality - you will be working on really big datasets (petabytes of data). - You will have a real impact on the direction of product development and technology choices. We utilize the latest and best available technologies, as we select them according to our own needs. - Our tech stack includes: GCP, BigQuery, (Py)Spark, Airflow. - We are a close -knit team where we work well together. - You will have the opportunity to work within a team of experienced engineers and big data specialists who are eager to share their knowledge, including publicly through allegro.tech Apply to Allegro and see why it is #dobrzetubyć (#goodtobehere)","[{""min"": 14200, ""max"": 19690, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14200,19690,Gross per month - Permanent
Full-time,Senior,Any,Remote,242,Data Product Engineer,The Ozone Project,"Dołącz do naszego zespołu pasjonatów technologii reklamy cyfrowej, którzy aktywnie wspierają bardziej ekologiczną i świetlaną przyszłość zarówno dziennikarstwa, jak i reklamy online. Będziesz w centrum strategii opartych na danych, pomagając markom nawiązywać kontakt ze społecznościami online w bezpiecznych i angażujących środowiskach. Współpracując z utalentowanymi, zaangażowanymi osobami, będziesz mieć realny wpływ na kształtowanie pozytywnego krajobrazu cyfrowego 💻. 🌟Zakres obowiązków: 🔮 Co oferujemy: ","[{""min"": 24494, ""max"": 29393, ""type"": ""Net per month - Any""}]",Data Engineering,24494,29393,Net per month - Any
Full-time,Senior,Permanent,Hybrid,243,Senior Data Engineer,KUBO,"We are seeking an experienced Senior Data Engineer to join our Data & Analytics team and play a critical role in designing, building, and maintaining high-quality data products. As a member of a cross-functional and agile team, you will collaborate with data architects, data scientists, analytics leads, front-end developers, and business stakeholders to support data-driven decision-making across the organization. This role is ideal for someone passionate about data engineering excellence, digital transformation, and delivering impactful analytical solutions. Key responsibilities: Design and implement scalable data pipelines and globally harmonized data models by integrating data from various domains Ensure data products adhere to architectural standards, data protection regulations, and quality guidelines Provide technical leadership and guidance to other data engineers Enhance and maintain implementation frameworks to support analytical use cases and evolving product needs Ensure accurate estimations of time and cost, high-quality delivery, and smooth handover of solutions to operations Ideal candidate profile: 5+ years of experience in Data & Analytics Min. 3 years of experience and good knowledge of the Azure data ecosystem: Azure Data Lake, Azure Synapse, Databricks Knowledge of data cataloguing, data quality management, and modern data management practices Proficiency with CI/CD tools and processes Fluency in English Knowledge of Snowflake is a plus Conditions: Work model: Hybrid – 1 day per week in the Warsaw office Salary: 20 000 - 25 000 PLN gross/month Employment type: Full-time employment contract (UoP) directly with the client Business trips to Germany 1-2 times a year Benefits: VIP Medical Care Package, Life & Travel Insurance, Company Bonus, Holiday allowance, Co-financed sport card *Relocation package and full support with relocation to Warsaw Recruitment steps: Phone call with a Recruiter (20 - 30 min.) First interview with a Manager (1h) Second interview with a technical team (1h) Feedback and decision","[{""min"": 20000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,25000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,245,Tech Lead (Azure),SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. projekt budowy nowoczesnej Data Platform w chmurze Azure, obejmującej warstwę Lakehouse (Bronze / Silver / Gold) oraz Analytical Platform (MLOps), modelowanie struktur bazodanowych w podejściu DDD (Data Domain Driven Design), opracowywanie modeli danych na podstawie dokumentacji z obszaru Data Governance (glosariusz danych, modele konceptualne/logiczne), projektowanie przepływu danych ze źródeł do warstw platformy danych (Ingest: direct query, API, event streaming), tworzenie i dokumentowanie Data Contracts, współpraca z właścicielami systemów źródłowych i docelowych w zakresie integracji danych i SLA, implementacja struktur danych w architekturze medallion (Bronze / Silver / Gold) przy użyciu ETL na Azure Data Platform, doradztwo w zakresie doboru narzędzi, architektury integracyjnej i praktyk CI/CD, możliwość realnego wpływu na standardy technologiczne i projektowe, praca 100% zdalna, a dla chętnych możliwość pracy z biura we Wrocławiu, stawka do 200 zł/h przy B2B, w zależności od doświadczenia. masz doświadczenie w modelowaniu danych (ERD), przygotowywaniu Data Contracts i implementacji struktur domenowych w środowisku Data Warehouse znasz procesy Data Ingestion i architekturę nowoczesnych DWH w Azure (Azure Synapse, Data Lake, Databricks), masz doświadczenie z platformami MLOps / Analytical Platform, mile widziane doświadczenie w: tworzeniu dokumentacji mapowania danych źródłowych, zarządzaniu metadanymi i jakością danych (np. Azure Purview), komunikujesz się płynnie w języku angielskim (B2/C1). długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed.","[{""min"": 26880, ""max"": 33600, ""type"": ""Net per month - B2B""}]",Data Engineering,26880,33600,Net per month - B2B
Full-time,Senior,B2B,Remote,246,Cloud Data Architect (AWS/Azure),Future Processing,"posiadasz min. 5 lat doświadczenia w IT, w tym min.3,5 roku w pracy z danymi w chmurze AWS lub Azure(potwierdzone projektami komercyjnymi wdrożonymi na produkcję) oraz min.rok pracy w roli Lead Developera/Architekta lub podobnej, masz wiedzę w zakresie usługData wAWS lub Azure bardzo dobrze znaszDatabricks, ze szczególnym naciskiem na najnowsze funkcjonalności tej platformy (np. Delta Live Tables i Unity Catalog), znasz narzędzia, mechanizmy i procesy, tj.CI/CD, IaaC/Terraform, Git, umiesz tworzyć i modyfikować modele danych oraz dobierać odpowiedni model do rozwiązywanego problemu (DWH, Data Marts, Data Lake, Delta Lake, Data Lakehouse), architektury Lambda i Kappanie kryją przed Tobą tajemnic, wiesz, jak budować rozwiązania do analizy danych w trybie „near to real time”, monitoring, diagnostyka oraz rozwiązywanie problemów w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanować infrastrukturę oraz obliczyć jej koszt, posiadasz wiedzę na temat migracji rozwiązań on-premise do chmury oraz znasz podstawowe typy migracji, wiesz, jak stosować mechanizmy związane z bezpiecznym przechowywaniem i przetwarzaniem danych w chmurze, brałeś/aś udział w spotkaniach przedsprzedażowych, cechuje Cię myślenie strategiczne i analityczne oraz dobra samoorganizacja pracy, samodzielność, adaptacyjność, komunikatywność oraz proaktywność, posiadasz umiejętność mentorowania oraz szkolenia osób o mniejszym doświadczeniu oraz prowadzenia działań mających na celu ich wsparcie rozwojowe, prowadzenie prezentacji i prelekcji podczas wydarzeń branżowych nie spędza Ci snu z powiek, jesteś otwarty na wyjazdy służbowe, posługujesz sięjęzykiem angielskim na poziomie zaawansowanym, min. C1. konsultowanie rozwiązań z klientami - zarówno obecnymi, jak i nowymi. Doradzanie w wyborze rozwiązania technicznego adekwatnego do problemu biznesowego klienta. Dążenie do wyboru rozwiązania, które jest optymalne kosztowo i odpowiada na potrzebę rozwiązującą problem klienta. udział w spotkaniach z klientem na wczesnym etapie - pitch naszego doświadczenia, procesu, podejścia do technologii, dopytywanie, zbieranie/doszczegóławianie wymagań; tworzenie wkładu merytorycznego do ofert - schemat proponowanej architektury, wyliczenia Total Cost of Ownership/Return of Investment/kosztów chmury za pomocą kalkulatorów dostawcy chmury; estymowanie projektu/wyceny zaangażowania przedstawicieli DS w projekcie; budowanie argumentów przekonujących klienta do naszego rozwiązania, pokazywanie przewag w stosunku do innego podejścia, udział w spotkaniach prezentujących ofertę, odpowiadanie na pytania klienta, prezentowanie oferty, za którą stoi nasz fragment rozwiązania, prace badawczo-rozwojowe w zakresie analizy funkcjonalności i przydatności nowych technologii i narzędzi w rozwiązaniach biznesowych klienta, regularny kontakt z osobami decyzyjnymi w obszarze Data po stronie klienta (VP, IT Director), tworzenie PoC w obszarze Data w celu zaprezentowania wyników prowadzonych prac R&D, projektowanie i tworzenie całości platformy przetwarzania danych uwzględniając wszystkie jej części oraz powiązania z pozostałymi rozwiązaniami (przykładowo BI i ML) z uwzględnieniem ekosystemu chmurowego, optymalizowanie całościowych rozwiązań/systemów przechowywania i analizy danych, utrzymywanie relacji z zespołem technicznym po stronie klienta, koordynowanie pracy inżynierów zaangażowanych w tworzenie rozwiązania, nadzorowanie przebiegu całego projektu, od początku do końca, zaangażowanie w rozwój linii biznesowej Data Solutions (pozyskiwanie pracowników, klientów, szkolenia, udział w konferencjach, mentoring itp.).","[{""min"": 165, ""max"": 245, ""type"": ""Net per hour - B2B""}]",Data Architecture,165,245,Net per hour - B2B
Full-time,Mid,B2B,Remote,249,Experienced Azure Data Engineer,Future Mind,"Future Mind is a brilliant, inspiring team, one of the most awarded tech consulting companies in the region with a broad portfolio of clients, including Żabka, Jeronimo Martins (Hebe, Biedronka), LPP (Reserved, Sinsay, Mohito), eObuwie, Modivo, and other well-recognized brands. We have received several industry awards for delivering some of the best eCommerce applications in Poland, listed among the most popular across app stores. Our expert engineers, designers, project managers, and analysts work on projects ranging from top mobile commerce apps used daily by millions of customers to IoT, and telematics platforms that produce vast amounts of data. In 2023, we joined forces with Solita, a Finnish tech powerhouse with a vibrant community of over 2000 specialists across Europe, that combines data, business, and technology skills to build and improve digital services for leading organizations in manufacturing, medical, shipping, and other major industries, including such clients as NATO, Nokian Tires, Pfizer and many others. Together we are dedicated to delivering cutting-edge, data-driven solutions. We value proactive professionals who take ownership, enjoy solving problems, sharing knowledge, and collaboration. Together, we create high-quality software solutions that fulfil our clients' business needs and impact their customers' lives every day. As part of the Solita Group, Future Mind provides digital advisory & delivery services with the support of our international partners and upholds equally high cultural standards. Role description: As an Azure Data Engineer, your role involves designing and implementing ingestion and transformation pipelines, and data models while collaborating closely with the business stakeholders. You have worked with the Azure data stack and cloud for several years and understand the value created for customers. This job is all about: Building end-to-end ELT data pipelines on top of an Azure Data Services ; Designing data models based on large, complex data sets that meet both functional and non-functional business requirements; Holistic platform view (source systems, networking, orchestration, etc.) with the ability to deliver business value; Collaborating within our project teams to meet client needs and deliver high-quality solutions. Here’s what we’re looking for: Vast experience with Azure Data Services; Deep understanding of dimensional data modeling techniques (data vault would be a plus); Proficiency with combining SQL and Python with a modern ELT toolset (Fabric, Synapse, Spark, or equivalent); Experience with ADF, Airflow, or other orchestration tools; Effective communication skills, proficient in Polish and English. You also must have the legal right to work in the EU to apply for this position. …and here’s what we offer: A dynamic work environment where innovation and collaboration are valued. Access to cutting-edge projects and technologies in a variety of industries. A supportive community of experts to foster your professional growth and development. Competitive compensation, comprehensive benefits, and a focus on work-life balance. Opportunities for continuous learning and career advancement, including specialized training in big data technologies and Snowflake certifications. The ability to work fully remotely or check into one of our offices whenever you like, Fully paid private health insurance, subsidized sports membership, mental health support, and language courses.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Engineering,14000,22000,Net per month - B2B
Full-time,Senior,B2B,Remote,250,Data Scientist,INFOPLUS TECHNOLOGIES,"🚀 Hiring Now: ETRM Data Scientist (Remote | B2B Contract) Are you a Data Scientist with deep expertise in time-series forecasting , machine learning , and Azure ML ecosystems ? Do you thrive in complex energy trading environments and love building scalable, production-ready ML systems ? We have the perfect opportunity for you! 🔍 Role : ETRM Data Scientist 📍 Location : Remote 📄 Type : B2B Contract 🎯 What You’ll Do Design and implement cutting-edge forecasting models using ARIMA, LSTM, Prophet, and more. Develop scalable and reusable machine learning pipelines using Azure ML, PySpark , and Python-based MLOps frameworks . Deploy, monitor, and fine-tune models in operational environments to ensure long-term accuracy and performance. Work on large-scale datasets and high-performance ML workflows using Azure Databricks and Data Lake . Apply ensemble methods (stacking, boosting, bagging) for robust and accurate predictions. Collaborate in a dynamic energy trading environment to optimize decision-making using advanced data science techniques. ✅ What We’re Looking For Education 🎓 Master’s in Mathematics, Statistics, Data Science, or related fields (Ph.D. preferred but not mandatory) Mandatory Skills Advanced Time-Series Forecasting and Predictive Modelling Deep understanding of ML frameworks: scikit-learn, XGBoost, TensorFlow, PyTorch, Darts Strong coding skills in Python and PySpark Solid hands-on with Azure Machine Learning SDK , Azure Databricks , and Azure Data Lake Expertise in MLOps and model lifecycle automation Proficiency in data handling libraries : Pandas, NumPy Preferred Skills K-Means Clustering , Bottom-Up Forecasting Experience with Azure Data Factory and pipeline orchestration Knowledge of Power/Energy Trading concepts Exposure to Generative AI (GenAI) and large language models like GPT 🌟 Why Join Us? Work with cutting-edge technology in the ETRM and energy analytics domain 100% remote – collaborate with global experts from the comfort of your space Competitive B2B contract with flexible terms High-impact role with real-world business value 📩 Interested? Apply now and let’s transform energy trading with data-driven intelligence! 🔗 [Apply Now] or send your CV to siva.s@infoplusltd.co.uk #ETRM #DataScience #MachineLearning #AzureML #MLOps #EnergyTrading #TimeSeriesForecasting #RemoteJobs #PythonJobs #DeepLearning #AzureDatabricks #ContractJobs #PySpark #GenAI #PowerTrading #B2BJobs #Contract #Hiring #Remotejobs","[{""min"": 31500, ""max"": 37800, ""type"": ""Net per month - B2B""}]",Data Science,31500,37800,Net per month - B2B
Full-time,Senior,B2B,Remote,251,Data Scientist,Altimetrik Poland,"Altimetrik Polandis a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. For our clientfrom FinTech UKwe are looking for an experiencedData Scientistwith a specialization intime series modelingand the application ofmachine learning and deep learning techniquesto temporal data. The candidate will focus on building predictive models and extracting insights from time-dependent datasets using both statistical and modern ML/DL approaches. Responsibilities: Design and train models for time series analysis. Develop ML and DL models specifically for time-based data. Analyze time series data for forecasting, classification, and pattern recognition. Collaborate with the wider team to support the deployment of models into production environments. If you possess... Hands-on experience with time series modeling (e.g., ARIMA, Prophet, LSTM). Proven use of ML and DL methods applied to temporal datasets. Strong skills in working with time-stamped data from preprocessing through to model evaluation. We work 100% remotely or from our hub inKraków. 🔥We grow fast. 🤓We learn a lot. 🤹We prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 25000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Data Science,25000,29000,Net per month - B2B
Full-time,Senior,B2B,Remote,252,Data Engineer / Big Data Analyst,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozwój i utrzymanie systemów informatycznych wspierających funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 5-letnie doświadczenie na stanowisku związanym z analizą danych lub analizą biznesową Min. 2-letnie doświadczenie na stanowisku wymagającym przetwarzania i analizy dużych zbiorów danych (Big Data) Doświadczenie projektowe w tworzeniu i optymalizacji zaawansowanych zapytań SQL Doświadczenie projektowe w programowaniu w języku Python Doświadczenie projektowe w przetwarzaniu i analizie dużych zbiorów danych Doświadczenie projektowe w Data Quality Doświadczenie w programowaniu w SQL Doświadczenie w programowaniu w Python Doświadczenie w programowaniu w PySpark Znajomość zagadnień związanych z procesami ETL Znajomość relacyjnych baz danych Dobra organizacja pracy własnej, orientacja na realizacje celów Umiejętności interpersonalne i organizacyjne, planowanie Komunikatywność, kreatywność, samodzielność, kultura osobista i dociekliwość Zdolność adaptacji i elastyczność, otwartość na stały rozwój i gotowość uczenia się Mile widziane Doświadczenie projektowe w obszarze ochrony zdrowia Doświadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarządzania projektem metodą zwinną (np.. Agile PM lub równoważny) Certyfikat potwierdzający znajomość Apache Airflow (np. Airflow Fundamentals lub równoważny) Certyfikat potwierdzający umiejętność tworzenia DAGów Airflow (np. Dag Authoring lub równoważny) Certyfikat potwierdzający znajomość Apache Spark (np. Spark Developer Associate lub równoważny) Certyfikat potwierdzający znajomość SQL (np.. W3Schools SQL Certificate lub równoważny) Kluczowe zadania Analiza danych biznesowych i technicznych w celu wspierania podejmowania decyzji oraz optymalizacji procesów Przetwarzanie, analiza i interpretacja dużych zbiorów danych (Big Data) z wykorzystaniem Python, SQL oraz PySpark Projektowanie, tworzenie i optymalizacja zaawansowanych zapytań SQL w środowiskach baz danych Współpraca z zespołami technicznymi i biznesowymi w celu definiowania potrzeb analitycznych oraz tworzenia rozwiązań opartych na danych Tworzenie i utrzymywanie procesów związanych z jakością danych (Data Quality), w tym ich weryfikacja, czyszczenie i walidacja Programowanie rozwiązań analitycznych i integracyjnych w języku Python (w tym PySpark) w środowiskach przetwarzania danych Praca z relacyjnymi bazami danych oraz narzędziami ETL w celu ekstrakcji, transformacji i załadunku danych Wspieranie inicjatyw związanych z automatyzacją raportów i analiz, w oparciu o duże zbiory danych i zapytania Utrzymywanie wysokiej jakości dokumentacji technicznej oraz przekazywanie wniosków i rekomendacji interesariuszom Monitorowanie integralności danych oraz proponowanie i wdrażanie usprawnień w procesach analitycznych","[{""min"": 150, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,150,185,Net per hour - B2B
Full-time,Senior,B2B,Remote,253,Staff Frontend Engineer,Adverity,"In this role, you’ll play a key part in shaping and developing our frontend applications. You’ll work closely with our Product Developers and Staff Engineers to drive technical decisions and ensure high-quality delivery. You’ll inspire confidence through your decisions, earn trust through your execution, and set a standard others are eager to follow. This is a full-time position, where you can work remotely in Europe. About Adverity: Our SaaS company was founded in Vienna in 2015, since then, Adverity has been driving marketing intelligence of some of the world’s leading brands and agencies including IKEA, Red Bull, GroupM, Unilever, Omnicom, Barilla, JD Sports and Forbes. Our mission is to empower Marketing and E-Commerce business users with our Marketing Analytics platform: we leverage all their data and turn it into powerful insights, decisions and actions. How do we do it? At Adverity, we bring together international and diverse teams in an inclusive and safe environment. We believe in an entrepreneurial mindset, we help each other succeed, we strive to provide an awesome customer and employee experience, we prioritise work-life harmony and we dare to make mistakes to learn from them. If you share our values, Adverity is a great place for you! Some of the things you’ll work on: Design and implement a clean frontend architecture with both strong performance requirements and complexity challenges. Work with useful programming tools such as extensive code reviews , pair programming , multiple layers of testing, continuous integration and deployments. Help build our design system to eliminate design pain points and streamline collaboration between design, UX, and engineering. Create documentation , usage guidelines, and proof of concepts. Advocate for quality and practicality with frontend development teams. Serve as a subject matter expert to designers, engineers, and product managers regarding the frontend domain. Understand the design needs of a large-scale software project. We're excited if you have: 5+ years of professional frontend engineering experience, especially with React , React Query, and React Hook Form. Solid skills in frontend testing using Cypress, Vitest, and/or Playwright. A strong grasp of complex application state management , side effects, and action orchestration. Experience mentoring engineers and working closely with stakeholders through clear, proactive communication . Expertise with modern styling approaches (Tailwind, styled-components, emotion) and design systems (design tokens, theming). Hands-on experience with monorepo management tools (NX, Turborepo, Lerna) and improving developer tooling and workflows. A passion for evaluating and implementing new technologies , and optimizing CI/CD pipelines for frontend deployments. Some of our benefits: Flexible working hours and remote work Regular team events (also remote) Sustainable merch for all Modern technology International & diverse teams Apply now if you are ready to revolutionise the way businesses work with marketing data. We look forward to meeting you!","[{""min"": 24763, ""max"": 29887, ""type"": ""Net per month - B2B""}]",Data Engineering,24763,29887,Net per month - B2B
Full-time,Mid,B2B,Remote,254,BI Consultant,Primaris,"Forma współpracy: kontrakt B2B Tryb: praca zdalna Aktualnie do jednego z projektów/zespołów poszukujemy osoby na stanowiskoBI Consultant, która posiada min.3lat komercyjnego doświadczenia. Dlatego jeśli szukasz pracy z: zaawansowanym SQL, tworzeniem raportów w Power BI, pracą z Microsoft SQL Server i SSRS, a także chcesz rozwijać swoje umiejętności w obszarze Business Intelligence –chętnie z Tobą porozmawiamy! Wiodący stack technologiczny: SQL, T-SQL, Power BI, Microsoft SQL Server, SSRS, Report Builder, Salesforce. Projekt który będziemy gotowi Ci zaproponować dotyczy: Rozwoju rozwiązań raportowych dla międzynarodowego systemu do zarządzania portfelem projektów, wykorzystywanego przez globalne firmy z obszaru HR, logistyki i produkcji. Praca obejmuje tworzenie zaawansowanych raportów na bazie produkcyjnej z wykorzystaniem SQL i Power BI, w oparciu o zgłoszenia w systemie Salesforce. Zakres raportowania obejmuje dane projektowe, finansowe i zasoby projektowe. Wymagania: Min. 3 roku komercyjnego doświadczenia na podobnym stanowisku Bardzo dobra znajomość SQL + tworzenie zapytań do bazy Biegła znajomość języka angielskiego (C1) Zaawansowana znajomość PowerBI (głównie Desktop) Doświadczenie w komunikacji i współpracy bezpośrednio z klientem Mile widziane będzie doświadczenie pracy z narzędziami raportowymi: Microsoft Report Bulider W naszej firmie będziesz mógł/mogła liczyć na: Pracę w organizacji z ugruntowaną pozycją rynkową Projekty, w których będziesz miał/miała wpływ na ich rozwój Współpracę z ciekawymi klientami biznesowymi z różnych branż (m.in.: finanse, bankowość, ubezpieczenia, healthcare, robotyzacja, energetyka, media), Permanentny mentoring zarówno techniczny jak i biznesowo-menedżerski, np. podczas naszych cyklicznych szkoleń (m.in. Git, Gitflow, Angular, Docker), czy wew. programów rozwojowych (Primaris x TechTalks, Primaris Leadership Academy) oraz zewnętrznych kursów.Już na etapie on-boardingu zapewniamy dostęp do naszych wewnętrznych szkoleń, cyklicznych spotkań, które serializujemy na Confluence oraz platformy e-learning Świetną atmosferę pracy, wśród zaangażowanych ludzi z pasją w płaskiej strukturze z prostymi procesami Kompleksowy pakiet benefitów skrojonych na miarę - prywatna opieka medyczna dla Ciebie oraz dla Twojej rodziny, Multisport dla Ciebie i os. towarzyszącej - Ty decydujesz, co wybierasz! Cały proces rekrutacyjny oraz onboarding prowadzony jest zdalnie. Proces rekrutacyjny składa się z: rozmowy telefonicznej z osobą z działu Rekrutacji & HR (do 30 min) zdalnej video rozmowy - weryfikacji techniczno-biznesowej z naszym specjalistą/specjalistką (60-90 min) zdalnego spotkania z liderem po stronie klienta projektu (30-60 min) finalnej decyzji dotyczącej oferty Primaris Services to ponad 250 ekspertów na pokładzie i 15 lat doświadczenia w branży IT na rynku polskim oraz zagranicznym.Realizujemy ambitne projekty o wysokiej złożoności z różnych obszarów -m.in. bankowości, ubezpieczeń, funduszy inwestycyjnych czy branży logistycznej (mamy ponad 40 aktywnych klientów!). Rośniemy w siłę oraz ciągle poszerzamy portfolio zarówno naszych usług jak i klientów. Zakres naszej działalności obejmuje budowę systemów od zera, ich rozwój oraz utrzymanie, wdrożenia produktowe, alokacje całych Zespołów, a także pojedynczych Ekspertów w strukturach Klienta. Ponadto od kilku lat działamy bardzo intensywnie jako złoty Partner firmy UiPath (obszar Robotic Process Automation) budując roboty i sprzedając licencje u naszych Klientów. Co miesiąc dołącza do nas 7 nowych osób! Wierzymy, że zgrany zespół i ludzie z pasją to klucz do naszego wspólnego sukcesu! Właśnie dlatego ciągle poszukujemy nowych, zdolnych osób, które zasilą nasze szeregi.","[{""min"": 80, ""max"": 100, ""type"": ""Net per hour - B2B""}]",Unclassified,80,100,Net per hour - B2B
Full-time,Senior,B2B,Remote,255,Data Modeler / Data Engineer,emagine Polska,"Informacje o projekcie: Branża: finanse/pożyczki Lokalizacja: zdalnie Umowa: B2B Stawka: 200 pln/h netto + VAT Długość projektu: długoterminowy Poszukujemy doświadczonego Data Engineera do zespołu Data Platform, który będzie modelować dane oraz implementować procesy ELT w chmurze. Obowiązki: Modelowanie struktur bazodanowych w podejściu DDD oraz tworzenie logicznych i fizycznych modeli danych. Data Mapping. Przygotowywanie warstwy Data Contracts (wymagań HD do systemów źródłowych pod merytoryczną płaszczyznę kontraktu na dane) na podstawie zamodelowanych uprzednio struktur dla poszczególnych domen danych. Współpraca w procesie ingerencji danych z systemów źródłowych. Implementacja modeli danych dla poszczególnych domen w Data Platform (warstwa Bronze, Silver i Gold) w podejściu ELT w środowisku Azure Databricks. Wymagania: Doświadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejętność implementacji procesów ELT. Umiejętność tworzenia dokumentacji technicznej. Doświadczenie w mapowaniu danych ze źródłowych do docelowych struktur w DWH. Umiejętność interpretacji fizycznego/logicznego modelu danych (ERD, modele relacyjne) i synchronizacji struktur tychże modeli z wymaganiami świata analityki. Doświadczenie w podejściu do projektowania słowników i zarządzania nimi (masterdata). Znajomość narzędziadbdiagram.io. Wiedza na temat zagadnień Data Quality, Data Lineage i zasad zarządzania danymi.","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,180,200,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,256,Senior GCP Data Engineer (Java + Market Data),ITDS,"GCP Data Engineer Join us, and code the backbone of financial intelligence! Kraków - based opportunity with hybrid work model (2 days/week in the office). As aGCP Data Engineer, you will be working for our client, a global financial institution developing a cloud-based risk management platform used to generate and deliver risk factor definitions, historical market data, and scenarios for advanced financial modeling. The project involves building and optimizing scalable data pipelines, microservices, and integration layers that process high volumes of real-time and historical market data. You will be joining an international team of engineers focused on innovation, automation, and delivering measurable business value in a highly regulated environment. Your main responsibilities: Translating business requirements into secure, scalable, and performant data solutions Integrating internal systems with an emphasis on fast data processing and cost optimization Developing and documenting data ingestion blueprints for market data pipelines Reviewing data solutions created by other team members Assessing and modernizing existing data pipelines and microservices Collaborating with engineers, analysts, and stakeholders to align technical solutions with business needs Implementing consistent logging, monitoring, error handling, and automated recovery Promoting automated unit and regression testing through test-centric development Designing and implementing performant REST APIs Applying industry-standard integration frameworks and patterns You're ideal for this role if you have: Strong knowledge ofJava Solid understanding of software design principles such as KISS, SOLID, and DRY Proficiency with Spring Boot and its ecosystem Experience building performant data processing pipelines Familiarity withApache Beamor similar technologies Experience working with relational andNoSQLdatabases, such as PostgreSQL and Bigtable Basic understanding of DevOps practices and CI/CD tools likeJenkinsandGroovy Ability to design and implement RESTful APIs Excellent problem-solving and analytical skills Strong communication and team collaboration abilities Experience withGCPservices like GKE, Cloud SQL, DataFlow, and BigTable It is a strong plus if you have: Knowledge of monitoring tools such as Open Telemetry, Prometheus, and Grafana Familiarity with Kubernetes and Docker Exposure to Terraform for infrastructure-as-code Experience with messaging and streaming platforms like Kafka We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #6923 You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere.","[{""min"": 28000, ""max"": 31080, ""type"": ""Net per month - B2B""}]",Data Engineering,28000,31080,Net per month - B2B
Full-time,Mid,B2B,Remote,257,Data Engineer_L4 with Databricks,Experis Manpower Group,"Tasks: Design and implement data processing solutions using Databricks for large-scale and diverse datasets Design, build, and enhance data pipelines with Python and cloud- native tools Work closely with solution architects to define and uphold best practices in data engineering Ensure data consistency, security, and scalability within cloud-based environments Requirements: Experience in data engineering, with hands-onDatabricksexpertise Strong experience inPythonfor automation and data transformation Experience working with at least one majorcloud platform(AWS, Azure, or GCP) Strong communication skills and strong English language skills Nice to have: Solid understanding of SQL with experience in query optimization and data modeling Familiarity with DevOps methodologies, CI/CD pipelines, and Infrastructure as Code (Terraform, Bicep) Experience with real-time data streaming technologies such as Kafka or Spark Streaming Knowledge of cloud storage solutions like Data Lake, Snowflake, or Synapse Hands-on experience with PySpark for distributed data processing Relevant certifications such as Databricks Certified Data Engineer Associate or cloud-based data certifications Our offer: Employment based onB2B contractvia Experis for a period of 12 months Compensation: 150-175 PLN per hour 100% remote work Multisport card Private healthcare system Life insurance","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,175,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,259,Senior Data Engineer (Microsoft),Onwelo,"🟠 Poznaj Onwelo: Onwelo to nowoczesna polska spółka technologiczna, która specjalizuje się w budowaniu innowacyjnych rozwiązań IT dla organizacji z szeregu sektorów na całym świecie. Główne obszary działalności Onwelo to: tworzenie oprogramowania, jego rozwój oraz utrzymanie, a także mocne wsparcie kompetencyjne. W krótkim czasie firma wdrożyła ponad 300 projektów w Europie i w USA, a także otworzyła biura w siedmiu miastach Polski oraz oddziały w Stanach Zjednoczonych, Niemczech i w Szwajcarii. 🚀 O projekcie: Szukamy doświadczonego Microsoft Data Engineera , który poprowadzi techniczne aspekty projektów i będzie kluczowym łącznikiem między IT a biznesem. Pracując z nami, będziesz mieć realny wpływ na sposób, w jaki przetwarzane są dane kluczowe dla biznesu i klientów. Jeśli masz doświadczenie w hurtowniach danych i lubisz wyzwania związane z presales oraz definiowaniem wymagań , dołącz do nas i współtwórz rozwiązania. 🎯 Z nami będziesz: Projektować, rozwijać i optymalizować hurtownie danych opartych na technologii Microsoft SQL Server Implementować i utrzymywać procesy ETL przy użyciu SSIS Tworzyć raporty i analizy z wykorzystaniem SSRS oraz modele wielowymiarowe i tabelaryczne w SSAS Integrować dane z różnych źródeł, w tym Oracle Optymalizować wydajności zapytań SQL oraz procesów przetwarzania danych Aktywnie współpracować z klientami w celu zbierania i definiowania wymagań biznesowych oraz ich przekładania na rozwiązania techniczne Prowadzić działania presales – przygotowywanie ofert, udział w spotkaniach z klientami, doradztwo w zakresie architektury danych Koordynować i nadzorować pracę zespołu developerskiego, pełnić rolę lidera technicznego 😎 Czekamy na Ciebie, jeśli: Masz m in. 5 lat doświadczenia w pracy z hurtowniami danych oraz rozwiązaniami Microsoft BI Bardzo dobra znasz SQL Server , w tym mechanizmów przechowywania i przetwarzania danych Posiadasz Doświadczenie w pracy z SSIS, SSRS, SSAS oraz umiejętność efektywnego wykorzystywania tych narzędzi. Pracujesz również z innymi źródłami danych, w szczególności Azure , Oracle Potrafisz prowadzić projekty i współpracować z biznesem – umiejętność definiowania wymagań, rekomendowania rozwiązań oraz prezentowania wyników. Masz doświadczenie w działaniach presales , tworzeniu ofert i doradztwie technologicznym. Możesz pochwalić się umiejętnością zarządzania zespołem oraz mentoringu młodszych członków zespołu. Znasz języka angielskiego na poziomie min. B2 🤝 Dowiedz się, jak skorzystasz, będąc w Onwelo: Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Potrzebujesz pracować zdalnie? Jesteśmy otwarci! Zaoszczędzisz czas na dojazdach – pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych i zewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,27000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,261,Data Engineer (Spark),SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. branży finansowej. Wykorzystywany stos technologiczny: Scala, Apache Spark, SQL/HQL, Airflow, Control-M, Jenkins, GitHub, Hive, Databricks, Azure, AWS S3, tworzenie kompleksowych procesów ETL z wykorzystaniem Spark/Scala – obejmujących transfer danych do/z Data Lake, walidacje techniczne oraz implementację logiki biznesowej, dokumentowanie tworzonych rozwiązań w narzędziach takich jak: JIRA, Confluence czy ALM, weryfikacja jakości dostarczanego rozwiązania – projektowanie i przeprowadzanie testów integracyjnych, zapewniających jego poprawne działanie i zgodność z innymi komponentami, praca w modelu hybrydowym: 1-2 razy w tygodniu praca z biura w Warszawie, stawka do 180zł/h przy B2B w zależności od doświadczenia. Ta oferta jest dla Ciebie, jeśli: masz co najmniej 7 lat doświadczenia w programowaniu, przez minimum 3 lata pracujesz z technologiami Big Data, najlepiej Spark i Scala, znasz podejście Agile/Scrum, korzystasz z narzędzi Continuous Integration – Git, GitHub, Jenkins, swobodnie posługujesz się SQL, komunikujesz się w języku angielskim na poziomie minimum B2, dodatkowym atutem będzie: doświadczenie z technologiami frontendowymi: React, TypeScript, Next.js, Qlik, umiejętność pisania skryptów w Bash, znajomość narzędzia Control-M, znajomość Docker, Kubernetes, AWS S3, Azure, AWS. Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 25200, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,30240,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,262,Tableau/BI Developer (in the EU),Andersen,"Andersen is seeking aTableau/BI Developerin the EUfor a major logistics project with a global UK-based transport leader. The role involves supporting data-related tasks for a mobile app focused on trip planning, ticket booking, and real-time transport information.","[{""min"": 10300, ""max"": 19400, ""type"": ""Gross per month - Permanent""}, {""min"": 10300, ""max"": 19400, ""type"": ""Net per month - B2B""}]",Data Engineering,10300,19400,Gross per month - Permanent
Full-time,Mid,B2B,Remote,264,Data Scientist,HR Contact,"Data Scientist ​ You'll takeend-to-end ownership of machine learning solutionsfrom concept to production. Join a team where data science fuels smarter e-commerce and marketing automation. Company: a European MarTech ecosystem of technology companies with a product portfolio includingAI-powered personalization engines, e-commerce platform and data management solutions. 📍 Poland, Remote. 🗃️​ B2B Contract. 🤖 What will you be doing? Owning full model lifecycle: from research, experimentation and prototyping to deployment and monitoring. Building and optimizing machine learning models using Python, e.g., customer lifetime value models, AI modules that support the recommendation engines and the use of a customer data platform. Designing data pipelines and integrating ML solutions into production (search personalization, clickstream analysis, email triggers). Improving and maintaining existing models to ensure performance, scalability, and robustness in production. Collaborating closely with engineering, product and business teams to turn complex data into actionable product features. Upholding high development standards by writing clean, maintainable code, conducting code reviews, and embracing DevOps practices like testing and CI/CD. Contributing to the continuous improvement of the team by mentoring junior colleagues and bringing new perspectives to modeling and methodology discussions. 🔧 What will help you get the job done? Experience inend-to-end data science projects (preferably in eCommerce, SaaS, MarTech space),witharound 5 years of experience. Strong background inPython. Experiencedeploying models in production environments. Experience with cloud infrastructure(Azure or AWS)would be an advantage Solid understanding of statistics, modeling, and software engineering practices. Analytical mindset with a passion forpractical impact, not just research. Excellent Englishand communication skills. Strongcommunication skills,with the ability to explain data-driven insights to diverse audiences. ✨ Nice to have: Experience with: Azure, DevOps, Docker, Kubernetes, Databricks, Cassandra, SQL. Experience with Delta Lake and Spark for handling large-scale data. Hands-on experience with deep learning (PyTorch, TensorFlow, MXNet). ✨ Why is it worth it? An ability to take charge of your own projects. Possibility to develop further in the MLOps space. Flexible B2B Contract Model. Private medical care (Luxmed or Medicover) & MultiSport package fully covered. This is a unique opportunity to be at the heart of product innovation, turning behavioral data into real business impact. Supportive company culture – flat structure, strong teamwork and openness to new ideas. Remote-first setup. Do you prefer slippers instead of shoes or maybe you are expecting Amazon delivery, no worries, we’ve got you covered. 👋 Meet Your Guide: Hello! I’m Olgaand I am responsible for this recruitment process. I will be happy to meet you! 🗺️ Recruitment stages: Online interview with Olga - we will talk in more details about the company, the role and your last projects : ) Online interview with a company - you will meet the team. Technical assignment - it is time for you to shine! Online interview - final assessment of your task and decision.","[{""min"": 19000, ""max"": 23000, ""type"": ""Net per month - B2B""}]",Data Science,19000,23000,Net per month - B2B
Full-time,Senior,B2B,Remote,268,Data Architect / Data Officer_L4,Experis Manpower Group,"Responsibilities: Building solutions that combine data from multiple sources to support our client's processes using the latest Azure technology stack. Supporting team members with technical issues, platform administration, and access rights. Understanding business targets, challenging choices and roadmaps, prioritizing needs based on technical capabilities, and designing target architectures. Acting as a data and cloud expert, sharing your knowledge with other architects, data engineers/data team members, business stakeholders, and platform experts. Developing data architecture with a solid understanding of data domain, data modeling, quality, and sensitive data protection topics. Documenting technical and architectural specifications and providing architectural advice in collaboration with business analysts. Act as an architect in data projects. Assist in creating Business Use Cases for several reporting projects. Define architecture of data management and advanced analytics solutions in the Azure cloud. Ensure solution compliance with client’s frameworks and standards. Develop documentation of system components following obligatory standards. Support IT team in technical implementation, testing, application deployment, and knowledge transfer to the IT support. Collaborate closely with business stakeholders to ensure the high quality of the final product. Code review and quality assurance Maintenance of Azure Data Relevant resources Requirements: Master’s degree in Computer Science or related field 5+ years of experience in Data Architecture & Engineering Experience with business intelligence, data analysis, and reporting in enterprise environments Experience in design, development, and implementation of solutions architecture on the Microsoft Azure platform Good understanding of data security topics, from networking layer to the end user interface Working experience with Azure Databricks (administration and data engineering) and Azure Data Factory Good understanding of Azure services (SQL database, administration, IAM, Data Lake) Experience with agile project delivery methodologies (Scrum, Kanban) SQL/Python programming skills Soft Skills: Fluent in English (C1) Hands-on mentality Self-managed Take end-to-end task ownership Excellent communication skills Ability to understand business stakeholders and the business landscape Ability to challenge decisions made by the platform or other architects Strong analytical, organizational, problem-solving, and time-management skills Comfortable working in a diverse, complex, and fast-changing landscape of data sources Proactive problem solver with innovative thinking and a strong team player Our offer: 100% remote work model Multisport card Private healthcare system Life insurance","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Architecture,180,220,Net per hour - B2B
Full-time,Senior,B2B,Remote,270,Senior and Mid BI Consultants,emagine Polska,"PROJECT INFORMATION: Industry: Marine infrastructure Client: from Denmark Remote work: yes Project language: English Business trips: no Project length: 12 months contract + option for prolongations Start: Flexible / at latest beginning of October 2025 Assignment type: B2B Remuneration: 160-200 PLN/h for a senior (depending on experience) Headcounts: 4 FTE: 100% Summary: The primary goal of this position is to support digital transformation efforts by migrating from a legacy data platform to a new cloud-based BI platform (from Databricks to Fabric), enabling enhanced data management and analytics capabilities. Responsibilities: Assist in the migration of data from a legacy platform to a new cloud-based BI platform. Implement large-scale enterprise data solutions and centralized cross-domain data models. Apply layered/medallion data architectures and dimensional modeling techniques. Utilize semantic modeling principles and create tabular models for data analysis. Ensure scalability, maintainability, and documentation of data models. Analyze business needs to align technical designs with requirements. Employ DevOps best practices in version control, CI/CD, and monitoring. Manage and execute tasks independently to meet project deadlines. Adopt agile methodologies to break down and track project work. Key Requirements: 5+ years of experience in data warehousing, BI, or data engineering. Experience in implementing large-scale enterprise data solutions. Strong understanding of layered and dimensional modeling techniques. Hands-on experience with semantic and tabular models. Proficiency in DevOps principles, including version control and CI/CD. Experience with Fabric, Power BI, (Py)spark, Azure DevOps, Git. Nice to Have: Familiarity with data normalization techniques. Experience with agile delivery methods. Understanding of best practices in data modeling and documentation. We offer: Long-term cooperation. Transparently built relations based on trust and fair play. Co-financed benefits: Medicover card, Multisport card.","[{""min"": 160, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Unclassified,160,200,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,271,Data Modeler 🧩,ITDS,"Join us, and craft the foundation for intelligent analytics! Kraków - based opportunity with hybrid work model (2 days/week in the office). As a Data Modeller, you will be working for our client, a leading global financial institution driving large-scale digital transformation and data strategy initiatives. You will be supporting a major data architecture project aimed at improving data quality, accessibility, and consistency across the organization. You will collaborate with cross-functional teams and lead the development of enterprise-level data models that support both operational efficiency and analytical insights. This role is central to implementing scalable, secure, and compliant data solutions that empower better decision-making and foster innovation across the business. Your main responsibilities: Leading the design and implementation of conceptual, logical, and physical data models Collaborating with stakeholders to gather and translate data requirements into scalable models Establishing and maintaining best practices and governance standards for data modelling Mentoring and training junior data modellers on tools, methodologies, and techniques Coordinating with data architects, engineers, and analysts to align data strategies Participating in architecture and design discussions to ensure consistency across platforms Developing and enforcing data quality and integrity standards across modelling initiatives Supporting the integration of data modelling solutions into enterprise systems Researching and recommending new modelling tools and technologies Ensuring compliance with data governance and protection regulations You're ideal for this role if you have: Demonstrated leadership experience in managing data modelling teams Deep expertise in data modelling for both operational and analytical systems Proven ability to translate complex business requirements into data models Excellent verbal and written communication skills for diverse audiences Strong consulting skills with the ability to advise cross-functional teams Experience in stakeholder management and navigating complex organizations Advanced problem-solving capabilities for data-related challenges Solid understanding of metadata and data lineage concepts Knowledge of data governance frameworks and data quality standards Proficiency with data modelling tools such as ER/Studio, ERwin, or similar It is a strong plus if you have: Hands-on experience with cloud data platforms such as AWS, Azure, or Google Cloud Familiarity with big data technologies like Hadoop or Spark Experience working with industry-standard models like BIAN, FSDM, or BDW Understanding of data architecture patterns in financial services Exposure to master data management (MDM) concepts and platforms We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #6913 📌 You can report violations in accordance with ITDS’s Whistleblower Procedure available here.","[{""min"": 23100, ""max"": 27720, ""type"": ""Net per month - B2B""}]",Data Science,23100,27720,Net per month - B2B
Full-time,Senior,B2B,Remote,274,DWH Developer (public/health),Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozwój i utrzymanie systemów informatycznych wspierających funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 5-letnie doświadczenie zawodowe na stanowisku Projektanta IT lub Starszego Programisty SQL Znajomość zasad i metodologii projektowania hurtowni danych i data martów analitycznych Doświadczenie w pracy z systemami relacyjnych baz danych PostgreSQL lub EDB Doświadczenie w projektowaniu i programowaniu przy wykorzystaniu języka PL/SQL Doświadczenie w tworzeniu modeli logicznych i fizycznych z wykorzystaniem narzędzia Enterprise Architect Doświadczenie w projektowaniu i programowaniu procesów ETL Znajomość standardów wymiany danych HL7 Znajomość narzędzi służących do orkiestracji procesów ETL Znajomość Airflow Znajomość Jira Doświadczenie w programowaniu w języku Python Dobra organizacja pracy własnej, orientacja na realizacje celów Umiejętności interpersonalne, w szczególności umiejętność planowania, definiowania, realizacji, oraz monitorowania i rozliczania celów Efektywna komunikacja, kreatywność, samodzielność, kultura osobista i proaktywność Zdolność adaptacji i elastyczność, otwartość na stały rozwój i gotowość uczenia się Mile widziane Doświadczenie projektowe w obszarze ochrony zdrowia Doświadczenie projektowe w obszarze Hurtownia Danych Znajomość słowników i rejestrów z obszaru zdrowia np.: ICD9, ICD10, OID, PESEL Certyfikat z obszaru zarządzania projektem metodą zwinną (np.. Agile PM lub równoważny) Certyfikat potwierdzający umiejętność programowania w języku pl/sql (np. PostgreSQL Certification lub równoważny) Certyfikat potwierdzający znajomość Apache Spark (np. Spark Developer Associate lub równoważny) Certyfikat potwierdzający wiedzę z zakresu administrowania EDB Certyfikat z obszaru administrowania środowiskiem Hadoop Kluczowe zadania Projektowanie hurtowni danych i data martów analitycznych Tworzenie modeli danych (logicznych i fizycznych) z użyciem Enterprise Architect Implementacja i optymalizacja procesów ETL z wykorzystaniem narzędzi orkiestracyjnych (np. Airflow) Programowanie w SQL, PL/SQL oraz Pythonie, głównie w środowisku PostgreSQL/EDB Praca ze standardami wymiany danych HL7 oraz narzędziami takimi jak Jira i Enterprise Architect Współpraca z zespołami projektowymi w zakresie analizy wymagań i dostarczania rozwiązań danych","[{""min"": 110, ""max"": 139, ""type"": ""Net per hour - B2B""}]",Data Engineering,110,139,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,275,Data Scientist/Analyst 🌟💻,ITDS,"Join Our Dynamic Team and Shape the Future of Digital Banking! Warsaw-based opportunity with hybrid work model (2 days in the office/month). As a Data Scientist/Analyst you will be working for our client, one of the leaders in online banking, on cutting-edge data-driven projects focused on areas such as process mining, churn analysis, fraud prevention, and NLP. You will play a key role in translating business problems into data models, uncovering insights, and developing impactful solutions. This role demands not only strong statistical and machine learning skills, but also the ability to understand business needs, communicate product vision, and collaborate with various stakeholders to drive data initiatives forward. Your main responsibilities: Build and validate machine learning models tailored to business needs Analyze and interpret complex data to extract actionable insights Collaborate with business stakeholders to define product vision and data strategy Identify, collect, and prepare internal and external data sources Engage in cross-functional teams to integrate data solutions into products Visualize and present findings to both technical and non-technical audiences Design scalable data workflows and pipelines using appropriate technologies Leverage statistical methods to support decision-making processes Participate in the continuous improvement of data infrastructure and tools Contribute to a culture of data-driven product development You're ideal for this role if you have: Strong foundation in machine learning concepts and algorithms Proficiency in SQL for data querying and manipulation Experience with Python and libraries such as NumPy, Pandas, SciPy, Matplotlib Familiarity with big data tools like Hadoop and Apache Spark Ability to translate business problems into analytical solutions Understanding of statistical analysis and data visualization techniques Strong communication and stakeholder management skills A product-oriented mindset with an engineering approach to problem solving Nice to have: Knowledge of DevOps principles and basic Linux administration skills Experience with Java or Scala for machine learning applications We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS’s Whistleblower Procedure available here . Ref. number 7005","[{""min"": 16800, ""max"": 19950, ""type"": ""Net per month - B2B""}]",Data Science,16800,19950,Net per month - B2B
Full-time,Mid,B2B,Remote,276,Data Modeler,emagine Polska,"Informacje o projekcie: Branża: finanse/pożyczki Lokalizacja: zdalnie Umowa: B2B Stawka: ~200 pln/h netto + VAT Długość projektu: długoterminowy Poszukujemy doświadczonego Data Engineera do zespołu Data Platform, który będzie modelować dane oraz implementować procesy ELT w chmurze. Obowiązki: Modelowanie struktur bazodanowych w podejściu DDD oraz tworzenie logicznych i fizycznych modeli danych. Data Mapping. Przygotowywanie warstwy Data Contracts (wymagań HD do systemów źródłowych pod merytoryczną płaszczyznę kontraktu na dane) na podstawie zamodelowanych uprzednio struktur dla poszczególnych domen danych. Współpraca w procesie ingerencji danych z systemów źródłowych. Implementacja modeli danych dla poszczególnych domen w Data Platform (warstwa Bronze, Silver i Gold) w podejściu ELT w środowisku Azure Databricks. Wymagania: Doświadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejętność implementacji procesów ELT. Mile widziane: Umiejętność tworzenia dokumentacji technicznej. Doświadczenie w mapowaniu danych ze źródłowych do docelowych struktur w DWH. Umiejętność interpretacji fizycznego/logicznego modelu danych (ERD, modele relacyjne) i synchronizacji struktur tychże modeli z wymaganiami świata analityki. Doświadczenie w podejściu do projektowania słowników i zarządzania nimi (masterdata). Znajomość narzędzia dbdiagram.io. Wiedza na temat zagadnień Data Quality, Data Lineage i zasad zarządzania danymi Znajomość narzędzi do zarządzania metadanymi (np. Azure Purview).","[{""min"": 160, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,160,200,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,277,Data Engineer,emagine Polska,"Project information: Industry: Insurance and IT services Rate: up to 160 zł/h netto + VAT Location: Warsaw (first 2-3 months of office visits once a week, then occasionally) Project language: Polish, English Summary The Data Engineer will be responsible for designing, building, and maintaining Data Hubs that integrate multiple data sources for efficient analytics and operational purposes, with a focus on real-time data processing. Main Responsibilities Data Hub Development – Design and implement scalable Data Hubs to support enterprise-wide data needs. Data Pipeline Engineering – Build and optimize ETL/ELT pipelines for efficient data ingestion, transformation, and storage. Logical Data Modeling – Structure Data Hubs to ensure efficient access patterns and support diverse use cases. Real-Time Analytics – Enable real-time data ingestion and updating models. Data Quality & Monitoring – Develop monitoring features to ensure high data reliability. Performance Optimization – Optimize data processing for large-scale datasets. Automation & CI/CD – Implement CI/CD pipelines for automating data workflows. Collaboration – Align data solutions with enterprise needs through teamwork. Monitoring & Maintenance – Continuously improve data infrastructure reliability. Agile Practices – Participate in Scrum/Agile methodologies. Documentation – Create and maintain clear documentation for data models and pipelines. Key Requirements Strong Python skills (or other relevant language) Experience with Azure Data Factory, ADLS, and Azure SQL Hands-on experience in building ETL/ELT pipelines Experience with real-time data processing Understanding of data preparation for AI/ML applications Experience in building data validation and monitoring features Proficiency in SQL for data transformation Familiarity with CI/CD and infrastructure-as-code principles Understanding data security and compliance best practices Proficient in English (B2 level minimum) Nice to Have Data Governance knowledge Experience with containerization technologies (Docker, Kubernetes/AKS) Agile collaboration experience Ability to produce high-quality technical documentation","[{""min"": 140, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,140,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,278,Senior Data Analyst,ERLI spółka akcyjna,"ERLI to jedna z czołowych platform marketplace w Polsce, która co miesiąc przyciąga ponad 9 milionów aktywnych kupujących. Współpracujemy z tysiącami sprzedawców, wspierając ich w maksymalizacji sprzedaży i rozwoju ich biznesów. Naszym celem jest dostarczanie szerokiej gamy produktów w konkurencyjnych cenach, zapewniając klientom najwyższy standard obsługi i satysfakcję z zakupów. Dołącz do ERLI jako Senior Data Analyst! Co będziesz robić? Jako Senior Data Analyst będziesz kluczową postacią w naszym zespole, odpowiedzialną za dogłębne zrozumienie zachowań użytkowników i optymalizację naszych produktów z focusem na udoskonalenia wyszukiwarki (search) i konwersji w ścieżkach zakupowych użytkowników (CVR). Twoje codzienne zadania będą obejmować: Prowadzenie wszechstronnych analiz ilościowych i jakościowych , koncentrując się na wyszukiwarce, ścieżkach użytkowników i innych kluczowych obszarach platformy. Wnioskowanie i formułowanie konkretnych rekomendacji na podstawie przeprowadzonych analiz, a następnie ich wdrażanie wraz z zespołem w celu poprawy funkcjonalności platformy i osiągania założonych metryk. Projektowanie i analizowanie eksperymentów A/B , w tym precyzyjne definiowanie kohort, KPI oraz metryk strażniczych, a także interpretowanie ich wyników. Aktywne wspieranie procesu projektowania i realizacji badań ilościowych . Tworzenie i optymalizowanie metryk jakościowych i ilościowych dla listingu oraz wyników wyszukiwania, aby zapewnić ich najwyższą efektywność. Wspieranie w definiowaniu i monitorowaniu kluczowych wskaźników efektywności (KPI) , które przekładają się na wyniki wyszukiwarki i konwersję na ścieżce użytkownika. Gromadzenie, monitorowanie i interpretowanie danych z różnorodnych źródeł i obszarów, takich jak performance marketing czy ścieżki konwersji . Bycie kluczowym wsparciem w przekładaniu danych ilościowych na identyfikację problemów użytkowników , co w efekcie będzie stanowić podstawę do aktywnego wsparcia w kreowaniu skutecznych rozwiązań. Jesteś idealnym kandydatem, jeśli: Jesteś w pełni samodzielnym analitykiem z silnym mindsetem produktowym , który doskonale odnajduje się w dynamicznym i zmiennym środowisku startupowym. Posiadasz minimum 5-letnie doświadczenie w pracy z dużymi zbiorami danych, wykorzystując narzędzia takie jak BigQuery, Tableau czy Snowflake . Znakomicie posługujesz się narzędziami takimi jak: Looker, Grafana, GA4, Firebase, BigQuery, Excel . Posiadasz bardzo dobrą znajomość metodyki eksperymentów A/B zarówno na poziomie teoretycznym, jak i praktycznym, ze szczególnym uwzględnieniem badań wyszukiwarek, a także potrafisz trafnie interpretować ich wyniki. Potrafisz samodzielnie wyciągać konkretne wnioski biznesowe z zachowań użytkowników i danych ilościowych, wspierając się w razie potrzeby danymi jakościowymi. Potrafisz odnaleźć się w gąszczu danych, narzędzi i procesów, wyciągając z nich nie tylko suche informacje, ale przede wszystkim wartościowe wnioski , które pomagają osiągać cele biznesowe. Nie boisz się dynamicznego i zmiennego środowiska , gdzie otwarta głowa i proaktywność w rozwiązywaniu problemów są fundamentem sukcesu. Dobrze znasz branżę e-commerce lub pasjonujesz się rozwojem wyszukiwarek oraz poprawą efektywności ścieżek użytkowników. Co oferujemy? Możliwość indywidualnego, nieograniczonego rozwoju biznesowego w dynamicznie rosnącej organizacji w branży e-commerce; Realny wpływ na rozwój naszej organizacji - Twoje pomysły się dla nas liczą! Dofinansowanie pakietu benefitów (karta sportowa, pakiet medyczny, ubezpieczenie grupowe); Różnorodne zadania i cele – wiesz, co masz robić, ale pozostawiamy Ci dużą swobodę działania i jesteśmy otwarci na Twoje pomysły; Zachęcamy do aplikowania!","[{""min"": 18000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,18000,25000,Net per month - B2B
Full-time,Senior,Permanent,Remote,279,Senior Consultant SAP CDC,INFOPLUS TECHNOLOGIES,"Key Responsibilities: • Architect and lead the implementation of secure, scalable CIAM solutions using SAP CDC, aligned with Coustomers digital strategy • Serve as a consultative partner to business and technology teams, translating business needs into secure, user-friendly identity experiences. • Drive the strategic integration of CIAM with digital platforms and applications, ensuring interoperability and performance across ecosystems. • Champion compliance and data privacy by designing solutions that meet global regulatory standards (GDPR, CCPA, etc.) and internal governance requirements. • Provide technical leadership and mentoring to developers and platform engineers, fostering a high-performing and knowledge-sharing team culture. • Oversee CIAM operations and continuous improvement, ensuring system reliability, security posture, and proactive incident resolution. • Engage in cross-functional program planning, contributing to timelines, resource strategies, and stakeholder alignment in an agile delivery environment. • Stay ahead of emerging technologies and trends in CIAM, identity protocols, and cybersecurity—proactively proposing enhancements to future-proof the platform. Your Profile: • 6+ years of experience in identity and access management, with a strong track record in delivering customer-centric solutions. • Deep hands-on expertise with SAP Customer Data Cloud (Gigya) and associated CIAM capabilities. • Strong understanding of identity standards and protocols such as SAML, OAuth 2.0, OpenID Connect, and their secure implementation. • Experience designing and managing privacy-first architectures, with knowledge of GDPR, CCPA, and industry data protection frameworks. • Backend experience with platforms such as Java, .NET, PHP, and front-end skills using JavaScript, HTML/CSS, REST APIs. • Proven ability to lead technical initiatives, influence architectural decisions, and collaborate effectively with both technical and business teams. • Professional certifications (SAP CDC, CIAM, CISSP, CISM, ITIL, etc.) are advantageous. • Excellent communication and stakeholder engagement skills—able to convey complex ideas in a clear and actionable way. • Agile working experience (Scrum, SAFe, etc.) and a mindset of continuous learning and adaptability.","[{""min"": 300000, ""max"": 325000, ""type"": ""Gross per year - Permanent""}]",Unclassified,300000,325000,Gross per year - Permanent
Full-time,Mid,B2B,Remote,281,DWH Developer,Connectis,"Wspólnie z naszym Partnerem,największą grupą energetyczną specjalizującą się w produkcji, dystrybucji i sprzedaży energii elektrycznej,poszukujemy osoby na stanowiskoProgramista Hurtowni Danych. Nasz Partner specjalizuje się w dostarczaniu i utrzymaniu nowoczesnych rozwiązań informatycznych, które wspierają kluczowe procesy operacyjne w sektorze energetycznym. Odpowiada on za projektowanie i rozwój systemów bilingowych, systemów klasy ERP, platform raportowych Business Intelligence oraz rozwiązań z zakresu cyberbezpieczeństwa. 👨‍💻 TWOJA ROLA: Współpraca z zespołem deweloperów hurtowni danych oraz analityków i kierowników projektów w implementacji. Udział w innowacyjnych projektachdotyczących rozwoju usługi oraz tworzenia. Opracowanie dokumentacji projektowej. Rozwój i utrzymanie hurtowni danych. Testowanie i wdrożenie rozwiązań. Wsparcie istniejących rozwiązań. 🔍CZEGO OCZEKUJEMY OD CIEBIE? Bardzo dobra znajomośćMS SQL- procedury, sql-e, indexy. Mile widziane podstawowe umiejętności optymalizacji zapytań/indeksów. Znajomość branży energetycznej lub rozwiązań billingowych potwierdzona co najmniej 2 letnim doświadczeniem. Podstawy optymalizacji procesówETL (SSIS). T-SQL- rozbudowany o funkcje analityczne. Dobra znajomośćSSIS. ZnajomośćMDM. ✨ OFERUJEMY: Uczestnictwo w spotkaniach integracyjnych oraz meetupach technologicznych, umożliwiających dzielenie się wiedzą i doświadczeniem. Szansę na rozwój zawodowy i osobisty w renomowanej firmie, która bierze aktywny udział w rewolucji branży energetycznej. Wsparcie dedykowanego opiekuna Connectis, który zawsze jest dostępny, by pomóc Ci w sprawach związanych z projektem. 5000 PLN za polecenie znajomego do naszych projektów. Szybki, zdalny proces. Pracę 100% zdalną. Dziękujemy za wszystkie zgłoszenia. Pragniemy poinformować że skontaktujemy się z wybranymi osobami. 12162/PZ","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,110,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,284,Senior Data Architect,N-iX,"N-iX is a software development service company that helps businesses across the globe develop successful software products. During 21 years on the market and by leveraging the capabilities of Eastern Europe talents the company has grown to 2000+ professionals with a broad portfolio of customers in the area of Fortune 500 companies as well as technological start-ups. N-iX has come a long way and increased its presence in nine countries - Poland, Ukraine, Romania, Bulgaria, Sweden, Malta, the UK, the US, and Colombia. The Data and Analytics practice, part of the Technology Office, is a team of high-end experts in data strategy, data governance, and data platforms, and contribute to shaping the future of data platforms for our customers. AsSenior Data Architect, you will play a crucial role in designing and overseeing the implementation of our strategic Databricks-based data and AI platforms. You will collaborate with data engineers and data scientists, define architecture standards, and ensure alignment across multiple business units. Your role will be pivotal in shaping the future state of our data infrastructure and driving innovative solutions within the automotive claims management domain. Key Responsibilities: Design scalable and robust data architectures using Databricks and cloud technologies (Azure/AWS) Oversee and guide the implementation of Databricks platforms across diverse business units Collaborate closely with data engineers, data scientists, and stakeholders to define architecture standards and practices Develop and enforce governance strategies, ensuring data quality, consistency, and security across platforms Lead strategic decisions on data ingestion, processing, storage, and analytics frameworks Evaluate and integrate new tools and technologies to enhance data processing capabilities Provide mentorship and guidance to engineering teams, ensuring architectural compliance and effective knowledge transfer Develop and maintain detailed architectural documentation. Requirements: 5+ years of experience as a Solution/Data Architect in complex enterprise environments Extensive expertise in designing and implementing Databricks platforms Strong experience in cloud architecture, preferably Azure or AWS Proficient in Apache Spark and big data technologies Advanced understanding of data modeling, data integration patterns, and data governance Solid background in relational databases (MS SQL preferred) and SQL proficiency Practical knowledge of data orchestration and CI/CD practices (Terraform, GitLab) Ability to articulate complex technical strategies to diverse stakeholders Strong leadership and mentorship capabilities Fluent English (B2 level or higher) Exceptional interpersonal and communication skills in an international team setting. Nice to have: Experience with Elasticsearch or vector databases Knowledge of containerization technologies (Docker, Kubernetes) Familiarity with dbt (data build tool) Willingness and ability to travel internationally twice a year for workshops and team alignment.","[{""min"": 20317, ""max"": 29553, ""type"": ""Net per month - B2B""}]",Data Architecture,20317,29553,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,285,👉 Senior GenAI Engineer (Future Opening),Xebia sp. z o.o.,"🟣About project: This role focuses on developing and deploying AI applications using tools such as Azure AI Studio, vector databases, and Retrieval-Augmented Generation (RAG) frameworks. You will collaborate closely with senior team members to deliver robust, high-quality solutions that drive innovation. 🟣You will be: designing, developing, and implementing AI solutions using Python, with a focus on LLMs, vision models, and generative AI technologies, building, testing, and optimizing RAG applications, leveraging effective prompt engineering techniques to improve model performance, integrating AI models with Azure AI Studio, SharePoint, and Azure Blob Storage for efficient deployment and data handling, utilizing vector databases and Agentic frameworks (e.g., LlamaIndex) to enhance the functionality and intelligence of AI systems, implementing event-driven architectures using tools like Event Hub and Kafka for real-time data processing and scalability, collaborating with the AI Lead and team members to troubleshoot issues, test models, and ensure successful deployment, writing clean, efficient, and well-documented code adhering to best practices and version control standards, staying current with emerging AI tools, frameworks, and technologies to continuously improve development processes and outcomes. 🟣Your profile: Bachelor’s degree in computer science, Data Science, Engineering, or a related field, 4+ years of hands-on experience in AI/ML development, with an emphasis on generative AI and related technologies, strong experience with Python, REST APIs, Git proven expertise in developing and deploying LLMs, vision models, vector databases, and RAG applications, strong proficiency in Azure AI Studio, Azure Blob Storage, Event Hub, Kafka, familiarity with Agentic frameworks and tools like LlamaIndex for advanced AI development, ability to thrive in a collaborative team environment while managing multiple tasks effectively, good verbal and written communication skills in English (min. B2). 🟣Nice to have: exposure to cloud fundamentals (Azure preferred) and containerization tools like Docker, experience with CI/CD pipelines for AI model deployment, understanding RESTful services and API integration. 🟣Recruitment Process: CVreview –HRcall –Interview(with Live-coding) –ClientInterview (with Live-coding) –Hiring ManagerInterview –Decision 🎁Benefits 🎁 ✍Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 23500, ""max"": 33500, ""type"": ""Net per month - B2B""}, {""min"": 18000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,23500,33500,Net per month - B2B
Full-time,Mid,B2B,Remote,287,Cloud Data Engineer with snowflake,Experis Manpower Group,"Requirements: at least 3 years of experience in Big Data or Cloud projects in the areas of processing and visualization of large and/or unstructured datasets (including at least 1 year of hands-on Snowflake experience) understanding of Snowflake's pricing model and cost optimization strategies for managing resources efficiently experience in designing and implementing data transformation pipelines natively with Snowflake or Service Partners familiarity with Snowflake’s security model practical knowledge of at least one Public Cloud platform in Storage, Compute (+Serverless), Networking and DevOps area supported by commercial project work experience at least basic knowledge of SQL and one of programming languages: Python/Scala/Java/bash very good command of English Tasks: design, develop, and maintain Snowflake data pipelines to support various business functions collaborate with cross-functional teams to understand data requirements and implement scalable solutions optimize data models and schemas for performance and efficiency ensure data integrity, quality, and security throughout the data lifecycle implement monitoring and alerting systems to proactively identify and address issues plan and execute migration from on-prem data warehouses to Snowflake develop AI, ML and Generative AI solution stay updated on Snowflake best practices and emerging technologies to drive continuous improvement Our offer: Remote work Multisport card Private healthcare system Life insurance","[{""min"": 150, ""max"": 175, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,175,Net per hour - B2B
Full-time,Senior,B2B,Remote,288,Software Engineer,Pretius,"WPretiusposzukujemySoftware Engineerado projektu dotyczącego rozwoju popularnej platformy eCommerce. Lokalizacja: zdalnie Wynagrodzenie: 140-170 pln netto/h O projekcie: Generowanie nazw produktów (również z użyciem AI), automatyczna ocena ich jakości i wyboru najlepszej nazwy dla produktu Weryfikacja poprawności numerów GTIN (EAN) oraz ich precyzyjne przyporządkowywanie do konkretnych produktów Automatyczne tłumaczenie nazw i parametrów produktów na wszystkie języki obsługiwane na platformie Allegro Oczekiwania: 5+ lat doświadczenia na podobnym stanowisku Doświadczenie w pracy w Kotlinie, wykorzystywaniu AI do projektowania, budowania i rozwijania rozwiązań Umiejętność efektywnej współpracy w zespole, proaktywność i samodzielność w działaniu Dbałość o wysoką jakość dostarczanych rozwiązań, z uwzględnieniem testowania i optymalizacji Stack: Kotlin/Spring, MongoDB, Apache Spark/Apache Beam, BigQuery Mile widziane: Prompt Engineering AI Co oferujemy w Pretius? Stawiamy na długofalowe relacje oparte na uczciwych zasadach i rzetelności Dofinansowanie karty sportowej Multisport i opieki zdrowotnej Medicover Możliwość pracy w nowoczesnym biurze Imprezy integracyjne, szkolenia wewnętrzne, konferencje, certyfikacje","[{""min"": 140, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,140,170,Net per hour - B2B
Full-time,Mid,B2B,Remote,289,SQL Developer,Britenet,"O projekcie Projekt realizowany dla instytucji odpowiedzialnej za rozwój i utrzymanie systemów informatycznych wspierających funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania Min. 3-letnie doświadczenie zawodowe na stanowisku Programisty SQL Doświadczenie projektowe związane z tworzeniem złożonych zapytań SQL, procedur oraz funkcji bazy danych Doświadczenie projektowe związane ze strojeniem i optymalizacją zapytań SQL Doświadczenie projektowe związane z hurtownią danych, procesami ETL Znajomość praktyk CI/CD Umiejętność pracy w systemie kontroli wersji GIT Doświadczenie w programowaniu w PL/pgSQL, PL/SQL Znajomość bazy danych PostgreSQL/EDB Znajomość JSON Znajomość zagadnień FDW/połączenia między różnymi bazami Doświadczenie w programowaniu w językach skryptowych w środowisku LINUX Doświadczenie w programowaniu w języku Python Dobra organizacja pracy i odpowiedzialność za powierzone zadania Komunikatywność, kreatywność, samodzielność, kultura osobista Dociekliwość w diagnozowaniu i rozwiązywaniu bieżących problemów Mile widziane Doświadczenie projektowe w obszarze ochrony zdrowia Doświadczenie projektowe w obszarze Hurtownia Danych Certyfikat z obszaru zarządzania projektem metodą zwinną (np.. Agile PM lub równoważny) Certyfikat potwierdzający umiejętność programowania w języku PL/SQL (np. PostgreSQL Certification lub równoważny) Certyfikat potwierdzający znajomość Apache Spark (np. Spark Developer Associate lub równoważny) Kluczowe zadania Tworzenie i rozwijanie złożonych zapytań SQL, procedur składowanych oraz funkcji bazy danych Optymalizacja i strojenie zapytań SQL w celu poprawy wydajności systemów Projektowanie i wdrażanie rozwiązań związanych z hurtownią danych oraz procesami ETL Praca z bazą danych PostgreSQL/EDB oraz implementacja rozwiązań z wykorzystaniem PL/pgSQL i PL/SQL Wykorzystywanie i zarządzanie danymi w formacie JSON oraz obsługa zagadnień związanych z FDW i połączeniami między różnymi bazami danych Programowanie w językach skryptowych w środowisku Linux oraz rozwijanie aplikacji i narzędzi w Pythonie Praca w systemie kontroli wersji GIT oraz stosowanie praktyk Continuous Integration/Continuous Deployment (CI/CD)","[{""min"": 80, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Database Administration,80,120,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,290,Data Engineer,Bayer Sp. z o.o.,"At Bayer we’re visionaries, driven to solve the world’s toughest challenges and striving for a world where ,Health for all, Hunger for none’ is no longer a dream, but a real possibility. We’re doing it with energy, curiosity and sheer dedication, always learning from unique perspectives of those around us, expanding our thinking, growing our capabilities and redefining ‘impossible’. There are so many reasons to join us. If you’re hungry to build a varied and meaningful career in a community of brilliant and diverse minds to make a real difference, there’s only one choice. Data Analysis and Synthesis Undertake data profiling and source system analysis Present clear insights to colleagues to support the end use of the data Data Development Process Design, build and test data products that are complex or large scale Build teams to complete data integration services Data Innovation Understand the impact on the organization of emerging trends in data tools, analysis techniques and data usage Data Integration Design Select and implement the appropriate technologies to deliver resilient, scalable and future-proofed data solutions and integration pipelines Data Modeling Produce relevant data models across multiple subject areas Explain which models to use for which purpose Understand industry-recognized data modelling patterns and standards, and when to apply them Compare and align different data models Metadata Management Design an appropriate metadata repository and present changes to existing metadata repositories Understand a range of tools for storing and working with metadata Provide oversight and advice to more inexperienced members of the team Problem Resolution Respond to problems in databases, data processes, data products and services as they occur Initiate actions, monitor services and identify trends to resolve problems Determine the appropriate remedy and assist with its implementation, and with preventative measures Programming and Build Use agreed standards and tools to design, code, test, correct and document moderate-to-complex programs and scripts from agreed specifications and subsequent iterations Collaborate with others to review specifications where appropriate Technical Understanding Understand the core technical concepts related to the role, and apply them with guidance Testing Review requirements and specifications, and define test conditions Identify issues and risks associated with work Analyze and report test activities and results WHO YOU ARE: Required: Bachelor’s or Master’s degree in Computer Science, Data Science, or a related field. 5+ years of experience in data engineering or related roles, with a strong understanding of data integration and modeling. Proficiency in programming languages such as Python, SQL, or Java, and experience with data processing frameworks (e.g., Apache Spark, Hadoop). Familiarity with data warehousing solutions and ETL (Extract, Transform, Load) processes. Experience with cloud data platforms (e.g., AWS, Azure, Google Cloud) and data storage solutions (e.g., relational databases, NoSQL databases). Strong analytical skills and the ability to present complex data insights clearly to stakeholders. Excellent problem-solving abilities and a proactive approach to identifying and resolving issues. Preferred: Experience with data visualization tools (e.g., Tableau, Power BI) and business intelligence practices. Knowledge of data governance frameworks and best practices. Familiarity with Agile methodologies and project management tools. You feel you do not meet all criteria we are looking for? That doesn’t mean you aren’t the right fit for the role. Apply with confidence, we value potential over perfection! WHAT DO WE OFFER: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360° Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure Increased tax-deductible costs for authors of copyrighted works VIP Medical Care Package (including Dental & Mental health) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English WORK LOCATION: WARSAW AL. JEROZOLIMSKIE 158 YOUR APPLICATION: Bayer welcomes applications from all individuals, regardless of race, national origin, gender, age, physical characteristics, social origin, disability, union membership, religion, family status, pregnancy, sexual orientation, gender identity, gender expression or any unlawful criterion under applicable law. We are committed to treating all applicants fairly and avoiding discrimination. Bayer is committed to providing access and reasonable accommodations in its application process for individuals with disabilities and encourages applicants with disabilities to request any needed accommodation(s) using the contact information below. Bayer offers the possibility of working in a hybrid model. We know how important work-life balance is, so our employees can work from home, from the office or combine both work environments. The possibilities of using the hybrid model are each time discussed with the manager.Bayer respects and applies the Whistleblower Act in Poland.","[{""min"": 15000, ""max"": 21000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15000,21000,Gross per month - Permanent
Full-time,Senior,Permanent,Remote,292,Staff Software Engineer,dotLinkers,"Salary: up to 37 500 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Staff Software Engineer focused on Structured Data Storage, you will lead the design and implementation of scalable, resilient cloud-native storage solutions using technologies like CosmosDB, Azure Parquet, ADLS, CockroachDB, TiDB, and Snowflake. Your work will enhance platform scalability, performance, and reliability, helping teams deliver greater value with less operational overhead.You’ll join the Infrastructure Services group, supporting core platform areas like compute, networking, and storage. The Structured Storage team provides reliable and cost-effective storage building blocks for product teams. Acting as a technical leader and mentor, you’ll drive architectural decisions and ensure collaboration across engineering teams to modernize and simplify data infrastructure. Job Responsibilities Lead the full software lifecycle and strategic implementation of modern cloud-native structured data storage technologies (such as CosmosDB, ADLS, Parquet, Snowflake, CockroachDB), aiming to enhance scalability, performance, and resilience across the data platform. Take hands-on responsibility for architecting and developing core structured storage services, ensuring high standards of reliability, security, and operational efficiency. Promote modern data design principles, storage strategies, and governance practices via design reviews, workshops, documentation, and knowledge-sharing initiatives. Serve as a trusted advisor to senior technical and product leadership (Directors, VPs, CTO), providing guidance on architectural decisions, technology trade-offs, and long-term platform direction. Act as technical owner and mentor for structured storage architecture, supporting multiple engineering teams and fostering technical excellence and collaboration. Evaluate new data storage technologies and drive their adoption where they align with business objectives and architectural evolution. Maintain and prioritize a roadmap for improvements and innovations in storage infrastructure, ensuring timely delivery through leadership and coordination. Detect and address scalability challenges, reliability risks, and performance limitations, driving cross-team initiatives to resolve them. Stay informed on trends in distributed databases, cloud storage architectures, and serverless data patterns to guide ongoing strategy development. Minimum Qualifications: 7+ years of experience in backend engineering, distributed systems, or data platform development. At least 4 years of practical experience designing and implementing cloud-native storage solutions on Azure, AWS, or GCP. Proven expertise in at least one large-scale data storage technology (e.g., CosmosDB, Snowflake, ADLS, Parquet). Strong understanding of data modeling, consistency models, data partitioning, and query optimization within distributed environments. Proficiency in C#, Java, Python, or similar programming languages. Experience developing production-grade APIs and integrating data solutions into enterprise systems. Preferred Qualifications: Familiarity with various structured storage models (relational, columnar, key-value, time-series) and associated technologies (e.g., Cassandra, MongoDB, Redis). Experience with infrastructure-as-code, container orchestration (Kubernetes), and serverless architectures for data management. Background in leading architectural and strategic platform initiatives at scale. Experience working within regulated industries (e.g., healthcare, finance, legal tech). Advanced degree in Computer Science or related discipline. Leadership Expectations: Define and drive technical strategy for structured data storage systems, ensuring adoption across engineering teams. Mentor senior and mid-level engineers, fostering high technical standards and continuous learning. Lead cross-functional initiatives requiring coordination between architecture, security, platform, and product teams. Represent structured storage topics in technical planning sessions, design reviews, and executive discussions. Cultivate a culture of ownership, innovation, and technical curiosity within the broader engineering community. Competencies and Skills: Technical Mastery: Expertise in cloud-native databases, storage design, and distributed systems scalability. System-Level Thinking: Ability to design holistic, optimized systems that balance performance, cost, and maintainability. Leadership & Influence: Comfortable guiding diverse engineering teams and aligning them to technical vision. Communication: Strong skills in articulating architectural strategies and technical recommendations to both engineers and leaders. Problem Solving: Skilled at analyzing and resolving complex scalability and performance challenges in cloud environments. Collaboration: Track record of building partnerships across platform, infrastructure, and application teams. Adaptability: Comfortable working in dynamic environments with shifting priorities and emerging technologies. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 32500, ""max"": 37500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,32500,37500,Gross per month - Permanent
Full-time,Senior,B2B,Remote,294,Senior Database Administrator,Link Group,"Job Summary: We are seeking a skilled and experiencedSenior Database Administrator (DBA)to join our IT team. In this role, you will be responsible for the installation, configuration, maintenance, and optimization of our database systems. You will collaborate closely with development teams to design and fine-tune database structures, support users, and ensure data security and integrity. Install, configure, and maintain database management systems (DBMS) to ensure high availability and optimal performance. Monitor database performance and proactively implement tuning and optimization measures. Ensure data integrity and security through robust access controls and best practices. Perform regular backups and manage recovery strategies to protect business-critical data. Collaborate with software developers to design, implement, and optimize database schemas and queries. Diagnose and resolve database issues, providing expert-level technical support to end-users and internal teams. Maintain accurate and up-to-date documentation for database architecture, configurations, and procedures. Stay current with new database technologies, tools, and industry trends to recommend and implement improvements. Bachelor’s degree in Computer Science, Information Technology, or a related field. Proven experience as a Database Administrator or similar role, ideally at a senior level. Strong proficiency with major database systems such as Oracle, SQL Server, or MySQL. Solid understanding of database design principles, data modeling, and performance tuning. Knowledge of backup, restore, and disaster recovery procedures. Excellent problem-solving skills and keen attention to detail. Strong communication and collaboration skills to work effectively within cross-functional teams. Experience with cloud-based database platforms (e.g., AWS, Azure). Familiarity with data warehousing concepts and ETL processes. Understanding of database security best practices and compliance requirements.","[{""min"": 100, ""max"": 135, ""type"": ""Net per hour - B2B""}]",Database Administration,100,135,Net per hour - B2B
Full-time,Senior,B2B,Remote,295,Senior Data Scientist (pharma),7N,"Senior Data Scientist (pharma) About the project For our client operating in thepharmaceutical industry,we are looking for aSenior Data Scientistto work on advanceddata scienceprojects related tomedical image analysis. Work Mode: 100% remote Research, develop and implementmedical image processingpipelines utilizing state of the artcomputer-vision-basedapproaches in the medical image analysis space. Implement and maintain local andcloud-based dataand computational environments and platforms to enable the work. Builddata pipelines(data curation, preparation, cleaning, and consolidation) as an integral part of data science activity. UtilizeAI/ML(Artificial Intelligence/Machine Learning) to develop complex methodologies and analyses, all around various medical imaging modalities. Perform informedsemi-automated quality assessmentsof the acquired imaging data and of the derived measures. Align with clinical expertson the requirements, constraints and deliverables of a project. PhD degreewith 3+ years’ relevant direct non-academic professional experience or PhD degree with strong post-doc experience inmedical image analysis Hands-on experience with deep learning for computer vision using frameworks likePyTorchandSklearn Strong practical knowledge in medical image analysis in multiple modalities -X-ray,US,MRI,CT,Digital Pathology Solid understanding of deep learning theory: CNNs,Transformers,RNN,GenAI Ability todevelop and validate algorithmsagainst clinical data Ability to do quickprototyping/proofs of conceptup to production ready models Ability toperform in-depth data analysisand present results to engineering and leadership teams Ongoing support from adedicatedagent, taking care of your project continuity, client contact, necessary formalities, work comfort and development, Consultant Development Program– advice on growth planning based on the latest trends and market needs in IT, including consultations withagents and growth mentors, Access to7N Learning & Development– a development and educational platform with webinars, a library of articles and industry reports, and regular invitations to one-time and recurring development events – technical, business, and lifestyle, Spectacular integration events, both for you (e.g.,annual Kick-Off trip, Christmas parties, or Summer Olympics sports events) and for your loved ones (e.g., family picnics, movie premieres), Professional developmentnot only during the project – you can get involved in knowledge transfer to others within the7N Servicesoffering directed at 7N clients, Relationships and access to the knowledge ofthe most experienced IT expertsin the market – the average professional tenure of our consultants in Poland is over 10 years, A complete benefits package, including funding for medical care, life insurance, sports cards for you and your loved ones, as well as discounts in stores in Poland and abroad. Constantly searching for projects, difficult rate negotiations, lack of development support – sounds familiar? At 7N, you gain not only stability of contracts but also the personal involvement of a dedicated agent who ensures your professional comfort and continuous access to development initiatives. Our mission is to provide stable and rewarding collaborations that drive your success as an IT expert and the success of our clients. We build long-lasting relationships based on Scandinavian values and 30 years of experience creating IT solutions for over 200 organizations.","[{""min"": 25200, ""max"": 31080, ""type"": ""Net per month - B2B""}]",Data Science,25200,31080,Net per month - B2B
Full-time,Mid,B2B,Remote,296,Mid Data Engineer (Python/Spark/Databricks),7N,"Nasz klient -jedna z największych firm na świecie zajmujących się analizą danych finansowych– poszukuje doświadczonych specjalistów do pracy nad nowoczesnym projektem transformacji danych. W ramach projektu będziesz odpowiadać zaprzekształcenie istniejących makr Excelowychw nowoczesne rozwiązania działające w środowiskuDatabricksprzy wykorzystaniuPythona i Apache Spark. Lokalizacja: 100% zdalnie Start: wrzesień/październik Min. 4-letnie doświadczenie jako Data / Python Engineer Bardzo dobra znajomośćPythonaiApache Spark Doświadczenie w pracy zDatabricks Umiejętność analizy i przekształcania makr Excelowych Dobra orientacja w zagadnieniach inżynierii danych Płynny j. angielski i doświadczenie we współpracy w międzynarodowych środowiskach (np. USA) Mile widziane: ZnajomośćSQL Doświadczenie w pracy z chmurą (AWS, Azure) ZnajomośćGitlub innych systemów kontroli wersji Analiza i zrozumienie logiki działania istniejących makr Excelowych Konwersja makr na kod w Pythonie (Databricks/Spark) Współpraca z inżynierami danych przy integracji kodu z pipeline’ami Optymalizacja kodu Spark i zapewnienie jego wydajności Dokumentowanie rozwiązań oraz udział w przeglądach kodu Stałe wsparcie osobistegoagenta, dbającego o Twoją ciągłość projektową, kontakt z klientem, niezbędne formalności, komfort pracy oraz rozwój Consultant Development Program– doradztwo w planowaniu kariery w oparciu o najnowsze trendy i potrzeby rynku IT, obejmującem.in.konsultacje z agentami i mentorami kariery Dostęp do7N Learning & Development– platformy rozwojowo-edukacyjnej z webinarami, biblioteką artykułów i raportów branżowych oraz regularnymi zaproszeniami na jednorazowe i cykliczne wydarzenia rozwojowe – techniczne, biznesowe oraz life-stylowe Spektakularne eventy integracyjne, zarówno dla Ciebie (np. corocznywyjazd Kick-Off, imprezy świąteczne czy sportowe Letnie Igrzyska), jak i dla Twoich bliskich (np. pikniki rodzinne, premiery filmowe) Rozwój zawodowynie tylko podczas projektu – możesz zaangażować się w przekazywanie wiedzy innym w ramachoferty 7N Serviceskierowanej do klientów 7N Relacje i dostęp do wiedzynajbardziej doświadczonych ekspertów IT na rynku– średni staż zawodowy naszego Konsultanta w Polsce to ponad 10 lat Pakiet benefitów zaplanowany od A do Z, czyli dofinansowanie do opieki medycznej, ubezpieczenia na życie, karty sportowej dla Ciebie i Twoich bliskich, a także zniżki do sklepów w Polsce i za granicą. Ciągłe poszukiwanie projektów, trudne negocjacje stawek, brak wsparcia w rozwoju – brzmi znajomo? W 7N zyskujesz nie tylko stabilność kontraktów, ale też zaangażowanie osobistego opiekuna dbającego o Twój komfort zawodowy i stały dostęp do inicjatyw rozwojowych. Naszym celem jest zapewnienie Ci stabilnej i komfortowej współpracy, która przyczyni się do sukcesu Twojego jako eksperta IT oraz naszych klientów. Budujemy trwałe relacje, bazując na skandynawskich wartościach i 30-letnim doświadczeniu w tworzeniu rozwiązań IT dla ponad 200 organizacji.","[{""min"": 21840, ""max"": 25200, ""type"": ""Net per month - B2B""}]",Data Engineering,21840,25200,Net per month - B2B
Full-time,Senior,B2B,Remote,297,Snowflake Engineer/ Architect,CRODU,"🌴 Forma pracy: długoterminowo, fulltime, 100% zdalnie 👈 ⏰ Start: ASAP👈 Cześć! 👋 Dla naszego klienta z USA poszukujemy Snowflake Engineera. Klient zajmuje się wsparciem firm w transformacjach chmurowych. Przekrój projektów i branż jest szeroki. Prace dotyczą działań w obszarachm.in. migracji, zbierania danych i optymalizacji rozwiązań. Klientowi zależy na długoterminowej współpracy. Prowadzone projekty są różnej długości, ale dzięki stałemu zapotrzebowaniu specjaliści zatrudnieni u klienta swobodnie przechodzą z zakończonego projektu do nowych tematów. Obecnie poszukiwani są kolejni specjaliści do projektu z wykorzystaniem Snowflake. Projekt dotyczy branży prawniczej. Klient ma legacy systemy i chcą zmigrować sie do Snowflake. Dodatkowo klient boryka się z jakością danych i procesów w pipelines, dlatego chcieliby zająć się również automatyzacją procesów (np. czyszczenia danych) i dopracowaniem struktury danych. Klient posiada lokalnie mały zespół, natomist widzi potrzebę rozszerzenia go o dodatkowych ludzi -> docelowo w planach jest powiększenie zespołu o 10 dodatkowych osób. Projekt prowadzony dla firmy z USA, ale wymagana jest praca jedynie z niewielką zakładką godzinową (od 10: 00 do 18: 00). Codzienne zadania: 📍 Projektowanie, rozwijanie i utrzymywanie zaawansowanej, skalowalnej i odpornej architektury danych oraz pipelines z wykorzystaniem Snowflake 📍 Budowanie i rozwijanie złożonych transformacji danych z użyciem dbt lub podobnych narzędzi 📍 Zapewnienie wysokiej jakości danych w pipeline'ach (kontrole i walidacje w celu wykrywania i korygowania nieprawidłowości) 📍Zbieranie danych do celów historycznych i audytowych (z naciskiem na spójność i dokładność zbieranych danych) 📍 Przestrzeganie data governance - ze szczególnym naciskiem na security i compliance 📍 Monitorowanie i optymalizacja działania pipeline’ów (przepustowość, opóźnienia) 📍 Identyfikowanie wąskich gardeł i nieefektywności w istniejących procesach oraz proponowanie rozwiązań usprawniających przepływ danych i czas przetwarzania 📍 Ścisła współpraca z zespołami data warehouse i analytics 📍 Dostarczanie dokumentacji technicznej i szkoleń dla innych członków zespołu w celu zapewnienia sprawnej obsługi i utrzymania systemów Wymagania: ⚡️ 7 lat doświadczenia w obszarach Data Engineering ⚡️ 4 lata doświadczenia w pracy z Snowflake ⚡️ Bardzo dobra znajomość SQL-a i umiejętność optymalizacji zapytań ⚡️ Bardzo dobra znajomość Python ⚡️ Dobra znajomość procesów ETL/ELT oraz narzędzi takich jak dbt ⚡️ Solidne zrozumienie zasad hurtowni danych i modelowania danych ⚡️ Doświadczenie z platformami chmurowymi takimi jak AWS lub Azure ⚡️ Angielski na poziomie umożliwiającym swobodną komunikację w zespole Mile widziane: ⚡️ Certyfikacja SnowPro lub SnowPro Advanced ⚡️ Certyfikacja DBT ⚡️ Znajomość Astronomer/Airflow Jak działamy i co oferujemy? 🎯 Stawiamy na otwartą komunikację zarówno w procesie rekrutacji jak i po zatrudnieniu - zależy nam na klarowności informacji dotyczących procesu i zatrudnienia 🎯 Do rekrutacji podchodzimy po ludzku, dlatego upraszczamy nasze procesy rekrutacyjne, żeby były możliwie jak najprostsze i przyjazne kandydatowi 🎯 Pracujemy w imię zasady ""remote first"", więc praca zdalna to u nas norma, a wyjazdy służbowe ograniczamy do minimum 🎯 Oferujemy prywatną opiekę medyczną (Medicover) oraz kartę Multisport dla kontraktorów","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Architecture,150,180,Net per hour - B2B
Full-time,Senior,B2B,Remote,298,Data Architect (public/health),Britenet,"Projekt dla instytucji odpowiedzialnej za rozwój i utrzymanie systemów informatycznych wspierających funkcjonowanie ochrony zdrowia w Polsce. Nasze oczekiwania: Minimum 5 lat doświadczenia na stanowisku Architekta IT, poparte projektowaniem architektur systemów zorientowanych na usługi (SOA, mikroserwisy), wielowarstwowych oraz o wysokiej wydajności i niezawodności. Praktyczne doświadczenie w roli Architekta w projektach o budżecie min. 1 mln zł brutto oraz w szacowaniu pracochłonności (min. 1000 roboczogodzin) i złożoności rozwiązań (ilość/wielkość komponentów). Umiejętność skalowania aplikacji (horyzontalne i wertykalne) oraz badania i oceny bezpieczeństwa systemów teleinformatycznych. Doświadczenie w architekturze rozwiązań data warehouse i narzędzi analitycznych, w tym znajomość projektowania hurtowni danych i data martów. Głęboka znajomość architektury, projektowania i integracji systemów IT, w tym wzorców projektowych i architektonicznych, serwerów aplikacyjnych oraz zagadnień DevOps i konteneryzacji. Biegłość w programowaniu SQL i Python, a także w projektowaniu i programowaniu procesów ETL. Praktyczna znajomość baz danych: PostgreSQL/EDB, MySQL, MongoDB, Oracle. Swobodna obsługa narzędzi: Enterprise Architect, MS Project, Jira. Doskonała organizacja pracy, zorientowanie na cel, umiejętności interpersonalne (planowanie, definiowanie, monitorowanie celów) oraz efektywna komunikacja. Kreatywność, samodzielność, wysoka kultura osobista, odporność na stres, proaktywność oraz otwartość na rozwój i nowe technologie. Mile widziane: Doświadczenie projektowe z obszaru Hurtowni Danych Doświadczenie projektowe w obszarze ochrony zdrowia Certyfikaty z obszaru zarządzania projektem metodą zwinną (np.. Agile PM lub równoważny); potwierdzający umiejętności z obszaru projektowania architektur rozwiązań IT (np.. TOGAF® EA Foundation lub równoważny); potwierdzający wiedzę z zakresu administrowania EDB (np. EDB Certification - PostgreSQL Essentials 15 lub równoważny); z obszaru administrowania środowiskiem Hadoop (np. ClouderaCertified Administrator for Hadoop (CCAH), Hortonworks Certified Apache Hadoop Administrator (HCAHA) lub równoważny) Kluczowe zadania: Projektowanie architektur systemów zorientowanych na usługi (SOA) i mikroserwisów Tworzenie wielowarstwowych systemów o wysokiej wydajności i niezawodności Skalowanie aplikacji zarówno horyzontalnie, jak i wertykalnie Zapewnienie spójności i integralności architektury systemów Aktywny udział w projektach IT, w tym w projektach o znacznym budżecie (minimum 1 mln zł brutto) Szacowanie pracochłonności zadań programistycznych i architektonicznych (na poziomie minimum 1000 roboczogodzin) Ocena złożoności aplikacji i rozwiązań (liczba i wielkość komponentów) Badanie i ocena bezpieczeństwa informacji w systemach teleinformatycznych Zapewnienie zgodności systemów z zasadami bezpieczeństwa IT Zastosowanie wzorców projektowych i architektonicznych w celu zapewnienia jakości i niezawodności Realizacja architektury rozwiązań zawierających hurtownie danych i narzędzia analityczne Projektowanie i programowanie procesów ETL Znajomość i wykorzystanie relacyjnych baz danych (PostgreSQL/EDB, MySQL, Oracle) oraz nierelacyjnych (MongoDB) Projektowanie i implementacja integracji systemów IT","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Architecture,150,180,Net per hour - B2B
Full-time,Mid,B2B,Remote,304,BI Developer with Snowflake,Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! 😊 Design, build, and develop data warehouses based on Snowflake; Create and optimize ETL/ELT processes using Matillion; Integrate data from various sources (databases, APIs, files); Develop reports and dashboards in Power BI for internal clients; Maintain, monitor, and further develop existing BI solutions; Collaborate with project teams and business stakeholders to gather and analyze requirements; Participate in data migration from legacy systems (e.g., Oracle, SSIS) to Snowflake; Ensure high-quality technical and business documentation; Implement best practices for data management, security, and version control; Actively participate in Agile team meetings (e.g., daily stand-ups, sprint planning, retrospectives); 2 years' experiencein a BI Developer role; At least 1 year of proven experience as a Snowflake developer,responsible for building ETL processes and creating tables and views within the Snowflake environment; Solid skills inPower BI; Eager to work as afull-stack BI Developer(both backend and frontend); Strong English skills (min. C1 level, daily communication with international clients); Proactive and creative- skills to drive improvements and engage with both technical and non-technical stakeholders; Strong documentation skills and a knack for business analysis. Experience withMatillion; Experience withOracle(we’re migrating to Snowflake, but legacy knowledge is a plus); Experience withSSAS/SSIS/SSRS; Familiarity withVisual Studio; Understanding ofversion control concepts(branching, merging, pushing; Git integrated with Matillion). Background in procurement, supply chain, or business data analysis related to orders and internal corporate stakeholders; Background inManaged Servicesdelivery models - you know how to take end-to-end ownership of BI solutions, ensuring their reliability, scalability, and alignment with client needs throughout the entire lifecycle; Experience working inAgileteams. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad – so far we've been in Cape Town, Are, and Barcelona). Fully remotework or in our office in Wrocław; B2B Contract: 120 – 140 PLN net/hour + VAT Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories.","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,305,AI Data Engineer,ITDS,"AI Data Engineer Join us, and build the future of AI-powered data systems! Kraków - based opportunity with hybrid work model (2 days/week in the office). As anAI Data Developeryou will be working for our client, a global financial institution currently investing in advanced AI-driven solutions to modernize its internal data ecosystems. You will play a key role in a pioneering project that focuses on creating agents to collect and synchronize data from multiple systems, powering Retrieval-Augmented Generation (RAG) pipelines for GenAI models. This is a technically challenging role that involves building scalable data pipelines, designing microservices, and leading a collaborative team of developers within a high-impact cloud-based infrastructure. Designing and implementing scalable data pipelines to support GenAI systems Creating and managing microservices that integrate into a broader AI solution architecture Leading a team of developers and AI specialists across various project phases Developing and maintaining RESTful APIs using FastAPI Managing data sourcing processes for RAG using Python and time-series/analytics databases Overseeing deployment workflows using tools like Ansible and Jenkins Ensuring system reliability and maintainability within a Unix/Linux environment Coordinating with stakeholders to ensure requirements are captured and implemented Writing shell scripts for task automation and process monitoring Documenting technical specifications and system architecture Proven experience developing in Python within Unix/Linux environments Hands-on experience with time-series or analytics databases such as Elasticsearch Proficiency in building RESTful APIs using FastAPI Experience with Azure Cloud infrastructure and services Background in building microservices architectures Strong knowledge of data pipelines for RAG or similar GenAI applications Familiarity with Git/GitHub and automated deployment tools (Ansible, Jenkins) Basic shell-scripting and process automation skills Solid understanding of software development lifecycle (SDLC) practices Ability to work independently with a proactive, team-oriented mindset Experience with Generative AI APIs and frameworks Understanding of big data modeling using both relational and non-relational techniques Familiarity with cloud-native design patterns Excellent communication and documentation skills Willingness to quickly adapt to evolving project requirements We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7227 You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere.","[{""min"": 21000, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Science,21000,31500,Net per month - B2B
Full-time,Senior,B2B,Hybrid,308,Data & Visualisation Specialist (Tableau),Antal Sp. z o.o.,"Data & Visualisation Specialist (Tableau) Location: hybrid, Cracow Contract Type: B2B Workload: Full-time Are you looking to make a real impact in a global financial project? We are working Data & Visualisation Specialist (Tableau) to join our Clients team. Company Description Our Client's team is dedicated to fostering a data-driven culture across the organization. Their strategy focuses on protecting the data through robust management policies, engaging colleagues with enhanced training opportunities, and building sustainable capabilities to unlock value for customers. Principal Accountabilities Developing data visualizations in the form of advanced dashboards and reports Understanding infrastructure requirements and best practices to support a Tableau deployment Data Source Management – ensuring data integrity, updating data as needed Managing Project site on Tableau Server Documenting processes, data sources and dashboards Analyze & drive data sharing best practices around user access Strong stakeholder management and influencing skills Essential Criteria Proficiency in Tableau 3+ years’ work experience in data analysis Ability to query and display large data sets while maximizing the performance of Tableau workbooks Working knowledge of Tableau administrator/architecture Ability to work collaboratively with cross-functional teams and stakeholders at all levels A high degree of mathematical competence Analytically minded Developing dashboards in QlikSense experience (nice to have) Major Challenges Manage self in a positive, engaging, thoughtful and constructive manner, from a role which often has a material influence on outcomes. Operate in a fast changing operating environment requiring quality decision making with often incomplete information. Maintaining well-regarded, thoughtful communications with principal stakeholders. Ensure data sharing compliance in a complex regulatory landscape What We Offer b2b contract and support of the Contractor Care Team Private Medical Care Cafeteria system Life insurance To learn more about Antal, please visit www.antal.pl","[{""min"": 30000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Unclassified,30000,33000,Net per month - B2B
Full-time,Mid,Permanent,Hybrid,310,Salesforce Consultant,Vaillant Group Business Services,"What we achieve together Are you ready to make a meaningful impact by joining our Service2Retention team? You will play a crucial role in enhancing service solutions for the Vaillant Group using Salesforce. In this position, you will have the opportunity to design and implement innovative solutions in collaboration with IT teams, customers, and product owners. Your work will span across Salesforce Service Cloud, Field Service (both Classic and LEX), and Marketing Cloud. Your responsibilities will also include writing and updating platform documentation, as well as conducting user trainings and demos to ensure everyone is equipped with the knowledge they need. You will manage stakeholder requirements and expectations through effective project management, ensuring that all parties are aligned and satisfied with the progress. Finally, you will contribute to a self-organizing and autonomous team, where your input and initiative are valued and encouraged. What makes us successful together Experience : You bring a minimum of 3 years of experience in designing salesforce or service processes. Ideally you designed and implemented such solutions in Salesforce Service Cloud or Field Service Know-how and skills: You have solid understanding of different types of service processes, such as orders, contracts, call center, invoicing, etc. Nice to have : You are familiar with Salesforce Marketing Cloud, including Journey Builder, Email Studio, and Automation Studio, experience in integrating Salesforce products and third-party applications, especially ERP systems. Personality : With your positive attitude and trustworthy personality, you can build strong stakeholder relationships. You are very well organized, have high standards for the quality of your work and communicate proactively and clearly. Language skills: You speak English fluently; Polish or German would be a plus. What makes us special Environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee Package of additional benefits: private medical care, multi-sport card. A fast growing, agile and very dynamic team that challenges established routines and helps transforming the Vaillant Group to a data informed business Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings","[{""min"": 17000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Unclassified,17000,20000,Gross per month - Permanent
Full-time,Mid,Mandate,Remote,314,IT Support & Systems Administrator (Salesforce CRM),BookingHost Sp. z o.o.,"Do zespołu IT poszukujemy osoby skrupulatnej, bystrej, komunikatywnej i samodzielnej, która wesprze nas w codziennej obsłudze i rozwoju systemów informatycznych, w tym rozwiązywaniu zgłoszeń naszych pracowników (ticketów). Administracji systemów informatycznych - Salesforce CRM i szeregiem innych aplikacji - zarządzaniem użytkownikami, konfiguracją kont, zakresem ich uprawnień itp.); Udziale w planowaniu procesów biznesowych w firmie, przygotowaniem pod nie wymagań technicznych oraz ich realizacji; Wdrażaniu nowych funkcjonalności m.in. automatyzacji z użyciem Salesforce Flows, Make; Integracji nowych narzędzi; Sprawowaniu pieczy nad porządkiem w modelu danych; Generowaniu raportów i analizie danych; Bieżącym wsparciu użytkowników. Dobra znajomość administracji CRM Salesforce (ewentualnie innego systemu CRM / ERP); Doświadczenie w administracji innych systemów i/lub wsparciu technicznym; Sprawne posługiwanie się arkuszami Google Sheets / Excel; Samodzielność i chęć uczenia się, także z wykorzystaniem dostępnych w sieci materiałów; Umiejętność pracy w dynamicznym środowisku, wielozadaniowość, szybkość adaptacji do zmian; Dobra organizacja pracy, umiejętność trafnej oceny priorytetów; Swoboda w komunikacji, wysoka kultura osobista; Dobra znajomość języka angielskiego (czytanie dokumentacji, aktywne uczestnictwo w spotkaniach). architekta rozwiązań Salesforce i/lub w programowaniu w APEX lub Java; z narzędziami takimi jak Front, Calendly, Make, Zapier, Google Workspace, Slack, wirtualna centralka, systemy ticketowe (np. Jira, ServiceNow); w nadzorowaniu projektów prowadzonych z zewnętrznymi podmiotami jak agencje deweloperskie; W integracji Salesforce z innymi systemami / aplikacjami. W pełni zdalną pracę, swobodną atmosfera w zespole, elastyczne godziny; Płatny urlop przy umowie B2B; Wsparcie w rozwoju umiejętności administrowania systemów; Realny wpływ na codzienną pracę firmy i rozwój struktury IT; Pracę w szybko rosnącej i dynamicznej firmie, aspirującej do pozycji lidera na rynku wynajmu mieszkań; Dostępne opcje Karty MultiSport i MultiLife. 📝Proces rekrutacji Proces rekrutacji składa się z dwóch etapów – oba odbywają się zdalnie: Rozmowa telefoniczna(ok. 15–30 minut) – krótkie poznanie się, omówienie doświadczenia i oczekiwań. Spotkanie online( ok. 60 minut) – rozmowa techniczna z członkiem zespołu IT, podczas której sprawdzimy wiedzę praktyczną i dopasowanie do roli.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 9000, ""type"": ""Gross per month - Mandate""}]",Database Administration,10000,13000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,315,IT Data Warehouse Analyst - Leader (banking),Primaris,"Forma współpracy: kontrakt B2B Tryb: gotowość do pracy w trybie hybrydowym z Wrocławia Aktualnie do jednego z projektów poszukujemy osoby na stanowiskoIT Data Warehouse Analyst - Leader, która posiada min.5lat komercyjnego doświadczenia. Dlatego jeśli szukasz pracy z: budową hurtowni danych w bankowości, SQL, Power BI/ Cognos, projektowaniem baz danych oraz analizą wymagańchętnie z Tobą porozmawiamy! Projekt który będziemy gotowi Ci zaproponować dotyczy: budowy hurtowni danych i systemu raportowego w bankowości. Doświadczenie w budowie i rozwoju hurtowni danych w sektorze bankowym – minimum 5 lat praktyki, ze zrozumieniem specyfiki danych i procesów w bankowości Bardzo dobra znajomość SQL – umiejętność efektywnego tworzenia zapytań oraz optymalizacji Znajomość narzędzia analityczno-raportowego: praktyczne doświadczenie z Power BI lub Cognos, w tym definicja struktur raportowych oraz tworzenie dashboardów i wizualizacji Doświadczenie w zbieraniu i analizie wymagań biznesowych na hurtownię danych i raportowanie – umiejętność współpracy z użytkownikami biznesowymi i tłumaczenia potrzeb na rozwiązania techniczne Doświadczenie w projektowaniu i tworzeniu struktur bazodanowych pod hurtownię danych – znajomość modelowania danych oraz pracy z dużymi zbiorami danych Znajomość języka angielskiego na poziomie B2 Umiejętność skutecznej współpracy i prezentacji wyników pracy Doświadczenie z platformą Teradata oraz narzędziami do integracji i zarządzania hurtowniami danych Znajomość narzędzi do przetwarzania danych, np. Ab Initio Praktyka w pracy w środowisku chmurowym (np. Azure, AWS, Google Cloud) i wdrażaniu rozwiązań data warehouse w chmurze Zakres obowiązków: Budowa i rozwój hurtowni danych w obszarze bankowości, z uwzględnieniem specyfiki i wymagań branży finansowej Projektowanie i tworzenie struktur bazodanowych dedykowanych hurtowniom danych, w tym modelowanie tabel faktów i wymiarów oraz relacji między nimi Zbieranie i analiza wymagań biznesowych dotyczących hurtowni danych i raportowania w ramach współpracy z różnymi działami firmy Implementacja oraz optymalizacja procesów ETL do zasilania hurtowni danych, dbanie o jakość i spójność danych Tworzenie i utrzymanie struktur raportowych oraz dashboardów w narzędziach analityczno-raportowych, przede wszystkim w Power BI lub IBM Cognos, zgodnie z wymaganiami użytkowników biznesowych Pisanie zaawansowanych zapytań SQL do ekstrakcji, transformacji i analizy danych Współpraca i komunikacja z zespołami projektowymi i biznesowymi, w tym zbieranie potrzeb i szkolenie użytkowników końcowych Utrzymywanie i rozwój środowisk hurtowni danych z opcjonalnym wykorzystaniem platform takich jak TeraData czy narzędzi Ab Initio(mile widziane) Dbanie o dokumentację techniczną i biznesową rozwiązań W naszej firmie będziesz mógł/mogła liczyć na: Pracę w organizacji z ugruntowaną pozycją rynkową Projekty, w których będziesz miał/miała wpływ na ich rozwój Współpracę z ciekawymi klientami biznesowymi z różnych branż (m.in.: finanse, bankowość, ubezpieczenia, healthcare, robotyzacja, energetyka, media), Permanentny mentoring zarówno techniczny jak i biznesowo-menedżerski, np. podczas naszych cyklicznych szkoleń (m.in. Git, Gitflow, Angular, Docker), czy wew. programów rozwojowych (Primaris x TechTalks, Primaris Leadership Academy) oraz zewnętrznych kursów.Już na etapie on-boardingu zapewniamy dostęp do naszych wewnętrznych szkoleń, cyklicznych spotkań, które serializujemy na Confluence oraz platformy e-learning Świetną atmosferę pracy, wśród zaangażowanych ludzi z pasją w płaskiej strukturze z prostymi procesami Kompleksowy pakiet benefitów skrojonych na miarę - prywatna opieka medyczna dla Ciebie oraz dla Twojej rodziny, Multisport dla Ciebie i os. towarzyszącej - Ty decydujesz, co wybierasz! Cały proces rekrutacyjny oraz onboarding prowadzony jest zdalnie. Proces rekrutacyjny składa się z: rozmowy telefonicznej z osobą z działu Rekrutacji & HR (do 30 min) zdalnej video rozmowy - weryfikacji techniczno-biznesowej z naszym specjalistą/specjalistką (60-90 min) zdalnego spotkania z liderem po stronie klienta projektu (30-60 min) finalnej decyzji dotyczącej oferty Primaris Services to ponad 250 ekspertów na pokładzie i 15 lat doświadczenia w branży IT na rynku polskim oraz zagranicznym.Realizujemy ambitne projekty o wysokiej złożoności z różnych obszarów -m.in. bankowości, ubezpieczeń, funduszy inwestycyjnych czy branży logistycznej (mamy ponad 40 aktywnych klientów!). Rośniemy w siłę oraz ciągle poszerzamy portfolio zarówno naszych usług jak i klientów. Zakres naszej działalności obejmuje budowę systemów od zera, ich rozwój oraz utrzymanie, wdrożenia produktowe, alokacje całych Zespołów, a także pojedynczych Ekspertów w strukturach Klienta. Ponadto od kilku lat działamy bardzo intensywnie jako złoty Partner firmy UiPath (obszar Robotic Process Automation) budując roboty i sprzedając licencje u naszych Klientów. Co miesiąc dołącza do nas 7 nowych osób! Wierzymy, że zgrany zespół i ludzie z pasją to klucz do naszego wspólnego sukcesu! Właśnie dlatego ciągle poszukujemy nowych, zdolnych osób, które zasilą nasze szeregi.","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,150,170,Net per hour - B2B
Full-time,Senior,B2B,Remote,316,Cloud Database Engineer,Altimetrik Poland,"Altimetrik Poland is a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. For our Fintech client from the UK, we are seeking a Cloud Database Engineer to join our growing team. The ideal candidate for this role will designs, manages, and optimises cloud-based database systems, ensuring scalability, performance, and data security while supporting business-critical applications. Responsibilities: Customer / client centricity: Ensures that cloud database services meet client expectations regarding reliability, performance, and security Actively works to troubleshoot and resolve client issues related to cloud database environments, ensuring minimal downtime and optimal performance Collaborates with clients to ensure that cloud database solutions align with their needs and support their operational goals Data & reporting: Gathers and analyses data on database performance metrics, resource usage, and incidents in cloud environments Produces regular reports that highlight database performance, cost management, and optimization opportunities within the cloud infrastructure Ensures data accuracy in reporting to support strategic decisions regarding cloud database management Industry knowledge: Stays up to date with cloud database technologies, trends, and best practices, including platforms like AWS RDS, Azure SQL Database, or Google Cloud SQL Applies knowledge of cloud database advancements to drive improvements in data management and retrieval efficiency Ensures the organization’s cloud database strategies are aligned with modern industry practices and innovations Planning & prioritisation: Manages the prioritisation of cloud database tasks, such as updates, patches, and incident responses Plans cloud database resource allocation effectively, ensuring high-priority issues are addressed promptly and efficiently Works with other teams to coordinate database deployments and updates, ensuring seamless transitions and minimal disruption to service Product Knowledge: Requires a detailed understanding of cloud-based data products and database technologies that support the company’s offerings Familiarity with database architecture, cloud solutions, and how they support the scalability and reliability of products. Strategic impact: Ensures that cloud database strategies and solutions align with the organization’s broader business objectives Recognizes how cloud database performance impacts organizational scalability and agility, contributing to the company's success Identifies ways to optimize cloud database resources to improve cost-efficiency and performance, ensuring alignment with strategic goals If you possess... Previously used SQL monitoring and performance tools. Demonstrable experience installing, scaling and maintaining MSSQL in HA architecture Securing and safeguarding sensitive data and maintaining referential integrity Architecting backup and disaster recovery strategies Performance tuning T-SQL for high throughput environments Experience with Installation, Performance monitoring, Security, and high availability and backups. Experience working with development life cycle models including CI/CD pipelines. … then we are looking for you! We work 100% remotely or from our hub in Kraków. 🔥We grow fast. 🤓We learn a lot. 🤹We prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,26000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,317,Data Analyst,Link Group,"Data Analyst Opis stanowiska: Poszukujemy analityka danych ze znajomością ETL oraz zasad zarządzania jakością danych. Kluczowe będzie doświadczenie w budowie przepływów danych (pipelines), zapewnianiu spójności danych oraz pracy z narzędziem Dataiku. Zakres obowiązków: Projektowanie, rozwój i utrzymanie procesów ETL zgodnych z potrzebami raportowymi i biznesowymi Analiza danych kredytowych i identyfikacja problemów jakościowych Monitorowanie jakości danych i wdrażanie zasad governance Pisanie specyfikacji technicznych, przeprowadzanie testów jednostkowych Wdrażanie usprawnień w istniejących przepływach danych Udział w spotkaniach projektowych i współpraca z zespołami IT/biznesowymi Wymagania: Doświadczenie z narzędziami ETL (Dataiku, Alteryx, Talend – preferowane Dataiku) Bardzo dobra znajomość SQL i pracy z dużymi zbiorami danych Umiejętność pracy w środowisku Agile Doświadczenie w tworzeniu specyfikacji technicznych i testów jednostkowych Podstawowa znajomość Pythona Znajomość zasad zarządzania jakością danych i kontroli wersji w Dataiku (GIT) Biegłość w języku angielskim (w mowie i piśmie) Otwartość na pracę w międzynarodowym środowisku Mile widziane: Znajomość procesów związanych z kredytem korporacyjnym Doświadczenie w pracy nad projektami międzyregionalnymi Umiejętność logicznego myślenia i modelowania danych Technologie: Dataiku 13.2.0 SQL Developer 24.3 360 Suite Jira","[{""min"": 100, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,100,140,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,318,Middle Data Analyst,N-iX,"#3683 We are seeking afor aMiddle Data Analystin Polandwith strong expertise in Tableau and a passion for turning data into actionable insights. In this role, you will lead the development of advanced visual analytics, collaborate with stakeholders to understand business needs, and help shape data-driven decision-making across the organization. Responsibilities: Develop and maintain dashboards and reports using Tableau to support key business functions Translate complex business questions into analytical solutions Work closely with stakeholders to gather requirements, understand KPIs, and deliver meaningful insights Use SQL to extract, transform, and analyze data from various sources Present findings in a clear, concise, and impactful way to both technical and non-technical audiences Continuously improve reporting performance and usability through iteration and feedback Requirements: 4+ years of experience in analytics, business intelligence, or data visualization roles Python skills for data processing (at least 1 year of experience) Advanced Tableau skills (dashboard development, calculated fields, LOD expressions, performance optimization) Strong SQL proficiency for data querying and preparation Proven ability to derive insights from data and explain them effectively Solid understanding of data modeling, joins, and ETL principles Strong analytical thinking and attention to detail Experience working with cross-functional teams and translating business requirements into data deliverables Must-Have: Expert-level Tableau development experience (at least 2+ years of experience) Hands-on experience creating scalable, interactive dashboards for enterprise use Advanced SQL skills applied to analytical/reporting contexts Ability to work independently and proactively in a data-driven environment Nice-to-Have: Experience with additional BI tools (Power BI, Looker, etc.) Knowledge of modern data warehouses (Snowflake, BigQuery) Familiarity with dbt or Python for data manipulation Exposure to A/B testing, experimentation frameworks, or product analytics Understanding of data visualization best practices and UX principles We offer: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits","[{""min"": 12560, ""max"": 16254, ""type"": ""Net per month - B2B""}, {""min"": 9974, ""max"": 12929, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,12560,16254,Net per month - B2B
Full-time,Mid,Permanent or B2B,Hybrid,322,Data Analyst,Remodevs,"Please note - hybrid from Warsaw (3 days from the office, 2 remotely). About us: We help businesses use AI and digital tools to work better and grow faster, especially in private capital markets. Our Core Platform improves workflows and gives useful insights with AI. Olympus Software is a fast, smart cloud system that grows with your needs. The Pantheon Suite offers flexible tools to manage and improve business performance. With over 10 years of experience, we know how to turn technology into real business value. About the Role We are seeking a Data Analyst to join our PaaS (Platform-as-a-Service) Customer Delivery team in Warsaw. In this role, you will be responsible for transforming data into actionable insights to support decision-making across product, operations, and strategy for customer-facing platform implementations. You will work closely with stakeholders across implementation projects to develop dashboards, conduct ad-hoc analyses, and ensure the integrity of platform usage and performance metrics. This role is ideal for someone who is curious, business-minded, and eager to make an impact through data. Key Responsibilities Partner with cross-functional teams to define key metrics and build dashboards and reports that provide visibility into business performance. Conduct deep-dive analyses to answer business questions, uncover trends, and identify opportunities for growth and optimization. Design and maintain scalable data models and SQL queries to support reporting and analytical needs. Collaborate with data engineers to ensure data availability, quality, and consistency across systems. Communicate findings and recommendations clearly to technical and non-technical audiences. Develop documentation and contribute to data literacy across the organization. Qualifications Required 2–4 years of experience in a data analyst or business intelligence role. Strong SQL skills and experience working with large datasets in a cloud data warehouse environment. Proficiency with BI tools such as Looker, Tableau, Power BI, or similar. Strong analytical thinking and attention to detail. Excellent communication and data storytelling skills. Preferred Experience working with dbt or similar modeling tools. Familiarity with A/B testing design and analysis. Some experience with Python, R, or another scripting language for data analysis. Exposure to product analytics platforms (e.g., Mixpanel, Amplitude).","[{""min"": 16623, ""max"": 18470, ""type"": ""Net per month - B2B""}, {""min"": 16623, ""max"": 18470, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,16623,18470,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,324,Data Engineer,Aristocrat Interactive,"Join Aristocrat in Bringing Joy to Life Through the Power of Play. Be part of our growing global team where people come first because they fuel our success. Here, it’s All About the Player and we create a world of its own for everyone, everywhere with premium casino and world-class digital and mobile products. Our value of Good Business, Good Citizen ensures that corporate growth and responsible gameplay go hand in hand to help our industry remain sustainable. Aristocrat offers a highly diverse, inclusive, and equitable culture as well as the professional tools and resources to ensure your Talent is Unleashed. We achieve success through Collective Brilliance. Individually, we are great, but together, we are unstoppable. Aristocrat enhances the player experience—and careers—with opportunities featuring meaningful challenges, strong advancement potential, and global exposure. Explore a Career with Our Team: Aristocrat Interactive We're looking for aData Engineerto help us build batch data pipelines, insightful dashboards, and data products that serve new gaming customers and internal teams. What You will Do Design and build scalable, maintainable data pipelines using Airflow and Snowflake Create data models and workflows that power dashboards, reports, and analytical products Collaborate with analysts, product teams, and game developers to deliver valuable insights Ensure data quality, documentation, and reliability across our data products Help shape the future of how data supports player engagement and product development What We're Looking For Hands-on experience with Airflow, Snowflake, and cloud data platforms (AWS/GCP) Strong SQL and Python skills for building and managing data workflows A knack for turning raw data into usable, impactful insights and visualizations Passion for games, analytics, and building cool stuff with data Very Independent and self-reliant What We offer High-level compensation on an employment contract and regular performance based salary and career development reviews; Medical insurance (health), employee assistance program; Multisport Card; English classes with native speakers, trainings, conferences participation; Referral program; Team buildings, corporate events. Why Aristocrat? Aristocrat is a world leader in gaming content and technology, and a top-tier publisher of free-to-play mobile games. Aristocrat has three operating business units, spanning regulated land-based gaming (Aristocrat Gaming), social casino (Product Madness), and regulated online real-money gaming (Aristocrat Interactive). Our team of over 7,300 employees worldwide is united by our company’s mission to bring joy to life through the power of play. We deliver great performance for our B2B customers and bring joy to the lives of the millions of people who love to play our casino and mobile games. And while we focus on fun, we never forget our responsibilities. We strive to lead the way in responsible gameplay, and to lift the bar in company governance, employee wellbeing and sustainability. We’re a diverse business united by shared values and an inspiring mission to bring joy to life through the power of play. Aristocrat is proud to be an equal opportunity employer. We celebrate diversity and do not discriminate based on gender, race, religion, color, national origin, sex, sexual orientation, age, veteran status, disability status, or any other applicable characteristics protected by law. Diversity and Inclusion are integral to our values of Talent Unleashed, Collective Brilliance, Good Business, Good Citizen, and It’s All About the Player.","[{""min"": 29553, ""max"": 33247, ""type"": ""Gross per month - Permanent""}]",Data Engineering,29553,33247,Gross per month - Permanent
Full-time,Mid,B2B,Hybrid,326,Database Developer,Connectis,"Wspólnie z naszym Partnerem Biznesowym zbranży ubezpieczeniowej,poszukujemy osoby na stanowiskoDatabase Developer, która dołączy do Zespołu Rozwoju Narzędzi HD/BI i będzie wspierać rozwój technologiczny jednej z największych Hurtowni Danych w Polsce. 💡 TWOJA ROLA: Budowa API integrującego różne technologie wykorzystywane w HD (SAS, Oracle, Office365, REST API itd.). Tworzenie narzędzi wspierających pracę developerów Hurtowni Danych (SAS / SAS Viya / Oracle / inne). Prowadzenie Proof of Concept (POC) dla nowych narzędzi (np. SAS Viya & Exadata w chmurze Azure). Integracja i wymiana danych pomiędzy systemami PZU z użyciemm.in. Kafki. Tworzenie narzędzi monitorujących aktywność użytkowników Hurtowni Danych. Udział w działaniach optymalizacyjnych dla przetwarzań HD – analiza i tuning. Budowa technicznych data martów i raportów BI (np. w SAS Viya, Power BI). Wsparcie projektów strategicznych w zakresie architektury i integracji danych. 🔍 CZEGO OCZEKUJEMY OD CIEBIE? Doświadczenie w projektowaniu rozwiązańHurtowni Danych i Business Intelligence. Praktyczna znajomość narzędziSASiSAS Viya (DI Studio, EG, SAS Studio). Doświadczenie z bazamiOracle oraz PL/SQL(mile widziane: Oracle Exadata). Znajomość narzędzi i procesówCI/CD (Git, Bitbucket, Bamboo, JIRA). Gotowość dopracy hybrydowej– część pracy w biurze w Warszawie. ✨ OFERUJEMY: Możliwość uczestnictwa w spotkaniach integracyjnych oraz meetupach technologicznych, gdzie będziesz mógł/mogła dzielić się wiedzą i doświadczeniem. Wsparcie dedykowanego opiekuna Connectis, który zawsze jest dostępny, by pomóc Ci w sprawach związanych z projektem. Pracę w modelu hybrydowym z biura w Warszawie (4 dni zdalnie, 1 dzień z biura / tyg.). Biuro w Warszawie przy wejściu do stacji metra– świetna lokalizacja i łatwy dojazd. Szybki, zdalny proces (rozmowa HR + rozmowa techniczna = decyzja). 5000 PLN za polecenie znajomego do naszych projektów. Sprzęt do pracy. Dziękujemy za wszystkie zgłoszenia. Pragniemy poinformować, że skontaktujemy się z wybranymi osobami. 12143/OM","[{""min"": 134, ""max"": 158, ""type"": ""Net per hour - B2B""}]",Data Engineering,134,158,Net per hour - B2B
Full-time,Senior,B2B,Remote,327,Greenplum Database Administrator,Calimala.ai,"Calimala.aiis seeking a seasonedGreenplum Database Administratorto join our dynamic team. With a focus on ensuring optimal performance, security, and resilience of our Greenplum clusters, you will take charge of database maintenance, backup & recovery, monitoring, and performance tuning. This role is perfect for a professional who is passionate about leveraging modern technologies in a fast-paced, innovative environment. Responsibilities: In this role, you will: Manage and perform daily administration tasks for Greenplum clusters while ensuring optimal system performance and data integrity. Conduct routine health checks, cleanup, vacuuming, and reindexing operations to maintain a robust database environment. Oversee backup and restore processes along with disaster recovery setups, ensuring data safety and business continuity. Monitor system performance and tune queries to achieve high responsiveness and efficiency. Collaborate with development teams to support schema changes, manage access controls, and enhance overall system functionality. Requirements: Applicants must have a minimum of 5 years of hands-on experience in managing and maintaining enterprise database systems, including proficiency in Greenplum DB and PostgreSQL. A deep understanding of Linux environments, SQL scripting, and performance tuning is essential, along with a proven track record in backup & recovery and disaster recovery implementations. Benefits: We offer a competitive compensation package, flexible work arrangements, and continuous learning opportunities in a collaborative, innovative work culture. Join us atCalimala.aiand contribute to cutting-edge projects in a company that values your expertise and drive for excellence. WhyCalimala.ai? Be part of a team that pushes the boundaries of technology and innovation. AtCalimala.ai, your expertise will be valued, and your professional growth nurtured. We are committed to supporting our staff and providing a stimulating work environment where you can thrive.","[{""min"": 15000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Database Administration,15000,25000,Net per month - B2B
Full-time,Mid,B2B,Remote,328,Data Integrations Engineer,dotLinkers,"Position: Data Integrations Engineer Working model: Fully remote Employment type: B2B Salary: 6 000 – 9 000 USD B2B/month We’re working with a fast-growing US tech company revolutionizing how organizations manage commissions, integrations, and operational data. As part of their Professional Services team, you’ll help bridge their product with third-party systems like Workday, Salesforce, and BigQuery—through both hands-on scripting and strategic integrations work. This is a backend-leaning role with some customer interaction, best suited for someone who’s solid with SQL, APIs, and light Python/JavaScript. If you’re interested in delivering real business impact – this one’s for you. Responsibilities: Build and maintain managed integrations between third-party platforms and the client’s system (e.g. Workday, Sage Intacct, MS Dynamics) Use tools likeWorkatoandSnowflaketo ingest and process customer data Handle support tickets related to integrations and act as the SME for integration architecture Work directly with enterprise clients and help design scalable data connection flows Support implementations and unblock client-facing solutions when product limitations arise Requirements: Strong command ofSQLandREST APIs(including JSON) Working knowledge ofPythonandJavaScript(for light scripting) Familiarity with CRM/HRIS systems likeSalesforce,Workday,Rippling, orMS Dynamics Comfort speaking with customers—ideally 1–2 years of client-facing or cross-functional experience Nice to have: experience withSSO/SCIM,ETL tools, orintegration platforms(e.g. Workato) What’s in it for you: Fully remote work Work directly with a senior Director in a flat, agile team structure Be part of a well-funded scale-up with IPO ambitions and an internal culture of excellence Working hours: Must have overlap withUS Eastern Time (EST)for at least4–5 days/week Full overlap expected for first 3 months (onboarding period) Flexibility increases post-ramp-up No formal on-call, but some flexibility is appreciated during launches","[{""min"": 22164, ""max"": 33247, ""type"": ""Net per month - B2B""}]",Data Engineering,22164,33247,Net per month - B2B
Full-time,Senior,B2B,Office,332,Oracle Database Administrator,Onwelo Sp. z o.o.,"Poznaj Onwelo: Onwelo to nowoczesna polska spółka technologiczna, która specjalizuje się w budowaniu innowacyjnych rozwiązań IT dla organizacji z szeregu sektorów na całym świecie. Główne obszary działalności Onwelo to: tworzenie oprogramowania, jego rozwój oraz utrzymanie, a także mocne wsparcie kompetencyjne. W krótkim czasie firma wdrożyła ponad 300 projektów w Europie i w USA, powiększyła zespół do 700 osób, a także otworzyła biura w siedmiu miastach Polski oraz oddziały w Stanach Zjednoczonych, Niemczech i w Szwajcarii. O projekcie: Jesteś ekspertem w zakresie baz danych Oracle? Dołącz do nas! Dla naszego szwajcarskiego klienta z branży bankowej poszukujemy Database Administratora , który będzie odpowiedzialny za zarządzanie, administrację oraz optymalizację baz danych Oracle. Jeśli jesteś otwarty/-a na wyjazd i pracę w Szwajcarii w siedzibie klienta , zostaw nam swoje CV! Z nami będziesz: Zarządzać i administrować bazami danych Oracle Monitorować wydajność i optymalizować bazy danych Implementować i zarządzać procedurami bezpieczeństwa Rozwiązywać problemy i zapewniać wsparcie techniczne Tworzyć dokumentację techniczną oraz utrzymywać kopie zapasowe Aktualizować i migrować bazy danych do nowszych wersji Oracle Przeprowadzać migrację do chmury Oracle (OCI) Czekamy na Ciebie, jeśli: Masz minimum 5 lat doświadczenia jako Oracle DBA Bardzo dobrze znasz SQL oraz PL/SQL Masz doświadczenie w administracji Oracle RAC, Data Guard, ASM Znasz narzędzia do monitorowania i optymalizacji (np. Oracle Enterprise Manager ) Masz wiedzę z zakresu systemów Unix/Linux Masz znajomość skryptowania w powłoce shell Znasz Infrastructure as Code (Ansible, Terraform, git) Masz doświadczenie z CI/CD Płynnie posługujesz się językiem angielskim na poziomie min. B2 Jesteś gotowy/wa do relokacji do Szwajcarii na czas kontraktu Dodatkowym atutem będzie: Doświadczenie jako MSSQL DBA Znajomość PostgreSQL DBA Doświadczenie z Exadata i RAC Dowiedz się, jak skorzystasz, będąc w Onwelo: Otrzymasz możliwość korzystania z elastycznych godzin pracy Możliwość pracy zdalnej Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych i zewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życi","[{""min"": 250, ""max"": 300, ""type"": ""Net per hour - B2B""}]",Database Administration,250,300,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,334,Senior Data Analyst,Jit Team,"Salary: 900 - 1100 PLN/day net+vat on B2B Work model: hybrid from Gdynia / Gdańsk / Warszawa / Łódź (2 days per week from the office) Why choose this offer? You can expect a flexible work organization The international work environment will give you the opportunity to interact with the English language on a daily basis Scandinavian organizational culture will provide you with work-life balance, you will gain time for additional training (financed by Jit) The Jit community will bring you a nice time during regular integration meetings Project We are looking for a data-driven professional to join a strategic KYC transformation initiative focused on improving data quality, governance, and regulatory compliance across the organization. This is a cross-functional role collaborating with business, IT, and architecture teams to implement a robust data model and reporting capabilities supporting a new Group-wide KYC system. Responsibilities you'll have Analyse existing KYC data to identify gaps, inconsistencies, and areas for improvement Work collaboratively with business and IT partners across business units to develop and implement data standards to ensure data accuracy & completeness Be a part of major projects for delivery of Group KYC Tool Partner with Data Architects and business SMEs to create and maintain a Business Information Model (BIM) for each data domain that is aligned across the enterprise Monitor data quality metrics and report on findings to management Investigate and resolve data quality issues, escalating complex problems as needed Provide consultative services to agile delivery teams and ensure data governance best practices Support the implementation of new KYC systems and processes, ensuring data migration and integration are seamless and accurate Utilize enterprise approved tools to develop reproducible data-flows that ultimately fulfill reporting requirements Implement testing and monitor quality of developed data-flows and reporting outputs, performing upgrades as identified and maintenance as required Work with multiple customers to analyze and visualize complex data in simple and accurate ways Document data definitions, data lineage, and data quality rules Participate in data governance/stewardship communities of practice Contribute to the development and maintenance of data governance policies and procedures Stay up-to-date with industry best practices and regulatory requirements related to KYC data management Proactively identify opportunities to improve data quality and efficiency Expected competences and knowledge Proven experience in data management, data quality, or a related role, preferably within the financial services industry Experience with advanced SQL queries and familiarity with another programming language, e.g. Python, R, Java and Scala Familiar with data taxonomy and capable to build small data models Experience with Power BI to produce reports Excellent stakeholder management is mandatory as you will be working closely with multiple business areas and key business leads across IT, Architecture, Financial Crime, Compliance, Legal - and the Business in order to implement changes Experience within financial crime (AML/CTF, KYC, Transaction Monitoring and Sanctions), working agile and experience with SAFe is an advantage Professional, organised, and structured in approach and work produced to a high quality of standard, and you always strive to find the solution that is best for the bank Excellent analytical and problem-solving skills, strong attention to detail and a commitment to data accuracy Ability to communicate effectively with both technical and non-technical audiences Experience with data governance frameworks and methodologies is a plus Experience with data migration and integration projects is a plus Ability to work independently and as part of a team Fluency in English is a requirement (speaking and writing) Technologies you'll work with SQL Python Power BI Client – why choose this particular client from the Jit portfolio? Jit Team has had an over-decade-long relationship with the leading financial group in the Nordic countries, and we are privileged to be our client's premier partner in Poland. At present, over 200 Jit personnel are engaged in the completion of more than 60 projects for this Norwegian major provider of financial services with a global presence and a strong focus on modern technology. Our customer's work atmosphere is epitomized by the Scandinavian culture , which is conducive to people who place emphasis on work-life balance and feedback culture . Furthermore, all projects are executed in international teams, giving constant exposure to the English language. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 18900, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,18900,23100,Net per month - B2B
Full-time,Senior,Permanent,Remote,335,Senior Azure Data Engineer,Link Group,"Data Engineer – Azure 📍 100% remote | 🕒 Full-time | 🌍 International environment We are looking for an experienced Data Engineer with strong expertise in the Azure ecosystem to join a dynamic data team delivering scalable and high-performance data solutions. You’ll play a key role in designing, building, and optimizing modern data pipelines and data lake architectures using cutting-edge cloud technologies. 🔧 Key Responsibilities: Design and develop robust and efficient data pipelines using Azure Databricks, Spark, and PySpark Work with Delta Lake architecture to manage structured and semi-structured data Perform data modeling, transformation, and performance tuning for large datasets Build and manage Azure Data Factory pipelines and Azure Functions for orchestrating workflows Integrate various data formats such as Parquet, Avro, and JSON Collaborate with cross-functional teams to understand data requirements and deliver optimal solutions Use Git for version control and manage code in a collaborative environment Write efficient Python and SQL code for data processing and querying Ensure data quality, consistency, and reliability across the platform ✅ Core Requirements: Solid hands-on experience in Azure Databricks, Spark, and PySpark Deep knowledge of Delta Lake and modern data lakehouse architectures Proficiency in data modeling and performance optimization techniques Experience with ADF (Azure Data Factory) and Azure Functions Strong skills in Python, SQL, and data serialization formats (Parquet, Avro, JSON) Familiarity with version control systems, especially Git Ability to work independently in a fully remote, distributed team Good communication skills in English","[{""min"": 18000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,25000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,337,Data Analyst with Tableau,Altimetrik Poland,"2-3 day per week you need to be available until 9: 00 PM for meetings with the US team. Altimetrik Polandis a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are seeking a skilledData Analyst with TableauforAirbnb-our customer, an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. Together with them, we are building a world-class payments platform that moves billions of dollars, in 191 countries, with 75 currencies, through a complex ecosystem of payments partners. They are also reinventing how to serve users to improve performance, scalability and extensibility. Responsibilities: Lead data visualization strategy for Policy, maintaining existing dashboards and developing new self-service resources. Optimize for bringing relevant data to the Policy team and bringing data created by the Policy team to cross-functional partners. Build and manage the primary Policy metrics and insights dataset and visualizations. Aggregate critical metrics across lobbying efforts, grassroots advocacy, partnerships, social and advertising campaigns, news articles, research, and regulatory compliance efforts. Architect this data to be flexibly leveraged at many geographic levels, including government administrative boundaries, custom shapes, and business geographic definitions. Partner closely with the Economics and Research Data Science team to ensure the Policy team is consuming all available data and that it meets the highest quality standards. Elevate the legitimacy of Policy Comms data assets to pass our rigorous internal data certification process. . Leverage Minerva, Airbnb’s in house metrics management tool, to make our data accessible to other data practitioners. Develop experimentation and attribution framework for political advocacy and regulatory compliance marketing initiatives. Support ad hoc analytics needs to support Policy, including supply composition, economic impact, growth and regulatory compliance, etc. Build relationships with local and regional Policy Comms teams around the world to support their local data and analytics needs. Partner with the Comms Data Strategy Taskforce. And if you possess.. 7+ years experience in business intelligence, data analytics, or data science. Expert in SQL and Python languages. Expertise in building data visualization, preferably in Tableau. Experience working with complex geographic data. Experience with knowledge graphs is a plus. Talent for breaking down complex technical concepts into common language and acting as a bridge between technical and departmental stakeholders. Experience working with complex and big data systems across a multitude of relationships and metrics. Ability to apply a creative and nuanced perspective to look beyond common data indicators in order to meet business goals. Expertise designing and running marketing experiments. Ability to self-serve and take the initiative to find answers to technical questions. Experience building and implementing machine learning models is a plus. Experience in survey population sampling and survey response analysis is a plus. Expertise in R, Java, REACT is a plus. … then we are looking for you! We work 100% remotely or from our hub inKraków. 🔥We grow fast. 🤓We learn a lot. 🤹We prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 20000, ""max"": 24300, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20000,24300,Net per month - B2B
Full-time,Mid,B2B,Remote,340,Data Engineer (Azure),SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. branży automotive. Wykorzystywany stos technologiczny: Azure, Databricks, PySpark, Azure Data Factory, Azure Synapse, Power BI, Spark, Python, MongoDB, Azure Functions, SQL Server, rozwój nowych funkcjonalności oraz utrzymanie aplikacji, wsparcie użytkowników wewnętrznych oraz procesów biznesowych poprzez rozwiązywanie problemów związanych z działaniem systemów i jakością danych, zapewnienie wydajnego i niezawodnego działania systemów, stawka do 140zł/h przy B2B w zależności od doświadczenia. Ta oferta jest dla Ciebie, jeśli: masz minimum 3 lata doświadczenia w pracy z danymi i inżynierią oprogramowania, posiadasz tytuł magistra z informatyki, inżynierii danych lub pokrewnej dziedziny, pracujesz z usługami Azure, takimi jak Azure Data Factory, Azure Synapse, Azure Functions, masz praktyczne doświadczenie z Databricks i PySpark, bardzo dobrze znasz bazy danych (np. SQL Server, Netezza, MongoDB), znasz narzędzia i technologie: Power BI, Spark, Python, biegle komunikujesz się w języku angielskim (min. B2). Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 21840, ""max"": 23520, ""type"": ""Net per month - B2B""}]",Data Engineering,21840,23520,Net per month - B2B
Full-time,Senior,B2B,Hybrid,341,Data Engineer,Antal Sp. z o.o.,"Join Us as a Data Engineer Location: Kraków, Poland (Hybrid – 5 days/month in office) Industry: FinTech / Data / Cloud / AI Are you passionate about working with impactful data in a modern tech stack? Want to help build solutions that directly protect millions of people and institutions from financial crime? This is your opportunity. We're part of Risk & Compliance Technology , a global team designing and delivering innovative solutions to protect the bank and its customers from financial crime, sanctions risk, identity threats, unauthorized trading, and regulatory breaches. Our mission: empower global growth through intelligent, data-driven risk management. Right now, we’re building a secure, scalable platform powered by Generative AI to automatically generate concise, accurate insights—helping investigators assess risk faster, spot suspicious patterns, and act quickly. You’ll be at the heart of this transformation. Who We’re Looking For We’re seeking a Data Engineer to join our FinCrime IT team , working across regions and disciplines. You’ll collaborate with developers, architects, and business stakeholders to shape a solution that makes a difference. This is a hands-on role in a forward-thinking, agile environment. We value curiosity, ownership, and a problem-solving mindset. What You’ll Do Design, build, and maintain components of a cutting-edge investigation support platform Design, build, and maintain components of a cutting-edge investigation support platform Take part in the entire development lifecycle: design, implementation, testing, and deployment Take part in the entire development lifecycle: design, implementation, testing, and deployment Work closely with cross-functional and international teams—including architecture, security, and compliance Work closely with cross-functional and international teams—including architecture, security, and compliance Build scalable, cloud-native processes that empower intelligent decision-making Build scalable, cloud-native processes that empower intelligent decision-making What You Bring Experience: Proven experience with data analysis and data engineering (Python, SQL) Proven experience with data analysis and data engineering (Python, SQL) End-to-end development lifecycle experience in large-scale IT projects End-to-end development lifecycle experience in large-scale IT projects Background in financial services or compliance-related technology (preferred) Background in financial services or compliance-related technology (preferred) Experience building and deploying solutions in a cloud environment (ideally Google Cloud) Experience building and deploying solutions in a cloud environment (ideally Google Cloud) Technical Skills: Languages & Tools : Python, SQL, Bash Languages & Tools : Python, SQL, Bash Cloud : Google Cloud Platform (BigQuery, Dataproc, Airflow) Cloud : Google Cloud Platform (BigQuery, Dataproc, Airflow) Containers & Infra : Docker, Kubernetes, GKE, Cloud Run Containers & Infra : Docker, Kubernetes, GKE, Cloud Run Security : IAM, roles, service accounts, secure development practices Security : IAM, roles, service accounts, secure development practices DevOps : Terraform, Jenkins, Ansible, Nexus DevOps : Terraform, Jenkins, Ansible, Nexus OS : Linux / Unix OS : Linux / Unix Agile methodology knowledge (Scrum, Jira, Confluence) Agile methodology knowledge (Scrum, Jira, Confluence) Soft Skills: Strong communication skills, with the ability to explain complex ideas clearly Strong communication skills, with the ability to explain complex ideas clearly Collaborative spirit and openness to working across time zones Collaborative spirit and openness to working across time zones Eagerness to learn and adapt in a fast-changing environment Eagerness to learn and adapt in a fast-changing environment Accountability and drive for quality Accountability and drive for quality Why Join Us? Make a tangible impact by fighting financial crime at a global scale Make a tangible impact by fighting financial crime at a global scale Work with modern tech: AI, GCP, containers, and automation tools Work with modern tech: AI, GCP, containers, and automation tools Collaborate with a global team of experienced professionals Collaborate with a global team of experienced professionals Be part of a purpose-driven organization that invests in innovation and ethical tech Be part of a purpose-driven organization that invests in innovation and ethical tech Ready to Apply? Help us protect the financial world through smarter technology. Apply now and be part of something meaningful. To learn more about Antal, please visit www.antal.pl","[{""min"": 170, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,220,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,344,Senior Power Platform Developer,Onwelo Sp. z o.o.,"Poznaj Onwelo: Onwelo to nowoczesna polska spółka technologiczna, która specjalizuje się w budowaniu innowacyjnych rozwiązań IT dla organizacji z szeregu sektorów na całym świecie. Główne obszary działalności Onwelo to: tworzenie oprogramowania, jego rozwój oraz utrzymanie, a także mocne wsparcie kompetencyjne. W krótkim czasie firma wdrożyła ponad 300 projektów w Europie i w USA, powiększyła zespół do kilkaset osób, a także posiada biura w sześciu miastach Polski oraz oddziały w Stanach Zjednoczonych, Niemczech i w Szwajcarii. O projekcie: Projekt dotyczy rozwoju i implementacji rozwiązań opartych na Microsoft Power Platform oraz usługach Azure. Celem jest stworzenie zintegrowanego środowiska wspierającego procesy wewnętrzne klienta, z naciskiem na automatyzację, raportowanie i poprawę efektywności operacyjnej. Z nami będziesz: Tworzyć rozwiązania w Power Platform Pracować w środowisku Azure Projektować architekturę i szacować czas realizacji Współpracować z biznesem Doradzać technologicznie i wspierać rozwój zespołu Czekamy na Ciebie, jeśli: Masz min. 3 lata doświadczenia w pracy z Power Platform Znasz Power BI, Power Apps, Power Automate, Copilot Umiesz analizować wymagania biznesowe Czujesz odpowiedzialność za realizowane zadania Mile widziane: JavaScript/TypeScript, C#, React) Dowiedz się, jak skorzystasz, będąc w Onwelo: Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Potrzebujesz pracować hybrydowo? Możemy zaproponować 4 dni pracy zdalnej i 1 dzień w tygodniu pracy z biura w Warszawie Uzyskasz dostęp do szkoleń wewnętrznych i zewnętrznych Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 800, ""max"": 1100, ""type"": ""Net per day - B2B""}]",Data Engineering,800,1100,Net per day - B2B
Full-time,Senior,B2B,Remote,345,Implementation Developer (more technical background),Ligo Headhunters,"Implementation Specialist – Remote | B2B | 25,000–28,000 PLN net/month I am currently recruiting for anImplementation Specialistposition for a US company.We are looking for someone withquick availability– a 3-month notice period is out of the question.We’re looking for someonemore technical than analytical. Location: Fully Remote Salary: 25,000–28,000 PLN net/month (B2B)We are flexible with salary for the right candidate Form of employment: B2B (Note: Since the client is based in the US, the0% VAT rateapplies under reverse charge rules.) Recruitment process: Two interviews via Zoom Vacation: 21 paid vacation days Company type: US Startup Blaze is building ano-code app building platform.We are looking for a person who will work directly with our customers to gather requirements and build apps for them using our platform. This roledoesn't involve writing any code, but requires the ability tothink like a developerand understand basic principles of software design. The role also involves customer interaction, soEnglish communication skills are essential. The successful candidates come from various backgrounds — typically: Ajunior developer Or afreelancerused to interacting with customers You don’t need to be a strong developer in any specific technology or have years of experience.Youmust be comfortable working with pre-made componentsand focusing onhigh-level app building— not coding. ExcellentEnglish communicationskills Goodeye for design Knowledge ofSQLand principles ofrelational databases Familiarity withdatabase schema design Fullyremote position Flexible working hours Very competitive salary Work in aninternational team Use ofinnovative tools and technologies We’re flexible, but: We expectfull-time commitment(no other daytime jobs or demanding side gigs).If there’s a deadline – we count on your extra effort. You should be able to: Schedule meetings with customers duringtheir working hours Attend team meetings with the US (usually around5–6 PM Polish time) Examples of working schedules within the team: A few hours in the morning + a few in the afternoon/evening Standard Polish working hours + short evening overlap Late afternoon start with evening/late-night work (for max US overlap) We’reflexible— as long as the work gets done. Seems like a place you’d like to be a part of?Our team is waiting for you! 📩 Send yourCVandfinancial expectations, notice period.","[{""min"": 25000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,25000,28000,Net per month - B2B
Full-time,Mid,B2B,Remote,349,Data Engineer (MS Azure) - Future Opportunities,Avanade Poland,"This is ""future opportunities"" ad - we will begin recruitment process after summer (September). You can expect to hear from us at that time. The role 100% remote so it is available for Candidates from all over the Poland. Do you love making sure that information is available and consumable? So do we. Are you passionate about transforming raw data into actionable insights? We're seeking a Data Engineer with expertise in the Microsoft Azure Data realm, including Delta Lake, Databricks, and ETL processes. Key Responsibilities: Design, build, and maintain scalable, efficient, and reliable data pipelines using Databricks and Azure services (e.g., Azure Data Lake, Azure SQL Database, Azure Databricks, Azure Data Factory). Lead the end-to-end development of data architectures and ETL processes to support data-driven decision-making across the organization. Collaborate with data scientists, analysts, and other engineering teams to ensure data integration and optimization. Implement data governance, data quality, and data security best practices across data systems. Mentor and provide guidance to junior data engineers, fostering a culture of continuous learning and improvement. Optimize performance of data workflows, troubleshoot issues, and ensure the reliability of data solutions. Stay up-to-date with the latest developments in cloud technologies, data engineering best practices, and emerging tools and frameworks Required Skills and Qualifications: Extensive experience with Databricks and Azure services (e.g., Azure Data Lake, Azure SQL Database, Azure Databricks, Azure Data Factory). Strong expertise in SQL, Python and other data engineering programming languages. Proven experience in building and managing complex data pipelines and distributed data architectures. Solid understanding of data engineering concepts, such as ETL, data modeling, data warehousing, and data governance. Ability to optimize large-scale data workflows and troubleshoot complex data issues. Strong leadership and communication skills, with the ability to guide teams, collaborate with cross-functional stakeholders, and communicate technical concepts to non-technical audiences. Proven track record of successfully delivering large-scale data projects and systems. Preferred Skills: Experience with other Azure tools such as Azure Microsoft Fabric, Azure Synapse Analytics, Azure IoT Hub, Azure HDInsight, Kafka and Azure Stream Analytics. Experience with containerization (Docker/Kubernetes) and orchestration tools (e.g., Apache Airflow). Familiarity with DevOps principles and CI/CD pipelines for data engineering workflows. Experience with data warehouses and real-time data processing frameworks. Experience with LLMs and orchestration frameworks (e.g., LangChain, Semantic Kernel) is a plus. Exposure to machine learning concepts and frameworks (e.g., MLflow, TensorFlow) is a plus. We offer: Remote/hybrid work – you choose! Access to Microsoft certifications, trainings and e-learning platforms (Pluralsight, Rosetta Stone) Career/development path – every employee has his/her Mentor Chill rooms at the offices: table football, game console, board games Free parking spot at the offices We participate in charity programs: Szlachetna Paczka, Business Run Sports section on the Strava: Poland Club | Avanade Poland Running on Strava Company events (Team outings, Meetups) - check our Instagram!Avanade Poland These are just a few of the Avanade benefits, let's talk about the others!","[{""min"": 110, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,110,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,351,LIMS Expert,Sii,"Minimum of 4 years of relevant experience in a regulated environment Hands-on laboratory experience and familiarity with analytical techniques (e.g., HPLC) Knowledge of GxP requirements and global regulatory standards Proficiency with LIMS (LabVantage experience preferred), eQMS, and document management systems Excellent communication and customer service skills, with the ability to translate technical information for diverse audiences Fluency in English (spoken and written) We are looking for a skilled, detail-oriented, and collaborative LIMS Expert to join our team in the medical and pharmaceutical industry, supporting global users of the Laboratory Information Management System. In this role, you will be part of a dynamic group committed to delivering both rapid and long-term solutions to enhance the laboratory user experience and ensure alignment with strategic goals. The ideal candidate will bring strong technical expertise, customer service skills, and a proactive attitude to a highly regulated environment. Support creation and updates of LIMS Master Data (e.g., specifications, sampling plans, LES worksheets) Troubleshoot and resolve user support tickets within the LIMS environment Communicate with QC departments to collect and verify data for LIMS integration Create and manage compliance documentation, including routing for approval Participate in training users and supporting routine QC operations Recommend process improvements and contribute to best practice development Rekrutacja online Język rekrutacji: polski&angielski Start ASAP Praca w pełni zdalna Darmowa kawa Darmowe śniadanie Bez wymaganego dress code'u Nowoczesne biuro Szkolenia wewnętrzne Pakiet sportowy Międzynarodowe projekty Prywatna opieka medyczna Budżet na szkolenia Małe zespoły Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title – get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers – Power People. Learn more atsii.pl.","[{""min"": 22000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Unclassified,22000,28000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,353,Senior DBA Consultant,Lumicode Sp. z o.o. (Pentacomp Group),"Kim jesteśmy Lumicode Sp. z o.o. należy do Grupy Pentacomp, która jest producentem rozwiązań informatycznych i dostawcą profesjonalnych usług IT dla dużych przedsiębiorstw i sektora publicznego. JakoPentacomptworzymy rozwiązania IT łączące innowacyjność z latami doświadczeń - a ich mamy całkiem sporo. Istniejemy na rynku prawie 30 lat i możemy pochwalić się wieloma zrealizowanymi projektami Aktualnie do projektunaszego Klienta poszukujemy Senior DBA Consultanta. Oferujemy: Praca hybrydowa (Warszawa, 2 razy w tygodniu w biurze) Stawka do190 pln/h netto + VAT B2B; Dofinansowanie do karty sportowej oraz możliwość skorzystania z prywatnej opieki zdrowotnej; 10+ lat doświadczenia w zarządzaniu bazą danych Oracle w wersji od 12c do 19c (RAC, Data guard) Doświadczenie z systemem operacyjnym Linux, Narzędzia do zarządzania Oracle (Data Guard, RMAN, Data pump), Znajomość zasad projektowania architektury, Obsługa różnych typów problemów Data Guard, Praktyczne doświadczenie ORACLE w produkcyjnym środowisku RAC (ASM) (19c), Dobra znajomość Oracle Golden gate - jasne zrozumienie przepływu procesu, bazy danych Downstream, OGG start stop, Dobra znajomość Storage, Dobra znajomość rozwiązywania problemów FSFO Dobra znajomość procesu - tj. zarządzanie zmianą, zarządzanie incydentami, Wiedza na temat obsługi Oracle SR, Wiedza na temat architektury OEM i zarządzania macierzami OEM. Przygotowanie środowiska prePROD w celu włączenia FSFO Synchronizacja DataGuard w przypadku jakichkolwiek problemów / błędów w DG, Przebudowa Standby zgodnie z potrzebami, Migracja starego PDB do nowego kontenera Aktywacja FSFO Rozwiązywanie problemów z FSFO Wykonywanie testów na prePROD: Switchover i Failover","[{""min"": 140, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Unclassified,140,190,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,357,Data and Cloud Architect,B2Bnetwork,"We are looking for an experienced Data and Cloud Architect to support transformation initiatives in the areas of data, cloud, and AI. You will be working in the Data & Analytics domain within the Financial Crime Prevention space, helping to drive solution design and ensure architectural governance. The role involves close collaboration with both business and technology stakeholders. Responsibilities: Design and implement solution architecture for cloud, data, and AI initiatives Create and maintain architecture documentation and blueprints Support architectural governance and ensure alignment with enterprise standards Collaborate with business and IT teams to define scalable, modern architecture solutions Contribute to large-scale cloud and AI transformation efforts Requirements (Must-Have): Strong experience in integration architecture and data integration patterns (batch, real-time, near-real-time) Hands-on experience with cloud-based big data platforms, preferably AWS Background in AI and ML architecture (cloud and on-prem), including GenAI solutions Familiarity with big data technologies (e.g., Hadoop, Spark) and related toolchains Knowledge of enterprise solution architecture implementation and governance Experience with modern tech stacks (open-source and vendor-based) Solid understanding of data architecture, data modeling, and data management technologies Nice-to-Have: Experience in the banking sector Knowledge of Financial Crime Prevention processes or technologies Location: Gdynia, Gdańsk or Warsaw (hybrid model – 2 days/week in the office) Contract Type: B2B Project Duration: Long-term","[{""min"": 170, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Architecture,170,185,Net per hour - B2B
Full-time,Mid,B2B,Remote,358,Data Engineer,emagine Polska,"Informacje o projekcie: Branża: finanse/pożyczki Lokalizacja: 100% zdalnie Umowa: B2B Stawka: 200 pln/h netto + VAT Długość projektu: długoterminowy Poszukujemy doświadczonego Data Engineera do zespołu Data Platform, który będzie modelować dane oraz implementować procesy ELT w chmurze. Obowiązki: Modelowanie struktur bazodanowych w podejściu DDD oraz tworzenie logicznych i fizycznych modeli danych. Opracowywanie warstwy Data Contracts na podstawie zamodelowanych struktur dla domen danych. Współpraca w procesie ingerencji danych z systemów źródłowych. Implementacja modeli danych w Data Platform na różnych warstwach (Bronze, Silver, Gold) w środowisku Azure Databricks. Wymagania: Doświadczenie w modelowaniu danych i tworzeniu Data Governance. Wiedza na temat Data Mapping. Umiejętność implementacji procesów ELT. Mile widziane: Umiejętność tworzenia dokumentacji technicznej. Doświadczenie w mapowaniu danych ze źródłowych do docelowych struktur. Znajomość narzędzi do zarządzania metadanymi (np. Azure Purview).","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,180,200,Net per hour - B2B
Full-time,Mid,B2B,Remote,359,Data Scientist,Experis Manpower Group,"Tasks: Analyze Standard Operating Procedures (SOPs), system logs, and structured datasets to support AI agent training and optimization. Validate AI-generated outputs and provide actionable feedback for continuous model improvement. Develop dashboards and define metrics to monitor and report on AI impact and performance. Collaborate closely with software developers and business analysts to align AI solutions with business needs. Requirements: Proven experience (4+ years) in data science, with a strong foundation in data analysis and machine learning. Proficiency in Python and SQL, with hands-on experience in data visualization tools (e.g., Power BI, Tableau, or similar). Familiarity with enterprise data systems such as SAP, OTM, or Denali is highly desirable. Strong analytical thinking, attention to detail, and ability to communicate technical insights to non-technical stakeholders. Ability to work independently in a remote or hybrid environment and manage multiple priorities effectively. Offer: 100% remote work MultiSport Plus Group insurance Medicover Premium e-learning platform","[{""min"": 190, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,190,200,Net per hour - B2B
Full-time,Mid,B2B,Remote,360,Data Engineer,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: Data Engineer Responsibilities: Develop, optimize & maintain big data (ELT/ETL) pipelines for purposes of business intelligence and statistical modelling Ensure & monitor data quality and integrity Set up and configure data storage systems (e.g. SQL databases, data lake) Manage and support production use of connections between key elements of data infrastructure Write and maintain secure, robust, scalable, and efficient code that turns business concepts into tangible solutions Drive engineering best practices like automation, CI/CD, and maintainability Support standardization and automation by following best-practices / common development standards Collaborate with BI analysts, data scientists, ML engineers and core IT in cross-functional teams delivering value through data solutions Requirements: Broad skills in Python, SQL, MDX and bash scripting 2+ years of hands-on experience within Azure data components (Data Factory, Synapse, Datalake, Blob Storage, Sharepoint, Databricks) Experience with data workflow orchestration (ADF, Databricks Jobs, possibly Airflow) Good command of version control and CI/CD pipelines using Azure DevOps or alike Experience with Snowflake and its use with Azure cloud Business-ready English (B2-C1) Familiarity with data governance concepts (e.g., catalogue & meta data, data lineage, master data, security & compliance) Nice to have: Familiarity with SAP Business Warehouse as a data source Azure Data Engineer / Data Analytics certificate Understanding of BI tools, eg. Power BI Experience with PySpark / Scala Offer: Private medical care Co-financing for the sport card Training & learning opportunities Constant support of dedicated consultant Employee referral program","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Mid,B2B,Remote,361,Administrator Baz Danych Oracle / Oracle DBA,Simora,"Dołącz do naszego zespołu jako Administrator Baz Danych Oracle! Poszukujemy osoby, która zajmie się zarządzaniem bazami danych naszych klientów. Jesteśmy firmą, która rozwija nowoczesne rozwiązania w obszarze baz danych oraz sztucznej inteligencji. Wykorzystujemy nowoczesne technologie do tworzenia narzędzi wspierających naszych klientów. Cenimy pozytywną atmosferę, wzajemny szacunek i zaangażowanie – to fundamenty naszego zespołu. Twój zakres obowiązków Tworzenie baz danych oraz środowisk bazodanowych na środowisku produkcyjnym i testowym Migrowanie baz danych na nowe środowiska Zarządzania i aktualizacja baz danych i środowisk bazodanowych Konfigurowanie i optymalizacja środowiska bazodanowego Automatyzacja zadań za pomocą języków skryptowych Bash, Python, i/lub innych Zapewnienie wysokiej jakości i bezpieczeństwa dla tworzonych rozwiązań Wdrażanie rozwiązań opartych na naszych flagowych produktach, takich jak SyncGuard i SimonAI Stałe doskonalenie sposobu Twojej pracy Nasze wymagania Doświadczenie w pracy na stanowisku administratora baz danych Oracle Dobra znajomość języka SQL i PLSQL Umiejętność dbania o szczegóły i jakość rozwiązań Komunikatywność Znajomość języka angielskiego na poziomie co najmniej B1 Kontrakt b2b Mile widziane Wykształcenie wyższe (preferowany kierunek: informatyka) Znajomość języków programowania: Python, Java itp. Znajomość systemów operacyjnych Linux / Windows Znajomość systemów wirtualizacyjnych np.: OLVM Oferujemy Ciekawą pracę w firmie o wysokiej dynamice rozwoju Możliwość podniesienia kwalifikacji w obszarach związanych z bazami danych, bezpieczeństwem danych oraz sztuczną inteligencją Benefity Karta Multisport Prywatna opieka zdrowotna – Medicover Praca zdalna Brak dress code’u Dofinansowanie szkoleń i kursów Elastyczny czas pracy","[{""min"": 7000, ""max"": 13000, ""type"": ""Net per month - B2B""}]",Database Administration,7000,13000,Net per month - B2B
Full-time,Mid,Permanent,Remote,362,Adoption Engineer,Link Group,"We are looking for a skilledAdoption Engineerwith strong hands-on experience indbt (Core & Cloud),DevOps,Infrastructure as Code, andAirflow. This role is critical to support the successful rollout, scaling, and enablement of modern data transformation platforms within enterprise environments. As an Adoption Engineer, you will serve as a technical bridge between platform engineering and data users, ensuring smooth adoption of tools and best practices across teams. You will be responsible for building robust, reusable infrastructure components, supporting automation, and guiding users through optimal implementation patterns. Act as asubject matter expert (SME)indbt Core and Cloudimplementation, configuration, and adoption strategy Design and build scalable and reusableinfrastructure componentsusingIaC tools(Terraform, CloudFormation, etc.) Enable and supportAirflow DAGcreation and scheduling for orchestration of dbt models and other workflows Collaborate with data teams to improve workflows, CI/CD pipelines, and DevOps processes Define and enforce best practices indata modeling,code versioning, anddeployment automation Provide onboarding and enablement support to teams migrating to dbt Cloud Support observability, testing, and documentation standards for all data transformation processes Ensure security, scalability, and operational excellence across the data transformation stack Proven experience withdbt Core and dbt Cloud(advanced user-level, admin or platform support experience is a plus) Strong proficiency inSQLand data modeling principles Experience withDevOps toolingandCI/CD pipelines Hands-on experience withAirflowand workflow orchestration Experience withInfrastructure as Code(Terraform preferred, but others acceptable) Cloud platform familiarity (AWS, Azure or GCP) Strong communication skills, with the ability to support teams across different technical maturity levels Experience in data platforms such asSnowflake,BigQuery,Databricks, orRedshift Familiarity withmonitoring & observabilitytools for data workflows Understanding ofdata governanceandaccess controlconcepts Background in platform evangelism, internal consulting, or enablement is a plus","[{""min"": 85, ""max"": 105, ""type"": ""Gross per hour - Permanent""}]",Data Engineering,85,105,Gross per hour - Permanent
Full-time,Mid,B2B,Remote,364,Data Engineer (Databricks),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition byForbesas one of the top 10 AI companies. As aData Engineer, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of a universal data platform for global aerospace companies.This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. Data Platform Transformation for energy management association body.This project addressed critical data management challenges, boosting user adoption, performance, and data integrity. The team is implementing a comprehensive data catalog, leveraging Databricks and Apache Spark/PySpark, for simplified data access and governance. Secure integration solutions and enhanced data quality monitoring, utilizing Delta Live Table tests, established trust in the platform. The intermediate result is a user-friendly, secure, and data-driven platform, serving as a basis for further development of ML components. Design of the data transformation and following data ops pipelines for global car manufacturer.This project aims to build a data processing system for both real-time streaming and batch data. We’ll handle data for business uses like process monitoring, analysis, and reporting, while also exploring LLMs for chatbots and data analysis. Key tasks include data cleaning, normalization, and optimizing the data model for performance and accuracy. 🚀 Your main responsibilities: Design scalable data processing pipelines for streaming and batch processing using Big Data technologies like Databricks, Airflow and/or Dagster. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using Databricks/DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. 🎯 What you’ll need to succeed in this role: At least 3 years of commercial experienceimplementing, developing, or maintaining Big Data systems. Strong programming skills inPython: writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity withBig Datatechnologies likeAirfloworDagster,Databricks, SparkandDBT. Experience implementing and deploying solutions in cloud environments (with a preference forAzure). Knowledge of how to build and deployPower BIreports and dashboards for data visualization. Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master’s or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. 🎁 Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage withtop-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you towork remotelyor from modern offices and coworking spaces. Accelerate your professional growth throughcareer paths,knowledge-sharinginitiatives,languageclasses, and sponsoredtrainingorconferences, including a partnership withDatabricks, which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days offavailable for B2B contractors and individuals under contracts of mandate. Participate inteam-building eventsand utilize theintegration budget. Celebratework anniversaries, birthdays,andmilestones. Accessmedicalandsports packages, eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you canboostyourpersonal brandby speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website (career page) and social media (Facebook,LinkedIn,Instagram).","[{""min"": 15120, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Data Engineering,15120,21000,Net per month - B2B
Full-time,Mid,B2B,Remote,368,Data Engineer,KUBO,"We’re hiring a Data Engineer! Designing and building robust ETL/ELT data pipelines in Microsoft Fabric using PySpark and SQL Modeling data using approaches like star schema, data vault, and lakehouse Creating well-structured datasets, reports, and Power BI dashboards focused on business usability and self-service Implementing best practices around data governance, security, and documentation Automating tests, CI/CD workflows, and monitoring using Azure DevOps or GitHub Actions Collaborating closely with product owners, analysts, and fellow engineers in cross-functional teams 3+ years of experience as a Data Engineer or in a similar role Advanced SQL skills – query optimization, indexing, partitioning Strong Python programming skills Solid knowledge of Apache Spark for batch & streaming data, Delta Lake Experience with Power BI – data modeling, DAX, RLS, deployment pipelines Familiarity with cloud platforms – ideally Azure (Data Lake Gen2, Data Factory, Synapse/Databricks) Proficiency with version control and DevOps tools (Git, pull requests, CI/CD basics) Fluency in English (minimum B2+) Work model: 100% remote Rate: 100–150 PLN/h net (B2B) Benefits: Private medical care, Life insurance, Multisport card","[{""min"": 100, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,100,150,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,369,Data Engineer,Tooploox,"We areTooploox 💎,an AI software development companyoffering custom AI solutions and services. We help innovative companies and startups design and build digital products with generative AI, mobile, and web technologies. Our team, consisting of nearly 200 experts including our R&D team of over 40 engineers, many with PhDs, has pioneered AI solutions across industries like healthcare, fashion, and e-commerce. We’ve published over 15 research papers in top conferences like NeurIPS and ICML. We're on the lookout for aData Engineer📊 to take on a pivotal role in our team.You'll be at the heart of working with data, focusing on scalable batch and streaming data pipelines. If you're someone who loves to merge traditional software development with innovative AI technologies, this role is tailor-made for you. Design, develop, and maintain scalable batch and streaming data pipelines. Work withPythonto transform, process, and integrate data. Handle a mix of structured and unstructured data, including work withNoSQL and vector databases. Optimize performance acrossbig data workflows, including tuningHive and Sparkjobs. BS/BA in Data Engineering/Computer Science+ 2 years of experience or related field or 5 years of relevant experience. Extensive expertise withApache Spark (especially PySpark), Hadoop, and Apache Hivewith a proven track record of optimizing large-scale data systems. Strong programming skills inPython. Comprehensive understanding of database concepts, including experience withNoSQL databases (e.g., MongoDB, Redis)and ideally vector databases. Proven hands-on experience with stream processing, preferably usingApache Flink. In-depth knowledge of distributed computing, data warehousing, and performance optimization techniques. Exceptional problem-solving and communication skills, with experience working in cross-functional teams. Fluency in Polish and English. Experience with LLMs, prompt engineering, or machine learning workflows (we use this in conjunction with vector DBs). Experience in Java or Scala - useful for deeper Spark optimization or contributing to broader engineering projects. Familiarity with Spring Boot for building and deploying data applications. 🏖️26 days of annual service break. 🤒An additional pool of 14 days per yearpaid at80% of your standard rate. 🇬🇧English lessonsonce a week or more frequently, depending on your needs. 📚Access to a curated libraryof books and e-books, regularly updated based on employee suggestions, plus recurringknowledge-sharing initiatives. 🏡Flexible hoursand the option to work100% remotelyor from one of our offices inWrocław or Warsaw. 💻Top-quality equipment– we provide MacBooks, new monitors, noise-cancelling headphones, and any additional gear you may need to work comfortably. 🏥Group insurancewith Warta andprivate medical carewith Enel-Med for just 1 PLN. 🧠Mental health support– we offer access to a psychologist with fully anonymous consultations if needed. 🏋️‍♂️Multisport card(we cover most of the cost – your contribution is currently no more than 45 PLN, or less depending on the selected package), access togyms in our Wrocław and Warsaw offices, andsports initiativeslike the annualBike 2 Work Challenge. 🍕🎮🕺🏻 We hostteam lunches, webinars, game nights, and social events. We enjoy the occasional barbecue, dance party, time on the terrace, foosball, or PlayStation session.","[{""min"": 18000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,22000,Net per month - B2B
Full-time,Senior,B2B,Remote,370,Manager Database Administrator,Link Group,"Role Overview We are looking for an experienced and versatileSenior Database Administratorto step into a managerial role within our growing IT team. This is an exciting opportunity for someone passionate about database technologies who is also ready to lead and develop a team. You will be responsible for ensuring the reliability, performance, and security of our database systems while driving strategic improvements and mentoring your team. Design, implement, and maintain robust, high-performance, and secure database environments. Monitor system performance, proactively troubleshoot issues, and optimize efficiency. Oversee backup strategies, ensure successful recovery processes, and manage disaster recovery plans. Lead, coach, and mentor a team of database professionals, supporting their growth and daily activities. Partner with cross-functional teams to understand and deliver on database requirements aligned with business objectives. Promote a culture of learning, innovation, and continuous improvement. Define and execute database strategies that align with broader IT and organizational goals. Stay updated on emerging technologies and best practices, recommending and implementing upgrades and enhancements. Maintain thorough documentation of database architecture, procedures, and configurations. Ensure adherence to security, data governance, and compliance standards. Provide technical guidance and training to IT staff and end-users. Work closely with development teams to optimize database integration and performance for business-critical applications. Bachelor’s degree in Computer Science, Information Technology, or a related field. At least5 years of experienceas a DBA, includingproven experience in a leadership or managerial role. Strong leadership and team management skills, with ability to mentor and inspire technical teams. Deep expertise with database systems such as Oracle, SQL Server, or MySQL. Strong background in database design, performance tuning, and optimization. Excellent problem-solving, analytical, and decision-making skills. Strong communication and interpersonal abilities to effectively collaborate across teams. Experience working with cloud database platforms (e.g., AWS, Azure). Understanding of data warehousing and business intelligence concepts. Familiarity with database security standards and best practices.","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Database Administration,150,180,Net per hour - B2B
Full-time,Senior,B2B,Remote,372,MOCNY GCP Data Engineer,Link Group,"Poszukujemy doświadczonego GCP Data Engineera do projektu realizowanego dla globalnej firmy konsultingowej w obszarze data & analytics. Praca dotyczy budowy i optymalizacji zaawansowanych pipeline’ów danych w środowisku Google Cloud Platform (BigQuery, Airflow, DBT). Kluczowe będą umiejętności projektowania architektury danych, integracji danych z różnych źródeł oraz optymalizacji wydajności systemów. Wymagania: Minimum 4 lata doświadczenia jako Data Engineer, w tym 3 lata z GCP Bardzo dobra znajomość BigQuery, Python, SQL Doświadczenie z Airflow, DBT/Dataform Znajomość zasad modelowania danych, optymalizacji zapytań Angielski B2/C1 Mile widziane: Certyfikaty GCP, doświadczenie z BI (Power BI, Tableau), znajomość Azure Informacje organizacyjne: 📍 100% zdalnie 🗓 Start: sierpień 💼 Proces: HR + techniczne + opcjonalne spotkanie z klientem","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,373,Snowflake Data Engineer,Onwelo Sp. z o.o.,"Jesteśmy nowoczesną polską firmą technologiczną, która dostarcza wsparcie eksperckie organizacjom na całym świecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiązania IT, oferując przy tym solidne zaplecze kompetencyjne. W ciągu kilku lat zrealizowaliśmy ponad 300 projektów w Europie i USA, dynamicznie rozbudowując zespół do kilkuset specjalistów i otwierając sześć biur w Polsce oraz oddziały w USA, Niemczech i Szwajcarii. Dołączysz do zespołu realizującego projekt dla globalnej organizacji z sektoralife science i healthcare, specjalizującej się w rozwiązaniach laboratoryjnych i biotechnologicznych. Klient prowadzi działalność na wielu rynkach i obsługuje tysiące jednostek operacyjnych na całym świecie.Celem projektu jest budowa i rozwój skalowalnej platformy danych w środowisku chmurowym, która wspiera analitykę biznesową, planowanie operacyjne oraz zaawansowane modele predykcyjne.Zespół Onwelo wspiera klientam.in. w rozwoju hurtowni danych, projektowaniu potoków ETL, modelowaniu danych i zapewnieniu jakości danych w środowisku enterprise.Szukamy osoby, która wniesie swoje doświadczenie i wesprze zespół w rozwoju architektury danych, zapewniając wydajność, jakość i bezpieczeństwo danych. Projektować, budować i rozwijać hurtownie danych oparte na platformieSnowflake Tworzyć oraz optymalizowaćpotoki danych (ETL/ELT)w środowiskach chmurowych Wdrażać i zarządzać komponentami Snowflake: Snowpipe, Streams, Tasks, Secure Views Projektować i rozwijaćmodele danychwspierające analitykę biznesową Współpracować z zespołami Data Science i BI w zakresie zasilania modeli i dashboardów Wspierać automatyzację procesów danych poprzez integrację z narzędziami CI/CD (GitLab, Jenkins) Posiadasz minimum 3-letnie doświadczenie jakoData Engineer– z naciskiem na Snowflake Znasz platformęSnowflake: strukturę danych, architekturę, optymalizację zapytań, zarządzanie schematami i dostępem Biegle posługujesz sięSQL(w tym: CTE, window functions, UDF, optymalizacja zapytań) Masz doświadczenie z procesamiETL/ELT, również z wykorzystaniem danych półstrukturalnych (JSON, XML, Parquet) Znasz zasady projektowania nowoczesnych modeli danych (np. Kimball, Data Vault) Maszwyższe wykształcenie techniczne(np. informatyka, matematyka, inżynieria danych) Komunikujesz siępo angielsku na poziomie min. B2 Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 16800, ""max"": 23100, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 17000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16800,23100,Net per month - B2B
Full-time,Mid,B2B,Remote,374,Data Engineer,Keyloop,"Keyloop bridges the gap between dealers, manufacturers, technology suppliers and car buyers. We empower car dealers and manufacturers to fully embrace digital transformation. How? By creating innovative technology that makes selling cars better for our customers, and buying and owning cars better for theirs. We use cutting-edge technology to link our clients’ systems, departments and sites. We provide an open technology platform that’s shaping the industry for the future. We use data to help clients become more efficient, increase profitability and give more customers an amazing experience.Want to be part of it? Job Summary: As a Data Engineer at Keyloop you will work within the Analytics Engineering team and be responsible for maintaining and developing our Data Lake and existing Data Pipelines, as well as continually exploring, analysing and proposing improvements to existing processes and tooling. You will also be required to contribute to scoping and discovery exercises being conducted elsewhere in the business, leveraging your expert knowledge of the data and business intelligence offering and the associated data requirements. You will therefore be working in a busy and multi-functional team, planning and prioritising a variable workload and delivering to deadlines. You will report to the Lead Analytics Engineer, and you may provide mentorship to other analysts and data engineers in Analytics Engineering where appropriate to support their knowledge and skills growth. Key Responsibilities: • Design, build and maintain scalable and robust ETL/ELT pipelines using various data integration tools and programming languages (e.g. Python, SQL) • Collaborate with Analytics Engineers to design and implement optimised data models within our data warehouse, ensuring data quality, consistency and ease of use for analytics • Manage and optimise our data warehouse including cost optimisation and ensuring data governance best practices • Implement robust data validation, monitoring and alerting mechanisms to ensure accuracy and completeness of our data • Work closely with Analytics Engineers to understand their data requirements, provide technical guidance and ensure the efficient delivery of data products • Pre and post release communications where necessary to relevant stakeholders • Documenting processes (or SOPs) for commonly performed tasks to assist with the training of other Data Engineers, or to aid business continuity as a general theme • Fully understanding the data landscape in databases, analytics and applications and all the front-end and back-end products in the global portfolio • Understanding of data compliance and laws, as well as the full data collection to output technology stack Technical Competencies: • Experience with Amazon Web Services cloud platform and with knowledge in particular of data related services they offer • SQL for data manipulation, transformation and query optimisation • Experience in Python for data engineering tasks • Hands-on experience with a modern data warehouse platform (e.g. Amazon Redshift) • Solid understanding of data warehousing concepts, dimensional modelling (star schema approach), and ETL/ELT principles • Experience with data pipeline orchestration tools • Familiarity with version control systems (e.g. Git) • Understanding of data governance principles and tools Behavioural & Personality Competencies: • Analytical and logical mindset • Time management skills • High standard of problem-solving skills and attention to detail • Good communication and listening skills • Team player and collaborative • Ability to manage multiple different projects · Organised & self-sufficient · Logical, methodical approach to problem and issue solving · Numerate, innovative and critical thinking Desirable Skills: • Familiarity with JIRA software • Experience, or an interest in, the automotive industry • Experience of Agile/SCRUM delivery environment Why join us? We’re on a journey to become market leaders in our space – and with that comes some incredible opportunities. Collaborate and learn from industry experts from all over the globe. Work with game-changing products and services. Get the training and support you need to try new things, adapt to quick changes and explore different paths. Join Keyloop and progress your career, your way. An inclusive environment to thrive We’re committed to fostering an inclusive work environment. One that respects all dimensions of diversity. We promote an inclusive culture within our business, and we celebrate different employees and lifestyles – not just on key days, but every day. Be rewarded for your efforts We believe people should be paid based on their performance so our pay and benefits reflect this and are designed to attract the very best talent. We encourage everyone in our organisation to explore opportunities which enable them to grow their career through investment in their development but equally by working in a culture which fosters support and unbridled collaboration. Keyloop doesn’t require academic qualifications for this position. We select based on experience and potential, not credentials. We are also an equal opportunity employer committed to building a diverse and inclusive workforce. We value diversity and encourage candidates of all backgrounds to apply.","[{""min"": 18000, ""max"": 23100, ""type"": ""Net per month - B2B""}]",Data Engineering,18000,23100,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,375,Data Scientist,Onwelo Sp. z o.o.,"Onweloto nowoczesna polska spółka technologiczna, specjalizująca się w budowie innowacyjnych rozwiązań IT dla organizacji z różnych sektorów na całym świecie. Firma oferuje kompleksowe usługi z zakresu tworzenia, rozwoju i utrzymania oprogramowania, oraz silne wsparcie kompetencyjne. Do naszego zespołuData & Analyticsposzukujemy Data Scientista, który będzie pracował nad budową i rozwojem modeli analitycznych oraz uczenia maszynowego dla klientów z różnych branż – zarówno z Polski, jak i z rynków zagranicznych. Będziesz współpracować z zespołami analitycznymi i biznesowymi, wspierać podejmowanie decyzji na podstawie danych i tworzyć rozwiązania, które realnie wpływają na działalność naszych klientów. Przeprowadzać eksploracyjnąanalizę danych (EDA) Poszukiwaćzależności, wzorców i insightów w danych biznesowych Budowaćmodele klasyfikacyjne, regresyjne i klasteryzacyjne Przeprowadzaćfeature engineering i przygotowywać dane do modelowania Współpracować z zespołami analitycznymi, technologicznymi i biznesowymi Wizualizować wyniki analiz i przygotowywać raporty oraz prezentacje Maszminimum 2-letnie doświadczeniew pracy jakoData Scientistlub na podobnym stanowisku Bardzo dobrze znaszPythona i pracowałeś z bibliotekami: pandas, scikit-learn, numpy, matplotlib, seaborn Swobodnie pracujesz zdanymi tabelarycznymii znasztechniki EDA Potrafisz wyciągać trafne wnioski z danych i prezentować je w przystępny sposób ZnaszSQLi masz doświadczenie z dużymi zbiorami danych Dodatkowo docenimy, jeśli: Korzystałeś znarzędzi BI i znasz metody interpretacji modeli Masz doświadczenie w automatyzacji procesów analitycznych Znasz system kontroli wersjiGITi technologie konteneryzacji (Docker) Pracowałeś z rozwiązaniami w chmurze obliczeniowej(Azure, AWS lub GCP) Wybierzesz wygodną dla Ciebie formę zatrudnienia Potrzebujesz pracować zdalnie? Jesteśmy otwarci! Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych i zewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Wydarzenia firmowe pozwolą Ci na bliższe poznanie zespołu","[{""min"": 15750, ""max"": 21000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 15000, ""type"": ""Gross per month - Permanent""}]",Data Science,15750,21000,Net per month - B2B
Full-time,Mid,B2B,Remote,377,Spatial Data Engineer,Spyrosoft,"Project Description Are you passionate about geospatial data and eager to work on cutting-edge GIS solutions? Join our team as a Spatial Data Engineer and help our clients implement and maintain advanced spatial data processing systems using ESRI products or Open-Source equivalents. Key Responsibilities Process and manage geospatial data from diverse sources Design and maintain ETL pipelines Integrate spatial data from multiple systems Develop automation solutions for data workflow Build GIS applications (web and mobile) Technical Requirements Solid understanding of spatial data models and best practices in spatial data management Proficiency in Python at an intermediate level, especially with ArcGIS API for Python and arcpy Strong experience with ESRI products , particularly: ArcGIS Pro ArcGIS Online (including: Experience Builder, Field Maps, Hub, Map Viewer, Survey123, Workflow Manager) ArcGIS Notebooks Managing and administering ArcGIS Online/Portal Automating spatial data processing (using ESRI or Open-Source tools) Integrating cloud solutions (Azure, AWS) with GIS environments Working in Azure DevOps Familiarity with data sharing methodologies and cloud-based environments Nice to have: Experience with FME Form , FME Flow , ArcGIS Velocity , or Sweet for ArcGIS ﻿ 💡 Soft Skills Passion for spatial data and strong analytical thinking Excellent communication skills and business-oriented mindset Fluency in English, both written and spoken","[{""min"": 70, ""max"": 120, ""type"": ""Net per hour - B2B""}]",Data Engineering,70,120,Net per hour - B2B
Full-time,Mid,B2B,Remote,378,Data Engineer (Palantir+Snowflake),Onwelo Sp. z o.o.,"Poznaj Onwelo: Jesteśmy nowoczesną polską firmą technologiczną, która dostarcza wsparcie eksperckie organizacjom na całym świecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiązania IT, oferując przy tym solidne zaplecze kompetencyjne. W ciągu kilku lat zrealizowaliśmy ponad 300 projektów w Europie i USA, dynamicznie rozbudowując zespół do kilkuset specjalistów i otwierając sześć biur w Polsce oraz oddziały w USA, Niemczech i Szwajcarii. Dołącz do zespołu Data, który wspiera jedną z największych grup technologicznych na świecie w budowie centralnej platformy danych. W projektach wykorzystujemy platformęPalantir Foundryjako warstwę aplikacyjną orazSnowflakejako warstwę magazynowania i przetwarzania danych. Pracujemy nad integracją danych z różnych źródeł (ERP, CRM, dane laboratoryjne) i tworzymy zaawansowane pipeline'y danych, modele analityczne i aplikacje wspierające decyzje biznesowe. Tworzyć i utrzymywać pipeline’y danych w platformiePalantir Foundry Projektować i wdrażać procesy przetwarzania danych oraz modele danych wSnowflake Integrować dane z różnych źródeł – ERP, CRM, dane laboratoryjne Zapewniać jakość danych, monitorować i optymalizować procesy Współpracować z architektami i analitykami danych przy projektach w różnych obszarach biznesowych (produkcja, finanse, logistyka, R&D) Masz min. 3–5 lat doświadczenia jako Data Engineer lub Developer Pracowałeś(aś) z platformąPalantir Foundry(np. Code Workbook, Pipelines, Ontology) Bardzo dobrze znaszSnowflakei językSQL Programujesz wPythoni/lubPySpark Masz doświadczenie w integracji danych z systemów klasyERP/CRM(np. SAP, Salesforce, MS Dynamics) Znasz rozwiązania chmurowe, szczególnieAWS Posługujesz się językiem angielskim na poziomie min.B2(międzynarodowe środowisko) Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczędzisz czas na dojazdach – pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 900, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Data Engineering,900,1000,Net per day - B2B
Full-time,Senior,B2B,Remote,379,Data Solution Architect (Azure),Scalo,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania tom.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! projekt budowy nowoczesnej Data Platform w chmurze Azure, obejmującej warstwę Lakehouse (Bronze / Silver / Gold) oraz Analytical Platform (MLOps), modelowanie struktur bazodanowych w podejściu DDD (Data Domain Driven Design), opracowywanie modeli danych na podstawie dokumentacji z obszaru Data Governance (glosariusz danych, modele konceptualne/logiczne), projektowanie przepływu danych ze źródeł do warstw platformy danych (Ingest: direct query, API, event streaming), tworzenie i dokumentowanie Data Contracts, współpraca z właścicielami systemów źródłowych i docelowych w zakresie integracji danych i SLA, implementacja struktur danych w architekturze medallion (Bronze / Silver / Gold) przy użyciu ETL na Azure Data Platform, doradztwo w zakresie doboru narzędzi, architektury integracyjnej i praktyk CI/CD, możliwość realnego wpływu na standardy technologiczne i projektowe, praca 100% zdalna, a dla chętnych możliwość pracy z biura we Wrocławiu, stawka do 240 zł/h przy B2B, w zależności od doświadczenia. masz doświadczenie w modelowaniu danych (ERD), przygotowywaniu Data Contracts i implementacji struktur domenowych w środowisku Data Warehouse znasz procesy Data Ingestion i architekturę nowoczesnych DWH w Azure (Azure Synapse, Data Lake, Databricks), masz doświadczenie z platformami MLOps / Analytical Platform, mile widziane doświadczenie w: tworzeniu dokumentacji mapowania danych źródłowych, zarządzaniu metadanymi i jakością danych (np. Azure Purview), komunikujesz się płynnie w języku angielskim (B2/C1). długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 33600, ""max"": 40320, ""type"": ""Net per month - B2B""}]",Data Architecture,33600,40320,Net per month - B2B
Full-time,Mid,B2B,Hybrid,382,BI Developer (WCL Qlik),ITDS,"BI Developer (WCL Qlik) Join us, and turn data into powerful business decisions! Kraków - based opportunity with hybrid work model (6 days/month in the office). As aBI Developer (WCL Qlik),you will be working for our client, a leading global financial institution undergoing a major digital transformation to enhance its data analytics capabilities. You will be part of a dynamic team focused on developing and maintaining scalable Qlik dashboards and reports, aimed at improving decision-making across various business units. The project involves managing complex data sets, implementing infrastructure best practices, and ensuring compliance within a fast-paced, highly regulated environment. Your role plays a key part in optimizing how data is shared, visualized, and utilized to deliver impactful business insights. Your main responsibilities: Developing advanced dashboards and reports using QlikSense Ensuring data integrity and managing updates to data sources Managing project sites and content on Qlik Server Documenting data sources, processes, and dashboards clearly Analyzing data sharing policies and promoting compliance best practices Collaborating with cross-functional teams and stakeholders at all levels Supporting Qlik deployment by following infrastructure best practices Enhancing dashboard performance for large data sets Influencing stakeholders through thoughtful data presentations Following established internal control standards and audit requirements You're ideal for this role if you have: Proficiency in Qlik with strong dashboard development experience 3+ years of experience in data analysis roles Practical knowledge of QlikSense and Qlik architecture Ability to work with large data sets while maintaining performance High level of mathematical and analytical skills Proven experience in stakeholder management and communication Strong collaboration skills within cross-functional teams Experience documenting technical processes and data sources Ability to adapt quickly in fast-changing environments Understanding of regulatory requirements for data sharing It is a strong plus if you have: Experience with Qlik administration Familiarity with financial services or highly regulated industries Knowledge of best practices for offshore project coordination Prior involvement in change or transformation projects Awareness of risk management and internal control standards We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #...7316 You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere.","[{""min"": 21000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,28000,Net per month - B2B
Full-time,Manager / C-level,B2B,Remote,383,AI/ML Data Practice Lead,Knowit Poland sp. z o.o.,"Role Overview: As the AI/ML & Data Practice Lead, you will be responsible for designing, implementing, and delivering AI/ML solutions that drive value for our clients. You will also play a key and active role in pre-sales activities, defining company offering, lead discovery workshops, and provide hands-on consulting. Your expertise will help establish and grow our AI/ML and Data Management practice, positioning Knowit Poland as a leader in the field. Key Responsibilities: Lead the development and delivery of AI/ML and Data Management solutions. Engage in pre-sales activities, including participation in defining company offer, client consultations, discovery workshops, and inspiration events. Design and implement AI/ML and Data solutions based on public cloud infrastructure (preferably Azure). Establish and grow Knowit Poland's AI/ML and Data practices. Serve as the technical and professional lead in AI/ML and Data areas.","[{""min"": 150, ""max"": 200, ""type"": ""Net per day - B2B""}]",Data Science,150,200,Net per day - B2B
Full-time,Senior,Permanent or B2B,Hybrid,384,DATA Architect,Power Media,"Nasz klient to firma, która odważnie podchodzi do wyzwań technologicznych i nie boi się szukać nieszablonowych rozwiązań. Innowacyjność łączy tu się z solidną wiedzą techniczną oraz doskonałą znajomością realiów branży. Specjalizują się w projektach z pogranicza eCommerce i Business Intelligence, oferując kompleksowe rozwiązania oparte na sprawdzonych technologiach. Co więcej – umiejętnie łączą te dwa światy, tworząc narzędzia, które zapewniają pełen wgląd w dane i realne wsparcie dla biznesu. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozwój rozwiązań Data Platform w środowisku chmurowym. Kluczowym zadaniem będzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejsów w złożonym krajobrazie systemowym klienta. Projekt obejmuje budowę hurtowni danych oraz platform danych od podstaw. Zespółskłada się z 22 osób: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiązków: Rozwój i utrzymanie Data Warehouse oraz Data Platform przy użyciu narzędzi ETL i SQL, Analiza wymagań biznesowych i proponowanie rozwiązań technicznych, Współpraca z zespołem w celu zapewnienia dostępności rozwiązań biznesowych, Wykorzystanie swojej wiedzy i doświadczenia do tworzenia innowacyjnych rozwiązań. Główne wymagania: Bardzo dobra znajomość metod i technik projektowania oprogramowania, Ponad 10-letnie doświadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych, Znajomość ekosystemów Big Data, Znajomośćrozwiązań chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejętności analityczne (analiza i dokumentowanie wymagań biznesowych oraz specyfikacji technicznych) Doświadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomość SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomość rozwiązań ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomość języka angielskiego (min. B2+/C1)– codzienna praca w międzynarodowym środowisku, Gotowość do podróży służbowych. Mile widziane: Znajomośćnarzędzi Informatica, Znajomość Machine Learning oraz frameworków ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomość języków programowania (Java, Scala, C++, Python). Znajomość języka niemieckiego. Firma oferuje: Stabilne, długofalowe zatrudnienie w oparciu o B2B lub UoP, Możliwość pracy w większości zdalnej(praca z biura 1 raz w tygodniu, we wtorek) Płaska struktura, anty korporacyjne podejście do pracy i zespołu, Ciekawe, międzynarodowe projekty, Zgrany zespół chętnie uczestniczący w aktywnościach sportowo – rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team – building), Dodatkowe zajęcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajęć sportowych, Nowoczesne biuro ze strefą relaksu, Co roczny 5 dniowy wyjazd firmowy (cała firma), warsztaty kulinarne, wyjścia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywnościami Świetna atmosfera, partnerskie podejście, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa „miękko-techniczna”. Gorąca prośba o CV w j. angielskim : )","[{""min"": 18000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,18000,30000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,385,Data Engineer,Antal Sp. z o.o.,"For Our Client in the Banking Industry, we are looking for a Data Engineer. Salary Range: 180-200 PLN/hour Work Model: Hybrid (6 times a month in the Krakow office) Responsibilities: This role involves: Responsibility for metrics implementation: data ingest, data quality, refinement, and presentation - design, development, testing, coding, and deploying to production Operating and iterating a cloud data platform capability to support our internal goals Focus on quality, time, and budget constraints Communicating efficiently within the team and business stakeholders Working with global diverse teams Working in an Agile product-oriented culture Skills Desired: Experience in data engineering, ETL processes At least 7 years of professional experience in SQL development Experience in GCP, Big Query Experience with data build tools (DBT) Experience with Airflow, Cloud Composer Experience in data modeling Experience with data streaming, e.g., Kafka Experience with BI tools – mainly Looker Studio Experience making trade-offs when designing schemas and marts Experience with implementing automated data quality solutions Experience in orchestration and scheduling Experience working in a DevOps environment, i.e., think, build, and run Experience with Continuous Integration, Continuous Delivery","[{""min"": 180, ""max"": 200, ""type"": ""Net per day - B2B""}]",Data Engineering,180,200,Net per day - B2B
Full-time,Senior,B2B,Remote,387,Senior Data Engineer with AWS and Snowflake,Sii,"Strong expertise in cloud technologies and the Snowflake ecosystem, particularly AWS, Data One Platform, Immuta, Collibra, and cloud security aspects Minimum 5 years of experience in Python programming for building and maintaining data pipelines Advanced knowledge of databases, including query optimization, relational schema design, and MPP using SQL Expertise in data storage technologies, including files, relational databases, MPP, NoSQL, and various data types (structured, unstructured, metrics, logs) Deep familiarity with Data Vault, Data Mesh, dimensional modeling, and metadata management Understanding of business and analytical processes, as well as integration with analytics and reporting systems Experience working in Agile environments, coaching development teams with the use of fluent English and Polish Residing in Poland required Nice-to-have requirements Knowledge of Pharma data formats is a big plus We are looking for a Data Engineer with expertise in Snowflake, AWS, and ETL processes, who will work closely with AI scientists and data analysts to design, develop, and maintain data pipelines and systems that support clinical and operational data use cases. Design and develop data architecture incorporating Snowflake, Immuta, Collibra, and cloud security Implement and optimize ETL/ELT processes, manage raw data, automate workflows, and ensure high performance and reliability of data processing systems Manage data quality and security, monitor quality, and implement metadata management strategies Collaborate with analytics and business teams to identify user needs and deliver comprehensive solutions supporting analytics and reporting Optimize and migrate data systems through data and process conversion from existing systems to Data Vault Define and enforce coding standards, data modeling, and ETL/ELT architecture, ensure compliance with Data Mesh and other modern approaches Coach and mentor the data engineering team, conduct code reviews, participate in architectural discussions, and initiate innovative technological solutions Recruitment language: Polish Start ASAP Permanent contract Fully remote Free coffee Free breakfast No dress code Modern office Sport subscription Training budget Private healthcare Small teams International projects Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title – get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers – Power People. Learn more atsii.pl.","[{""min"": 22000, ""max"": 26000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,26000,Net per month - B2B
Full-time,Manager / C-level,Permanent,Hybrid,389,Big Data Staff Engineer,Relativity,"Job Overview At Relativity, we make software to help users organize data, discover the truth, and act on it. Our e-discovery platform is used by more than 13,000 organizations around the world to manage large volumes of data and quickly identify key issues during litigation, internal investigations, and compliance projects. We are seeking a Staff Data Engineer to join our Relativity Data Services organization, a team dedicated to developing data and AI infrastructure that powers AI-driven applications in support of our mission to drive pursuit of justice. Relativity’s scale and breadth provide significant opportunities for rich data exploration and insights. Our market position and advanced products ensure that our latest models and insights can quickly benefit our users. Great insights stem from excellent data, and the best insights arise from substantial data. Our data infrastructure and engineering guarantee that Relativity's vast data is accessible for insights, confidential data remains secure, and data protection is always upheld. We are making substantial investments in data pipeline and data lake technology for the future. In this role, you will partner with teams across the Data Services organization to scale and optimize our data platforms, advance tooling for large-scale distributed processing, and enable critical use cases such as reporting, analytics, and audit. Additionally, you will lead efforts to redefine and strengthen our approach to audit and behavioral analytics. Role overview: Staff Engineer serves as a technical liaison between his or her teams and other internal and external development teams to identify and resolve dependencies, to identify, improve, and apply software engineering best practices and processes, and to identify and mitigate risks to the on-time delivery of software. Staff Engineer – thinks what to buy or what to build, designs the architecture to serve the user needs and support the system scale. Understands the trade-offs to be made and that there are no silver bullets. But ultimately builds systems that work, deliver value in time and are predictable to operate and extend. Staff Engineer serves as a mentor to other team members to improve technical and process expertise and promotes collaboration. Job Description and Requirements Responsibilities Lead design of software using abstraction, low coupling and high cohesion, modularization, encapsulation and information hiding, and separation of concerns. Lead implementation of software using practical application of algorithms, defensive programming and exception handling, fault tolerance, design patterns. Be pragmatic – in using object-oriented principles, applying SOLID principles and design patterns in a variety of languages. Build systems that are low maintenance but not overengineered – balancing security, observability and extensibility with time-to-market and user value. Specify non-functional software requirements and analyze all requirements to determine design feasibility within time and cost constraints. Test and lead test of software emphasizing the practice of Test-Driven Design and the use of autonomous frameworks and Continuous Integration. Identify and offer solutions to reduce technical debt. Display an ownership mindset; be accountable for and beyond the features your team and larger organization develops. Provide solutions to varied and ambiguous issues, utilizing judgment to select methods and techniques for obtaining solutions. Offer coaching to ensure the team stays focused and delivers against the goals, adapting to changing business requirements. Advocate for and ensure adherence to best practices in coding standards, quality assurance, and security, while aligning solution with company-wide architectural principles. Your Skills 10+ years of professional software development experience on commercial-grade systems and applications with a proven track record of building and shipping successful software. 6+ years of hands-on experience with large-scale data infrastructure and cloud-native distributed systems. Proven proficiency in multiple programming languages, with a strong aptitude for rapidly learning and adapting to new technologies. Experience with at least two of the following is required: Java, Python, Scala, Rust and C#. Experience building and optimizing data pipelines using Apache Spark for large-scale batch workloads. Familiarity with deploying and managing data workloads on Kubernetes, including containerization and orchestration best practices. Extensive knowledge of and adherence to SDLC (Software Development Life Cycle) standards and best practices. Ability to consistently identify and deliver technical improvement feedback to team members in a supportive and constructive manner, to achieve demonstrable results over time. Excellent problem solving-solving skills, with a clear ability to present trade-offs, make informed decisions, and drive strategic execution. Excellent verbal and written communication to clearly, succinctly, and completely communicate intent (both technical and non-technical) in interactions with team members and management. Nice to have: Experience working with Data Lake and Lakehouse architectures on cloud storage platforms like ADLS. Nice to have: Hands-on experience or practical understanding of machine learning systems and their integration into production environments. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law. #LI-MM5 Relativity is committed to competitive, fair, and equitable compensation practices. This position is eligible for total compensation which includes a competitive base salary, an annual performance bonus, and long-term incentives. The expected salary range for this role is between following values: 300 000 and 450 000PLNThe final offered salary will be based on several factors, including but not limited to the candidate's depth of experience, skill set, qualifications, and internal pay equity. Hiring at the top end of the range would not be typical, to allow for future meaningful salary growth in this position.","[{""min"": 300000, ""max"": 450000, ""type"": ""Gross per year - Permanent""}]",Data Engineering,300000,450000,Gross per year - Permanent
Full-time,Senior,Permanent,Hybrid,392,"Director, Data Engineering & Analytics",Optiveum,"Director, Data Engineering & Analytics Location: Warsaw (remote for now, hybrid soon) Salary: up to 33 000 PLN/month Contract type: Employment contract (UoP only) Our client is a US-based technology company headquartered in New York City, delivering innovative digital solutions and cloud-based platforms for private capital markets. With offices in multiple countries, the company is now investing in a new engineering centre in Warsaw, highly valuing the technical expertise and strong work ethic of software engineers in Poland. Currently, our client is building a new team in Warsaw. While the work is fully remote for now, within a few months, the role will require working from the Warsaw office 3 days per week. About the role As Director of Data Engineering & Analytics , you will lead a high-performing team of data engineers and be responsible for building and maintaining scalable data platforms and architectures. You will play a key role in shaping data-driven strategies and empowering analytics and data science teams with robust tools and pipelines. What you will do Lead, recruit, mentor, and develop a world-class data engineering team Design and maintain scalable data pipelines and architectures Optimize infrastructure for data extraction, transformation, and loading (ETL) Model data for optimal performance and insights Build and implement tools and systems that empower analytics and data science functions What we are looking for 8+ years of experience in data engineering or cloud/web engineering 5+ years in leadership roles with direct client communication Hands-on experience with ETL tools (AWS Glue, Azure Data Factory) Knowledge of orchestration tools (Airflow, Prefect) Proficiency with DBT, SQL, and data modeling techniques Experience with data lakes, data warehouses, and data mesh architecture Programming skills in Python or R Knowledge of reporting best practices and tools (Power BI, Tableau, Redash, etc.) Experience working with cloud platforms like AWS, Azure, or GCP Bachelor’s degree in Computer Science or a related field what is offered Competitive salary (up to 33 000 PLN/month) Employment contract (UoP only) Flexible work arrangements Great paid time off policy Comprehensive benefits package Opportunities for professional growth and leadership Supportive, inclusive, and fast-paced work environment","[{""min"": 28000, ""max"": 33000, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,28000,33000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,398,Data Engineer with Databricks,Margo,"We are looking for data engineers perfectly familiar with data literacy and proficient with data platform architecture patterns. The data platform is leveraging MS Azure services, including ingestion, staging, processing, and serving. The client now entering a phase of scaling Databricks solution, then we are looking for resources with relevant skills on this technology. The position is ideal for you if you have: 3+ years of experience as a Data Engineer Experience with Azure Databricks and Power BI Fluent in English (written and spoken) Joining Margo you can expect: Ability to work in an international consulting company on ambitious projects, Permanent contract or B2B cooperation, Benefits such as medical care and sports card, Co-finantrainings, certification exams and post-graduate studies, Internal training and the possibility of using our know-how, Possibility to use our library free of charge, Individual approach and development opportunities (career path planning, ability to change the project and position, possibility to get involved in outside-project activities with additional remuneration), Possibility to influence the shape of the company, openness to your ideas and willingness to implement them, Excellent working atmosphere, integration events. Data engineer, with proven skills with Databricks. Missions : support our dev team to develop our data pipelines & data transformation & data exposure services Actively Contribute to the continuous improvement of our dev patterns Document developments & unit testing Improve platform observability capabilities Key skills : Proficient in SQL, Python and PySpark At least 2 years of experience in Databricks, with proven experience deploying Unity Catalog solutions Proven experience working in Azure, and more specifically with the following services within Azure: Data Factory, SQL Database, Data Lake, Logic Apps Skilled in Data modelization, whatever the supporting tool. Comfortable with business-specific discussions.","[{""min"": 170, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,200,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,399,Regulatory Reporting IT Analyst / DevOps,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: 🌍hybrid work model from Warsaw - 3 days HO, 2 days from the office 📝 ASAP project start Responsibilities: Design and develop solutions for regulatory reporting on the Snowflake platform Collaborate with developers and SMEs to gather requirements and translate them into technical designs Build and maintain ETL pipelines and orchestration workflows Apply DevOps practices to ensure high code quality, automation, and efficient deployment Support testing, troubleshooting, and documentation of the implemented solutions Requirements: Minimum 3 years of a professional experience Solid experience with DevOps practices and tools Strong hands-on expertise in Snowflake Proficiency in Python development Experience designing and building ETL pipelines Familiarity with orchestration tools (e.g., Airflow, Prefect, etc.) Very good command of English (oral and written) Nice to have: Knowledge of DORA (DevOps Research and Assessment) principles Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 140, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,140,150,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,402,Data Engineer,dotLinkers,"Type of contract: contract of employment (UoP) / B2B Salary ranges: up to 24 000 PLN a month Working model: 100% remote Join our client, one of the leading logistics and transport solutions providers. About the role: Looking for a skilled Lead Data Engineer to drive the design and implementation of robust data systems, while effectively connecting technical teams with business goals. This role involves hands-on development, strategic planning, and cross-team collaboration. Responsibilities: Designing scalable data systems and tools to support analytics, modeling, and decision-making Building advanced data pipelines and platforms on Azure Collaborating closely with technical and business teams Establishing best practices and reusable standards in data engineering Engaging with international projects and external partners Requirements: 5+ years in IT, data engineering, or information systems Expertise in Azure, Spark (Scala/PySpark), Databricks, Kafka, Event Hubs, Python, Java, SQL/NoSQL Experience with DevOps tools and practices (CI/CD, Terraform, Kubernetes) Skilled in streaming data and big data environments Strong leadership and communication skills (English and Polish) Experience working with distributed teams The offer: Flexible remote work with occasional office visits Benefits include private healthcare, insurance, and a sports card High-end equipment Development budget for learning and growth","[{""min"": 20000, ""max"": 24000, ""type"": ""Net per month - B2B""}, {""min"": 20000, ""max"": 24000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,24000,Net per month - B2B
Full-time,Senior,B2B,Remote,403,Senior Data Analyst,Kodiak Hub,"Join Kodiak Hub🐻🚀 Senior Data Analyst – Drive Insights for a Smarter, Sustainable Future Who We Are At Kodiak Hub , we’re revolutionizing how companies manage supplier relationships and elevate procurement strategies. Through our dynamic SaaS platform and international team spirit, we tackle today’s global supply chain challenges head-on. Guided by our values — Caring, Curious, Cool, and Courageous — we build a future where strategic, sustainable, and smart procurement is the standard. About the Role We’re looking for a Senior Data Analyst who’s passionate about turning complex data into actionable insights — for both our internal teams and our global end-users. In this role, you’ll bridge raw data and real-world decisions: optimizing Kodiak Hub’s SaaS platform, shaping our internal analytics landscape, and delivering smart reporting that drives user success. You’ll collaborate cross-functionally, working closely with our Engineering, Product, and Customer Success teams to unlock the full power of data. What You’ll Do Advanced Analytics : Dive deep into SQL/NoSQL data structures to extract, clean, and analyze key datasets. Insight for Action : Generate analytics that inform internal business development and enhance the end-user experience within the Kodiak Hub platform. Data Storytelling : Build dashboards and reports that tell compelling stories through tools like Power BI , Looker , or Tableau . Platform Optimization : Contribute to smarter, more intuitive analytics features within Kodiak Hub’s SaaS modules, driving greater value for users. Stakeholder Collaboration : Partner with Engineering, Product, and Customer Success to translate needs into smart, actionable data solutions. Cloud Proficiency : Work within a cloud-native environment (primarily Azure) , leveraging ready-pulled datasets to support analysis. What We’re Looking For Strong hands-on experience with SQL/NoSQL and database optimization. Expertise in Power BI and/or other visualization tools ( Looker, Tableau ). Familiarity with cloud-native environments, especially Azure (AWS knowledge a plus but not required). Experience translating business and product needs into actionable analytics. Understanding of SaaS platforms and how analytics can drive platform and user success. A collaborative, solutions-focused mindset that aligns with Kodiak Hub’s values . Excellent communication and data storytelling skills — the ability to turn complexity into clarity. Why Join Kodiak? Impact at Scale : Help shape the future of sustainable supply chains and empower users worldwide. Team Spirit : Join a caring, curious, and courageous team with a passion for innovation. Growth Opportunities : Take a senior role with room to grow and make strategic impacts. Modern Ways of Working : Flexibility, ownership, and a commitment to your well-being. Ready to turn data into decisions that matter? Apply now and join us in building the future of smart, sustainable procurement!","[{""min"": 20000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20000,29000,Net per month - B2B
Full-time,Senior,B2B,Remote,405,Data Architect (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As a Data Architect , you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company. This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies. This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. 🚀 Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Hadoop, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. 🎯 What you'll need to succeed in this role: 5+ years of proven commercial experience in implementing, developing, or maintaining Big Data systems. Strong programming skills in Python or Java/Scala : writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity with Big Data technologies like Spark , Cloudera, Airflow , NiFi, Docker , Kubernetes , Iceberg , Trino or Hudi. Proven expertise in implementing and deploying solutions in cloud environments (with a preference for AWS ). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master’s or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. Fluent English (C1 level) is a must. 🎁 Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website ( career page ) and social media ( Facebook , LinkedIn , Instagram ).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Architecture,21000,31920,Net per month - B2B
Full-time,Mid,B2B,Hybrid,406,SAS/Oracle Data Warehouse Engineer,Onwelo Sp. z o.o.,"Poznaj Onwelo: Jesteśmy nowoczesną polską firmą technologiczną, która dostarcza wsparcie eksperckie organizacjom na całym świecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiązania IT, oferując przy tym solidne zaplecze kompetencyjne. W ciągu kilku lat zrealizowaliśmy ponad 300 projektów w Europie i USA, dynamicznie rozbudowując zespół do kilkuset specjalistów i otwierając sześć biur w Polsce oraz oddziały w USA, Niemczech i Szwajcarii. Dołącz do projektu realizowanego dla klienta z branży ubezpieczeniowej, gdzie będziesz częścią zespołu odpowiedzialnego za rozwój narzędzi wspierających Hurtownię Danych. Projekt obejmuje budowę narzędzi automatyzujących, integracyjnych i raportowych, jak również eksplorację nowych technologii i podejść architektonicznych. Tworzyć narzędzia wspierające pracę developerów Hurtowni Danych (SAS, SAS Viya, Oracle i inne technologie) Budować API umożliwiające integrację różnych systemów (REST, SAS, Office365) Projektować rozwiązania wspierające wymianę danych pomiędzy systemami (np. z wykorzystaniem Apache Kafka) Realizować POC nowych technologii i narzędzi (np. SAS Viya, Exadata w chmurze Azure) Opracowywać komponenty optymalizujące przetwarzanie danych i wydajność procesów Projektować techniczne data mart-y i raporty BI (SAS Viya, Power BI) Monitorować aktywność użytkowników oraz tworzyć narzędzia raportowe Wspierać strategiczne projekty poprzez dostarczanie rozwiązań technologicznych Masz doświadczenie w projektowaniu rozwiązań dla Hurtowni Danych i systemów BI Znasz narzędzia i środowisko SAS oraz SAS Viya (DI Studio, Enterprise Guide, SAS Studio, Visual Analytics) Pracujesz z bazami danych Oracle oraz językiem PL/SQL (mile widziane doświadczenie z Oracle Exadata) Potrafisz pracować z narzędziami CI/CD (Git, Bitbucket, Bamboo, JIRA) Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Zaoszczędzisz czas na dojazdach – pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 6 miast w Polsce Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 700, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Data Engineering,700,1000,Net per day - B2B
Full-time,Senior,B2B,Remote,407,Senior Cloud Data Engineer (GCP),Future Processing,"Do naszej linii biznesowej Data Solutions poszukujemy osoby na stanowisko Senior Cloud Data Engineer ze znajomością GCP masz min. 5 lat doświadczenia w IT, w tym min.3,5 roku w pracy z danymi w chmurze GCP(potwierdzone projektami komercyjnymi wdrożonymi na produkcje), budowałeś(aś) i utrzymywałeś(aś)hurtownie/data lake w BigQuery, łącząc wiele źródeł danych, korzystasz zSQLiPythonna poziomie zaawansowanym, znasz metodyki i stosujesz biegleGit oraz CI/CD, tworzysz i optymalizujesz rozwiązania przetwarzające dane (ETL,ELT, itp.) poprzedzone projektem technicznym oraz alternatywami rozwiązań, masz praktyczne doświadczenie w przetwarzaniu danych przy użyciuDataflow / Dataprocoraz orkiestracjiAirflow (Cloud Composer), monitoring, diagnostyka oraz rozwiązywanie problemów w chmurze nie stanowi dla Ciebie problemu i dobrze wiesz, jak zaplanować infrastrukturę oraz obliczyć jej koszt, pracujesz w duchuDevSecOps & FinOps, znasz koncepcjeBigLake / Lakehouse / Data Mesh, znaszarchitekturę SMP oraz MPPwraz z przykładami rozwiązań opartych o te architektury, potrafisz zaplanować infrastrukturę GCP, estymować i optymalizować jej koszty, masz doświadczenie wmigracji rozwiązań on-premise do chmuryi w ochronie danych (IAM, DLP, GDPR), swobodnie współpracujesz z klientemi interdyscyplinarnymi zespołami, posługujesz sięj. angielskimna poziomie średniozaawansowanym (min. B2). odpowiedzialność end-to-endza rozwiązania data-platformowe tworzone wspólnie z zespołem, tworzenie i optymalizację potokówETL/ELT w GCP(BigQuery, Dataflow, Dataproc, Cloud Composer), tworzenie i modyfikowanie dokumentacji, budowanie i utrzymywaniekatalogu oraz modeli danychzgodnie z najlepszymi praktykami Data Governance, analizę wymagań biznesowych i dobór optymalnych rozwiązań technologicznych, analizowanie potencjalnych zagrożeń, monitoring, diagnostykę i FinOps-owe optymalizacjekosztów chmury.","[{""min"": 135, ""max"": 200, ""type"": ""Net per month - B2B""}]",Data Engineering,135,200,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,408,Sr Cloud Engineer - Data Integration Platform,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Sr. Cloud Engineer - Data Integration Platform The Data, Analytics and AI Organization of the Digital Transformation Unit of Bayer Consumer Health operates a global transactional data integration backbone which is used to connect internal and external applications and systems into a uniform experience for our customers, consumers and employees. The backbone (platform) provides composable infrastructure modules, development frameworks, CI/CD automations and developer tooling to internal and external software engineering teams that build integrations on top of the platform. As Sr. Cloud Engineer - Data Integration Platform you are responsible for planning, designing and implementing platform capabilities, steering external software engineering teams and overseeing operations of the cloud infrastructure. To do so, you will engage with Enterprise Architects, Integration Architects and Software Engineers from Bayer and partner companies as well as incorporate industry best practices and trends into your designs. Key Tasks & Responsibilities: Contribute to platform roadmap together with Enterprise and Integration Architects, derive requirements for missing capabilities and actively shape the future of the platform Plan, design and implement capabilities on Amazon Web Services and/or the GitHub ecosystem based on derived requirements in line with security guidelines & non-functional requirements like cost efficiency, elasticity & scalability, maintainability, developer experience and documentation guidelines. Engage and consult with software engineering teams building integrations on the platform on design, best practices and integration patterns Document and share newly built capabilities within the corporate software engineering community Observe and assess platform compliance and implement solutions to ensure platform operations within compliance guidelines such as security or regulatory aspects Engage with technology and implementation partners to overcome issues if required Engage with central cloud teams, network and security teams, cyber security teams to overcome issues if required and follow company best practices Qualifications & Competencies (education, skills, experience): Master’s degree in computer science, Engineering, or similar Proficient knowledge of distributed systems and their challenges such as eventual consistency, horizontal scaling and parallel processing, consensus, operations and maintenance of a microservices architecture Good knowledge of data integration technologies and concepts such as message brokers, streaming systems, batch processing, API design and - management 3-5 years of working experience in AWS cloud development with focus on serverless services such as AWS Lambda, AWS SQS, AWS DynamoDB, AWS API Gateway and augmenting services such as AWS CloudWatch, AWS IAM. Experience with AWS networking services, AWS DocumentDB (MongoDB) and AWS container services are a plus. Proficient knowledge of infrastructure as code tooling (preferred: Terraform) to automate infrastructure deployment. Additional coding skills such as shell scripting for task automation are a plus. 2-3 years of experience of software development with JavaScript/ECMAScript (preferred: server-side development with NodeJS). Good knowledge of the GitHub ecosystem including the core VCS, GitHub Actions for CI/CD and working with tickets/issues and pull requests Problem solving and analysis skills, combined with profound business judgement as well as good documentation and communication skills (active listening, consulting, challenging). Intercultural awareness and willingness to travel from time to time. Fluent in English, both written & spoken What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360° Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (“Wczasy pod gruszą”) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn’t mean you aren’t the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,28500,Gross per month - Permanent
Full-time,Mid,B2B,Remote,410,"BI Developer (Snowflake, Matillion)",Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! 😊 Design, build, and develop data warehouses based on Snowflake; Create and optimize ETL/ELT processes using Matillion; Integrate data from various sources (databases, APIs, files); Develop reports and dashboards in Power BI for internal clients; Maintain, monitor, and further develop existing BI solutions; Collaborate with project teams and business stakeholders to gather and analyze requirements; Participate in data migration from legacy systems (e.g., Oracle, SSIS) to Snowflake; Ensure high-quality technical and business documentation; Implement best practices for data management, security, and version control; Actively participate in Agile team meetings (e.g., daily stand-ups, sprint planning, retrospectives); 3-5 years' experiencea BI Developer role; At least 2 years’ hands-on with Snowflake; Strong experience withMatillion; Solid skills inPower BI; Eager to work as afull-stack BI Developer(both backend and frontend); Strong English skills (min. C1 level, daily communication with international clients); Proactive and creative - skills to drive improvements and engage with both technical and non-technical stakeholders; Background inManaged Servicesdelivery models - you know how to take end-to-end ownership of BI solutions, ensuring their reliability, scalability, and alignment with client needs throughout the entire lifecycle; Experience working inAgileteams; Strong documentation skills and a knack for business analysis. Experience withOracle(we’re migrating to Snowflake, but legacy knowledge is a plus); Experience withSSAS/SSIS/SSRS; Familiarity withVisual Studio; Understanding ofversion control concepts(branching, merging, pushing; Git integrated with Matillion). Background in procurement, supply chain, or business data analysis related to orders and internal corporate stakeholders; By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad – so far we've been in Cape Town, Are, and Barcelona). Fully remotework or in our office in Wrocław; B2B Contract: 120 – 140 PLN net/hour + VAT Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories.","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Senior,B2B,Remote,411,Senior Data Scientist,hubQuest,"We are a team of tech enthusiasts on a mission to bring together the best minds in IT services and analytics. Our goal? To create cutting-edge IT and Analytical Hubs that empower our partners to become truly data-driven organizations. Tackle real-world challenges with advanced analytics and machine learning Lead impactful projects that influence decision-making across global markets Flexible remote or hybrid work model (with a modern office in Warsaw) Join a diverse and experienced international team No red tape – just real tech challenges, autonomy, and ownership Enjoy private medical care, Multisport, access to online learning platforms and certifications Supportive, collaborative, and relaxed work culture About the Team What You’ll Do Take technical ownership of an end-to-end data science project – from business understanding to deployment Develop predictive and prescriptive models that support strategic and operational goals Design, test, and implement scalable data pipelines and modeling workflows (Python, PySpark) Work closely with stakeholders to identify opportunities for impact through data science Lead experimentation, model validation, and performance monitoring in production Contribute to the design of robust and maintainable analytics architecture usingAzure cloud services What We’re Looking For Hands-on experience with the Azure ecosystem(e.g., Azure Databricks, Azure ML, Azure Data Factory, Azure DevOps) AdvancedPythonandPySparkskills, including writing production-level code Experience with large-scale data processing and analytics Good understanding of CI/CD pipelines and deployment automation Strong background in building and deploying machine learning models 5+ years of data science experience in production environments 2+ years in senior or lead roles with proven project ownership Strong portfolio of deployed ML models used in production at scale Ability to independently lead projects from conception to launch Excellent communication and collaboration skills across technical and non-technical teams Adaptability to new tools, processes, and environments Self-driven learner with a proactive approach to skill development Ready to take ownership and build data science solutions that drive global decisions?Join us and help shape the future of data-driven transformation. Please add to your CV the following clause: ""I hereby agree to the processing of my personal data included in my job offer by hubQuest spółka z ograniczoną odpowiedzialnością located in Warsaw for the purpose of the current recruitment process.” If you want to be considered in the future recruitment processes please add the following statement: ""I also agree to the processing of my personal data for the purpose of future recruitment processes.”","[{""min"": 28500, ""max"": 37000, ""type"": ""Net per month - B2B""}]",Data Science,28500,37000,Net per month - B2B
Full-time,Mid,B2B,Remote,412,Reporting Developer (Power BI/Tableau),Britenet,"About the role Project carried out for the lottery industry.The work is conducted remotely, with occasional visits to the office in Warsaw. Our expectations Minimum 3 years of experience in a similar position Proficient inPower BIand familiar withTableau Basic knowledge of project management principles, especially using Tableau/Power BI Understanding of best practices in data visualization Experience working with SQL for data extraction and manipulation Experience with Microsoft SQL Server and ETL processes Knowledge of data preparation, modeling, and visualization techniques Strong communication and teamwork skills Ability to translate business requirements into effective solutions Willingness to learn and adapt to new technologies and tools Very good command of English (minimum B2 level) Openness to occasional on-site work in the Warsaw office Main responsibilities Managing projects related to PowerBI/Tableau implementation, data visualization, and analytics Collaborating with other teams to gather requirements, design solutions, and deliver Tableau dashboards and reports Analyzing data trends and providing strategic insights to support business decision-making Working with team members on project tasks and deliverables Participating in project meetings and contributing to project status updates","[{""min"": 90, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,90,140,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,415,Tester Data&BI,summ-it,"Uprzejmie informujemy, że z uwagi na sezon urlopowy czas odpowiedzi w procesie rekrutacji może się wydłużyć. Jeśli chcesz nauczyć się nowych technologii, a następnie pracować w projektach dla marek znanych na całym świecie, a jednak wciąż w ramach niekorporacyjnej struktury, to idealne miejsce dla Ciebie! W summ-it stwarzamy przestrzeń na zgłaszanie swoich pomysłów i dbamy o Twój rozwój. Nasi pracownicy biorą udział w konferencjach branżowych i wydarzeniach dotyczących szeroko pojętej tematyki IT, a także udoskonalają swoje umiejętności dzięki różnego rodzaju szkoleniom i warsztatom. Pracujemy z bazami danych o łącznej wielkości liczonej w PB, i optymalizujemy systemy walcząc o każdą ms. Obecnie zarządzamy ponad 10 000 systemów baz dla naszych klientów. Zatrudnienie na podstawie: umowy o pracę, umowy B2B, umowy zlecenie Pracę w atmosferze koleżeństwa i zaufania – w ankiecie satysfakcji ponad 92% pracowników zgadza się z tym stwierdzeniem Odpowiednie wsparcie i współpracę w swoim zespole – w ankiecie satysfakcji 100% pracowników zgadza się z tym stwierdzeniem Elastyczne godziny pracy oraz pracę w modelu hybrydowym Pracę z biura w centrum Poznania Dostęp do najnowszych technologii IT Możliwość rozwoju w międzynarodowej firmie Szkolenia zewnętrzne i wewnętrzne: Miękka środa, Bezpieczne czwartki, Science Friday Spotkania firmowe: Summer Party, Winter Party, AllHands, Talk to Your Boss, Lokalne środy, summ‑itowe śniadania Programy doceniania pracowników: summ-it heores i nagrody za przyznane kudosy Program poleceń pracowniczych Możliwość dołączenia do benefitów (opieka medyczna, karta Multisport, ubezpieczenie grupowe) Pracę w zrównoważonym zespole – 3 generacji: X, Y, Z Przeprowadzanie testów w obszarze danych (Azure Databricks, Azure Data Factory, Azure Synapse, Azure Analysis Services, Power BI, MDS) zgodnie z opisem w User stories Rejestrowanie wyników testów w Azure DevOps zgodnie z procesem Analiza wyniku testów i informacja zwrotna dla developer’ów i SME Opracowywanie scenariuszy testowych (unit, integration, regression) Analiza testów, identyfikacja i wdrażanie automatyzacji testów oraz usprawnień w procesach QA Proponowanie usprawnień w zakresie testów Identyfikacja wzorców na powtarzające się błędy Min. 3 lata doświadczenia w testowaniu danych Znajomość Azure Databricks (notebook, job, cluster Spark) Znajomość Azure Data Factory (pipeline’s datasets, linked services, monitor) Znajmość Azure Synapse (Dedicated pool) Znajomość modeli tabularnych (Analysis Services) i serwisu Power BI (data lineage, model semantyczny, połączenia) Biegłość w SQL (weryfikacja jakości danych, przygotowywanie zapytań testowych) Zrozumienie procesów ETL Znajomość narzędzi do automatyzacji testów i CI/CD (w szczególności Azure DevOps, TestPlans) Znajomość języka angielskiego na poziomie min. B2 Dziękujemy za zainteresowanie naszą ofertą i nadesłanie aplikacji. Uprzejmie informujemy, że skontaktujemy się z wybranymi kandydatami.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 10000, ""type"": ""Gross per month - Permanent""}]",Unclassified,10000,13000,Net per month - B2B
Full-time,Senior,B2B,Remote,421,Senior Backend Engineer (with Data Science Skills) at PeakData,PeakData,"PeakData provides AI-powered market intelligence to optimize drug launch execution and resource allocation for pharmaceutical companies. Our platform delivers actionable insights on healthcare professionals (HCPs) and healthcare organizations (HCOs), empowering commercial leaders with real-time, data-driven decision-making. We're looking for aSenior Engineerwith strongdata science capabilitiesto join our Data Platform team. In this role, you’ll design and build cloud-native data solutions that support large-scale processing, analytics, and AI-powered automation across our platform. This is ahands-on, senior-levelrole. You will be expected to work independently, own end-to-end pipelines and infrastructure, and drive initiatives forward both individually and within the team. You should have a strong foundation inPython, SQL, AWS, and/orGCP, with experience using or integratingLLMsinto data workflows. You’ll work with and expand upon: Pythonfor data pipelines and automation SQL(PostgreSQL) for transformation and analytics AWS (S3, Glue, Lambda, ECS, Bedrock)as primary cloud environment GCP (Vertex AI)for select workloads and integrations Medallion architecturewith RAW/CLEANED/CURATED layers LLM integrations for automation, enrichment, and insight generation Data quality frameworks and orchestration tools (e.g., Argo) Design, implement, and maintain scalable and efficient data pipelines across AWS and GCP Build data products and services supporting both internal analytics and client-facing insights Own ETL/ELT workflows from ingestion to curation Implement observability and alerting for pipeline health and data integrity Integrate LLMs into workflows to support enrichment, automation, or intelligent data handling Act as a technical lead for data engineering projects, driving execution independently Collaborate cross-functionally with Data Science, Product, and Engineering teams Contribute to architectural decisions and long-term data platform evolution Champion best practices for performance, security, and scalability Apply data science techniques where appropriate (e.g., clustering, statistical inference) Prototype and validate LLM-powered solutions using tools like AWS Bedrock or Vertex AI Use prompt engineering and evaluation frameworks to fine-tune LLM interactions Help bridge engineering and AI innovation across the platform 6+ years of experience indata engineeringor back-end systems with data-heavy workloads Strong hands-on skills withPythonandSQL Deep understanding ofAWScloud data tooling (S3, Lambda, Glue, Step Functions, etc.) Working experience withGCPservices, especially BigQuery and Vertex AI Exposure toLLMsand how they integrate into data workflows Experience buildingdata pipelinesat scale with monitoring and alerting Ability to work independently and take ownership of technical topics Experience withArgo, Airflowor similar orchestration frameworks Familiarity withIaC tools(Terraform) for deploying infrastructure Experience withdata quality monitoring, validation frameworks, or anomaly detection Previous work in healthcare, life sciences, or regulated data environments Proactive: You take initiative and don’t wait for tasks to be assigned Autonomous: You can own projects from design to production with minimal oversight Curious: You explore new approaches (especially LLMs/AI) and bring them to the table Collaborative: You work well with cross-functional teams Customer-aware: You understand the real-world impact of your pipelines and models Purpose-driven work: support pharmaceutical innovation and better patient outcomes Ownership: real autonomy in shaping our data systems and how they scale Innovation: work on LLM integration and next-gen data workflows A collaborative, fast-moving environment Competitive compensation Access to both AWS and GCP ecosystems in production If you're a hands-on data engineer who enjoys owning end-to-end systems, loves solving real business problems, and thrives in a hybrid cloud + AI environment — we want to talk to you.","[{""min"": 28000, ""max"": 36000, ""type"": ""Net per month - B2B""}]",Data Science,28000,36000,Net per month - B2B
Full-time,Mid,B2B,Remote,422,Data Migration Specialist with German,in4ge sp. z o.o.,"Key Responsibilities: Performing data migration from Dynamics 365 system. Analyzing data migration requirements. Creating a comprehensive data migration plan. Preparing and cleaning data (data remediation) to ensure high data quality. Testing and verifying migrated data. Executing and monitoring the end-to-end data migration process. Minimum 1 year of experience working with Dynamics 365, especially in the area of data migration. Strong knowledge of SQL and Microsoft Excel. Good understanding of business processes. Fluent English and German (C1) and ability to work effectively in a distributed, international team. Fully remote work with flexible working hours -EMEA Timezone. Long-term collaboration on B2B contract. Opportunity to work on complex cloud projects for international clients. Professional growth in a highly skilled and supportive team. Collaborative and open working culture.","[{""min"": 18000, ""max"": 29000, ""type"": ""Net per month - B2B""}]",Unclassified,18000,29000,Net per month - B2B
Full-time,Mid,B2B,Remote,425,Insurance Data Analyst,Link Group,"Opis stanowiska: Poszukujemy doświadczonego Insurance Data Analyst do pracy w międzynarodowych projektach technologicznych. Osoba na tym stanowisku będzie odpowiedzialna za analizę danych, przygotowanie wymagań biznesowych oraz współpracę z klientami z sektora ubezpieczeń. Szukamy specjalisty o podejściu technologicznym, który wesprze nasze zespoły w realizacji ambitnych projektów. Zakres obowiązków: Analiza biznesowa oraz zbieranie wymagań funkcjonalnych i niefunkcjonalnych Budowanie i utrzymywanie relacji z klientami oraz onsite koordynatorami Wsparcie w planowaniu i realizacji projektów Analiza, przygotowanie i eksploracja danych Tworzenie raportów oraz wizualizacji danych przy użyciu narzędzi no-code/low-code Przestrzeganie standardów jakości oraz wspieranie inicjatyw organizacyjnych Współpraca w środowisku Agile (Scrum, Kanban) Wymagania: Minimum 3 lata doświadczenia w pracy na podobnym stanowisku w środowisku korporacyjnym Doświadczenie w branży ubezpieczeniowej (min. 2 lata, aktualne) Biegła znajomość SQL oraz technik projektowania relacyjnych baz danych, data martów, hurtowni danych i data lakes Umiejętności w zakresie czyszczenia, eksploracji i analizy danych Znajomość narzędzi do wizualizacji danych Bardzo dobra znajomość języka angielskiego (w mowie i piśmie) Wykształcenie wyższe techniczne (Inżynieria, IT, Nauki Ścisłe) Mile widziane: Znajomość OpenL Tablets Certyfikaty potwierdzające umiejętności (Java, Spring, SQL, AWS lub Azure Cloud, Angular, React) Doświadczenie w bezpośredniej pracy z klientem","[{""min"": 95, ""max"": 125, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,95,125,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,426,Data Engineer,Tesco Technology,"Tesco Technology is multi-functional and specialist team that drives operational excellence of services improves scale for our systems and processes globally and creates business leading capabilities. We are an agile team of an industry-leading team of engineers. We create the future continuous integration and delivery tools for Colleague and Customer & Loyalty areas, solving problems, and developing new features through quality, scalable, performant, and maintainable technical solutions. The solutions that we are responsible for will have a global reach, impacting hundreds of thousands of Tesco colleagues worldwide. We operate in a DevOps philosophy. We take responsibility for the software through its entire lifecycle. We practice continuous integration, delivery, and support of our code through to production and beyond. As Tech Hub we cooperate within the group of Tesco Technology Hubs located in the UK, Poland, Hungary, and India. We always welcome conversations about flexible working, so feel free to talk to us during your application about how we can support you.We value connecting, collaborating, and innovating with our colleagues in person. At Tesco Technology, we work in a hybrid model. This role requires you to be based in or near Kraków, as we currently meet in the office three days a week. The Data Engineering department at Tesco Technology is at the forefront of data processing within the retail and technology industry. This vital department handles a range of responsibilities, including: Analyzing order and delivery data to optimize logistics processes and enhance delivery efficiency. Managing critical data related to customer orders, suppliers, and products to ensure the seamless flow of our fulfillment operations. Upholding data integrity and security during the processing of order and delivery-related information. As we continue to expand, we are actively seeking a skilled Data Engineer to join our team of analytics experts. In this role, you will take charge of expanding and refining our data and data pipeline architecture. Additionally, you will be instrumental in optimizing data flow and collection to cater to the needs of cross-functional teams. Our ideal candidate is an experienced data pipeline builder and data enthusiast who relishes the opportunity to optimize data systems and construct them from the ground up. As a Data Engineer, you will collaborate closely with software developers, database architects, data analysts, and data scientists on various data-driven initiatives. You will play a crucial role in ensuring that the optimal data delivery architecture remains consistent across all ongoing projects. This role calls for a high level of self-direction and the ability to effectively support the data requirements of multiple teams, systems, and products. If you are enthusiastic about the prospect of optimizing, and possibly even redesigning, our company's data architecture to support our next generation of products and data initiatives, we encourage you to apply and be part of our dynamic team shaping the future of our data operations! Responsibilities Create and maintain optimal data pipeline architecture Assemble large complex data sets that meet functional / non-functional business requirements. Identify design and implement internal process improvements: automating manual processes optimising data delivery re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction transformation and loading of data from a wide variety of data sources Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition operational efficiency and other key business performance metrics. Work with stakeholders including the Executive Product Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure Create data tools for analytics and data scientist team members that assist them in building and optimising our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Mandatory skills: Data Processing: Apache Spark - Scala or Python Data Storage: Apache HDFS or respective cloud alternative Resource Manager: Apache Yarn or respective cloud alternative Lakehouse: Apache Hive/Kyuubi or alternative Workflow Scheduler: Airflow or alternative Nice to have skills: Functional programming Apache Kafka Kubernetes Stream processing CI/CD Unsure if you fit all the criteria? Apply and give us the chance to evaluate your potential – you could be the perfect fit! We value flexibility at Tesco; therefore, this position is also available for candidates who are interested in working part time – about 120 hours a month or more. Please let us know what would work for you. Hybrid work We know life looks a little different for each of us. That’s why at Tesco, we always welcome chats about different flexible working options. Some people are at the start of their careers, some want the freedom to do the things they love. Others are going through life-changing moments like becoming a carer, adapting to parenthood, or something else. So, talk to us throughout your application about how we can support.This role requires you to be based in or near Kraków, as you will spend 60% (3 days) of your week collaborating with colleagues at our office locations or local sites and the rest remotely. Benefits Tesco is a diverse and exciting employer, dedicated to being #aplacetogeton, providing career-defining opportunities to all of our colleagues. If you choose to join our business, we will provide you with (for all): MacBook as your tool for work Learning opportunities - certified technical training and learning platforms like Udemy, Pluralsight and O’reily Referral Bonus Sports activities with a personal trainer in the office Benefits for colleagues on employment of contract only: Additional 4 days of paid leave to support your well-being and family life Up to 20% yearly salary bonus – based on both individual and business performance Private healthcare (LuxMed) Cafeteria & Multisport Supporting those, who are not yet eligible for full holiday entitlement, by expanding their pool from 20 to 25 days Relocation Help IP Tax Deductible Costs If that sounds exciting, then we'd love to hear from you. Tesco is committed to celebrating diversity and everyone is welcome at Tesco. As a Disability Confident Employer, we’re committed to providing a fully inclusive and accessible recruitment process, allowing candidates the opportunity to thrive and inform us of any reasonable adjustments they may require.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 18500, ""max"": 26000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,30000,Net per month - B2B
Full-time,Senior,B2B,Remote,429,Data Engineer MS Fabric,1dea,"Dla jednego z dużych klientów poszukujemy osoby do roli: Data Engineer MS Fabric! Warunki zaangażowania: Lokalizacja: 100% zdalnie Start: ASAP (akceptujemy kandydatury z max 1msc okresem wypowiedzenia) Stawka (ustalana indywidualnie): 130 - 160 PLN net / h Zaangażowanie: B2B (outsourcing z 1dea), full-time, długofalowo Minimum 3 lata doświadczenia jako Data Engineer, Znajomość technologii: Azure Data Factory, Azure, PySpark, Microsoft Fabric, Solidne podstawy w zakresie modelowania danych oraz architektury hurtowni danych. Język angielski na poziomie min. B2 Przejrzysty model współpracy: Zatrudnienie przez 1dea na podstawie umowy B2B Stabilne i bezpieczne środowisko pracy: Dołączysz do firmy z solidną pozycją na rynku Nowoczesne wyposażenie: Firma zapewnia nowoczesny sprzęt, oprogramowanie i konfigurację Elastyczny czas pracy: Możliwość pracy w elastycznych godzinach Praca zdalna: możliwa w 100% Profesjonalne doradztwo i wsparcie: Profesjonalne doradztwo i wsparcie w rozwoju kariery od doświadczonego zespołu specjalistów 1dea Przyjemna atmosfera w zespole: Cenimy sobie koleżeńskość, otwartość, szacunek, wzajemną pomoc i wsparcie w rozwijaniu kompetencji zarówno własnych, jak i kolegów i koleżanek z zespołu Kultura kreatywności: Wspieramy kulturę kreatywności. Każdy członek zespołu ma możliwość proponowania własnych pomysłów i rozwiązań, a jego głos jest zawsze brany pod uwagę","[{""min"": 130, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,130,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,430,ETRM Data Scientist,INFOPLUS TECHNOLOGIES,"We are looking for a highly skilled ETRM Data Scientist with 7+ years of experience to join our advanced analytics team. The ideal candidate will have deep expertise in time-series forecasting, scalable ML systems, and MLOps, with hands-on experience in energy trading (ETRM) environments. This is a high-impact role for someone passionate about building intelligent solutions in a cloud-native setting. Mandatory Skills Data Science & Machine Learning Expertise in time-series forecasting , predictive modelling , and deep learning . Experience with ML algorithms such as ARIMA , LSTM , Prophet , Linear Regression , Random Forest . Strong proficiency in scikit-learn , XGBoost , Darts , TensorFlow , PyTorch , Pandas , and NumPy . Proven knowledge of ensemble techniques ( stacking , boosting , bagging ) for robust model design. Ability to optimize and retrain production ML models based on evolving data and business needs. MLOps Implementation Proficiency in Python-based MLOps frameworks for automated pipelines, monitoring, and retraining. Hands-on experience with Azure Machine Learning Python SDK : Designing parallel model training workflows. Utilizing distributed computing for large-scale data processing. Big Data Analytics Strong experience with PySpark for distributed data processing and large-scale analytics. Azure Cloud Expertise Azure Machine Learning: Model deployment, training orchestration, lifecycle management. Azure Databricks: Data engineering and collaborative development using PySpark/Python. Azure Data Lake: Managing scalable storage and analytics pipelines for large datasets. Preferred Skills K-Means Clustering for segmentation and pattern analysis. Bottom-Up Forecasting for hierarchical business insight generation. Azure Data Factory for pipeline orchestration and ETL integration. Basic understanding of power/energy trading and ETRM systems. Exposure to Generative AI (GenAI) such as GPT or similar technologies.","[{""min"": 32000, ""max"": 36800, ""type"": ""Net per month - B2B""}]",Data Science,32000,36800,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,432,Senior Data Science/AI Engineer,N-iX,"(3767) About client: Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management.Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact. Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company’s R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 22164, ""max"": 25859, ""type"": ""Net per month - B2B""}, {""min"": 18470, ""max"": 21056, ""type"": ""Gross per month - Permanent""}]",Data Science,22164,25859,Net per month - B2B
Full-time,Mid,B2B,Remote,433,Data Scientists with Generative AI (mid/senior),Yosh.AI,"We are excited to announce an open position for: Data Scientists with Generative AI (mid/senior) - Location: Warsaw/remote We are looking for a hands-on generative AI engineer to architect, build, and deploy the next generation of autonomous and agentic AI systems. In this role, you will bridge the gap between rapid prototypes and robust, production-ready solutions that solve complex business challenges for our enterprise clients. You will be at the forefront of developing innovative technologies on a global scale, leveraging the full power of Google's Generative AI stack, LangChain, and other advanced models to create solutions that think, reason, and act. This is a unique opportunity to own end-to-end projects and work with a team of passionate experts, in close collaboration with Google, to drive real-world impact. Key Responsibilities: Architect and build enterprise-grade, agentic AI systems and conversational agents Engineer and deploy scalable AI/ML solutions on Google Cloud Platform (GCP) Develop and maintain serverless systems and containerized Python REST APIs Leverage LLMs to perform deep analysis on structured and unstructured data, enhancing our big data analysis platforms Drive innovation by rapidly prototyping new solutions and championing out-of-the-box thinking Required Experience: Proven experience building and deploying AI/ML models in a production environment Excellent programming skills in Python and experience building and deploying REST APIs Hands-on experience with a major cloud platform (GCP, AWS, or Azure). Practical experience with LLM orchestration frameworks Deep knowledge of Generative AI concepts Understanding of machine learning concepts Proficiency with version control systems (Git) Highly Desirable: Specific, in-depth experience with the Google Cloud Platform (GCP) AI/ML stack Experience designing and building fully autonomous or agentic AI systems Knowledge of technologies related to LLM models (e.g., LangChain, ADK, Vertex AI) Practical experience with Conversational AI platforms (e.g., Google Dialogflow CX/Playbooks) Experience with containerization technologies, specifically Docker Salary Range: 10.000-20.000 PLN gross per month (full time contract equivalent, depending on experience) We offer: Opportunity for professional development in the area of GenAI Private Medical insurance Multisport card Google certification paths Hybrid or remote location - our office is located in the center of Warsaw Cooperation with a great team of energetic and open-minded people","[{""min"": 10000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Science,10000,20000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,436,Middle/Senior Data Engineer,N-iX,"#3682 We are seeking aMiddle/Senior Data Engineerwith proven expertise inAWS, Snowflake, and dbtto design and build scalable data pipelines and modern data infrastructure. You'll play a key role in shaping the data ecosystem, ensuring data availability, quality, and performance across business units. 4+ years of experience in Data Engineering roles. Experience with theAWScloud platform. Proven experience withSnowflakein production environments. Hands-on experience building data pipelines usingdbt. Pythonskills for data processing and orchestration. Deep understanding of data modeling and ELT best practices. Experience with CI/CD and version control systems (e.g., Git). Strong communication and collaboration skills. Strong experience withSnowflake(e.g., performance tuning, storage layers, cost management) Production-level proficiency withdbt(modular development, testing, deployment).. Experience developingPythondata pipelines. Proficiency in SQL (analytical queries, performance optimization). Experience with orchestration tools like Airflow, Prefect, or Dagster. Familiarity with cloud platforms (e.g., GCP, or Azure). Knowledge of data governance, lineage, and catalog tools. Experience in working in Agile teams and CI/CD deployment pipelines. Exposure to BI tools like Tableau or Power BI. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 110, ""max"": 195, ""type"": ""Net per hour - B2B""}, {""min"": 81, ""max"": 162, ""type"": ""Gross per hour - Permanent""}]",Data Engineering,110,195,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,437,👉 Data & AI Enterprise Architect,Xebia sp. z o.o.,"🟣You will be: leading and inspiring cross-functional architecture efforts across our client’s organization, filling an internal strategic role designed to bring added value to the client by aligning data architecture with business innovation and long-term growth, acting as a thought leader and enabler, supporting dedicated architects and teams across multiple domains, including: AI & ML Projects (40–60%), B2B & B2C E-commerce Platforms (20%), DataOps & Data Engineering (20%). 🟣Your profile: proven experience as an Enterprise or Lead Data Architect in complex, enterprise-scale environments, deep expertise in Microsoft Azure and Databricks, strong understanding of data architecture principles, data governance, and modern data platforms, ability to work across business and technical domains, with a focus on value creation and business impact, willingness to occasionally work on site in Amsterdam, very good command of English (min. C1). 🟣Nice to have: familiarity with technologies used across the client’s ecosystem, such as: Salesforce, Event Hubs / Kafka, Contentful or other headless CMS platforms, experience in customer-facing roles, pre-sales, or innovation consulting.in. Work from the European Union region and a work permit are required. 🟣Recruitment Process: CVreview –HRcall –Technical Interview–ClientInterview (with Live-coding) –Hiring ManagerInterview –Decision 🎁Benefits 🎁 ✍Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 43500, ""type"": ""Net per month - B2B""}, {""min"": 26850, ""max"": 35500, ""type"": ""Gross per month - Permanent""}]",Data Architecture,33500,43500,Net per month - B2B
Full-time,Senior,B2B,Remote,438,Senior Backend Engineer (Cloud & Data Infrastructure Focus) at PeakData,PeakData,"PeakData provides AI-powered market intelligence to optimize drug launch execution and resource allocation for pharmaceutical companies. Our platform delivers actionable insights on healthcare professionals (HCPs) and healthcare organizations (HCOs), empowering commercial leaders with real-time, data-driven decision-making. We are seeking aSenior Backend Engineerwho is passionate about building scalable, cloud-native systems with a focus ondata pipelines,infrastructure, andbackend services— not traditional frontend or UI-heavy applications. This role blends backend engineering, cloud architecture, and infrastructure thinking. You’ll work alongside data engineers and AI specialists to develop services, APIs, and frameworks that power our platform. If you have strong Python skills, experience in AWS and/or GCP, and love solving deep technical problems in distributed systems, this is your opportunity. You’ll be contributing to systems using: Python(FastAPI, asyncio, pandas, etc.) SQLandNoSQLdata stores (PostgreSQL, DynamoDB, BigQuery) AWS(Lambda, ECS, S3, Step Functions, CDK) – core platform GCP– supporting workflows Event-driven architecture and async processing LLM integrations viaAWS BedrockorVertex AI Infra-as-code withCDKorTerraform Containerized services via Docker, ECS, and Kubernetes (limited scope) Design and implement scalable backend services and APIs that support platform data flow and automation Build resilient infrastructure components to handle large-scale data and AI-driven workloads Support data ingestion, transformation, and LLM integration with performant back-end systems Contribute to the modularization and standardization of internal backend tooling Collaborate with data engineers to bridge platform and data systems effectively Own infrastructure components through IaC (e.g., CDK, Terraform) Drive observability (logging, tracing, alerting) for backend and data systems Participate in performance tuning, scalability planning, and cost optimization Ensure reliability and resilience across critical services and pipelines Lead by example in delivering high-quality code and stable services Work independently on initiatives and drive solutions end-to-end Collaborate cross-functionally with Data Science, DevOps, and Product Engineering Mentor peers and support a culture of technical excellence 6+ years of experience in backend or infrastructure-focused engineering roles Strong Python backend development skills (FastAPI, Flask, asyncio) Deep experience withAWS(and familiarity withGCP) Comfortable with SQL and working knowledge of NoSQL datastores Solid understanding of distributed system design and asynchronous processing Experience working with infrastructure-as-code (e.g., CDK, Terraform) Experience with containerized deployment (Docker, ECS, or K8s) Exposure toLLM integration, prompt engineering, or AI-supported pipelines Experience with event-driven architecture (e.g., SQS, Pub/Sub, Kafka) Familiarity with data workflows, orchestration tools (e.g., Airflow), and data validation frameworks Previous work in regulated or high-sensitivity data environments Experience bridging backend systems and AI/data platforms Regular visits to ourWrocław office (ideally twice per month)arevery welcome and positively received— we value face-to-face collaboration when possible! Self-driven: You take ownership and move projects forward independently Infrastructure-minded: You enjoy making systems robust, scalable, and observable Curious: You’re eager to learn about emerging tech like LLMs and AI ops Team player: You work well across disciplines and share knowledge openly Pragmatic: You solve problems efficiently and avoid over-engineering Opportunity to shape the technical foundation of our platform infrastructure Work that contributes to faster, more effective healthcare outcomes A collaborative, agile environment that values autonomy Exposure to both AWS and GCP in real-world deployments Hands-on work with LLMs and cutting-edge AI applications Competitive compensation and real responsibility from day one If you’re a backend engineer who thrives on owning infrastructure and scaling real-time, data-powered systems — we’d love to hear from you.","[{""min"": 28000, ""max"": 36000, ""type"": ""Net per month - B2B""}]",Data Engineering,28000,36000,Net per month - B2B
Full-time,Mid,B2B,Remote,439,Data Engineer,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmą rekrutacyjną, w której wierzymy, że wszystko jest możliwe dzięki odpowiednim ludziom. Naszym celem jest połączenie najbardziej utalentowanych pracowników z odpowiednimi firmami, tworząc synergiczne relacje, które przyczyniają się do wzrostu i sukcesu każdej ze stron. Uważamy, że prawdziwą wartość stanowią ludzie pracujący wspólnie w atmosferze wzajemnego szacunku i zaufania. Poszukujemy doświadczonego Data Engineer do współpracy przy tworzeniu i rozwijaniu rozwiązań przetwarzania danych dla naszych klientów. Twoim zadaniem będzie projektowanie i wdrażanie skalowalnych systemów przetwarzania danych, a także współpraca z zespołami Data Science, Analityki i IT. Twoje zadania Projektowanie, rozwój i utrzymanie hurtowni danych, pipelines ETL/ELT i systemów przetwarzania dużych zbiorów danych. Optymalizacja wydajności procesów przetwarzania danych. Tworzenie i zarządzanie infrastrukturą w chmurze (AWS, GCP, Azure) lub on-premise. Współpraca z zespołami ds. analizy danych i biznesu w celu identyfikacji potrzeb i wymagań. Implementacja rozwiązań monitorujących jakość danych i ich bezpieczeństwo. Wymagania Min. 3 lata doświadczenia na stanowisku Data Engineer. Znajomość przynajmniej jednego narzędzia chmurowego (AWS, GCP, Azure). Doświadczenie z systemami big data, np. Apache Spark, Kafka, Hadoop. Umiejętność pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL) i bazami NoSQL (np. MongoDB, Cassandra). Praktyczna znajomość narzędzi ETL/ELT (np. Airflow, dbt). Nice-to-have : Certyfikaty z zakresu technologii chmurowych. Doświadczenie z modelowaniem danych i optymalizacją zapytań SQL. Znajomość narzędzi monitorujących (Prometheus, Grafana). Zrozumienie zasad CI/CD oraz praktyczne doświadczenie z narzędziami DevOps. Oferujemy Możliwość bycia częścią międzynarodowych projektów. Ambitne i rozwojowe projekty. Wsparcie merytoryczne na każdym etapie wdrożenia. Dostęp do najnowszych technologii. Praca w systemie zdalnym lub hybrydowym, w zależności od projektu. Nasza oferta Możliwość bycia częścią międzynarodowych projektów. Ambitne i rozwojowe projekty. Wsparcie merytoryczne na każdym etapie wdrożenia. Dostęp do najnowszych technologii. Praca w systemie zdalnym lub hybrydowym, w zależności od projektu. Jak wygląda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klientów. Składając aplikację, możesz liczyć na nasz obiektywizm, szacunek i pełny profesjonalizm. We connect you with the right people","[{""min"": 18000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Engineering,18000,28000,Net per month - B2B
Full-time,Mid,B2B,Remote,445,Data Management Engineer,Experis Manpower Group,"Join a dynamic team of engineers responsible for the development, maintenance, and enhancement of technologies in the Data Analytics Catalog and Data Management domains. The scope includes tools such as Collibra, Schedulix, Trillium, Tibco CIM, Tibco EBX, Informatica MDM, and Informatica SaaS MDM. The role involves software development, configuration, AWS infrastructure setup, and quality assurance. Tasks: Develop, configure, and maintain Collibra and MDM/RDM tools. Implement and manage AWS-based serverless solutions using Infrastructure as Code (Terraform preferred). Design and maintain scalable applications with REST and Graph APIs. Integrate and manage metadata, data lineage, profiling, and quality processes. Collaborate in Agile/Scrum or Kanban teams under the guidance of a Technical Product Manager. Conduct code reviews and ensure adherence to best practices. Create and maintain technical documentation with high attention to detail. Support DevOps practices including CI/CD pipelines (GitHub Actions preferred). Work with containerization technologies like Docker and Kubernetes. Perform data transformation tasks (ETL/ELT) and manage relational and NoSQL databases. Requirements: 5+ years of experience in Python. Proficiency in Groovy (especially for Collibra-related tasks). Experience with AWS, especially serverless architecture and Terraform. Understanding of microservices architecture and design patterns. Deep knowledge of Collibra Data Governance (configuration, metadata ingestion, workflows, etc.). Experience connecting to diverse data sources for metadata and data quality management. Strong skills in designing and maintaining scalable applications. Experience with REST and Graph APIs. Proficiency in Git. Experience working in Agile/Scrum environments. Familiarity with DevOps methodologies and CI/CD pipelines (GitHub Actions preferred). Experience with Docker and Kubernetes. Knowledge of testing frameworks like pytest or unittest. Ability to conduct thorough code reviews. Experience with ETL/ELT processes. Familiarity with relational (PostgreSQL, MySQL) and NoSQL (DynamoDB) databases. Strong documentation skills and attention to detail. Offer: 100% remote work MultiSport Plus Group insurance Medicover Premium e-learning platform","[{""min"": 160, ""max"": 172, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,172,Net per hour - B2B
Full-time,Senior,B2B,Remote,447,Lead Data Engineer,Link Group,"About the Role We’re looking for aLead Data Engineerto take ownership of the architecture and delivery of data solutions in a modern, cloud-based environment. You will lead technical direction, mentor team members, and collaborate closely with cross-functional teams to ensure scalable, reliable, and high-performance data platforms. Design and build robustdata pipelines,ETL/ELT workflows, and data architecture Lead the engineering team in best practices, code quality, and solution design Collaborate with stakeholders, product owners, and architects to define data strategies Set and enforce standards around data governance, quality, and performance Review code, provide technical mentorship, and support engineering growth Take ownership of end-to-end solution delivery — from data ingestion to consumption Support DevOps, CI/CD, and cloud infrastructure related to data workloads 6+ years of experience in Data Engineering Proven experience in atechnical leadershiporlead engineerrole Expert inPython,SQL, and cloud-native data processing tools Strong knowledge ofAWS(preferred), GCP or Azure also welcome Experience with orchestration tools (Airflow,Dagster, etc.) Proficiency with modern data platforms likeDatabricks,Snowflake,Redshift Familiar withdata modeling,data lakes,lakehouse architectures, andstreaming(Kafka, Kinesis, etc.) Knowledge ofCI/CD,Terraform, andGitOpsworkflows Experience withteam managementor mentoring junior engineers Knowledge ofdbt,Spark,Delta Lake,Glue, or similar Exposure toMLOps,analytics engineering, orBI/data visualizationtools Understanding ofdata security,compliance, andprivacy regulations","[{""min"": 150, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,180,Net per hour - B2B
Full-time,Mid,B2B,Remote,450,Analytics Engineer,Datumo,"We’re looking for a Analytics Engineer ready to push boundaries and grow with us. Datumo specializes in providing Data Engineering and Cloud Computing consulting services to clients from all over the world, primarily in Western Europe, Poland and the USA. Core industries we support include e-commerce 🛒, telecommunications 📡 and life sciences 🧬. Our team consists of exceptional people whose commitment allows us to conduct highly demanding projects. Our team members tend to stick around for more than 3 years, and when a project wraps up, we don't let them go - we embark on a journey to discover exciting new challenges for them. It's not just a workplace; it's a community that grows together! Must-have: ✅ at least 3 years of commercial experience in programming ✅ proven record with a selected cloud provider GCP (preferred), Azure or AWS ✅ good knowledge of JVM languages (Scala or Java or Kotlin), Python, SQL ✅ experience in one of data warehousing solutions: BigQuery/Snowflake/Databricks or similar ✅ in-depth understanding of big data aspects like data storage, modeling, processing, scheduling etc. ✅ data modeling and data storage experience ✅ ensuring solution quality through automatic tests, CI/CD and code review ✅ proven collaboration with businesses ✅ English proficiency at B2 level, communicative in Polish Nice to have: 🌟 knowledge of dbt, Docker and Kubernetes, Apache Kafka 🌟 familiarity with Apache Airflow or similar pipeline orchestrator 🌟 another JVM (Java/Scala/Kotlin) programming language 🌟 experience in Machine Learning projects 🌟 understanding of Apache Spark or similar distributed data processing framework 🌟 familiarity with one of BI tools: Power BI/Looker/Tableau 🌟 willingness to share knowledge (conferences, articles, open-source projects) What’s on offer: 🔥 100% remote work, with workation opportunity 🔥 20 free days 🔥 onboarding with a dedicated mentor 🔥 project switching possible after a certain period 🔥 individual budget for training and conferences 🔥 benefits: Medicover Private Medical Care , co-financing of the Medicover Sport card 🔥 opportunity to learn English with a native speaker 🔥 regular company trips and informal get-togethers Development opportunities in Datumo: 🚀 participation in industry conferences 🚀 establishing Datumo's online brand presence 🚀 support in obtaining certifications (e.g. GCP, Azure, Snowflake) 🚀 involvement in internal initiatives, like building technological roadmaps 🚀 training budget 🚀 access to internal technological training repositories Discover our exemplary project: 🔌 IoT data ingestion to cloud The project integrates data from edge devices into the cloud using Azure services. The platform supports data streaming via either the IoT Edge environment with Java or Python modules, or direct connection using Kafka protocol to Event Hubs. It also facilitates batch data transmission to ADLS. Data transformation from raw telemetry to structured tables is done through Spark jobs in Databricks or data connections and update policies in Azure Data Explorer. ☁️ Petabyte-scale data platform migration to Google Cloud The goal of the project is to improve scalability and performance of the data platform by transitioning over a thousand active pipelines to GCP. The main focus is on rearchitecting existing Spark applications to either Cloud Dataproc or Cloud BigQuery SQL, depending on the Client’s requirements and automate it using Cloud Composer. 📈 Data analytics platform for investing company The project centers on developing and overseeing a data platform for an asset management company focused on ESG investing. Databricks is the central component. The platform, built on Azure cloud, integrates various Azure services for diverse functionalities. The primary task involves implementing and extending complex ETL processes that enrich investment data, using Spark jobs in Scala. Integrations with external data providers, as well as solutions for improving data quality and optimizing cloud resources, have been implemented. 🛒 Realtime Consumer Data Platform The initiative involves constructing a consumer data platform (CDP) for a major Polish retail company. Datumo actively participates from the project’s start, contributing to planning the platform’s architecture. The CDP is built on Google Cloud Platform (GCP), utilizing services like Pub/Sub, Dataflow and BigQuery. Open-source tools, including a Kubernetes cluster with Apache Kafka, Apache Airflow and Apache Flink, are used to meet specific requirements. This combination offers significant possibilities for the platform. Recruitment process: 1️⃣Quiz - 15 minutes 2️⃣ Soft skills interview - 30 minutes 3️⃣ Technical interview - 60 minutes Find out more by visiting our website - https: //www.datumo.io If you like what we do and you dream about creating this world with us - don’t wait, apply now!","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,14000,25000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,451,Senior Database Engineer,AUCTANE Poland,"About us At Auctane, we are united by a passion to help sellers — wherever they are, however they operate — fulfill the promises they make to consumers. The Auctane mission is to fuel commerce through exceptional delivery. We make it possible for businesses to meet the ever rising expectations of their customers, and we make the world smaller and more accessible to consumers everywhere. Auctane brands enable hundreds of thousands of merchants to annually deliver billions of products — over $200 billion worth — to customers around the globe. And Auctane is just getting started. Auctane is a team of shipping and software experts with a passion for helping merchants move their ideas, dreams and innovations around the globe. The Auctane family includes ShipStation, ShipWorks, ShipEngine, ShippingEasy, Stamps, Endicia, Metapack, GlobalPost, and Packlink. Our partners include Amazon, UPS, USPS, eBay, BigCommerce, Shopify, WooCommerce, and Walmart. Our values Salary range for this role: 18 900- 23 600 PLN gross/month About the role As Senior Database Engineer, you will be responsible for the design, implementation, maintenance, and optimization of the organization's relational database systems. This role will ensure the integrity, availability, and performance of critical data to support business operations and decision-making processes. The DBE will work closely with IT teams, developers, and business stakeholders to deliver efficient and scalable database solutions that meet the organization's current and future needs. The ideal candidate will have strong technical skills in SQL Server and Oracle database management systems, excellent problem-solving abilities, and the capacity to translate business requirements into effective relational database solutions. This role will be critical in maintaining the organization's data infrastructure, enabling data-driven decision-making, and ensuring the optimal performance of mission-critical applications across the enterprise. What will you be doing? Develop database schemas, tables, and indexes to optimize performance and ensure data integrity. Implement and maintain database security measures, including user authentication, access controls, and data encryption. Analyze and interpret database performance metrics to proactively identify areas for improvement Monitor database performance and troubleshoot issues such as slow queries, database locks, and resource contention. Identify and address performance bottlenecks through query optimization, index tuning, and database configuration adjustments. Perform regular backups and disaster recovery procedures to ensure data availability and integrity. Optimize database indexes, query execution plans, and caching strategies to improve application responsiveness and scalability. Write and optimize SQL queries and stored procedures for efficient data retrieval and manipulation. Configure and maintain database replication, clustering, or failover mechanisms to provide resilience against hardware failures, network outages, or natural disasters. Stay abreast of emerging database technologies, trends, and best practices through self-study, training, and participation in industry forums and conferences. Collaborate with software developers to design database schemas that align with application requirements and development workflows. Participate in code reviews to ensure adherence to database best practices, performance guidelines, and data access patterns. Work closely with application developers to optimize SQL queries, data access patterns, and database interactions for maximum efficiency. What are we looking for? Bachelor's degree in Computer Science, Information Technology, or a related field In-depth knowledge and proficiency in SQL (Structured Query Language) Proficiency in managing SQL database systems Strong understanding of database design principles, including normalization, indexing, and data modeling Experience in database administration tasks such as installation, configuration, backup and recovery, security management, and performance tuning is important Knowledge of database security best practices and experience in implementing security measures such as user authentication, access controls, encryption, and auditing are important Strong troubleshooting and problem-solving skills Effective communication and collaboration skills Communicative level of English Tech Stack SQL Oracle Amazon Web Services C# .Net What do we offer? 🗓️ Annual Salary Review: We are reviewing the salaries of all our teams annually in order to evaluate an increase according to individual performance and the business results. 📙 Personal Training Budget. Up to 7.000 PLN/year training budget (certifications, conferences attendance, etc.) to invest in your professional development. We want to help you improve your technical skills, feel involved in the product community, and develop your soft skills to lead teams and manage other stakeholders. 🌅 Up to 30 days of vacation per year ( additional days are granted along with seniority at AUCTANE). 💙 Up to 500 PLN/year to match your NGO donations ! We are happy to support your initiatives by duplicating the amount donated. 💜 Lunch card 😊 Volunteer day. You can take 1 day off per year in order to participate in volunteering activities! 🔗 Referral Fee We need your support in hiring top-class talent! We offer a referral bonus of 4k-20k PLN, depending on the complexity of the role and the hiring process. 👩‍⚕️We have an Employee Assistance Program with psychological assistance free of charge. 🗺️ Languages classes every week. Thirsty for knowledge? Learn a new language by joining our free English/Spanish/German classes. You can connect and enjoy taking up a new language or improving your current skills with one of our great instructors. 🏥 Free private medical insurance 📄 Attractive life insurance 🏐 Gym membership co-financing 🏢 Nice office in Zielona Gora (free drinks, snacks…) or new office in Wrocław (in both locations 3 days remotely, 2 days in the office/per week) ⚖️ Great work-life balance We offer a flexible work schedule and will do our best to adapt to your personal situation. Working in a fast-paced environment can be intense, but that doesn’t mean you shouldn’t enjoy your free time! 💜 An inclusive and upbeat work environment Leave your suit behind... we’re a t-shirt and converse kind of place! More importantly, our company culture promotes diversity and inclusion. The personality and opinions of each of our team members are important and valid, and we aim to offer all employees a safe environment where they can be themselves and thrive. 🌍 A cross-cultural atmosphere We are a truly international team of 20 nationalities that speak 10 languages. Our company language is English and all internal communication and company-wide meetings are in English. 🏟️ Company events Work hard, play hard! We do our best every day, even at our regular team-building events. 📺 Internal and external training, free access to online training platforms such as Linkedin Learning 🏠 Possibility to work in a home-office using equipment provided by AUCTANE, or in our office prepared in accordance with all safety requirements. Do you want to know a little bit more about the team? Please, don’t hesitate to reach out to Amazing Auctane-PL and our Instagram","[{""min"": 18900, ""max"": 23600, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18900,23600,Gross per month - Permanent
Full-time,Senior,B2B,Hybrid,452,Data Business Analyst,emagine Polska,"Project Information: Industry : Banking Location : Gdańsk ( 2 day per week in office) Rate : up to 155 zł/h netto + VAT (B2B) Summary: The Senior Data Business Analyst will serve as a crucial liaison between the business and IT development teams, primarily focusing on advanced data analysis to ensure business requirements are accurately translated into effective solutions. Main Responsibilities: Conduct advanced data analysis using SQL. Manage and analyze large datasets efficiently. Demonstrate strong analytical and problem-solving skills. Understand business terminology to effectively bridge gaps between business needs and IT. Utilize a functional background in Credit Risk (if applicable). Independently convert business requirements into high-level and low-level solution designs. Support unit testing (UT), system testing (ST), and user acceptance testing (UAT). Create functional test cases and validate results. Exhibit strong communication skills and team collaboration in an agile environment. Leverage a minimum of 8-10 years of IT experience and 3-4 years of analytical experience. Key Requirements: Proficient in SQL data analysis Experience with large datasets Strong analytical and problem-solving skills Business terminology comprehension Minimum 8-10 years of IT experience Minimum 3-4 years of analyst experience Nice to Have: Knowledge of SAS/Python Functional background in Credit Risk Experience creating functional test cases","[{""min"": 120, ""max"": 155, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,120,155,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,454,"Data Engineer - Spark, Scala/Java",Link Group,"Developing, managing, and optimizing data pipelines. Engaging with team developers and architects to shape the process implementation framework. Estimating workload and regularly updating progress on solution development. Cooperating closely with stakeholders to refine solution details and validate requirements. Supporting business teams in understanding technical possibilities, offering optimal solutions, and clarifying constraints. Establishing efficient workflows, proposing, and executing process enhancements. Proficiency in Scala or Java with substantial experience. Minimum 4–5 years of relevant experience Hands-on expertise in working with Spark. Familiarity with CI/CD methodologies and tools such as GitHub Actions. Knowledge of cloud platforms (GCP) and infrastructure as code (Terraform) is a plus. Experience with Airflow is an added advantage. Background in data pipeline testing is beneficial. A business- and product-centric approach with direct experience in stakeholder collaboration. Banking sector experience is a plus. Prior work experience in Scrum and agile methodologies. Strong analytical skills with the ability to address intricate technical challenges. Passion for tackling challenges that foster both personal and professional development. A collaborative mindset with a willingness to learn and grow. Proficiency in English is essential.","[{""min"": 22000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,30000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,461,Senior Data Scientist - ML&AI,VISA,"Company Description Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid. Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa. Visa’s Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world’s most sophisticated processing networks capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. While working with us you’ll get to work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cyber security, and B2C platforms. The Opportunity: We are looking to hire a Data Scientist to lead the AI/ML initiatives in the Cybersecurity Products team. This position will be based out of Warsaw, Poland. In this role, you will: Be responsible for driving, designing, and building cutting edge innovation in the space of cybersecurity through Artificial Intelligence and Machine Learning - current scope of problems includes behavior biometrics, risk-based authentication, Account Takeover protection, advanced threat detection, smart incident response, AI model threat analysis, and many more. Drive the continued innovation and engineering of our existing behavior-based adaptive authentication product and bot/fraud protection product with a high-performance team of data scientists and engineers. Build innovative solutions and collaborate with engineering and product partners across the global Visa organization that help secure Visa against a variety of threats and attacks. Provide consultation to more experienced leaders in order to recommend solutions which solve security & other business challenges. You must have strong technical depth and experience in application of Machine Learning, Deep Learning, and Data Science techniques. On top of that, you should also have a genuine interest in cybersecurity and a desire to build solutions that deliver real impacts to the world. We will rely on your leadership to establish a roadmap and vision as this team engages in existing and new emerging areas. Support transfer technical knowledge to facilitate implementation of the business solution provided. Document all projects developed, including clear and efficient coding, and write other documentation as needed. Identify relevant market trends by country, based on a deep analysis of payment industry information. Interacting with several internal and external stakeholders for the strategic definition of analysis and initiatives. Continuously develop and present innovative ideas to improve current business practices within Visa. Essential Functions: Cyber Analytics Product: Research innovation in digital authentication using behavior biometrics capabilities, build applied AI based models and engineer them into the product called Visa Behavior Analytics. You will engage in data science and applied AI related activities for a Visa engineered product to protect against account takeover related threats, continuously enhancing it to combat threats in the secure authentication and perimeter defense space. Cyber/AI R&D: Research innovation in applying AI to the more general field of cybersecurity, including the protection with and against AI driven technologies, as well as the AI models themselves. You will be using your core competencies around AI and data science and help drive the teams to build models and solutions that work at scale, harnessing Petabytes of data while applying it to products that need to respond with cyber analytics in milliseconds. Influence & Collaborate: Be able to present results to a cross section of employees, including C-Level and other senior leadership at Visa. You will engage with internal technology, and cyber teams along with global product orgs. In addition, you will collaborate with colleagues in technology and product offices to establish effective, productive business relationships. This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs. Basic Qualifications: 2+ years of relevant work experience and a Bachelor’s degree, OR 5+ years of relevant work experience. Preferred Qualifications: 3 or more years of work experience with a Bachelor’s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD,MD). 3+ years of experience in modern data mining and data science techniques(e.g., regressions, decision trees, ensemble algorithms, neural networks, time series analytics, clustering, anomaly detection, text analytics, etc.). Candidates with a PhD in a quantitative field, such as Statistics, Mathematics, Operational Research, Computer Science, Economics, or engineering preferred. Experience in developing and deploying products using Docker, Kubernetes, and the containerization technology stack. Experience in development of advanced machine learning and deep learning models such as RNN, LSTM, Graph Neural Networks. Experience and proficiency in working with large language models (LLMs), and familiarity with the associated solution architecture and infrastructure, including Nvidia GPUs. Experience in leading, building, and supporting scalable and reliable AI/ML-powered systems, enabling rapid prototyping and advanced analytics using modern big data and AI/ML technologies (e.g., Spark, Kafka, TensorFlow, PyTorch) in an agile environment. Software Engineering background: The candidate must be proficient in Python and at least one object-oriented programming language (Java, Golang, C++,etc.) Golang experience is desired. Candidate must possess software engineering skills and be able to take end-to-end ownerships of analytical models. Data Wrangling: The candidate must have proficient data wrangling skills with Python, SQL, and other data processing tools/scripts. Experience with the end-to-end machine learning lifecycle and MLOps, including data preprocessing and feature extraction, model training and evaluation, deployment, and monitoring of AI models in production environments. Experience working with Docker in both development and deployment workflows, ensuring smooth transitions from development to production environments. Distributed Systems: practical experience with NoSQL data platforms (e.g., Cassandra, Lakehouse, DynamoDB) and caching technologies like Redis is a plus. A solid understanding of the Linux networking subsystem, contributing to the stability and performance of deployed AI/ML systems. A solid understanding of the web applications and APIs, contributing to the front-end accessibility and integration of AI-driven solutions. Cloud domain: Familiarity with infrastructure and analytics services on cloud(e.g., AWS, Azure) is a plus. Domain Knowledge - Candidate with background in one or multiple of the following domains is a plus: Cybersecurity, AI security/privacy research and Biometrics Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","[{""min"": 16000, ""max"": 26000, ""type"": ""Gross per month - Permanent""}]",Data Science,16000,26000,Gross per month - Permanent
Full-time,Senior,B2B,Hybrid,463,Data Engineer Hadoop,Antal Sp. z o.o.,"Hadoop Data Engineer (GCP, Spark, Scala) – Kraków / Hybrid We are looking for an experienced Hadoop Data Engineer to join a global data platform project built in the Google Cloud Platform (GCP) environment. This is a great opportunity to work with distributed systems, cloud-native data solutions, and a modern tech stack. The position is based in Kraków (hybrid model – 2 days per week in the office). Your responsibilities: Design and build large-scale, distributed data processing pipelines using Hadoop, Spark, and GCP Develop and maintain ETL/ELT workflows using Apache Hive, Apache Airflow (Cloud Composer), Dataflow, DataProc Work with structured and semi-structured data using BigQuery, PostgreSQL, Cloud Storage Manage and optimize HDFS-based environments and integrate with GCP components Participate in cloud data migrations and real-time data processing projects Automate deployment, testing, and monitoring pipelines (CI/CD using Jenkins, GitHub, Ansible ) Collaborate with architects, analysts, and product teams in Agile/Scrum setup Troubleshoot and debug complex data logic at the code and architecture level Contribute to cloud architecture patterns and data modeling decisions Must-have qualifications: Minimum 5 years of experience as a Data Engineer / Big Data Engineer Hands-on expertise in Hadoop, Hive, HDFS, Apache Spark, Scala, SQL Solid experience with GCP and services like BigQuery, Dataflow, DataProc, Pub/Sub, Composer (Airflow) Experience with CI/CD processes and DevOps tools: Jenkins, GitHub, Ansible Strong data architecture and data engineering skills in large-scale environments Experience working in enterprise environments and with external stakeholders Familiarity with Agile methodologies such as Scrum or Kanban Ability to debug and analyze application-level logic and performance Nice to have: Google Cloud certification (e.g., Professional Data Engineer) Experience with Tableau, Cloud DataPrep, or Ansible Knowledge of cloud design patterns and modern data architectures Work model: Hybrid – 2 days per week from the Kraków office (rest remotely) Opportunity to join an international team and contribute to global-scale projects To learn more about Antal, please visit www.antal.pl","[{""min"": 180, ""max"": 220, ""type"": ""Net per hour - B2B""}]",Data Engineering,180,220,Net per hour - B2B
Full-time,Senior,Permanent,Hybrid,464,Senior Data Architect,KUBO,"We are looking for a Senior Data Architect to join our global Data & Analytics team and lead the design and implementation of scalable, reusable, and high-performing data and AI solutions. This role is critical to delivering analytical products that empower data-driven decision-making across various business domains. As a Senior Data Architect, you will take ownership of architecture and implementation, ensuring solutions are scalable, cost-efficient, and aligned with data governance and security standards. Key responsibilities: Lead the technical implementation of analytical and AI solutions in e.g. finance, controlling Own the development of harmonized, enriched, and reusable data models and KPIs based on data from multiple sources Translate business and functional requirements into technical architecture, considering scalability, reusability, performance, security, and cost efficiency Lead and mentor internal and external development teams Design and orchestrate end-to-end data pipelines, implement data quality and governance processes, and ensure delivery of production-ready data assets Ideal candidate profile: At least 5+ years of experience in Data & Analytics or AI, ideally in supply chain Proven experience in leading technical teams Strong technical knowledge in data architecture and engineering using technologies such as Azure Data Lake, Azure Synapse, Databricks Experience with BI tools like Tableau or Power BI and understanding of data cataloging and data quality management Fluent English Conditions: Work model: Hybrid – 1 day per week in the Warsaw office Salary: 20 000-28 000 PLN gross/month Employment type: Full-time employment contract (UoP) directly with the client Business trips to Germany 1-2 times a year Benefits: VIP Medical Care Package, Life & Travel Insurance, Company Bonus, Holiday allowance, Co-financed sport card *Relocation package and full support with relocation to Warsaw Recruitment steps: Phone call with a Recruiter (20 - 30 min.) First interview with a Manager (1h) Second interview with a technical team (1h) Feedback and decision","[{""min"": 20000, ""max"": 28000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,20000,28000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,467,👉 GenAI Lead,Xebia sp. z o.o.,"🟣About project: GenAI Lead will take ownership of designing, developing, and deploying advanced AI solutions, specializing in machine learning, deep learning, and generative AI technologies. This role demands a strategic leader with deep technical expertise to architect scalable GenAI solutions, guide a team of AI professionals, and deliver transformative business outcomes through innovative AI applications. 🟣You will be: designing and implementing scalable Generative AI (GenAI) systems using Large Language Models (LLMs), vision models, and vector databases, leading the design, development, and deployment of AI-driven solutions, including machine learning models, deep learning frameworks, and generative AI applications, overseeing seamless integration of AI solutions with Azure AI Studio, SharePoint, and Power BI for deployment, reporting, and visualization, collaborating with cross-functional teams to shape AI strategies and deliver high-impact solutions, providing technical leadership in implemention of Agentic frameworks and tools like LlamaIndex to advance AI workflows, mentoring and empowering a team of AI developers, fostering a culture of innovation, collaboration, and technical excellence, staying at the forefront of AI advancements, proposing creative and practical solutions to address complex challenges, enforcing best practices in code quality, model optimization, and solution scalability to ensure robust production-ready systems. 🟣Your profile: Bachelor’s or Master’s degree in computer science, Data Science, Engineering, or a related field (PhD preferred), 8+ years of hands-on experience in AI/ML development, including at least 3 years in leadership or solution architect capacity, demonstrated success in delivering machine learning, deep learning, and generative AI projects in production environments, exceptional programming proficiency in Python and experience with frameworks such as TensorFlow, PyTorch, or equivalent. in-depth expertise in LLMs, vision models, vector databases, and RAG applications, strong command of Azure AI Studio, SharePoint, and basic Power BI for integration and reporting purposes, proven experience with Agentic frameworks and tools like LlamaIndex for intelligent system development, outstanding problem-solving abilities, with a knack for translating business needs into technical solutions, superior communication and leadership skills to manage teams, align stakeholders, and drive project success, excellent verbal and written communication skills in English (min. C1). 🟣Nice to have: experience with cloud platforms beyond Azure (e.g., AWS, GCP), knowledge of DevOps practices (e.g., CI/CD pipelines, Kubernetes), familiarity with AI ethics, governance, and compliance standards, exposure to advanced visualization tools or BI platforms. 🟣Recruitment Process: CVreview –HRcall –Interview(with Live-coding) –ClientInterview (with Live-coding) –Hiring ManagerInterview –Decision 🎁Benefits 🎁 ✍Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 40000, ""type"": ""Net per month - B2B""}, {""min"": 27000, ""max"": 32500, ""type"": ""Gross per month - Permanent""}]",Unclassified,33500,40000,Net per month - B2B
Full-time,Senior,B2B,Remote,470,Mid/Senior Data Engineer,CodiLime,"Get to know us better CodiLime is a software and network engineering industry expert and the first-choice service partner for top global networking hardware providers, software providers and telecoms. We create proofs-of-concept, help our clients build new products, nurture existing ones and provide services in production environments. Our clients include both tech startups and big players in various industries and geographic locations (US, Japan, Israel, Europe). While no longer a startup - we have 250+ people on board and have been operating since 2011 we’ve kept our people-oriented culture.Our values are simple: Actto deliver. Disruptto grow. Team upto win. The project and the team The goal of this project is to build a centralized, large-scale business data platform for one of the biggest global consulting firms. The final dataset must be enterprise-grade, providing consultants with reliable, easily accessible information to help them quickly and effectively analyze company profiles during Mergers & Acquisitions (M&A) projects. You will contribute to building data pipelines that ingest, clean, transform, and integrate large datasets from over 10 different data sources, resulting in a unified database with more than 300 million company records. The data must be accurate, well-structured, and optimized for low-latency querying. The platform will power several internal applications, enabling a robust search experience across massive datasets and making your work directly impactful across the organization. The data will provide firm-level and site-level information, including firmographics, technographics, and hierarchical relationships (e.g., GU, DU, subsidiary, site). This platform will serve as a key data backbone for consultants, delivering critical metrics such as revenue, CAGR, EBITDA, number of employees, acquisitions, divestitures, competitors, industry classification, web traffic, related brands, and more. Technology stack: Languages: Python, SQL Data Stack: Snowflake + DBT, PostgreSQL, Elasticsearch Processing: Apache Spark on Azure Databricks Workflow Orchestration: Apache Airflow Cloud Platform: Microsoft Azure- Compute / Orchestration: Azure Databricks (Spark clusters), Azure Kubernetes Service (AKS), Azure Functions, Azure API Management.- Database & Storage: Azure Database for PostgreSQL, Azure Cosmos DB, Azure Blob Storage- Security & Configuration: Azure Key Vault, Azure App Configuration, Azure Container Registry (ACR)- Search & Indexing: Azure AI Search CI/CD: GitHub Actions Static Code Analysis: SonarQube AI Integration (Future Phase): Azure OpenAI What else you should know: Team Structure: Data Architecture Lead Data Engineers Backend Engineers DataOps Engineers Product Owner Work culture: Agile, collaborative, and experienced work environment. As this project will significantly impact the organization, we expect a mature, proactive, and results-driven approach. You will work with a distributed team across Europe and India. We work on multiple interesting projects at the time, so it may happen that we’ll invite you to the interview for another project if we see that your competencies and profile are well suited for it. As a part of the project team, you will be responsible for: Data Pipeline Development: Designing, building, and maintaining scalable, end-to-end data pipelines for ingesting, cleaning, transforming, and integrating large structured and semi-structured datasets Optimizing data collection, processing, and storage workflows Conducting periodic data refresh processes (through data pipelines) Building a robust ETL infrastructure using SQL technologies. Assisting with data migration to the new platform Automating manual workflows and optimizing data delivery Data Transformation & Modeling: Developing data transformation logic using SQL and DBT for Snowflake. Designing and implementing scalable and high-performance data models. Creating matching logic to deduplicate and connect entities across multiple sources. Ensuring data quality, consistency, and performance to support downstream applications. Workflow Orchestration: Orchestrating data workflows using Apache Airflow, running on Kubernetes. Monitoring and troubleshooting data pipeline performance and operations. Data Platform & Integration: Enabling integration of 3rd-party and pre-cleaned data into a unified schema with rich metadata and hierarchical relationships. Working with relational (Snowflake, PostgreSQL) and non-relational (Elasticsearch) databases Software Engineering & DevOps: Writing data processing logic in Python. Applying software engineering best practices: version control (Git), CI/CD pipelines (GitHub Actions), DevOps workflows. Ensuring code quality using tools like SonarQube. Documenting data processes and workflows. Participating in code reviews Future-Readiness & Integration: Preparing the platform for future integrations (e.g., REST APIs, LLM/agentic AI). Leveraging Azure-native tools for secure and scalable data operations Being proactive and motivated to deliver high-quality work, Communicating and collaborating effectively with other developers, Maintaining project documentation in Confluence. As a Data Engineer, you must meet the following criteria: Strong experience with Snowflake and DBT (must-have) Experience with data processing frameworks, such as Apache Spark (ideally on Azure Databricks) Experience with orchestration tools like Apache Airflow, Azure Data Factory (ADF), or similar Experience with Docker, Kubernetes, and CI/CD practices for data workflows Strong SQL skills, including experience with query optimization Experience in working with large-scale datasets Very good understanding of data pipeline design concepts and approaches Experience with data lake architectures for large-scale data processing and analytics Very good coding skills in Python- Writing clean, scalable, and testable code (unit tests)- Understanding and applying object-oriented programming (OOP) Experience with version control systems: Git Good knowledge of English (minimum C1 level) Beyond the criteria above, we would appreciate the nice-to-haves: Experience with PostgreSQL (ideally Azure Database for PostgreSQL) Experience with GitHub Actions for CI/CD workflows Experience with API Gateway, FastAPI (REST, async) Experience with Azure AI Search or AWS OpenSearch Familiarity with developing ETL/ELT processes (a plus) Optional but valuable: familiarity with LLMs, Azure OpenAI, or Agentic AI system Flexible working hours and approach to work: fully remotely, in the office or hybrid Professional growth supported by internal training sessions and a training budget Solid onboarding with a hands-on approach to give you an easy start A great atmosphere among professionals who are passionate about their work The ability to change the project you work on","[{""min"": 16500, ""max"": 27500, ""type"": ""Net per month - B2B""}]",Data Engineering,16500,27500,Net per month - B2B
Full-time,Senior,B2B,Remote,474,Data Engineer,Link Group,"About the Role We're looking for aData Engineerto join a growing team working on modern data platforms. You will play a key role in designing, developing, and maintaining scalable data pipelines and infrastructure to support data analytics and reporting initiatives. This is a great opportunity to work with cutting-edge cloud and big data technologies. Design, build, and maintain scalableETL/ELT pipelines Work with structured and unstructured data from diverse sources Optimize data workflows for performance, reliability, and cost Implement data quality checks and monitoring Collaborate with analysts, architects, and other engineers to support data needs Build data integrations with internal and third-party APIs Support cloud data infrastructure and automation 3+ years of experience as a Data Engineer or similar role Strong knowledge ofSQLand data modeling principles Experience withPythonorScalafor data processing Hands-on experience with cloud platforms (ideallyAWS, but Azure/GCP also valuable) Familiarity with tools likeApache Spark,Airflow,Kafka, or similar Experience withdata lakes,data warehouses, orlakehouse architectures Git and CI/CD workflows Strong problem-solving skills and ability to work independently Experience withSnowflake,Redshift, orBigQuery Familiarity withdbt,Terraform, or other IAC tools Background indata governanceorsecurity Experience with real-time data processing (e.g., Flink, Kinesis) Exposure toML pipelinesorMLOps","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,476,Data Analyst,Vasco Electronics,"Miejsce pracy: Kraków Tryb pracy: Hybrydowy, 3 dni z biura, 2 dni zdalnie Etat: Full time Rodzaj umowy: Umowa o Pracę, Umowa Zlecenie, B2B Wynagrodzenie: UoP (brutto): 9900 - 12000 PLN UZ (brutto): 59 - 71 PLN/h B2B (netto/h FV): 73 - 93 PLN/h Zakres obowiązków Kompleksowa analiza danych (marketingowych, sprzedażowych, finansowych, magazynowych) z wykorzystaniemGoogle BigQueryiSQL Łączenie, agregowanie, porównywanie i weryfikowanie danych z różnych źródeł Projektowanie, tworzenie i utrzymywanie interaktywnych dashboardów oraz raportów wPower BI Tłumaczenie wymagań biznesowych na specyfikacje techniczne, przygotowywanie analiz i prezentowanie rekomendacji dla interesariuszy Ścisła współpraca zzespołem Data Engineeróworaz dbałość o jakość i spójność dokumentacji Wykorzystywanie LLMów do automatyzacji swojej pracy Nasze oczekiwania 3 - 5 lat doświadczenia Zaawansowana, praktyczna znajomośćSQLoraz środowiska bazodanowegoGoogle BigQuerylub innych baz danych Praca z wersjonowaniem kodu (Dataform, dbt lub podobne) Biegłość w wizualizacji danych i budowaniu raportów wPower BIlub podobne Doświadczenie w samodzielnym prowadzeniu analiz, od ekstrakcji danych po prezentację wniosków Wysoko rozwinięte umiejętności komunikacyjne i zdolność do efektywnej współpracy z odbiorcami biznesowymi i technicznymi Proaktywne podejście, umiejętność jasnego formułowania rekomendacji i otwartość na kulturę feedbacku Język angielski na poziomie B2 Mile widziane BigQuery Power BI Dataform Python Oferujemy Środowisko oparte na wartościach i przyjazną, nieformalną atmosferę – bez nadęcia, z fajnymi ludźmi i dobrą kawą Duży wpływ na kształt pracy zespołu Budżet do wykorzystania na platformie Worksmile, która oferuje dostęp do takich benefitów jak m.in. Multisport, Allianz, Luxmed, PZU oraz wiele innych Elastyczny czas pracy Inicjatywy rozwojowe Dofinansowanie okularów korekcyjnych 800 zł Częste integracje Atrakcje w biurze tj. PS5 + VR2, biuro przyjazne zwierzętom, przekąski i owoce w biurze Parking przy biurze","[{""min"": 9900, ""max"": 12000, ""type"": ""Gross per month - Permanent""}, {""min"": 73, ""max"": 93, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,9900,12000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,479,Senior Data Engineer,emagine Polska,"Summary: The role of the Senior Data Engineer focuses on enhancing a Data Lakehouse platform built on Snowflake and DBT, driving innovation, and ensuring the platform's operational excellence. Start: ASAP Duration: 3 months with possible extension until the end of 2025 Location: 100% Remote Salary: 170-190 zł/h Main Responsibilities: Design and implement solutions for the data platform using Snowflake and Azure. Enhance the platform with new features and optimize CI/CD pipelines. Automate processes and improve governance and monitoring. Collaborate within an international team and gather requirements from stakeholders. Work within Scrum and Kanban methodologies using Jira. Key Requirements: DBT Snowflake SQL Python or other object-oriented languages (Java, C#, etc.) Experience with relational databases CI/CD operations Version control using Git (preferably GitHub) Data architecture and integration skills English B2/C1 proficiency Nice to Have: PowerBI Other Details: Ability to work in Scrum and Kanban environments using Jira. Experience working independently with international stakeholders. Strong problem-solving skills. Experience working in an international team setting. Project Start Date: As soon as possible Team Composition: Three other data engineers Contract Type: B2B Work Type: Remote (with office options in Poznan and Warsaw) Travel: Unlikely but possible to Poznan, Warsaw, or Copenhagen Working Hours: Flexible; generally available from 9 AM - 4 PM.","[{""min"": 170, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,190,Net per hour - B2B
Full-time,Senior,B2B,Remote,480,Data Engineer with Palantir,Link Group,"About the Role We are looking for aData Engineer experienced with Palantir Foundryto join a cross-functional team working on large-scale data integration, modeling, and analytics platforms. The ideal candidate is hands-on, proactive, and capable of navigating complex data ecosystems in an enterprise environment. Design and build data pipelines and models usingPalantir Foundry Integrate multiple data sources (structured and unstructured) into usable, high-quality data assets Collaborate with data scientists, analysts, and business stakeholders to support advanced analytics initiatives Apply data governance, lineage, and cataloging principles within Foundry Develop and maintain Foundry “Objects”, Code Workbooks, and other tooling Ensure quality, performance, and scalability of the implemented data solutions Support and document platform usage and development best practices 3+ years of experience in Data Engineering Hands-on experience withPalantir Foundryin a commercial or enterprise setting Proficiency inSQL,Python, and data transformation techniques Good understanding ofdata modeling(dimensional, relational, and graph-based) Familiarity withdata governanceandmetadata management Experience working incloud-based environments(AWS, GCP, or Azure) Excellent communication skills and ability to work with cross-functional teams Previous experience in highly regulated industries (finance, pharma, defense, etc.) Experience integrating Foundry with external tools and systems via APIs Knowledge ofCI/CD,Git, and software engineering best practices Exposure to tools likeAirflow,dbt,Databricks,Snowflake, etc. Experience withdata privacy regulations(GDPR, HIPAA, etc.)","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,481,SAP Master Data Subject Matter Expert,emagine Polska,"Location: Poland, 100% Remote Expected Start Date: ASAP Contract Period: 12 months (+possible extension) Language Requirement: English B2/C1 Salary: 190-205 zł/h B2B contract Introduction & Summary: We are seeking a highly skilled Master Data Subject Matter Expert with over 5 years of relevant experience in Master Data Management, particularly with SAP ECC and/or S/4HANA. The ideal candidate will possess a deep understanding of Master Data processes, cross-functional knowledge across various business processes, including O2C, P2P, and R2R, and exceptional soft skills for effective communication and stakeholder engagement. Evaluate and improve current Master Data processes as part of SAP S/4HANA implementation. Participate in data clean-up and harmonization efforts. Design and document SAP Master Data configuration activities. Support implementation of new SAP features for Master Data enhancements. Assist in writing test plans and support end-user training. Create and maintain comprehensive Master Data documentation. Analyze large data sets for migration and validation purposes. Provide end-user support and troubleshooting for Master Data-related issues. Work closely with project teams to prepare for ERP rollout. Minimum of 5 years of experience in Master Data management. Operational experience with SAP ECC and/or S/4HANA. Deep knowledge of Business Partner setup. Cross-functional understanding of Master Data objects. Experience in the retail sector is a plus. Experience in SAP implementation or transformation programs is advantageous. Expertise in Microsoft Office tools, especially Excel and PowerPoint. Working knowledge of Jira is preferred. Nice to Have: Experience in retail industry operations. Participation in SAP implementation or rollout projects. Experience in data mapping and migration into SAP. This position offers an opportunity to contribute to significant ERP initiatives in a dynamic environment. The role is remote with flexible working hours, depending on project needs.","[{""min"": 190, ""max"": 205, ""type"": ""Net per hour - B2B""}]",Data Engineering,190,205,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,484,Data Modeller,Antal Sp. z o.o.,"Position: Data Modeller Location: Kraków hybrid Employment Type: Full-time | B2B We are seeking aData Modellerto design enterprise-grade data models and lead a team of data modellers in shaping our data foundation. You'll collaborate closely with cross-functional teams to ensure consistency, scalability, and data quality across systems. Design and develop conceptual, logical, and physical data models to support business operations and analytics. Work with stakeholders to gather requirements and translate them into scalable, efficient data models. Define and enforce data modeling standards, best practices, and governance policies. Mentor and train the data modelling team in tools, techniques, and methodologies. Collaborate with data architects, engineers, and analysts to align data models with broader data strategies. Contribute to design and architecture discussions to ensure end-to-end data consistency. Stay up to date on data technologies, trends, and tools; evaluate and recommend improvements. Experience indata modeling for both transactional and analytical systems. Strong understanding ofmetadata, data analysis, and requirement gathering. Ability to clearly communicate complex data modeling concepts to technical and non-technical audiences. Leadership experience, including mentoring and guiding teams. Strong stakeholder management and consulting skills. Solid knowledge ofdata governance, data quality, and protection practices. Experience with cloud data platforms (AWS, Azure, GCP). Familiarity with Big Data ecosystems (Hadoop, Spark). Knowledge of industry models (BIAN, FSDM, BDW) or experience in designing enterprise-wide data models. To learn more about Antal, please visitwww.antal.pl","[{""min"": 130, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Science,130,180,Net per hour - B2B
Full-time,Senior,Permanent or B2B,Remote,487,👉 Senior GCP Data Engineer,Xebia sp. z o.o.,"🟣 You will be: developing and maintaining data pipelines to ensure seamless data flow from the Loyalty system to the data lake and data warehouse, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, ensuring data integrity, consistency, and availability across all data systems, integrating data from various sources, including transactional databases, third-party APIs, and external data sources, into the data lake, implementing ETL processes to transform and load data into the data warehouse for analytics and reporting, working closely with cross-functional teams including Engineering, Business Analytics, Data Science and Product Management to understand data requirements and deliver solutions, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, optimizing data storage and retrieval to improve performance and scalability, monitoring and troubleshooting data pipelines to ensure high reliability and efficiency, implementing and enforcing data governance policies to ensure data security, privacy, and compliance, developing documentation and standards for data processes and procedures. 🟣 Your profile: 7+ years in a data engineering role, with hands-on experience in building data processing pipelines, experience in leading the design and implementing of data pipelines and data products, proficiency with GCP services, for large-scale data processing and optimization, extensive experience with Apache Airflow, including DAG creation, triggers, and workflow optimization, knowledge of data partitioning, batch configuration, and performance tuning for terabyte-scale processing, strong Python proficiency, with expertise in modern data libraries and frameworks (e.g., Databricks, Snowflake, Spark, SQL), hands-on experience with ETL tools and processes, practical experience with dbt for data transformation, deep understanding of relational and NoSQL databases, data modelling, and data warehousing concepts, excellent command of oral and written English, Bachelor’s or Master’s degree in Computer Science, Information Systems, or a related field. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ 🟣 Nice to have: experience with ecommerce systems and their data integration, knowledge of data visualization tools (e.g., Tableau, Looker), understanding of machine learning and data analytics, certification in cloud platforms (AWS Certified Data Analytics, Google Professional Data Engineer, etc.). 🟣 Recruitment Process: CV review – HR call – Interview (with Live-coding) – Client Interview (with Live-coding) – Hiring Manager Interview – Decision 🎁 Benefits 🎁 ✍ Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺 We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️ We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 21500, ""max"": 33000, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,21500,33000,Net per month - B2B
Full-time,Mid,B2B,Remote,491,Junior/ Mid Data Engineer,Altimetrik Poland,"Altimetrik Polandis a digital enablement company. In an agile way, we deliver bite-size outcomes to enterprises and startups from all industries, to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a big focus on core development, attacking challenging and complex problems of the biggest companies in the world. For our client, founded in 1884 as a Swiss luxury watchmaker, known for precision-made chronometers designed for aviators, we seek aJunior/ MidData Engineerto join our team and drive the development and success of the high-end timepieces. Responsibilities: Collaboration: collect and understand requirements and work closely with the business analysts, data architects, BI engineers and other data engineers. Developing data pipelines: develop, optimize, and maintain reliable and scalable ETL/ELT pipeline on a cloud platform through the collection, storage, processing, and transformation of large datasets Support production issues related to application functionality and integrations And if you possess.. Related Experience: 2+ years of experience in the role of data engineer Hands-on experience in gathering, cleaning, processing and visualizing data in Databricks with Python/Pyspark Proficient in SQL development skills with the ability to write complex, efficient queries for data integration Experience in Azure cloud ecosystem (Azure Data Factory, Azure Storage Account, Azure SQL database, . Experience in data modelling in a Data lake environment Must have strong experience in data warehouse concepts Analytical skills to support Business Analysts and the ability to translate user stories Good diagnosis skills for identifying job issues Ability to manage and complete multiple tasks within tight deadlines Proficient in spoken and written communication skills (verbal and non-verbal) … then we are looking for you! We work 100% remotely or from our hub in Kraków We grow fast. We learn a lot. We prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 15000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,15000,20000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,494,Data Modeler - banking 💥,ITDS,"Join us, and turn complex data into powerful business insights! Kraków-based opportunity with a hybrid work model (2 days/week in the office). As a Data Modeller, you will be working for our client, a global financial services institution undergoing a large-scale digital transformation to modernise and enhance its data infrastructure. The project focuses on building scalable, high-quality data models that support operational efficiency and strategic decision-making. You will be collaborating with cross-functional teams to design, standardise, and govern data solutions that drive business value across regulatory, analytical, and customer-facing initiatives. This is a key role where your expertise in data modelling will shape the foundation of a forward-looking data strategy. Your main responsibilities Designing and developing conceptual, logical, and physical data models Designing and developing conceptual, logical, and physical data models Collaborating with stakeholders to gather and translate data requirements Collaborating with stakeholders to gather and translate data requirements Establishing and maintaining data modelling standards and governance policies Establishing and maintaining data modelling standards and governance policies Participating in architecture and design discussions to align data models Participating in architecture and design discussions to align data models Working closely with data engineers and architects on data integration strategies Working closely with data engineers and architects on data integration strategies Creating documentation to support metadata management and data lineage Creating documentation to support metadata management and data lineage Reviewing and optimising existing data models for scalability and performance Reviewing and optimising existing data models for scalability and performance Advising teams on best practices for data modelling and data quality Advising teams on best practices for data modelling and data quality Supporting regulatory and compliance requirements through accurate modelling Supporting regulatory and compliance requirements through accurate modelling Staying informed on emerging data technologies and recommending improvements Staying informed on emerging data technologies and recommending improvements You're ideal for this role if you have: Demonstrated expertise in data modelling across operational and analytical systems Demonstrated expertise in data modelling across operational and analytical systems Proven experience eliciting, documenting, and verifying data requirements Proven experience eliciting, documenting, and verifying data requirements Strong communication skills for engaging both technical and non-technical stakeholders Strong communication skills for engaging both technical and non-technical stakeholders Excellent consulting skills to advise and guide teams on data architecture Excellent consulting skills to advise and guide teams on data architecture Solid relationship management abilities across complex organisational structures Solid relationship management abilities across complex organisational structures Deep knowledge of metadata management and data cataloguing Deep knowledge of metadata management and data cataloguing Strong problem-solving skills for analysing and resolving data challenges Strong problem-solving skills for analysing and resolving data challenges Experience applying data governance principles and standards Experience applying data governance principles and standards Ability to develop scalable and efficient data models for enterprise systems Ability to develop scalable and efficient data models for enterprise systems Proficiency in using data modelling tools such as Erwin, PowerDesigner, or similar Proficiency in using data modelling tools such as Erwin, PowerDesigner, or similar It is a strong plus if you have: Familiarity with financial services data domains and regulatory reporting needs Familiarity with financial services data domains and regulatory reporting needs Experience working in Agile or DevOps environments Experience working in Agile or DevOps environments Understanding of cloud-based data platforms (e.g., AWS, Azure, GCP) Understanding of cloud-based data platforms (e.g., AWS, Azure, GCP) Knowledge of data warehouse and data lake design principles Knowledge of data warehouse and data lake design principles Exposure to master data management and reference data modelling Exposure to master data management and reference data modelling We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to an attractive Medical Package Access to Multisport Program Access to Multisport Program #GETREADY Internal job ID #7435 You can report violations in accordance with ITDS’s Whistleblower Procedure available here .","[{""min"": 1200, ""max"": 1400, ""type"": ""Net per day - B2B""}]",Data Science,1200,1400,Net per day - B2B
Full-time,Mid,B2B,Remote,495,Programista Baz Danych MS-SQL,People More P.S.A.,"Cześć! Nazywamy się People More bowiem traktujemy naszych współpracowników z szacunkiem ale również dlatego, że projekty przy których pracujemy są dla ludzi i powinny być łatwe oraz przyjemne w użyciu. Jesteśmy technologiczni ale patrzymy szerzej : ) People More istnieje już ponad cztery lata i wywodzi się z jednej z najstarszych agencji interaktywnych w kraju - Insignia. Spółkę tworzą osoby z ogromnym zapleczem klientów w kraju i zagranicą dla których budujemy projekty od zera (UX, UI, frontend, backend, mobile) lub w części. Tworzymy bezpośrednio dla naszych klientów jak również wspieramy naszych partnerów w pracy nad własnymi rozwiązaniami. Gwarantujemy tym samym bogaty wachlarz projektów i możliwość ich zmian! Pracujemy przy projektach z całego świata. Do projektu naszego bezpośredniego klienta poszukujemy specjalisty na stanowiskoProgramista Baz Danych MS-SQL Zakres obowiązków: Projektowanie i rozwój struktur baz danych MS SQL Tworzenie oraz optymalizacja zapytań i procedur składowanych (T-SQL) Wsparcie zespołów projektowych w obszarach związanych z bazami danych Ścisła współpraca z developerami .NET Diagnozowanie problemów i wsparcie w zakresie działania aplikacji oraz modyfikacja danych Generowanie raportów zgodnie z ustalonym harmonogramem Wymagania: Minimum 2 lata doświadczenia w programowaniu MS SQL, T-SQL Bardzo dobra znajomość SQL oraz procedur składowanych Umiejętność priorytetyzacji zadań i dobrej organizacji pracy własnej Znajomość środowiska .NET Znajomość systemu kontroli wersji GIT Znajomość narzędzi RED GATE (SQL Source Control, SQL Prompt) Umiejętność analizy potrzeb wewnętrznego klienta i dostosowania rozwiązań Znajomość języka angielskiego i polskiego O nas / co oferujemy: Jesteśmy otwarci, szczerzy i rozwiązujemy problemy zamiast je generować. Może to oczywiste ale naprawdę szanujemy pracowników i współpracowników. My też byliśmy programistami i cenimy tę pracę Międzynarodowe środowisko i projekty Prywatną opiekę medyczną Kartę sportową Praca w 100% zdalna (chyba że preferujesz inny system) Mamy biuro w Krakowie, ale jeśli lubisz pracować zdalnie, nie ma sprawy. Nie mamy nic przeciwko pracy w pełni zdalnej. Dla nas możesz znajdować się w dowolnym miejscu : ) Dlaczego warto pracować z People More? Jeśli nie jesteś zadowolony ze swojej pracy lub zadań, wspólnie znajdziemy wyjście! Jeśli się znudzisz, zaproponujemy Ci nowy produkt i nowe, fascynujące zadania Wspólnie popracujemy nad Twoją marką: będziesz miał okazję uczestniczyć w konferencjach, w tym jako prelegent, pomożemy Ci publikować w uznanych czasopismach i online Ułatwimy Ci dostęp do wyzwań, które zazwyczaj są trudne do zdobycia. W każdej chwili możesz porozmawiać bezpośrednio z zarządem People More - mówimy Twoim językiem, ponieważ założyciele firmy są programistami i projektantami! Jak wygląda proces rekrutacji? Przyjazna, zdalna rozmowa wstępna Zdalna rozmowa techniczna Decyzja o podjęciu współpracy!","[{""min"": 110, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Database Administration,110,170,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,496,Data & BI Software Engineer,Spyrosoft,"Requirements: Experience in any SQL (or similar) Knowledge of BI tools (particularly Power BI) Ability to design data models Optimizing interactive dashboards and reports Familiarity with ETL/ELT processes Experience with GCP, especially Looker and LookML Data governance principles and experience in monitoring data quality Fluency in English We are seeking a Data & BI Specialist responsible for designing, optimizing, and developing BI solutions. Depending on your experience level (Regular/Senior), your tasks will include advanced data modeling, performance optimization, and data visualization. Additionally, you may work on developing infrastructures and processing large datasets, ensuring their high quality and availability across the organization. Data Visualization: Creating and optimizing interactive dashboards and data visualizations. Performance optimization of reports. UX/UI development. Data Modeling & SQL: Designing scalable data models. Advanced SQL queries. Supporting Master Data Management (MDM). Infrastructure & Data Processing: Building and optimizing ETL/ELT processes. Incident management and monitoring cloud infrastructure. Data Governance & Quality: Implementing data governance principles. Monitoring data quality (KPIs). Managing data catalogs (e.g., Purview). Business Support: Supporting stakeholders through data analysis and delivering key insights. Gathering business requirements. Proficiency in SQL (or Python, Scala, etc.) and ability to design data models (Lookml, AAS) Hands-on experience with BI tools (Looker, Power BI) and optimizing interactive dashboards and reports. Knowledge of ETL/EL processes Strong expertise in Google Cloud Platform, particularly with Looker and LookML. Understanding of data governance principles and experience in monitoring data quality. Fluent communication in English, especially in the context of technical documentation and collaboration with international teams. Ability to create scripts for process automation (Python, Scala). Experience in Fabric, Databricks, Snowflake, etc. Basic knowledge of frameworks for processing large datasets, such as Hadoop or Spark. Experience with data management tools (e.g., Purview, Collibra).","[{""min"": 70, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Data Engineering,70,150,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,497,Senior Data Engineer (stałe zatrudnienie - sektor bankowy),DEVTALENTS Sp. z o.o.,"Senior Data Engineer | Tworzenie skalowalnych rozwiązań chmurowych O DEVTALENTS oraz model zatrudnienia W DEVTALENTS łączymy wybitnych specjalistów IT z ambitnymi projektami, stosując nasz unikalny model współpracy „Build-Operate-Transfer”. Jako członek zespołu DEVTALENTS będziesz pracować nad innowacyjnymi rozwiązaniami dla naszych klientów, mając jasno określoną ścieżkę prowadzącą do bezpośredniego zatrudnienia u klienta. Prowadzenie projektowania, rozwoju i utrzymania potoków danych oraz procesów ETL/ELT obsługujących duże, zróżnicowane zbiory danych. Optymalizacja procesów pobierania, transformacji i dostarczania danych z wykorzystaniem SQL, PySpark i Pythona. Wykorzystywanie frameworków takich jak Apache Airflow, AWS Glue, Kafka i Redshift w celu zapewnienia wydajnej orkiestracji danych, przetwarzania wsadowego/strumieniowego i wysokiej wydajności analiz. Wdrażanie najlepszych praktyk w zakresie kontroli wersji (Git), infrastruktury jako kodu (Terraform, Ansible) oraz pipeline’ów CI/CD, aby zapewnić solidne, powtarzalne i skalowalne wdrożenia. Ścisła współpraca z zespołami Data Science, Analityki i Product Management przy projektowaniu modeli danych i architektur wspierających cele biznesowe. Monitorowanie, debugowanie i optymalizacja potoków ETL, zapewnianie wysokiej niezawodności, niskich opóźnień i efektywności kosztowej. Mentoring inżynierów na poziomie średnim i juniorskim oraz budowanie kultury dzielenia się wiedzą, ciągłego doskonalenia i innowacji. Duża biegłość w SQL, PySpark i Pythonie w zakresie transformacji danych oraz tworzenia skalowalnych potoków danych (minimum 6 lat doświadczenia komercyjnego). Praktyczne doświadczenie w pracy z Apache Airflow, AWS Glue, Kafka i Redshift. Znajomość pracy z dużymi wolumenami danych strukturalnych i częściowo strukturalnych. Mile widziane doświadczenie z DBT. Biegłość w korzystaniu z Gita do kontroli wersji. Airflow jest kluczowy do orkiestracji procesów. Solidne doświadczenie w pracy z AWS (Lambda, S3, CloudWatch, SNS/SQS, Kinesis) oraz znajomość architektur serverless. Doświadczenie w automatyzacji i zarządzaniu infrastrukturą za pomocą Terraform i Ansible. Umiejętności w zakresie monitorowania potoków ETL, rozwiązywania problemów z wydajnością oraz utrzymywania wysokiej niezawodności operacyjnej. Znajomość procesów CI/CD w celu automatyzacji testów, wdrożeń i wersjonowania potoków danych. Umiejętność projektowania rozproszonych systemów, które skalują się horyzontalnie dla dużych wolumenów danych. Wiedza o architekturach przetwarzania w czasie rzeczywistym (Lambda) i wsadowym (Kappa) będzie dodatkowym atutem. Doświadczenie w tworzeniu API (REST, GraphQL, OpenAPI, FastAPI) do wymiany danych. Znajomość zasad Data Mesh i narzędzi self-service do danych będzie dużym plusem. Wcześniejsze doświadczenie w budowaniu skalowalnych platform danych i przetwarzaniu dużych zbiorów danych jest wysoko cenione. Wyższe wykształcenie w zakresie informatyki lub kierunków pokrewnych. Znajomość języka angielskiego na poziomie co najmniej B2. Proaktywne podejście do rozwiązywania problemów, pasja do podejmowania decyzji w oparciu o dane i nieustannego doskonalenia. Doskonałe umiejętności komunikacyjne pozwalające przekładać złożone koncepcje inżynierii danych na zrozumiały język dla odbiorców technicznych i nietechnicznych. Umiejętność współpracy w środowisku wielofunkcyjnym i zwinnym oraz gotowość do wspierania i mentorowania członków zespołu. Chęć śledzenia trendów branżowych, eksperymentowania z nowymi technologiami i wdrażania innowacji w praktykach inżynierii danych.","[{""min"": 25000, ""max"": 32000, ""type"": ""Net per month - B2B""}]",Data Engineering,25000,32000,Net per month - B2B
Full-time,Senior,B2B,Remote,502,Senior Data Engineer,edrone,"We're a hard-working, fun-loving, get-things-done type of team dedicated to providing unique marketing automation solutions for clients. We understand the challenges of eCommerce and the importance of seamless customer service and satisfaction. We roll our sleeves up, act fast, and learn together. We're looking for a Senior Data Engineer who will do the same! 🚀 Who are we? Edrone is a SaaS-based product that helps thousands of small and medium-sized businesses compete with major brands. Our mission is to provide simple solutions to big challenges in eCommerce. We achieve results through a strong feedback culture and clearly defined, transparent expectations. Currently, we work with nearly 2,000 online stores — primarily in Poland and Brazil. Brands such asBielenda,Mosquito,2005,orLilouhave placed their trust in our product. If you want to learn more about our culture and what it’s like to work at edrone, check it outhere. Our social media: LinkedIn,Instagram,YouTube. Sounds great? Keep on reading! 🚀 What’s in it for you: Be part of a small, fast-paced team that values innovation, efficiency, and a positive work culture. We thrive on challenges, embrace change, and keep things moving. We value initiative and ownership—if something makes sense, we act on it quickly and take full responsibility for delivering it. Direct responsibility for projects, regular 1: 1s with your leader with a blameless postmortems, code reviews B2B contract (20-25k) & covering all the costs of accounting services Hybrid or remote work or a modern, well-equipped office - whatever you prefer! 26 paid days off so you can relax properly! Benefits - MultiSport card, LuxMed medical package, group accident insurance, English and Portuguese classes, and Mindgram - a portal for mental health and development 🚀 How you will spend your time: Leadership, Innovation & Excellence Drive technical design discussions, lead critical implementations, and support decision-making across the team Mentor other engineers and raise the bar for code quality and system thinking Stay current with evolving technologies and proactively introduce improved tools, patterns, or approaches when beneficial. Champion engineering excellence by questioning the status quo and influencing product/technical direction Backend System Development Design, build, and maintain robust Python-based services and microservices Develop and optimize RESTful APIs and background services supporting core business logic and integrations Ensure code quality, reusability, and scalability through modular design and adherence to best practices Cloud-Native Application Engineering Develop serverless and containerized applications usingAWS Lambda,ECS, and other cloud-native tools LeverageAWS services(S3, RDS, DynamoDB, Step Functions, etc.) to support backend operations and workflows Collaborate with DevOps to provision, deploy, and monitor cloud infrastructure Automation and Task Orchestration Automate recurring tasks, background jobs, and workflows using Python scripts and AWS orchestration tools Build and maintain task schedulers and asynchronous workers for time-sensitive operations Implement monitoring, logging, and alerting systems for observability and proactive issue resolution Data Access and Integration Build data access layers and connectors for interfacing with relational and NoSQL databases Develop data integration scripts or services to move and sync data between systems when needed Write efficient, production-grade SQL and Python code to support internal tools and services Performance and Reliability Optimize application performance, API response times, and system throughput Implement retries, fallbacks, and circuit breakers to increase fault tolerance Continuously assess and improve system design for scalability and maintainability 🚀Who you are: 5+ years of professional experienceas a Data or Python Engineer Experience inData Engineering– including schema design, query optimization, and managing pipelines in production Experienced withAWS services (Redshift, Aurora, DynamoDB, S3, Glue, Lambda, Step Functions, etc.)to build data pipelines and scalable cloud-native applications. You’re familiar withdbtor eager to work with it Experience withdata orchestration tools(e.g., Airflow, Step Functions) — scheduling, monitoring, and troubleshooting data pipelines. Experience in building and maintainingRESTful APIs, microservices, and backend systems. Have acted as atechnical/feature leadon multiple projects, owning solutions from design through production operation. Comfortableworking closely with the Product team, aligning goals, making independent technical decisions, and challenging assumptions when needed. Experienced inleading design reviews, engaging in pair programming, and documenting key decisions for long-term clarity. 🚀 It’s nice if you have: Experience in Java 📝How does the recruitment process look like: A 30-minute phone interview with thePeople and Culture Partner-Milena Micor, where we aim to get to know you a little better! A technical online interview with theData Team Lead - Krystian Andruszek and another panelist 30-minute conversation with ourCTO Maciej Mendrelato align on vision, culture, and expectations Decision regarding the offer and welcome on board! After each stage, you will always receive feedback regarding your candidacy.","[{""min"": 20000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,20000,25000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,503,👉 Senior Azure Data Engineer,Xebia sp. z o.o.,"🟣 You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. 🟣 Your profile: ready to start immediately , 3+ years’ experience with Azure (Data Factory, SQL, Data Lake, Power BI, Devops, Delta Lake, CosmosDB), 5+ years’ experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools – Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, experience with CI/CD tooling (GitHub, Azure DevOps, Harness etc), working knowledge of Git, Databricks will be benefit, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ Please note that we are currently looking to expand our talent pool for future opportunities within the IT industry. While we may not have an immediate project for you at the moment, we are proactively recruiting to ensure that we have the right expertise when new projects arise. We will contact you when a potential project matching your skills and experience becomes available. Thank you for your interest in joining our team. 🟣 Nice to have: ﻿ experience with Azure Event Hubs, Azure Blob Storage, Azure Synapse, Spark Streaming, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification. 🟣 Recruitment Process: CV review – HR call – Technical Interview (with live-coding elements) – Client Interview (live-coding)– Hiring Manager call – Decision 🎁 Benefits 🎁 ✍ Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺 We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️ We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,22300,33700,Net per month - B2B
Full-time,Mid,B2B,Remote,504,Power BI Developer,in4ge sp. z o.o.,"Everything is possible with the right people In4ge jest firmą rekrutacyjną, w której wierzymy, że wszystko jest możliwe dzięki odpowiednim ludziom. Naszym celem jest połączenie najbardziej utalentowanych pracowników z odpowiednimi firmami, tworząc synergiczne relacje, które przyczyniają się do wzrostu i sukcesu każdej ze stron. Uważamy, że prawdziwą wartość stanowią ludzie pracujący wspólnie w atmosferze wzajemnego szacunku i zaufania. Dla naszego Klienta, międzynarodowej organizacji, poszukujemy osób na stanowisko Power BI Developer. Zakres obowiązków: Projektowanie, rozwój i utrzymanie raportów oraz dashboardów w Power BI. Współpraca z zespołami analitycznymi i biznesowymi w celu identyfikacji wymagań oraz przekształcania danych w wartościowe informacje. Integracja danych z różnych źródeł (SQL, Excel, API, itd.) i ich przetwarzanie. Praca z zapytaniami SQL w celu ekstrakcji, transformacji i ładowania danych (ETL). Optymalizacja raportów i zapytań w Power BI pod kątem wydajności. Tworzenie i wdrażanie modeli danych w Power BI i SQL. Wspieranie procesów analizy danych i podejmowania decyzji biznesowych dzięki interaktywnym dashboardom i raportom. Wymagania: Minimum 3-letnie doświadczenie w pracy z Power BI, w tym tworzeniu raportów, dashboardów i modeli danych. Umiejętność pracy z SQL (zapytania, agregacje, joiny, optymalizacja zapytań). Doświadczenie w pracy z danymi: przetwarzanie, analiza i modelowanie danych. Znajomość narzędzi ETL i integracji różnych źródeł danych. Umiejętność tworzenia rozwiązań frontendowych w Power BI (dynamiczne raporty, wykresy, tabele). Znajomość podstawowych zasad analizy danych oraz tworzenia wizualizacji, które wspierają procesy podejmowania decyzji. Dobra znajomość języka angielskiego w mowie i piśmie. Dodatkowe atuty: Doświadczenie z Power Query i DAX. Znajomość narzędzi do automatyzacji procesów analitycznych. Zrozumienie procesów biznesowych i zdolność do przekształcania danych w użyteczne informacje. Oferujemy: Udział w dynamicznych, międzynarodowych projektach, Możliwość pracy w pełni zdalnej (sporadyczne wizyty w zależności od potrzeb biznesowych), Elastyczne godziny pracy oraz przyjazną atmosferę w zespole, Możliwość rozwoju w nowoczesnych technologiach i realny wpływ na innowacyjne projekty. Jak wygląda proces rekrutacyjny in4ge? Nasz proces rekrutacyjny jest transparentny i skoncentrowany na znalezieniu odpowiedniego kandydata dla naszych klientów. Składając aplikację, możesz liczyć na nasz obiektywizm, szacunek i pełny profesjonalizm. Czekamy na Twoje CV. We connect you with the right people","[{""min"": 14000, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,14000,24000,Net per month - B2B
Full-time,Mid,B2B,Remote,506,Data Engineer,DataMass,"About the Role: We are looking for an experiencedData Engineerto join our delivery team working for aleading financial client in the UK. This is an exciting opportunity to contribute to the development of a scalable and secure data platform leveragingDatabricks,Apache Spark, andAzure. The ideal candidate will be comfortable working in regulated environments and building robust pipelines that support critical analytics and reporting needs. Build, optimize, and maintain data pipelines usingApache SparkandDatabricks Develop and deploy ETL workflows usingPySparkandPython Write clean, optimizedSQLqueries for complex data transformations Work closely with data analysts, data scientists, and business users to deliver well-structured, reliable datasets Implementdata modelingbest practices for both batch and streaming data LeverageAzureservices to manage data storage, orchestration, and compute workloads Ensure data quality, lineage, and compliance within afinancial servicescontext Contribute to solution architecture and participate in code reviews and Agile ceremonies Strong experience withApache Spark(Advanced) Proficient inDatabricks,PySpark,Python, andSQL(Regular) Hands-on experience working withAzurecloud services Solid understanding ofdata modelingand data warehousing principles Experience working with financial or highly regulated datasets (preferred) Strong communication and stakeholder collaboration skills Experience withDelta Lake,Unity Catalog, orAzure Synapse Familiarity with data governance, security, and compliance standards (e.g., GDPR, FCA) Exposure toCI/CD,Git, or workflow/orchestration tools (e.g., Azure Data Factory) Knowledge ofdata quality frameworks(e.g., Great Expectations) Opportunity to work on a high-impact project in the financial domain Access to cutting-edge technologies and cloud infrastructure Collaborative, diverse, and inclusive team environment Flexible working model (remote)","[{""min"": 20300, ""max"": 25700, ""type"": ""Net per month - B2B""}]",Data Engineering,20300,25700,Net per month - B2B
Full-time,Mid,B2B,Remote,507,Data Engineer,Datumo,"We’re looking for a Data Engineer ready to push boundaries and grow with us. Datumo specializes in providing Data Engineering and Cloud Computing consulting services to clients from all over the world, primarily in Western Europe, Poland and the USA. Core industries we support include e-commerce 🛒, telecommunications 📡 and life sciences 🧬. Our team consists of exceptional people whose commitment allows us to conduct highly demanding projects. Our team members tend to stick around for more than 3 years, and when a project wraps up, we don't let them go - we embark on a journey to discover exciting new challenges for them. It's not just a workplace; it's a community that grows together! Must-have: ✅ at least 3 years of commercial experience in programming ✅ proven record with a selected cloud provider GCP (preferred), Azure or AWS ✅ good knowledge of JVM languages (Scala or Java or Kotlin), Python, SQL ✅ experience in one of data warehousing solutions: BigQuery/Snowflake/Databricks or similar ✅ in-depth understanding of big data aspects like data storage, modeling , processing , scheduling etc. ✅ data modeling and data storage experience ✅ ensuring solution quality through automatic tests, CI/CD and code review ✅ proven collaboration with businesses ✅ English proficiency at B2 level, communicative in Polish Nice to have: 🌟 knowledge of dbt, Docker and Kubernetes, Apache Kafka 🌟 familiarity with Apache Airflow or similar pipeline orchestrator 🌟 another JVM (Java/Scala/Kotlin) programming language 🌟 experience in Machine Learning projects 🌟 understanding of Apache Spark or similar distributed data processing framework 🌟 familiarity with one of BI tools: Power BI/Looker/Tableau 🌟 willingness to share knowledge (conferences, articles, open-source projects) What’s on offer: 🔥 100% remote work, with workation opportunity 🔥 20 free days 🔥 onboarding with a dedicated mentor 🔥 project switching possible after a certain period 🔥 individual budget for training and conferences 🔥 benefits: Medicover Private Medical Care , co-financing of the Medicover Sport card 🔥 opportunity to learn English with a native speaker 🔥 regular company trips and informal get-togethers Development opportunities in Datumo: 🚀 participation in industry conferences 🚀 establishing Datumo's online brand presence 🚀 support in obtaining certifications (e.g. GCP, Azure, Snowflake) 🚀 involvement in internal initiatives, like building technological roadmaps 🚀 training budget 🚀 access to internal technological training repositories Discover our exemplary project: 🔌 IoT data ingestion to cloud The project integrates data from edge devices into the cloud using Azure services. The platform supports data streaming via either the IoT Edge environment with Java or Python modules, or direct connection using Kafka protocol to Event Hubs. It also facilitates batch data transmission to ADLS. Data transformation from raw telemetry to structured tables is done through Spark jobs in Databricks or data connections and update policies in Azure Data Explorer. ☁️ Petabyte-scale data platform migration to Google Cloud The goal of the project is to improve scalability and performance of the data platform by transitioning over a thousand active pipelines to GCP. The main focus is on rearchitecting existing Spark applications to either Cloud Dataproc or Cloud BigQuery SQL, depending on the Client’s requirements and automate it using Cloud Composer. 📈 Data analytics platform for investing company The project centers on developing and overseeing a data platform for an asset management company focused on ESG investing. Databricks is the central component. The platform, built on Azure cloud, integrates various Azure services for diverse functionalities. The primary task involves implementing and extending complex ETL processes that enrich investment data, using Spark jobs in Scala. Integrations with external data providers, as well as solutions for improving data quality and optimizing cloud resources, have been implemented. 🛒 Realtime Consumer Data Platform The initiative involves constructing a consumer data platform (CDP) for a major Polish retail company. Datumo actively participates from the project’s start, contributing to planning the platform’s architecture. The CDP is built on Google Cloud Platform (GCP), utilizing services like Pub/Sub, Dataflow and BigQuery. Open-source tools, including a Kubernetes cluster with Apache Kafka, Apache Airflow and Apache Flink, are used to meet specific requirements. This combination offers significant possibilities for the platform. Recruitment process: 1️⃣Quiz - 15 minutes 2️⃣ Soft skills interview - 30 minutes 3️⃣ Technical interview - 60 minutes Find out more by visiting our website - https: //www.datumo.io If you like what we do and you dream about creating this world with us - don’t wait, apply now!","[{""min"": 14000, ""max"": 25000, ""type"": ""Net per month - B2B""}]",Data Engineering,14000,25000,Net per month - B2B
Full-time,Senior,B2B,Remote,508,Workplace SCCM/Intune Specialist,emagine Polska,"We are looking for an experiencedWorkplace SCCM/Intune Specialist, specializing in Modern Workplace to support the carve-out project. The focus of the role is to support the transition team with modern end-user technology services, with a strong focus on SCCM and Intune. This role is pivotal in ensuring a seamless, secure, and efficient digital workplace experience for our users. Strong documentation and communication skills are essential. Start: 1st September Duration: first contract for 6 months + possible extension Location: Fully Remote (Poland) Salary: B2b contract- 150-170 zł/hour Skills and Experience: Strong experience with Microsoft Intune, SCCM, Endpoint Manager (policy management, app deployments, reporting). Knowledge of hybrid-joined and Azure AD-joined device scenarios. Familiarity with Windows Autopilot, co-management, compliance policies, and Conditional Access. Good understanding of security controls on endpoints (AV, encryption, firewall, patching). Strong knowledge of end user devices, printers, device inventory, policies. Previous experience with Microsoft Exchange – important nice to have. Previous involvement in large-scale device migration or M&A carve-out projects is a plus. Key Responsibilities: Perform complete device inventory for in-scope users, validating ownership, management method, and compliance state. Plan and execute endpoint separation strategy (re-provisioning or re-enrollment into buyer’s management platform). Adjust Intune MDM and SCCM policies, profiles, and configurations for separation readiness. Manage application packaging and redeployment for devices transitioning to the buyer. Handle BitLocker, encryption keys, and endpoint security policies to ensure secure handover. Support co-management scenarios (SCCM and Intune) for hybrid-managed devices during migration. Provide technical guidance to user support teams for device wipe/rebuild/re-enrollment procedures. Document device management changes and handover processes for operational continuity.","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Unclassified,150,170,Net per hour - B2B
Full-time,Senior,B2B,Office,509,Data Engineer,1dea,"Dla jednego z dużych klientów poszukujemy osoby do roli: Data Engineer Warunki zaangażowania: Obszar: Finansowy Lokalizacja: Irlandia, 5 dni w tygodniu Start: ASAP (akceptujemy kandydatury z max 1msc okresem wypowiedzenia) Stawka (ustalana indywidualnie): 150 - 170 PLN net / h Zaangażowanie: B2B (outsourcing z 1dea), full-time, długofalowo Zakres obowiązków Projektowanie, tworzenie i utrzymywanie wysokiej jakości potoków danych (data pipelines). Analiza danych oraz modelowanie danych zgodnie z wymaganiami biznesowymi. Tworzenie i utrzymywanie procesów ETL do efektywnego pobierania i transformacji danych (znajomość SnapLogic będzie dodatkowym atutem). Zapewnienie wydajności, bezpieczeństwa i skalowalności rozwiązań danych. Współpraca z analitykami, naukowcami danych i innymi interesariuszami w zakresie projektowania i wdrażania rozwiązań. Udział w przeglądach kodu i projektów w celu utrzymania wysokiego poziomu standardów inżynierii danych. Tworzenie i utrzymywanie procesów CI/CD dla płynnej integracji i dostarczania danych. Współpraca z liderami technicznymi i architektami nad ulepszaniem rozwiązań. Wymagania Minimum 5 lat doświadczenia na stanowisku Data Engineera w środowisku korporacyjnym. Zaawansowana znajomość architektury danych i technologii bazodanowych. Biegłość w pracy z MS SQL, w tym ze środowiskiem SQL MI w chmurze Azure . Doświadczenie w analizie danych, modelowaniu danych oraz procesach ETL. Znajomość koncepcji hurtowni danych. Doświadczenie w pracy z narzędziami CI/CD (Git, Jenkins, Docker). Zrozumienie i doświadczenie w metodykach Agile. Oferujemy Zatrudnienie na podstawie umowy B2B na czas nieokreślony Dołączysz do firmy z solidną pozycją na rynku Firma zapewnia nowoczesny sprzęt, oprogramowanie i konfigurację Profesjonalne doradztwo i wsparcie w rozwoju kariery od doświadczonego zespołu specjalistów 1dea Cenimy sobie koleżeńskość, otwartość, szacunek, wzajemną pomoc i wsparcie w rozwijaniu kompetencji zarówno własnych, jak i kolegów i koleżanek z zespołu Wspieramy kulturę kreatywności. Każdy członek zespołu ma możliwość proponowania własnych pomysłów i rozwiązań, a jego głos jest zawsze brany pod uwagę","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Manager / C-level,B2B,Remote,513,Head of Data & Quant Engineering,RedStone,"RedStoneis a fast-growing Polish blockchain startup revolutionizing oracle infrastructure. With a team of 40, over half being senior engineers and technical experts, we deliver scalable, secure, and low-latency off-chain data to smart contracts across multiple chains. We’ve secured over $6B in TVS, raised $15M in Series A funding, and were recognized by Forbes asthe top VC-backed startup in Poland. Backed by leading Web3 names like Arrington Capital, Stani Kulechov (Aave), and Gnosis, our team includes alumni from Google, OpenZeppelin, and major crypto projects. We work remotely across time zones, with a Warsaw HQ for deep work and collaboration. JOIN THE REDSTONE TEAM! As Head of Data, you'll own the strategy and execution of RedStone’s data architecture. You’ll lead a high-performing technical team, define standards for data quality and reliability, and help deliver real-time intelligence for smart contracts across multiple chains and financial ecosystems. This is a high-impact leadership role at the intersection of DeFi, infrastructure, and big data. Lead theData Engineering and Analytics functionacross the company Architect robust systems for collecting, validating, and aggregating off-chain and on-chain data Define and enforcedata reliability, accuracy, and uptime standards Design monitoring systems to detect anomalies, manipulation attempts, and cross-source inconsistencies Work cross-functionally with Engineering, Product, and Business teams to turn raw data into mission-critical insights Grow and mentor a team of high-caliber data and backend engineers Research and prototype advanced data products (e.g., asset pricing models, latency-adjusted feeds, cross-DEX aggregation) Own the roadmap and delivery of the data infrastructure used by our oracle network Strong backend engineering background withPython, Go, or Rust Experience working withAWS (especially Lambda), event-driven architectures, andmessage queueslike RabbitMQDeep knowledge oftime-series databases(InfluxDB, TimescaleDB) and monitoring systems (Grafana, CloudWatch) Familiarity withon-chain data structures, smart contract logs, block timing, and decentralized data fetching Advanced understanding offinancial or trading data, including anomaly detection, latency compensation, and pricing validation Quality-Driven Mindset– extreme attention to detail; cares deeply about data integrity in high-stakes environments Leadership & Mentoring– experience managing or mentoring technical teams, building culture, and scaling impact Ownership & Execution– strong project management and execution skills, especially in unstructured and fast-moving environments Experience withDEX protocols(Uniswap, GMX, dYdX, Curve, etc.) or DeFi lending platforms (Aave, Compound, Euler) Knowledge ofCEX infrastructure, order books, matching engines, and arbitrage mechanics Understanding ofliquidations,slippage, and pricing risk in lending/AMM environments Contributions toWeb3 projects, DAOs, or open-source blockchain tooling Exposure toquantitative financeor financial modeling Hands-on experience withsmart contracts, oracles, and price feed architecture The role can be fully remote but we also have an office in the center of Warsaw at Zgoda 3. Competitive salary (based on skills and experience) +token allocation after 3 months Multisport card and private healthcare Long-term stability and growth (Series A closed, backed by top global investors) Flexible hours A fixed, pre-agreed number of paid service break days. Top-tier equipment (MacBooks, external displays) Regular team off-sites, hackathons, and Web3 events Full ownership over your work A rare opportunity to build foundational infrastructure for the future of finance 🌐Website 🧠Docs 💻GitHub 🐦X Be part of a world-class team building mission-critical Web3 infrastructure. Apply now and redefine the future of decentralized data.","[{""min"": 25000, ""max"": 35000, ""type"": ""Net per month - B2B""}]",Data Engineering,25000,35000,Net per month - B2B
Full-time,Senior,B2B,Remote,514,Data Engineer,Haddad Brands Europe,"Building a scalable Data Warehouse that consolidates transactional data from our AS400 (IBM i) ERP and DB2 databases Integrating data from MS SQL-based applications to support analytics and reporting Automating ETL pipelines for nightly and real-time data ingestion Implementing data quality, lineage, and governance processes Design, develop, and maintain ETL processes to extract data from AS400 DB2 and MS SQL systems Model and optimize Data Warehouse schemas (star, snowflake) for performance and scalability Collaborate with Business Analysts and stakeholders to translate requirements into data solutions Implement and monitor data quality checks, alerts, and remediation workflows Automate deployments and version control of data pipelines using tools like Git Troubleshoot data issues and tune database performance for large datasets Proven experience as a Data Engineer or similar role in a corporate environment Strong SQL skills and hands-on experience with DB2 on AS400 (IBM i) and MS SQL Server Proficiency in ETL frameworks or orchestration tools Programming experience in Python, Shell scripting, or similar languages Solid understanding of data modeling, warehousing concepts, and performance tuning Familiarity with data governance, lineage, and quality best practices Excellent communication skills and ability to work with cross-functional teams English proficiency at C1 level or higher Initial meeting in Polish to get acquainted and discuss your background Technical interview within two weeks, focusing on your data architecture and ETL skills Brief English conversation to assess collaboration and communication abilities Private medical insurance 21 days of paid vacation per year (B2B contract) Company-provided hardware and software Training programs and clear career-development path Freedom to choose tools and technologies Supportive, trust-based work environment Get to know us video: -<http: //gofile.me/20s0Q/YMS1drNMp Password: HaddadFamilyB2BA@Retail","[{""min"": 17000, ""max"": 24000, ""type"": ""Net per month - B2B""}]",Data Engineering,17000,24000,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,515,Middle Data Engineer (Databricks),N-iX,"#3821 Join our team to work on enhancing a robust data pipeline that powers ourSaaS product,ensuring seamless contextualization, validation, and ingestion of customer data. Collaborate withproduct teamsto unlock new user experiences by leveragingdata insights.Engage with domain experts to analyze real-world engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Clientwas established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client’s platform empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renownedScandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities: Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements: Programming: Minimum of3-4 yearsas data engineer, or in a relevant field. Python Proficiency: Advanced experience inPython, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights. Cloud: Familiarity with cloud platforms (preferablyAzure). Data Platforms: Experience withDatabricks, Snowflake, or similar data platforms. Database Skills: Knowledge of relational databases, with proficiency inSQL. Big Data: Experience using Apache Spark. Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability. Version Control: Experience withGitlabor equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have: Experience withDockerandKubernetes. Experience with document and graph databases. Ability to travel abroad twice a year for an on-site workshops.","[{""min"": 18101, ""max"": 21536, ""type"": ""Net per month - B2B""}, {""min"": 14591, ""max"": 17547, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18101,21536,Net per month - B2B
Full-time,Mid,Permanent or B2B,Hybrid,517,Data Scientist,NeuroSYS,"Nasz klient toglobalna firma farmaceutyczna, która rozbudowuje swoje systemy zarządzania produkcji opierając się na analityce danych z użyciem Data Science i Machine Learning. Obecnie poszukujemy doświadczonego Data Scientista, który pomoże namwykorzystać zgromadzone dane do optymalizacji procesów produkcyjnych. W ramach projektu klient pracuje nad przygotowaniem zestawu narzędzi analitycznych wykorzystujących Power BI jako interfejs do wizualizacji danych oraz bazujący na algorytmach Machine Learning do analizy trendów, odkrywania nieoczywistych wzorców w celu usprawnienia procesu produkcyjnego i wykrywania wczesnych oznak zużycia lub nieprawidłowego działania elementów maszyn (Predictive Maintenance). Twoje zadania: Wykorzystywanie technologiiOCR do ekstrakcji danychz dokumentów papierowych, Ścisła współpraca z ekspertami branżowymi w celu zrozumienia procesów i danych produkcyjnych Analiza potrzeb i wymagań biznesowych w zakresie analizy danych i oczekiwanych rezultatów, Ocena wykonalności i szacowanie czasochłonności dla zgłaszanych potrzeb, Dobór, implementacja i usprawnianiemodeli analitycznych, Przetwarzanie i eksploracja danych pochodzących zsystemu typu historian, Regularne raportowanie postępów prac i prezentacja wyników interesariuszom. Wymagane umiejętności: Doświadczenie wMachine Learning i Data Science: min.3 lata doświadczeniaw komercyjnych projektach wykorzystujących ML, umiejętność trenowania i oceny modeli ML, doświadczenie z technikami uczenia nadzorowanego i nienadzorowanego. Umiejętności programistyczne: znajomość narzędziOCR(AWS Textract, Tesseract), biegła znajomość językaPythonoraz popularnych bibliotekML(m.in. aiohttp, scikit-learn, TensorFlow, PyTorch, XGBoost), doświadczenie w przetwarzaniu i analizie dużych zbiorów danych (Big Data), znajomość narzędzi doprzetwarzania danych, takich jak Pandas, NumPy czy SQL, doświadczenie w pracyz bazami danychSQL i noSQL, praktyczna znajomość GIT oraz GitFlow, podstawowa znajomość Power BI będzie dodatkowym atutem. Umiejętności analityczne i komunikacyjne: zdolność do analizy wymagań biznesowych i przekładania ich na techniczne rozwiązania, umiejętność prezentacji wyników oraz klarownego wyjaśniania złożonych koncepcji technicznych osobom nietechnicznym, swoboda komunikacji wjęzyku angielskimw mowie i piśmie na poziomie C1. Mile widziane: Znajomość koncepcji i metod Predictive Maintenance i Przemysłu 4.0. (np. prognozowanie stanu maszyn, analiza drgań, wykrywanie awarii), zrozumienie pojęć takich jak IoT, SCADA, DCS, IIoT, Znajomość systemów typu historian (np. AVEVA Historian, OSIsoft PI) lub protokołów przemysłowych (OPC, Modbus), Doświadczenie z technologiami chmurowymi (np. Azure, AWS, Google Cloud), zwłaszcza w kontekście ML Ops, Znajomość technik DevOps i CI/CD (np. Docker, Kubernetes) używanych w środowisku ML, Doświadczenie w analizie danych procesowych i integracji z systemami przemysłowymi, Doświadczenie w pracy z danymi produkcyjnymi i procesowymi (np. dane z PLC, SCADA, Historian), Doświadczenie w tworzeniu dashboardów Power BI i ich integracji ze źródłami danych. Oferujemy: Atrakcyjną, pełną wyzwań pracę w zgranym zespole pasjonatów IT i luźnej atmosferze, Udział w innowacyjnych projektach realizowanych dla globalnego klienta, Dowolną formę zatrudnienia, elastyczne godziny pracy, Prywatną opiekę medyczną, Możliwość pracy zdalnej, jak i w biurze; ze sporadycznymi, obowiązkowymi spotkaniami w biurze, w którym czekają świeże owoce, przekąski i pyszna kawa non stop!","[{""min"": 15000, ""max"": 21800, ""type"": ""Net per month - B2B""}, {""min"": 11200, ""max"": 16200, ""type"": ""Gross per month - Permanent""}]",Data Science,15000,21800,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,520,Lead Data Scientist,Bayer Sp. z o.o.,"Lead Data Scientist For Digital Hub Warsaw, we are looking for: Lead Data Scientist Are you ready to make a significant impact in the world of data science and AI agents? We are seeking a talented Lead Data Scientist to become a vital part of our dynamic Data Assets, Analytics and AI Platform at Bayer Consumer Health. In this role, you will play a pivotal role in implementing global, cutting-edge AI solutions across commercial, product supply, marketing, and R&D domains. Our diverse, international team, spanning Poland and Germany, is dedicated to managing the entire product life cycle – from proof of concept to the seamless operation of fully industrialized products. We pride ourselves on delivering innovative solutions that leverage traditional Machine Learning, Generative AI, and Mathematical Optimization, all powered by a modern tech stack featuring Python, Azure and Databricks. If you’re passionate about driving data-driven decision-making and AI driven process automation and eager to collaborate with a forward-thinking team, we want to hear from you! Key Tasks & Responsibilities: Creates, employs, evaluates, optimizes, and maintains generative AI, machine learning, time series forecasting, mathematical optimization, simulation, and NLP models and workflows that deliver actionable insights and meet business objectives. Participates in the development, delivery, and maintenance of Agentic AI Platform by implementing and proposing new features, designing evaluation experiments, and researching the latest advancements in the GenAI field. Extracts valuable information from large structured and unstructured datasets, identifying key variables and metrics that influence consumer behavior, commercial activities, product supply, R&D and RMSQC. Builds prototypes to prove modeling concepts. Industrializes and scales up successful advanced analytics prototypes into IT products. Creates intuitive and interactive data visualizations, reporting tools and dashboards to communicate complex analytical findings to technical and non-technical stakeholders as well as enable scenario creation and management. Develops generalized frameworks and libraries for repetitive data science activities. Leads exploratory projects that investigate new data sources, tools, and analytical techniques, keeping Bayer at the forefront of data science in the consumer health sector. Builds data processing pipelines, analyzes data used for modelling and performs other data related activities if required. Manages code in Github repositories and performs peer code reviews. Fosters a culture of continuous learning within the data science team, taking active part in workshops and training sessions to share knowledge and skills. Presents compelling data driven stories to all levels of the organization, including peers, senior management, and internal customers to drive both strategic and operational changes in the business. Acts as a subject matter expert in data science, advising on best practices and emerging technologies that can enhance Bayer Consumer Health’s data science capabilities. Develops and train other members of the Data Science team. Qualifications & Competencies (education, skills, experience): Master’s or PhD degree with 8+ years of experience in Data Science or related fields. Proven educational background or applied experience in at least one of the following: Machine Learning, Statistics, Mathematics, Computer Science, Quantitative Finance/Economics/Marketing, Biostatistics, Bioinformatics, or other related quantitative disciplines. Expert proficiency and practical experience in machine learning, generative AI, forecasting, and mathematical optimization. Expert knowledge of data science tools, libraries, frameworks, and platforms: Python, SQL, Vector Stores, Spark, Azure, Databricks, Github, LangChain, RAG, prompt and context engineering. Ability to write production grade code using object-oriented programming paradigm. Proven track record of developing advanced analytics products within a cloud environment and delivering valuable analysis through the application of domain and business knowledge. Ability to create architecture for advanced analytics products. Problem solving and analytical skills. Interpersonal and communication skills, active listening, consulting, challenges, presentation skills. Leadership, strategic and design thinking. Fluent in English, both written & spoken. What do We offer: A flexible, hybrid work model. Great workplace in a new modern office in Warsaw. Career development, 360° Feedback & Mentoring programm. Wide access to professional development tools, trainings, & conferences. Company Bonus & Reward Structure. VIP Medical Care Package (including Dental & Mental health). Holiday allowance (“Wczasy pod gruszą”). Life & Travel Insurance. Pension plan. Co-financed sport card - FitProfit. Meals Subsidy in Office. Additional days off. Budget for Home Office Setup & Maintenance. Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs. Tailored-made support in relocation to Warsaw when needed.","[{""min"": 26000, ""max"": 34000, ""type"": ""Gross per month - Permanent""}]",Data Science,26000,34000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Hybrid,522,Senior Data Engineer,Remodevs,"Please note it's now remote role but later turns into hybrid - so only candidates from Warsaw and surroundings are required. About: We are seeking a highly motivated and self-driven data engineer for our growing data team -who is able to work and deliver independently and as a team. In this role, you will play a crucial part in designing, building and maintaining our ETL infrastructure and data pipelines. Major Responsibilities: ● Design, develop, and deploy Python scripts and ETL processes with Prefect and Airflow to prepare data for analysis. ● Model dimensional and denormalized schemas for optimal performance reporting and discovery. ● Design AI-friendly DB schemas and ontologies. ● Architect cloud ops solutions for data topologies. ● Transform and migrate data with Python, DBT, and Pandas. ● Work with event-based/streaming technologies for real-time ETL. ● Ingest and transform structured, semi-structured, and unstructured data. ● Optimize ETL jobs for performance and scalability to handle big data workloads. ● Monitor and troubleshoot ETL jobs to identify and resolve issues or bottlenecks. ● Implement best practices for data management, security, and governance with Prefect, DBT, and Pandas. ● Write SQL queries, program stored procedures, and reverse engineer existing data pipelines. ● Perform code reviews to ensure fit to requirements, optimal execution pattern,s and adherence to established standards. ● Assist with automated release management and CI/CD processes. ● Validate and cleanse data and handle error conditions gracefully. Skills ● 3+ years of Python development experience, including Pandas ● 5+ years writing complex SQL queries with RDBMSes. ● 5+ years of Experience with developing and deploying ETL pipelines using Airflow, Prefect, or similar tools. ● Experience with cloud-based data warehouses in environments such as RDS, Redshift, or Snowflake. ● Experience with data warehouse design: OLTP, OLAP, Dimensions, and Facts. ● Experience with Cloud-based data architectures, messaging, and analytics. Pluses: Experience with ● Docker ● Kubernetes ● CI/CD automation ● AWS lambdas/step functions ● Data partitioning ● Databricks ● Pyspark ● Cloud certifications","[{""min"": 24012, ""max"": 25859, ""type"": ""Net per month - B2B""}, {""min"": 24012, ""max"": 25859, ""type"": ""Gross per month - Permanent""}]",Data Engineering,24012,25859,Net per month - B2B
Full-time,Senior,B2B,Remote,523,Senior Power BI Developer,Holisticon Insight,"Holisticon Insightis a division ofhttp: //nexergroup.comfocused on IoT, AI, and advanced analytics. We assist our customers in developing IoT services, portals, and mobile apps and, above all, analyze data using AI and advanced analytics to reduce costs, streamline, and find new opportunities for the business. Our goal is to grow strong but rather in competencies than in numbers. We get things done and, on the way, we build a dynamic company culture to create the best place for our development. We love the atmosphere of a family business with its flat structure and self-organized teams where every opinion matters and influences our way of working Check us out! https: //holisticon.pl/holisticon-insight/ 🚀 We are looking foraSenior Power BI Developerto lead the frontend development of a proprietary Business Intelligence (BI) and analytics tool within the Dealer Management System (DMS) stream to work on a project in a team of our client,(a Swedish-based leading provider of transport solutions.) In the role of Senior Power BI Developer, you willbe responsible for building modern BI solutions usingPower BI Premium (Cloud). You will play a key role in enhancing our data visualization and reporting capabilities across complex data environments. Develop and maintain dashboards, cubes, and reports usingPower BI Premium (Cloud) Collaborate with cross-functional teams to gather requirements and deliver data-driven insights Participate in the migration from on-premise solutions to cloud-based BI infrastructure Drive innovation and best practices in frontend data engineering and analytics Expert-level proficiency in Power BI and Power Automate StrongSQL skillsand hands-on experience with data modeling and BI tools Proven track record working oncomplex projectswith multiple reports Analytical thinker with a passion for solving challenges through data Experience collaborating inlarge, diverse development teams Excellentcommunication skillsto engage with both technical and non-technical stakeholders Experience withPythonandSnowflake Familiarity withcloud migration projects Life insurance Multisport card Fully remote job Private medical care Flexible working hours Amazing integration events on a regular basis Training budget (e.g. Microsoft Azure Certifications) Opportunity to impact our company culture build-up Work equipment (laptop, 2 monitors, and accessories) 1️⃣ Initial chat with Joanna, our recruiter – 30 minutes 2️⃣ Technical interview with our expert – 30-45 minutes 3️⃣ Final interview with the client – 1 hour","[{""min"": 130, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,130,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,525,Senior Data Engineer,KMD Poland,"#Data Engineer #Apache Spark #Databricks #Java #Apache Kafka #Batch Processing #Structured Streaming #Azure #SQL #Microservices #CI/CD #Docker #DDD Are you ready to join our international team as aLead/Senior Data Engineer? We shall tell you why you should... What product do we develop? We are building an innovative solution,KMD Elements, on Microsoft Azure cloud dedicated to the energy distribution market (electrical energy, gas, water, utility, and similar types of business). Our customers include institutions and companies operating in the energy market as transmission service operators, market regulators,distribution service operators, energy trading, and retail companies. KMD Elements delivers components allowing implementation of the full lifecycle of a customer on the energy market: meter data processing, connection to the network, physical network management, change of operator, full billing process support, payment, and debt management, customer communication, and finishing on customer account termination and network disconnection. The key market advantage of KMD Elements is its ability to support highly flexible, complex billing models as well as scalability to support large volumes of data. Our solution enables energy companies to promote efficient energy generation and usage patterns, supporting sustainable and green energy generation and consumption. We work with always up-to-date versions of: • Apache Spark on Azure Databricks • Apache Kafka • Delta Lake • Java • MS SQL Server and NoSQL storages like Elastic Search, Redis, Azure Data Explorer • Docker containers • Azure DevOps and fully automated CI/CD pipelines with Databricks Asset Bundles, ArgoCD, GitOps, Helm charts • Automated tests How do we work? #Agile #Scrum #Teamwork #CleanCode #CodeReview #Feedback #BestPracticies • We follow Scrum principles in our work – we work in biweekly iterations and produce production-ready functionalities at the end of each iteration – every 3 iterations we plan the next product release • We have end-to-end responsibility for the features we develop – from business requirements, through design and implementation up to running features on production • More than 75% of our work is spent on new product features • Our teams are cross-functional (7-8 persons) – they develop, test and maintain features they have built • Teams’ own domains in the solution and the corresponding system components • We value feedback and continuously seek improvements • We value software best practices and craftsmanship Product principles: • Domain model created using domain-driven design principles • Distributed event-driven architecture / microservices • Large-scale system for large volumes of data (>100TB data), processed by Apache Spark streaming and batch jobs powered by Databricks platform Your responsibilities: •Develop and maintainthe leading IT solution for the energy market using Apache Spark, Databricks, Delta Lake, and Apache Kafka •Have end-to-end responsibilityfor the full lifecycle of features you develop •Designtechnical solutions for business requirements from the product roadmap •Maintainalignment with architectural principles defined on the project and organizational level •Ensure optimal performancethrough continuous monitoring and code optimization. •Refactor existing codeandenhance system architectureto improve maintainability and scalability. •Design and evolvethe test automation strategy, including technology stack and solution architecture. •Preparereviews,participatein retrospectives,estimateuser stories, andrefinefeatures ensuring theirreadinessfor development. Personal requirements: • Have4+years ofApache Sparkexperience and have faced various data engineering challenges in batch or streaming • Have an interest instream processingwith Apache Spark Structured Streaming on top of Apache Kafka • Have experienceleading technical solution designs • Have experience withdistributed systems on a cloud platform • Have experience withlarge-scale systems in a microservice architecture • Are familiar withGit and CI/CD practices and can design or implement the deployment process for your data pipelines • Possess aproactive approachand can-do attitude • Are excellent inEnglish and Polish, bothwritten and spoken • Have ahigher educationin computer science or a related field • Are ateam playerwith strong communication skills Nice to have requirements: • Apache Spark Structured Streaming • Azure • Domain Driven Development • Docker containers and Kubernetes • Message brokers (i.e. Kafka) and event-driven architecture • Agile/Scrum Our offer: • Contract type: B2B •Work Mode: Flexible — this role supportson-site,hybrid, andremotearrangements, depending on your individual preferences. •Occasional on-site presence may be required— for example, onboard new team members, explore new business domains, or refine requirements in close collaboration with stakeholders or team building activities. What does the recruitment process look like? • Phone conversation with Recruitment Partner • Technical interview with the Hiring Team • Cognitive test • Offer KMD (an NEC company) is committed to providing equal opportunities. Hence, we invite all qualified interested applicants to apply for career opportunities. At KMD all aspects of employment and cooperation including the decision to hire/cooperate with will be based on merit, competence, performance, and business needs without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other status protected under local anti-discrimination legislation.","[{""min"": 115, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,115,180,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,530,Senior Data Analyst,Jit Team,"Salary: 900-1050PLN/day on B2B Work model: hybrid from Gdańsk (at least 2 day per week from the office) Why choose this offer? You can expect aflexible work organization Theinternational work environmentwill give you the opportunity to interact with the English language on a daily basis Scandinavian organizational culturewill provide you with work-life balance, you will gain time for additional training (financed by Jit) TheJit communitywill bring you a nice time during regular integration meetings Project Join a team responsible foranalyzing and modernizing a complex data environmentwithin alarge organization,operating in thefinancial sector.The project focuses on understanding and documenting current data lineage across legacy systems, as well as designing afuture-state solution fordata acquisition, integration, and modeling from strategic sources. The goal is not only to map existing data processes, but also to propose optimized solutions that meet both business and technical needs. Responsibilities you'll have Analyze legacy systemsto identify data sources,transformations, and destinations Create detaileddocumentation ofdata flows (data lineage)anddata models Evaluate the qualityand relevance of existing data Collaborate withbusiness stakeholders and IT teams to understand data requirements Design and propose afuture-state data solutionand architecture Prepare technical documentation and project-related deliverables Expected competences and knowledge Min.4 years of experiencein data management, IT/business analysis, or data architecture Strong knowledge ofdata modeling, data flows, mapping techniques, and transformations Experience insolution design and system integration Experience withdata warehouses and databases(e.g., Snowflake, DB2) as well asETL/ELT tools(e.g., DBT, Alteryx) Familiarity withdata visualization tools(e.g., Power BI, QlikView) Excellent communication skills in English Self-driven withstrong stakeholder management skills, including communication, collaboration, requirement gathering, and expectation management Nice to have Experience in thebanking or financial sector Background in data programming Technologiesyou'll work with Databases & Data Warehouses: Snowflake, DB2 ETL / ELT Tools: DBT, Alteryx Data Visualization: Power BI, QlikView Languages & Techniques: SQL, data analysis, data mapping Documentation: UML, data lineage diagrams, data models Client– why choose this particular client from the Jit portfolio? Jit Team has had anover-decade-long relationshipwith the leading financial group in the Nordic countries, and we are privileged to be our client's premier partner in Poland. At present,over 200 Jit personnelare engaged in the completion ofmore than 60 projectsfor this Norwegian major provider of financial services with a global presence and a strong focus on modern technology. Our customer's work atmosphere is epitomized by theScandinavian culture, which is conducive to people who place emphasis onwork-life balance and feedback culture. Furthermore, all projects are executed in international teams, giving constant exposure to the English language. About Jit Team The Human factor of IT- it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employover 500 experienced experts. We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share whatwe have achieved over 15 years of activity. By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 17100, ""max"": 22050, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,17100,22050,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,533,"Senior Data Engineer, Active Journey Alliance (m/f/x)",HelloFresh,"HelloFresh Group, the world’s leading integrated food solutions provider, is expanding with a new R&D Tech office in Poland. With brands offering meal kits, ready-to-eat meals, and specialty products such as meat, seafood, and pet food, we are seeking individuals who are ready to make an impact from day one. Joining us in Wrocław means shaping the culture, working on meaningful R&D Tech projects, and contributing to a global company changing how people eat. At HelloFresh Group, we are driven by a high-performance culture that values speed, agility, and continuous learning. We believe in hands-on contribution and fostering a truly collaborative, egoless environment where every team member contributes to our mission ofchanging the way people eat, foreverand welcome team players who thrive in a dynamic environment, lead with ownership, and bring diverse perspectives to the table. Our teams thrive on in-person collaboration, with an expectation to work from the office four days a week. This approach creates a dynamic space where ideas flourish, decisions are made efficiently, and our collective impact is accelerated. It's how we stay closely connected to our shared goals and drive swift execution. The position is part of the Active Journey Alliance, which has the mission to maximize retention of active customers. Active Journey Alliance helps customers to create habits with our products, manage their subscription, order the products they love and provide help in the face of errors. Our Data Engineers assume development and operational responsibility for the HelloFresh Platform that enables serving millions of customers globally by providing the best & efficient experience for our customers and internal users. Take ownership of the end-to-end process, including architecture, design, development, deployment, and operations of data pipelines, applying DevOps practices, pair programming, and other cutting-edge methodologies. Be active, solution-oriented member of autonomous, cross-functional agile teams collaborating with Product Owners, Back-end Engineers, Front-end Engineers, Product Analysts, and Business Intelligence teams Demonstrate an in-depth understanding of HelloFresh’s core product and architecture, and act as ambassador for software solutions offering support and mentorship to colleagues Work with state-of-the-art technologies like AWS (EMR, Glue, s3, etc), Kafka, PySpark, Kubernetes, Airflow, Prefect, Tecton, Databricks, as well as our in-house Data Pipeline tools Strong background in software engineering with a focus on writing clean, maintainable Python code Proficient in working with cloud platforms and data technologies such as PySpark, Kubernetes, and Kafka Skilled in designing scalable data models and working with relational databases (e.g., PostgreSQL) and object stores (e.g., AWS S3) Track record of building and optimizing efficient, reliable data pipelines Committed to ensuring data quality, implementing monitoring practices, and maintaining high testing standards across the development lifecycle Comfortable in agile environments, embracing fast iterations, lean principles, and continuous delivery A collaborative team player who actively mentors others, stays curious, and is eager to learn and share new trends Health- You’re covered from your first day with private health insurance Hybrid Working Schedule- We work in-office 4 days a week to align on goals, with flexible hours to support work-life balance and personal needs Holidays- You receive 26 days of paid vacation each year, providing you time to rest and recharge Learning and Development- An annual Learning & Development budget and a Mentoring Program to support your ongoing professional growth Employee Referral Program- Our team members can participate in our internal employee referral program and receive a bonus for recommending successful candidates to open roles Daily Comforts- Free coffee, drinks, and fresh fruit are available to keep you refreshed throughout the day If you are passionate about making a tangible impact and thrive in a fast-paced environment where your work directly contributes to a global purpose, we encourage you to apply – even if your experience doesn't tick every single box, we believe there are many ways to develop skills and grow with us.","[{""min"": 14700, ""max"": 22100, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14700,22100,Gross per month - Permanent
Full-time,Senior,B2B,Remote,535,Data Vault Expert with Snowflake,Experis Manpower Group,Main Responsibilities: Assess the current platform and propose a path forward Stakeholder and change management Define platform strategy and vision Coordinate delivery and planning Required Skills: Min. 5 years of experience Experience withData Vault 2.0 Proficiency inSnowflake Hands-on experience withDBT Familiarity withAWS What We Offer: B2B contract via Experis Sports card Life insurance Private medical care Fully remote work - candidate must be located in Poland,"[{""min"": 28560, ""max"": 30240, ""type"": ""Net per month - B2B""}]",Data Architecture,28560,30240,Net per month - B2B
Full-time,Senior,B2B,Hybrid,536,Big Data Engineer,Link Group,"🚀 Big Data Engineer Full-time | AdTech Platform | Python/Java/Scala We’re looking for an experienced Big Data Engineer to join our high-impact team building the backbone of a global advertising platform delivering personalized content to millions of media-enabled devices. Your work will directly influence data-driven decision making, real-time targeting, and analytics pipelines for one of the most advanced AdTech ecosystems in the industry. 🔍 What You’ll Be Doing Build and maintain robust, scalable data pipelines to support attribution, targeting, and analytics Collaborate closely with Data Scientists, Engineers, and Product Managers Design and implement efficient storage and retrieval layers for massive datasets Optimize data infrastructure and streaming processing systems (e.g. Flink, Apache Ignite) Drive quality through unit tests, integration tests, and code reviews Develop and maintain Airflow DAGs and other orchestration pipelines Translate business needs into robust, technical data solutions Lead or support A/B testing and data-driven model validation Contribute to R&D initiatives around cloud services and architecture ✅ What You Bring 5+ years of experience in backend/data engineering using Python, Java, or Scala Strong experience with Big Data frameworks (Hadoop, Spark, MapReduce) Solid knowledge of SQL/NoSQL technologies (e.g. Snowflake, PostgreSQL, DynamoDB) Hands-on with Kubernetes, Airflow, and AWS (or similar cloud platforms) Stream processing experience: Flink, Ignite Experience with large-scale dataset processing and performance optimization Familiarity with modern software practices: Git, CI/CD, clean code, Design Patterns Fluent in English (B2+) Degree in Computer Science, Telecommunications, or related technical field ⭐ Bonus Points Experience with GoLang or GraphQL Hands-on with microservices or serverless solutions Experience in container technologies (Docker, Kubernetes) Previous work in AdTech, streaming media, or real-time data systems","[{""min"": 100, ""max"": 130, ""type"": ""Net per hour - B2B""}]",Data Engineering,100,130,Net per hour - B2B
Full-time,Senior,Permanent,Remote,537,Senior Application Manager Microsoft M365,Simon-Kucher CBS,"Become part of a unique entrepreneurial team.Think independently, use your initiative, and take some risks. Entrepreneurship is a powerful force that drives the growth not only of our firm but our clients and people. Unlock the power of opportunity.Advance your career in a thriving company that creates positive impact. We invest in your professional development every step of the way. Enjoy balance and flexible working.Be empowered to do your best work – we offer flexible and hybrid working, sabbaticals, and paid time off. Prioritize your health and wellbeing.No matter where you live, we offer a competitive suite of health benefits to help keep you and your loved ones safe. Work in a values-driven culture.At Simon-Kucher, our vision is to become the world's leading growth specialist. Our values guide the way we do business and communicate our distinctiveness. They sum up what we stand for, influence our culture, and drive how and why we do things. Be responsible for the planning, development, implementation and operation of Microsoft M365 tools (SharePoint, Teams) and Power Platform applications (Power Apps, Power Automate, etc.). Collaborate with and consult stakeholders to understand business needs, recommend application enhancements, and drive the adoption of best practices. Participate in strategic planning to align Microsoft services with business goals, staying abreast of industry trends and emerging technologies. Define operational level agreements within the IT organization and ensure compliance with them. Oversee configuration and maintenance of the Microsoft applications in close collaboration with the Simon-Kucher MS operations team. Ensure advanced technical support to end-users, addressing and resolving service-related issues according to agreed service level agreements. Liaise with software vendors for support, updates, enhancements, and developments, ensuring service level agreements are met. Support and enhance knowledge management and digital collaboration based on Microsoft tools and solutions. Develop and deliver training sessions for end-users (in collaboration with our Learning & Skill Development team), create detailed documentation, and maintain up-to-date knowledge bases. Manage incident resolution and problem-solving processes, conducting root cause analysis and implementing preventive measures. Apprenticeship as IT specialist or bachelor’s degree in information systems, computer science, IT, or a related field. Around 5 years of experience in Microsoft application management or a similar role. Proven experience in developing and customizing applications within the Microsoft 365 environment, including SharePoint, Power Apps, and Power Automate. Understanding of web technologies such as HTML5, CSS, and JavaScript. Experience with SAAS enterprise software solutions and cloud technologies. Familiarity with integration scenarios and process automation scenarios as well as API management. Advanced communication skills to effectively convey technical information to non-technical stakeholders and collaborate with cross-functional teams. A keen focus on user satisfaction, understanding their needs, and ensuring applications meet their expectations. Strong analytical, problem-solving, and organizational skills. Proficient in both spoken and written English to effectively communicate with international teams, vendors, and stakeholders.","[{""min"": 25000, ""max"": 28000, ""type"": ""Gross per month - Permanent""}]",Unclassified,25000,28000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,539,Python Developer with AI,Pretius,"At Pretius we are looking for Python Developer with AI to a project in the financial industry, international team. Location: remote Salary: 150-210 pln netto/h Project / Role Build and validate intelligent assistants with cutting-edge LLMs (e.g., GPT-4, Falcon 2, LLAMA 3, Mixtral) using techniques like RAG and agent frameworks (e.g., Langraph, CrewAI) Recommend technical approaches and architectures for business challenges Implement efficient Python-based data pipelines for AI model training and deployment Communicate insights to both technical and non-technical audiences Contribute to project documentation Requirements 4+ years of relevant experience Proficiency in Python and LLM frameworks (e.g., Langchain) Experience with transformer-based models and large datasets (e.g. Pandas, NumPy, SQL) Strong knowledge of ML/AI concepts: types of algorithms, machine learning frameworks,model efficiency metrics, model life-cycle, AI architectures Experience with data engineering, including preprocessing, transformation, and pipeline automation Fluent in English (C1) Nice to have: Familiarity with cloud-based ML services (AWS, Azure, GCP) What we offer? We focus on long-term relationships based on fair principles and reliability Co-financing of the Multisport card and Medicover private healthcare Modern office available Team bonding activities, internal courses, conferences, certifications","[{""min"": 25200, ""max"": 35280, ""type"": ""Net per month - B2B""}]",Data Science,25200,35280,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,540,Sr Data Engineer - Product Core Data Asset,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Senior Data Engineer - Product Core Data Asset The PS Data & Analytics team at Bayer Consumer Health focuses on driving digital transformation and innovation by creating best-in-class analytical solutions that enable data-driven decision making and performance optimization for Bayer Consumer Health’s Supply Chain organization. You will be part of the data & analytics organization and will be responsible for building data products in the area of product supply and supply chain. You partner with business stakeholders, data architects, data scientists, analytics leads as well as other engineers. You will build data pipelines, data models and provision data for Analytics products and data scientists. You will also make sure that proper development processes are followed within the team, enhance implementation frameworks and guide other team members in building scalable, secure and well performing data products. If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Ingest data from transactional systems and data warehouses of other business functions into the divisional data analytics platform. Key Tasks & Responsibilities: Integrate data from different sources (e.g. supply chain planning data, product hierarchy, quality data, inventory data, procurement data, distributor and transportation data) to develop globally harmonized data models and KPI calculations Contribute to define data management and data quality standards. Ensure that data is well-managed to build stable, reusable and quality assured data assets. Collaborate with other IT functions (enabling functions data asset teams, analytics teams, platform product managers & integration architects) to ensure the aforementioned activities are executed effectively. Ensure that product supply data products adhere to the data protections standards. Continuously enhance implementation frameworks based on the needs of the analytics products in your responsibility. Guide other data engineers in your team (internal or external engineers) and ensure that all engineers apply same design principles. Together with the assigned data architect, ensure that cost and time estimations are accurate, quality of delivery is assured, and deliverables are properly tested, documented and handed over to the operations team Qualifications & Competencies (education, skills, experience): Bachelor/Master’s degree in Computer Science, Engineering, or a related field. 5+ years of working experience in the field of Data & Analytics, preferably in the area of product supply and the CPG industry Excellent data engineering & technology knowledge (Azure Data Lake Gen2, Azure Synapse, Databricks, Snowflake, potentially also legacy stack SAP Hana, SQL, Python as well as data management knowhow (data cataloguing, data quality management) Knowledge of CI/CD processes and tools (GitHub, Azure DevOps Pipelines) Profound data content knowledge (Supply Chain, Logistics, Quality Management) and Product Supply process knowhow Experience in Agile methodologies (Scrum, Kanban) Strong problem solving and analytical skills, combined with impeccable business judgment. Excellent interpersonal and communication skills, active listening, consulting, challenging, presentation skills. Fluent in English, both written & spoken, intercultural awareness and willingness to travel What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360° Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (“Wczasy pod gruszą”) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn’t mean you aren’t the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,28500,Gross per month - Permanent
Full-time,Senior,B2B,Remote,541,Senior Data Engineer (Azure/Databricks),GS Services,Poszukujemy superbohaterów – Senior Azure Data Engineer’ów! 🦸‍♀️🦸‍♂️,"[{""min"": 170, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,190,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,545,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for aData Engineerwho can help us to provide solutions for customers for making informed decisions in volatile short-term markets. You will design, implement, and maintain data pipelines and storages, become a data manager of our model inputs, and create insightful and powerful analysis and visualization for day-ahead, intraday and balancing data. In our day-to-day work, we include pair programming, joint learning sessions, and recurring hacking days to explore new ideas. We have very much an agile and digital way of working with fast feedback loops and embracing a culture of learning and personal growth. What you will be doing to make a difference? Thrive in an empowered, self-driven team where you take ownership across the entire data lifecycle: Work together with in-house analysts to understand the domain and the data in question. Design, build and maintain flexible and scalable end-to-end data pipelines together with software engineers. Monitor quality and reliability of data and implement required tooling in coordination with data scientists. Visualize data in a meaningful way for in-house analysis and together with product manager and UX designer, for customer facing dashboards. What do you need to succeed in the role? A Bachelor’s or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. A good understanding of database systems. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Why will you love being part of our team? Supportive Onboarding: Begin your journey with a thorough introduction and a steep learning curve. Room to Grow: Shape and develop your role with a large degree of influence. Mission-Driven Culture: Join one of Europe’s most exciting green tech companies and contribute to building a more sustainable future. Inclusive Environment: Work in an innovative, international, and supportive atmosphere. Competitive Benefits: Enjoy salaries that reflect your professional experience, flexible working hours, and a hybrid work model that fits your lifestyle. Team Spirit: Collaborate with talented, inspiring colleagues who believe in succeeding together. Attractive Perks: Benefit from our referral program and other employee-focused initiatives. We are looking to hire for Volue office in Gdańsk but will be ready to consider other locations for the right candidate. In Volue, we cherish each employee’s competence, ideas and personality. Let your skills and talent be a part of our team – and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14000,22000,Net per month - B2B
Full-time,Mid,B2B,Remote,547,Data Engineer,edrone,"We're a hard-working, fun-loving, get-things-done type of team dedicated to providing unique marketing automation solutions for clients. We understand the challenges of eCommerce and the importance of seamless customer service and satisfaction. We roll our sleeves up, act fast, and learn together. We're looking for a Data Engineer who will do the same! 🚀 Who are we? Edrone is a SaaS-based product that helps thousands of small and medium-sized businesses compete with major brands. Our mission is to provide simple solutions to big challenges in eCommerce. We achieve results through a strong feedback culture and clearly defined, transparent expectations. Currently, we work with nearly 2,000 online stores — primarily in Poland and Brazil. Brands such asBielenda,Mosquito,2005,orLilouhave placed their trust in our product. If you want to learn more about our culture and what it’s like to work at edrone, check it outhere. Our social media: LinkedIn,Instagram,YouTube. Sounds great? Keep on reading! ✨ What’s in it for you: Be part of a small, fast-paced team that values innovation, efficiency, and a positive work culture. We thrive on challenges, embrace change, and keep things moving. We value initiative and ownership—if something makes sense, we act on it quickly and take full responsibility for delivering it. Direct responsibility for projects, regular 1: 1s with your leader with a blameless postmortems, code reviews B2B contract (15-20k) & covering all the costs of accounting services Hybrid or remote work or a modern, well-equipped office - whatever you prefer! 26 paid days off so you can relax properly! Benefits - MultiSport card, LuxMed medical package, group accident insurance, English classes, and Hedepy - a portal for mental health and development 🚀 How you will spend your time: Backend System Development Design, build, and maintain robust Python-based services and microservices Develop and optimize RESTful APIs and background services supporting core business logic and integrations Ensure code quality, reusability, and scalability through modular design and adherence to best practices Cloud-Native Application Engineering Develop serverless and containerized applications usingAWS Lambda,ECS, and other cloud-native tools LeverageAWS services(S3, RDS, DynamoDB, Step Functions, etc.) to support backend operations and workflows Collaborate with DevOps to provision, deploy, and monitor cloud infrastructure Automation and Task Orchestration Automate recurring tasks, background jobs, and workflows using Python scripts and AWS orchestration tools Build and maintain task schedulers and asynchronous workers for time-sensitive operations Implement monitoring, logging, and alerting systems for observability and proactive issue resolution Data Access and Integration Build data access layers and connectors for interfacing with relational and NoSQL databases Develop data integration scripts or services to move and sync data between systems when needed Write efficient, production-grade SQL and Python code to support internal tools and services Contribute to Innovation and Excellence Stay informed on modern Python practices, libraries, and AWS developments Take initiative in proposing improvements and new ideas to enhance our platform 👀Who you are: 3+ years of experienceas a Data Engineer. Hands-on experience withschema design, complex SQL/query optimization,and running data pipelines in production. Experienced withAWS services (Redshift, Aurora, DynamoDB, S3, Glue, Lambda, Step Functions, etc.)to build data pipelines and scalable cloud-native applications. dbtexperience (or strong SQL/ELT background and eagerness to learn dbt quickly). Familiarity withdata orchestration tools(e.g., Airflow, Step Functions) — scheduling, monitoring, and troubleshooting data pipelines. Ability tobuild and maintain RESTful APIs/microservicesin Python (e.g., FastAPI/Flask) and understand basic backend architecture. 👀 It’s nice if you have: Experience in Java is a plus. 📝How does the recruitment process look like: A 30-minute phone interview with the recruiter -Milena Micor, where we aim to get to know you a little better! A technical online interview with theTeam Lead Krystian Andruszek and another panelist A short call with ourCTO – Maciej Mendrela– where we’ll share more about the direction of our organization and how we see this role evolving Decision regarding the offer and welcome on board! ⭐ After each stage, you will always receive feedback regarding your candidacy.","[{""min"": 15000, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,15000,20000,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,549,Senior Data Engineer,N-iX,"#3071 We are seeking a proactive Senior Data Engineer to join our vibrant team. As a Senior Data Engineer, you will play a critical role in designing, developing, and maintaining sophisticated data pipelines, Ontology Objects, and Foundry Functions within Palantir Foundry. The ideal candidate will possess a robust background in cloud technologies, data architecture, and a passion for solving complex data challenges. Key Responsibilities: Collaborate with cross-functional teams to understand data requirements, and design, implement and maintain scalable data pipelines in Palantir Foundry, ensuring end-to-end data integrity and optimizing workflows. Gather and translate data requirements into robust and efficient solutions, leveraging your expertise in cloud-based data engineering. Create data models, schemas, and flow diagrams to guide development. Develop, implement, optimize and maintain efficient and reliable data pipelines and ETL/ELT processes to collect, process, and integrate data to ensure timely and accurate data delivery to various business applications, while implementing data governance and security best practices to safeguard sensitive information. Monitor data pipeline performance, identify bottlenecks, and implement improvements to optimize data processing speed and reduce latency. Troubleshoot and resolve issues related to data pipelines, ensuring continuous data availability and reliability to support data-driven decision-making processes. Stay current with emerging technologies and industry trends, incorporating innovative solutions into data engineering practices, and effectively document and communicate technical solutions and processes. Tools and skills you will use in this role: Palantir Foundry Python PySpark SQL TypeScript Required: 5+ years of experience in data engineering, preferably within the pharmaceutical or life sciences industry; Strong proficiency in Python and PySpark; Proficiency with big data technologies (e.g., Apache Hadoop, Spark, Kafka, BigQuery, etc.); Hands-on experience with cloud services (e.g., AWS Glue, Azure Data Factory, Google Cloud Dataflow); Expertise in data modeling, data warehousing, and ETL/ELT concepts; Hands-on experience with database systems (e.g., PostgreSQL, MySQL, NoSQL, etc.); Proficiency in containerization technologies (e.g., Docker, Kubernetes); Effective problem-solving and analytical skills, coupled with excellent communication and collaboration abilities; Strong communication and teamwork abilities; Understanding of data security and privacy best practices; Strong mathematical, statistical, and algorithmic skills. Nice to have: Certification in Cloud platforms, or related areas; Experience with search engine Apache Lucene, Webservice Rest API; Familiarity with Veeva CRM, Reltio, SAP, and/or Palantir Foundry; Knowledge of pharmaceutical industry regulations, such as data privacy laws, is advantageous; Previous experience working with JavaScript and TypeScript.","[{""min"": 18470, ""max"": 19579, ""type"": ""Net per month - B2B""}, {""min"": 14776, ""max"": 15515, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18470,19579,Net per month - B2B
Full-time,Mid,B2B,Hybrid,550,GenAI Productivity Analyst,ITDS,"GenAI Productivity Analyst Join us, and lead the charge in AI-powered developer tools! Kraków - based opportunity with hybrid work model (6 days/month in the office). As a GenAI Productivity Analyst, you will be working for our client, a global financial services leader pioneering developer productivity enhancements through innovative data science projects. Your role focuses on advancing the initial proof-of-concept linking GenAI coding assistant usage to developer productivity, operationalizing these insights, and identifying actionable patterns and use cases. Collaborating with data scientists and programme teams, you will help unlock industry-leading knowledge that supports developer performance improvements across thousands of users, setting new standards in GenAI adoption and value realization within a complex, fast-paced environment. Your main responsibilities: Reviewing and enhancing the existing GenAI CA Productivity Analysis proof-of-concept Developing a future roadmap and delivery strategy for GenAI CA productivity insights Identifying specific use cases and technical patterns where GenAI CAs impact productivity most Highlighting challenges, accelerators, and best practices for GenAI CA adoption Contributing to a data science strategy for measuring GenAI CA benefits Gathering, analyzing, and interpreting SDLC, Deployment, and DORA metrics data Collaborating with data analysts and scientists to refine analytical models and methodologies Defining delivery strategy, phased plans, and MVP implementation objectives Communicating actionable insights clearly to senior stakeholders Supporting delivery teams with inputs to develop initiatives improving GenAI CA benefits for developers You're ideal for this role if you have: Proven experience in data analysis within software development contexts Strong understanding of SDLC, Deployment, and DORA metrics and their impact on developer productivity Experience translating data analysis into actionable business or operational insights Familiarity with GenAI technologies, specifically coding assistants like GitHub Copilot Proficiency in data analytics and processing tools such as Python, R, SQL, or Jupyter notebooks Solid knowledge of data modeling concepts Experience working in global matrix organizations and cross-cultural environments Excellent written and verbal communication skills in English, including report and presentation creation Ability to work independently in a fast-paced, dynamic environment with tight deadlines Strong interpersonal and influential communication skills It is a strong plus if you have: Exposure to machine learning libraries such as Scikit-learn, XGBoost, Keras, or PyTorch Track record in knowledge acquisition, transfer, and community building processes Experience rewriting or refining English content authored by non-native speakers Willingness to explore and implement emerging technologies Prior experience contributing to developer productivity or adoption programs We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7159 You can report violations in accordance with ITDS’s Whistleblower Procedure available here .","[{""min"": 23100, ""max"": 29400, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,23100,29400,Net per month - B2B
Full-time,Senior,Permanent,Remote,552,Senior/Lead Data Scientist,Link Group,"Dołącz do zespołu, który wykorzystuje dane, by ulepszać procesy logistyczne. Szukamy osoby o analitycznym umyśle, która potrafi przekształcać liczby w konkretne działania i wspierać decyzje operacyjne. Twoje zadania: Oczyszczanie i przygotowanie danych do analiz i testów Budowa i doskonalenie modeli uczenia maszynowego Weryfikacja modeli, wyciąganie wniosków i prezentowanie wyników Współpraca z zespołami biznesowymi i technologicznymi Proponowanie nowych sposobów usprawnienia procesów na podstawie danych Czego oczekujemy: Umiejętności programowania w Pythonie lub R Znajomości SQL oraz środowisk danych, takich jak Databricks lub Snowflake Doświadczenia w pracy z modelami ML i ich wdrażaniem Podstaw matematycznych (algebra, statystyka, rachunek różniczkowy) Zdolności analitycznych i praktycznej znajomości takich metod jak regresje, drzewa decyzyjne, prognozowanie szeregów czasowych Umiejętności wizualizacji danych (np. z matplotlib, seaborn, ggplot2) Nice to have: Doświadczenie w środowisku produkcyjnym ML Wiedzę z zakresu łańcucha dostaw lub logistyki","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Science,20000,28500,Gross per month - Permanent
Full-time,Mid,B2B,Remote,553,Programista Baz danych,Eyzee S.A.,"Poszukujemy Programistów Baz danych, którzy mają doświadczenie w projektowaniu i eksploatacji baz danych dla naszego klienta w branży medycznej. Celem projektu jest opracowywanie i wdrażanie innowacyjnych rozwiązań, programowanie nowych funkcjonalności, rozwijanie, modyfikowanie i testowanie oprogramowania. Tworzymy przyjazne miejsce pracy i rozwoju dla specjalistów w branży IT, zapewniamy ciekawe wyzwania, dbając o dobrą komunikację i atmosferę w zespole. Z nami przyspieszysz rozwój swojej kariery! Praca zdalna, pełen etat. Zadania dla Ciebie: projektowanie, implementacja i utrzymanie struktur relacyjnych baz danych (np. PostgreSQL, MySQL, MSSQL, Oracle) dla systemu medycznego tworzenie i optymalizacja logiki biznesowej w bazie danych przy użyciu języków proceduralnych (np. PL/SQL, PL/pgSQL), w tym procedur składowanych, funkcji i wyzwalaczy pisanie złożonych i wydajnych zapytań SQL oraz skryptów do zarządzania danymi i ich przetwarzania zapewnienie integralności, bezpieczeństwa i optymalnej wydajności bazy danych współpraca z zespołem deweloperskim i analitykami w celu realizacji wymagań projektowych Wymagania: min. 5 lat doświadczenia w pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL, MSSQL, ORACLE) praktyczne doświadczenie w programowaniu w proceduralnych językach bazodanowych (np. PL/SQL, PL/pgSQL) znajomość technik migracji danych pomiędzy różnymi platformami bazodanowymi i systemowymi biegłej znajomości SQL doświadczenie z bazą danych PostgreSQL. umiejętność strojenia zapytań SQL i procedur PL/pgSQL. znajomość zasad zarządzania, konfiguracji i optymalizacji PostgreSQL Mile widziane: doświadczenie w branży medycznej Co oferujemy? stabilne zatrudnienie w oparciu o kontrakt B2B służbowy laptop i monitor dofinansowanie prywatnej opieki medycznej sportową kartę Multisport nauka języka angielskiego omawianie postępów i rozwoju co pół roku transparentna komunikacja z pracownikami możliwość zaangażowania się w rozwój organizacji chętnie dzielimy się wiedzą - dołącz do Akademii Eyzee mocny kompetencyjnie zespół składający się w większości z seniorów praca z narzędziami JIRA, Confluence, BitBucket dbamy o integracje i chętnie wspólnie spędzamy czas Kim jesteśmy? Jesteśmy polską firmą specjalizującą się w realizacji złożonych projektów informatycznych oraz doradczych dla firm z sektora finansowego, telekomunikacyjnego i publicznego. Stanowimy zgrany zespół konsultantów z wiedzą i wieloletnim doświadczeniem w tworzeniu i utrzymywaniu rozwiązań. Ważne dla nas są: doprecyzowanie wymagań przed napisaniem kodu, jakość tworzonego kodu, testowanie oraz CI/CD. Nasze projekty to głównie tworzenie nowych mikroserwisów lub nowych funkcjonalności do istniejących rozwiązań. Dodatkowo rozwijamy własne aplikacje i plugin’y, które nie tylko usprawniają pracę, ale też pozwalają rozwinąć nasze doświadczenie. Jeden z nich możesz pobrać tutaj (eZee Worklog). Jesteśmy partnerem Atlassian.","[{""min"": 15000, ""max"": 21000, ""type"": ""Net per month - B2B""}]",Database Administration,15000,21000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,555,Developer Hurtowni Danych,Clurgo,"Clurgo to firma stworzona przez developerów dla developerów. Odnajdą się u nas pasjonaci nowych technologii takich jak Java, Node, Frontend, jak i mikroserwisów. Jesteśmy firmą projektową skupioną na dostarczaniu rozwiązań IT dla naszych Klientów, przy zachowaniu dobrych praktyk programistycznych oraz work-life balance. Realizujemy projekty dla różnych branż - od małych projektów start-upowych, przez wysokobudżetowe projekty dla międzynarodowych firm. ✅Dołącz do projektu dla klienta z branży ubezpieczeń, gdzie Zespół Rozwoju Narzędzi HD/BI rozwija Hurtownię Danych w oparciu o technologie SAS i Oracle, jednocześnie eksplorując nowe rozwiązania jak Azure, Kafka czy REST API. Będziesz wspierać zespoły biznesowe Hurtowni Danych oraz prowadzić projekty Proof of Concept nowych narzędzi ✅Technologie i narzędzia: SAS, Oracle, Azure, Kafka, REST API ✅Praca hybrydowa (1 raz w tygodniu w biurze w Warszawie) ✅Praca w metodyce SCRUM Szukamy Ciebie, jeśli: 👉posiadasz doświadczenie w projektowaniu rozwiązań z zakresuHurtowni Danych i Business Intelligence 👉dobrze znasz narzędzia / środowiska SAS i SAS Viya (DI Studio, Enterprise Guide, SAS Studio, Visual Analytics) 👉masz doświadczenie w pracy z Oracle (mile widziane doświadczenie z Oracle Exadata i PL/SQL) 👉znasz zagadnienia i technologie CI/CD (Git / Bitbucket / Bamboo / JIRA) Zadania: budowa narzędzi wspomagających pracę developerów Hurtowni Danych - m. in. wspólne re-używalne komponenty (SAS / SAS Viya / Oracle / inne technologie) budowa API łączącego wykorzystywane w HD technologie (SAS / SAS Viya/Oracle / Office365 / REST API / inne technologie) budowa narzędzi wspomagających wymianę danych pomiędzy systemami informatycznymi m. in. Kafka POC nowych narzędzi/technologii do potencjalnego wdrożenia w HD m. in. SAS Viya & Exadata w chmurze Azure budowa narzędzi monitorujących aktywność użytkowników HD udział w pracach optymalizujących przetwarzania HD, poprzez budowę wspierających komponentów a także punktowe analizy i optymalizacje przetwarzań budowa technicznych data mart-ów oraz raportów BI (m. in. w SAS Viya lub Power BI) wsparcie projektów strategicznych w aspektach technologicznych Czego możesz się spodziewać? Współpracy w oparciu o kontrakt B2B Onboarding w biurze w Warszawie (dobrze skomunikowana lokalizacja) Pracy hybrydowej 1 raz w tygodniu z biura w Warszawie Prawdziwego work-life balance - w tej kwestii nie wierzymy w półśrodki 👌 Płaskiej struktury i niekorporacyjnej atmosfery 🏃‍♀️Benefitów: opieka medyczna, karta multisport, lekcje języka angielskiego 🙌 Elastycznych godzin pracy 🎉Integracji - lubimy spędzać razem czas Profesjonalnego i procesu rekrutacyjnego – zawsze otrzymasz od nas feedback niezależnie od decyzji. Poznaj nas lepiej👉https: //www.facebook.com/clurgo/","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,559,Senior Data Engineer,Jit Team,"Salary: 1100–1440 PLN net/day ( B2B ) Work mode: Hybrid – 1 day/week in Warsaw or Gdynia office Why choose this offer? You’ll join a long-term , high-impact project focused on modern data processing and software asset management You’ll work with an international team in a cloud-native environment ( GCP , Cloud Run , Pub/Sub , FastAPI ) Opportunity to develop scalable backend systems using Apache Beam and modern serverless patterns Full autonomy in technical decision-making and architecture evolution Stable , long-term collaboration with a global HR industry leader Project You’ll join the Global SAM (Software Asset Management) team responsible for developing a cloud-native , data-driven platform used to analyze and manage enterprise software usage. The system is built on Google Cloud Platform , processes data in near real-time , and integrates with multiple systems using an event-driven architecture . You’ll be responsible for designing and implementing data pipelines , backend APIs, and scalable serverless functions. This is a hands-on engineering role with lots of space for ownership and technical influence . Expected competences and knowledge Solid Python backend development experience, ideally with FastAPI Experience building data pipelines using Apache Beam or equivalent frameworks Familiarity with GCP services, especially Dataflow, Pub/Sub, Cloud Run Understanding of event-driven architecture and observability best practices Experience with CI/CD tools, preferably GitLab Ability to work independently in a distributed team Optional but welcome: knowledge of Google Cloud Firestore Technologies you'll work with Python backend development (FastAPI, data pipelines) Experience with Apache Beam or streaming frameworks Strong knowledge of GCP services (Dataflow, Pub/Sub, Cloud Run) Understanding of event-driven architecture and observability Familiarity with CI/CD workflows (GitLab preferred) Ability to work in a distributed team environment Optional: experience with Firestore or other GCP data stores Client – why choose this particular client from the Jit portfolio? We have partnered with our Client, a leading HR company with over 60 years of experience and a presence in multiple countries around the world. Our Client provides a wide range of recruitment and job placement services and is dedicated to helping individuals achieve their career goals. In Poland, our Client has been operating for over 20 years and is at the forefront of digitization in the HR services industry. Our team has been building the Work Time system for our Client since 2019 , a complex tool for registering and settling the working time of full-time and temporary employees working under different legal and organizational conditions, supporting on-boarding processes, and integrating with the employee portal and mobile app. As a member of Jit Team, you will have the opportunity to work on a modern, complex enterprise-class system built in modern technologies such as Java and the AWS cloud . With a business-critical operation, the Work Time system settles working time for tens of thousands of employees every month and contributes to our Client's revenue. About Jit Team The Human factor of IT - it's not just a slogan, it's a philosophy. The foundation of Jit Team is people, which is why we prioritise you. We employ over 500 experienced experts . We create highly specialised teams for clients from all over the world. We offer team members developmental projects, a wide range of benefits and a proprietary professional development programme. Behind our maxim are also charitable and educational activities. We support pupils and students by donating learning equipment. We offer internships to help launch careers in IT. We support water rescuers and hospitals by providing the necessary equipment. We are a Polish company and we share what we have achieved over 15 years of activity . By supporting indigenous initiatives, we ensure the circulation of good energy.","[{""min"": 22000, ""max"": 28800, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,28800,Net per month - B2B
Full-time,Senior,B2B,Hybrid,560,"Senior Data Engineer – Python, Spark, AWS",Link Group,"We’re looking for aSenior Data Engineerto join a highly skilled team responsible for building a scalable, multitenant data platform that powers data ingestion, transformation, cataloging, and distribution for internal and external stakeholders. This role is central to designing robust data pipelines, integrating modern data lakehouse components, and supporting the company’s AWS-based infrastructure. Design and build scalabledata ingestion pipelinesin AWS Work extensively withDatabricks, including Unity Catalog, streaming, Delta Lake, and data transformation tools Lead technical delivery and implementation of core platform components acrossraw, normalized, operational, and historical layers Collaborate with engineering managers and cross-functional teams across the US and Poland Drive technical excellence by improving team processes, architecture standards, and engineering best practices Support and consult with stakeholders to ensure successful delivery of data-driven solutions Contribute to the growth and maturity of the team’s cloud and data engineering capabilities 5+ years of experience in Pythonand strong software engineering skills Solid knowledge ofAWS cloud servicesand best practices Experience building scalableSparkpipelines inPySparkorScala Practical experience withSpark Streamingfor low-latency pipelines Familiarity withDelta Lakeand modern data lakehouse architectures Hands-on experience withKubernetesand container orchestration Experience withMongoDBand other NoSQL data stores Understanding ofmicroservicesandevent-driven architecture Excellent communication skills and ability to work in a collaborative, global team Commitment to high standards ofethics, quality, and delivery Familiarity withMachine Learning,AI models, orData Science workflows","[{""min"": 35000, ""max"": 40000, ""type"": ""Net per month - B2B""}]",Data Engineering,35000,40000,Net per month - B2B
Full-time,Senior,B2B,Remote,561,Data Architect,Connectis,"Wraz z naszym partnerem z branży ubezpieczeniowej, poszukujemy eksperta/ekspertkina stanowiskoData Architect, który dołączy do strategicznego projektu budowyDataHubu– rozwiązania klasy mini hurtowni danych, wspierającego działalność operacyjną, raportową, analityczną oraz rozwiązania z zakresu generatywnej sztucznej inteligencji. Projektowanie i nadzór nad wdrażaniem skalowalnych, wydajnych modeli danych integrujących wiele źródeł. Tworzenie dokumentacji architektonicznej i mentoring mniej doświadczonych inżynierów. Zapewnienie jakości danych: automatyczna walidacja, wykrywanie anomalii, monitoring. Definiowanie standardów, wzorców i najlepszych praktyk w zakresie inżynierii danych. Projektowanie rozwiązań do przetwarzania danych strumieniowego i wsadowego. Ustalanie polityk dostępu, zasad zarządzania danymi i ram bezpieczeństwa. Bliska współpraca z zespołami DevOps, Data Engineering oraz AI/ML. 🔍CZEGO OCZEKUJEMY OD CIEBIE? Doświadczenie zAzure DatabricksorazAzure DevOps. Bardzo dobra znajomośćMicrosoft Azure. ZnajomośćApache Spark/PySpark. Biegłość wPythoniSQL. Mile widziane: Doświadczenie zDBT (Data Build Tool). Praktyczna znajomość narzędziCI/CD Znajomość modeluData Vault 2.0. ✨OFERUJEMY: Tryb pracy zdalnej z okazjonalnymi spotkaniami zespołu w biurze w centrum Warszawy(raz w miesiącu). Udział w strategicznym projekcie realizowanym dla międzynarodowego Partnera Biznesowego. Realny wpływ na architekturę i rozwój rozwiązania wdrażanego w wielu krajach. Dedykowane wsparcie opiekuna Connectis przez cały czas trwania współpracy. Współpracę z doświadczonym zespołem inżynierów danych i DevOpsów. Możliwość przedłużenia współpracy w kolejnych fazach programu. 5000 PLN za polecenie znajomego do naszych projektów. Szybki, zdalny proces rekrutacyjny. Dziękujemy za wszystkie zgłoszenia. Pragniemy poinformować, że skontaktujemy się z wybranymi osobami. 12170/DK","[{""min"": 150, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Architecture,150,200,Net per hour - B2B
Full-time,Senior,B2B,Remote,563,Senior Data Engineer,7N,"O projekcie: Poszukujemy doświadczonych Senior Data Engineer'ów, którzy dołączą do nowo tworzonego w Polsce zespołu specjalizującego się w migracji danych do chmury. Projekt realizowany jest dla międzynarodowej duńskiej organizacji z sektora finansowego. Kluczowym zadaniem będzie zapewnienie płynnej transformacji istniejących rozwiązań oraz stworzenie trwałych fundamentów pod rozwój nowoczesnej platformy danych opartej na architekturze Lakehouse. Praca w 100% zdalna, wymagana otwartość na podróż do Danii na początku projektu. Zakres obowiązków Migracja istniejących rozwiązań danych do środowiska chmurowego Projektowanie oraz implementacja procesów ETL Modelowanie danych oraz tworzenie transformacji danych w oparciu o wymagania użytkowników końcowych Implementacja kontraktów danych i domen danych (np. Umowy, Polisy, Klienci, Roszczenia) Współpraca z interesariuszami w celu zrozumienia wymagań biznesowych Przeprowadzanie testów jakości danych, preferencyjnie automatycznych, i tworzenie mechanizmów kontrolnych zapewniających spójność danych Tworzenie i utrzymywanie dokumentacji technicznej i biznesowej Udział w planowaniu prac z wykorzystaniem Azure DevOps Udział w analizach potrzeb biznesowych i współpraca z lokalnymi przedstawicielami klienta w celu określenia wymagań funkcjonalnych i niefunkcjonalnych. Dbanie o jakość i spójność kodu źródłowego, w tym przestrzeganie dobrych praktyk programistycznych i standardów kodowania przy pracy z SQL i Pythonem. Udział w rozwoju i utrzymaniu fundamentów danych wspierających produkty biznesowe Współpraca z zespołami międzynarodowymi Oczekiwania Minimum 5 lat doświadczenia komercyjnego w roli Data Engineera Doświadczenie w migracji danych i projektowaniu rozwiązań chmurowych Bardzo dobra znajomość języków SQL i Python, w tym umiejętność tworzenia skalowalnych i czytelnych skryptów oraz testów automatycznych Praktyczna znajomość narzędzi z ekosystemu Azure, w szczególności: - Azure Data Factory (ADF) -Databricks - Azure DevOps (zarządzanie backlogiem i planowaniem pracy) - (mile widziana znajomość: Microsoft Purview w zakresie rozumienia, niekoniecznie rozwoju) Doświadczenie w budowaniu procesów ETL oraz tworzeniu i implementacji data pipelines Zrozumienie zasad tworzenia data contracts i domen danych Wiedza z zakresu testowania jakości danych w procesach przetwarzania danych (ETL/ELT), np. Great Expectations – mile widziane Znajomość zagadnień związanych z migracją danych i integracją danych legacy Umiejętność pracy w międzynarodowym zespole oraz komunikacji z interesariuszami - zdolność zrozumienia ich potrzeb i przekształcania ich w wymagania techniczne Mile widziane certyfikaty: Databricks Certified Data Engineer Associate Microsoft Azure Data Fundamentals (DP-900) Mile widziana wiedza domenowa z zakresu ubezpieczeń (w szczególności umów, osób, polis, roszczeń itp.) Bardzo dobra komunikacja w języku angielskim Oferujemy Stałe wsparcie osobistego agenta , dbającego o Twoją ciągłość projektową, kontakt z klientem, niezbędne formalności, komfort pracy oraz rozwój, Consultant Development Program – doradztwo w planowaniu rozwoju w oparciu o najnowsze trendy i potrzeby rynku IT, obejmujące m.in. konsultacje z agentami i mentorami rozwoju , Dostęp do 7N Learning & Development – platformy rozwojowo-edukacyjnej z webinarami, biblioteką artykułów i raportów branżowych oraz regularnymi zaproszeniami na jednorazowe i cykliczne wydarzenia rozwojowe – techniczne, biznesowe oraz life-stylowe, Spektakularne eventy integracyjne, zarówno dla Ciebie (np. coroczny wyjazd Kick-Off , imprezy świąteczne czy sportowe Letnie Igrzyska), jak i dla Twoich bliskich (np. pikniki rodzinne), Rozwój zawodowy nie tylko podczas projektu – możesz zaangażować się w przekazywanie wiedzy innym w ramach oferty 7N Services kierowanej do klientów 7N, Relacje i dostęp do wiedzy najbardziej doświadczonych ekspertów IT na rynku – średni staż zawodowy naszego Konsultanta w Polsce to ponad 10 lat, Pakiet benefitów zaplanowany od A do Z, czyli dofinansowanie do opieki medycznej, ubezpieczenia na życie, karty sportowej dla Ciebie i Twoich bliskich, a także zniżki do sklepów w Polsce i za granicą. O 7N Ciągłe poszukiwanie projektów, trudne negocjacje stawek, brak wsparcia w rozwoju – brzmi znajomo? W 7N zyskujesz nie tylko stabilność kontraktów, ale też zaangażowanie osobistego opiekuna dbającego o Twój komfort zawodowy i stały dostęp do inicjatyw rozwojowych. Naszym celem jest zapewnienie Ci stabilnej i komfortowej współpracy, która przyczyni się do sukcesu Twojego jako eksperta IT oraz naszych klientów. Budujemy trwałe relacje, bazując na skandynawskich wartościach i 30-letnim doświadczeniu w tworzeniu rozwiązań IT dla ponad 200 organizacji.","[{""min"": 160, ""max"": 185, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,185,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,564,BI Developer / SQL Developer,Beesafe,"About Us: Join our trailblazing team as we expand our digital horizons. From our beginnings as visionary InsurTech to becoming a key player in the Polish digital insurance market, we are part of the esteemed Vienna Insurance Group. We're redefining the rules in the insurance industry with our innovative approach. Our hybrid working model supports both the collaborative energy of office work and the flexibility of remote work. About the Role: We're seeking passionate BI/SQL Developer to join our team. You'll be at the heart of developing and implementing high-quality application software, using state-of-the-art tools and technologies. This is your chance to make a significant impact on one of the most exciting and unique products in the Polish digital insurance market. Why it is worth to work with us? You’ll be contributing to the reporting solution and data model design of our Data Platform with close cooperation with our Data Engineers You’ll gather business requirements and work closely with business stakeholders You’ll deliver end-to-end business intelligence/reporting solutions using SQL/PowerBI What you need to start the adventure with us: +1 year of commercial experience in data extraction, ETL, and report development Proficiency in SQL Understanding of Relational Database Management System and Business Intelligence concepts Business and collaboration skills, and responsive to service needs and operational demands Domain knowledge gained across the insurance or financial sector Nice to have: Experience with BI tools (Power BI would be a plus) Experience with cloud solutions (we use Azure) Familiarity with code version control systems such as GIT Understanding of the principles of Agile and Scrum (we work in Scrum) Enthusiastic approach to coffee breaks (we love informal discussions with a cup of favorite coffee or tea) Why Join Us? Be part of a dynamic team driving digital innovation in the insurance and eCommerce sectors Opportunity to work in a collaborative and forward-thinking environment Contract options: B2B cooperation Engage in meaningful work that directly impacts business success Join a company that values work-life balance and fosters a positive team culture Comprehensive onboarding, including a dedicated Buddy program Remote work flexibility with hybrid office visits Flexible working hours Access to the latest tools and cloud-native solutions A comprehensive benefits package, including health insurance and MultiSport card Employee discounts on insurance products Referral program and sports club memberships Sounds interesting? Join us and help shape the future! 🚀","[{""min"": 9000, ""max"": 12000, ""type"": ""Net per month - B2B""}, {""min"": 9000, ""max"": 12000, ""type"": ""Gross per month - Permanent""}]",Database Administration,9000,12000,Net per month - B2B
Full-time,Senior,B2B,Remote,566,Data Engineer/Data Modeler,Altimetrik Poland,"5 days per week you need to be available until 10: 00pm due to meetings with the US team Altimetrik Polandis a digital enablement company. We deliver bite-size outcomes to enterprises and startups from all industries in an agile way to help them scale and accelerate their businesses. We are unique in Poland's IT market. Our differentiators are an innovation-first approach, a strong focus on core development, and an ability to attack the challenging and complex problems of the biggest companies in the world. We are looking forData Engineer/ Data Modelerfor our client which is an online marketplace connecting people who want to rent out their homes with people looking for accommodations in specific locales. As a Data Engineer/Data Modeler you will be responsible for designing and implementing various scalable data solutions for our customers providing hospitality and travel services. This role requires a strong analytical mindset, technical expertise, and the ability to collaborate across teams to ensure data solutions align with business needs. Responsibilities: Build and maintain scalable data models and metrics to support variety of UC such as: fraud detection, safety incident tracking, and ID verification processes. Collaborate with Product Managers (PMs) to translate product requirements into comprehensive logging needs. Scope and implement metrics for experimentation, KPI tracking, and operational insights. Migrate existing data assets (pipelines, tables, metrics) to modern technology stacks. Identify, deprecate, and replace legacy data assets with optimized, scalable solutions. And if you possess... Advanced SQL skills for querying and data transformation. Proficiency in Python for data analysis, scripting, and automation. Hands-on experience with AWS (e.g., S3, Redshift, Athena) for cloud-based data management. Experience with Airflow for orchestration and Hive for large-scale data processing. Strong experience in data modelling, metric design, and data pipeline development. Proven ability to work cross-functionally with Product Managers, Engineers, and Analysts. Exceptional problem-solving skills and a data-driven decision-making approach. Experience in fraud detection, safety incident monitoring, or related domains. We work 100% remotely or from our hub inKraków. 🔥We grow fast. 🤓We learn a lot. 🤹We prefer to do things instead of just talking about them. If you would like to work in an environment that values trust and empowerment... don't hesitate, just apply!","[{""min"": 21000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Science,21000,28000,Net per month - B2B
Full-time,Mid,B2B,Remote,568,Data Engineer/Analyst - Remote!,ITDS,"Join us, and develop your data competencies Opportunity with the possibility to work 100% remotely! As a Data Engineer/Analyst, you will be working for our client, a dynamic and rapidly expanding organization in the home and lifestyle sector. The client is embarking on an exciting project to build and optimize their data infrastructure to gain deeper insights into customer behavior and drive business growth. You will play a pivotal role in designing and implementing robust ETL/ELT pipelines, leveraging cutting-edge technologies to transform raw data into actionable intelligence. Your expertise will be crucial in shaping the future of their data-driven decision-making processes and contributing to the overall success of the company. Your main responsibilities: Design and implement ETL/ELT pipelines Work with Apache Spark, understanding cluster operation and resource management Configure the number of executors, memory, and partitions in Spark Utilize strong SQL skills for data manipulation and querying Apply understanding of data warehouse concepts and data modeling basics Build and monitor data flows using a data orchestration tool Employ practical knowledge of Python to write functions and work with data structures Create basic ETL processes involving reading, transforming, and writing data Work with a cloud platform for data storage and processing Apply basic knowledge of Git and understanding of CI/CD concepts You're ideal for this role if you have: 2 years of experience in designing and implementing ETL/ELT pipelines Knowledge of Apache Spark, including cluster operation Understanding of Spark resource management Good knowledge of SQL Understanding of data warehouse concepts and data modeling basics Knowledge of any data orchestration tool Practical knowledge of the Python language Experience working with a cloud platform Basic knowledge of Git Practical knowledge of Power BI","[{""min"": 15000, ""max"": 16800, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,15000,16800,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,569,Azure Data Engineer z kompetencjami Devopsowymi,UNIVIO,"Jesteśmy polską firmą technologiczną z ponad 25-letnim doświadczeniem jako partner cyfrowej transformacji handlu. Realizujemy międzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocześnie luźną, niezobowiązującą atmosferę. Nasza organizacja opiera się na kulturze otwartości i dzielenia się wiedzą. Dowiedz się kogo szukamy i zaaplikuj, jeśli spełniamy Twoje oczekiwania 😉","[{""min"": 16800, ""max"": 23520, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 17300, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16800,23520,Net per month - B2B
Full-time,Mid,B2B,Hybrid,570,Cloud Data Engineer,Optiveum,"Data Engineer – Cloud ETL & Platform Development 📍 Hybrid in Warsaw (3 days/week onsite required) | 💼 Full-time | B2B contract up to $7,000/month Our client is a US-based technology company headquartered in New York City, delivering digital solutions and consulting services that transform businesses and drive measurable value. With offices in multiple countries, the company is now investing in a new engineering centre in Warsaw, recognizing the strong talent and culture of Polish software professionals. We are looking for a highly motivated and self-drivenData Engineerto join a fast-growing data platform team. In this role, you will design, build, and maintain robust, scalable, and cloud-native ETL infrastructure and data pipelines, enabling real-time analytics and AI-ready architecture for high-performance business applications. This is a hybrid position — you will be expected to work from the Warsaw office at least 3 days per week. Design, develop, and deploy Python-based ETL pipelines using Airflow and Prefect Build and optimize data warehouse structures for analytics, OLAP, and dimensional modeling Model AI-compatible DB schemas and ontologies for future-facing analytics Migrate and transform structured, semi-structured, and unstructured data using DBT and Pandas Work with streaming/event-driven architectures for real-time data processing Optimize pipeline performance and scalability for large data volumes Ensure data quality, validation, cleansing, and graceful error handling Perform code reviews to ensure standards, scalability, and best practices Collaborate with DevOps on CI/CD and automated release pipelines Implement data governance, security, and observability across ETL processes 5+ years of experience designing and building enterprise-scale ETL pipelines Strong Python skills, including use of Pandas for data processing Proficient in SQL — writing complex queries and procedures across RDBMSes Experience with Airflow, Prefect, or similar workflow orchestration tools Solid understanding of data warehousing (OLTP, OLAP, Facts & Dimensions) Familiarity with cloud-based data platforms such as RDS, Redshift, or Snowflake Knowledge of cloud data architectures, messaging systems, and scalable design Hands-on experience with code versioning, deployment automation, and CI/CD Preferred Qualifications Experience with Docker, Kubernetes, AWS Lambdas, Step Functions Exposure to Databricks, PySpark, and advanced distributed data processing Cloud certifications are a plus What’s Offered B2B contract with monthly compensation up to $7,000 Strong career growth opportunities in a global fintech environment High-impact projects in a fast-growing sector Friendly, open, and ambitious team culture Hybrid model – minimum 3 days/week in the Warsaw office","[{""min"": 16623, ""max"": 25859, ""type"": ""Net per month - B2B""}]",Data Engineering,16623,25859,Net per month - B2B
Full-time,Senior,B2B,Remote,571,Data Modeller,ITDS,"Join Us and Build a Cutting-Edge Data Platform! As aData Modeller,you will be working for our client in the debt collection sector, helping to build and maintain a robust, cloud-native data platform. The role focuses heavily on data modelling, requiring an expert who can translate conceptual business needs into logical and physical data models, build data contracts, and implement scalable ELT pipelines using Azure Databricks. Your main responsibilities: Design and maintain logical and physical data models based on DDD (Domain-Driven Design) principles Translate conceptual models and business glossaries into technical data structures for the Data Warehouse Perform data mapping and create data contracts between the Data Platform and source systems Collaborate with source system owners to define data contract requirements Work on data ingestion processes from source systems using various methods: Direct database queries (bulk read/CDC), API communication, Event streaming Implement ELT processes across Bronze, Silver, and Gold layers in Azure Databricks Ensure alignment of data models with business and analytical requirements You're ideal for this role if you have: Strong experience in Data Modelling (logical & physical), preferably in DDD-based environments Proven ability to work with Data Governance inputs: glossaries, conceptual models, HLD/LLD documentation Experience preparing and maintaining data contracts Solid knowledge of data ingestion techniques and working with source systems Experience with Azure Databricks (or similar cloud platforms like GCP) Ability to develop and maintain ELT pipelines in cloud-native environments Nice to have: Experience in writing clear technical documentation (e.g. data contracts, field definitions, extraction rules) Background in mapping source data to target DWH structures Ability to interpret and work with ERDs and relational models Knowledge of master data management practices Familiarity withdbdiagram.io Awareness of Data Quality, Data Lineage, and metadata management concepts Experience using tools like Azure Purview or other metadata management platforms","[{""min"": 1200, ""max"": 1500, ""type"": ""Net per day - B2B""}]",Data Science,1200,1500,Net per day - B2B
Full-time,Mid,B2B,Remote,574,Big Data Engineer,EndySoft,"Position Overview: We are seeking a skilled Big Data Engineer to join our data engineering team. The ideal candidate will have extensive experience in building and managing large-scale data processing systems. This role involves designing, implementing, and optimizing data pipelines and infrastructure to support analytics, machine learning, and business intelligence efforts. MD rate: 16600 – 20000 PLN Roles and Responsibilities: Design, develop, and maintain big data pipelines to process and analyze large datasets. Implement data ingestion , processing , and storage solutions using big data frameworks such as Apache Spark , Hadoop , and Kafka . Optimize data pipelines for performance , scalability , and fault tolerance . Collaborate with data scientists, analysts, and other stakeholders to ensure data availability and usability. Develop and maintain data storage solutions such as HDFS , Amazon S3 , Google Cloud Storage , or Azure Data Lake . Ensure data quality and integrity through automated testing and validation processes. Monitor and troubleshoot big data infrastructure to ensure optimal performance and reliability. Document technical solutions, workflows, and best practices. Required Skills and Experience: Proficiency in big data technologies such as Apache Spark , Hadoop , Kafka , or Flink . Strong programming skills in languages like Python , Scala , or Java . Experience with SQL and NoSQL databases such as PostgreSQL , MongoDB , or Cassandra . Familiarity with cloud platforms such as AWS , Azure , or Google Cloud , including their big data services (e.g., EMR , BigQuery , Databricks ). Knowledge of data modeling , ETL processes , and data pipeline orchestration tools like Apache Airflow , Luigi , or Dagster . Strong understanding of distributed computing principles and parallel processing . Experience with containerization tools such as Docker and orchestration tools like Kubernetes . Strong problem-solving skills and ability to troubleshoot large-scale data systems. Nice to Have: Experience with real-time data processing and streaming platforms such as Apache Kafka Streams , Kinesis , or Pulsar . Familiarity with machine learning pipelines and integration with big data systems. Knowledge of data governance , security , and compliance in big data environments. Experience with CI/CD tools for automating data pipeline deployment and management. Exposure to Agile/Scrum methodologies. Understanding of data visualization tools such as Power BI , Tableau , or Looker . Additional Information: This role offers an opportunity to work on complex, large-scale data projects and help shape the future of data-driven decision-making. If you are passionate about big data technologies and thrive in a fast-paced, innovative environment, we encourage you to apply.","[{""min"": 16600, ""max"": 20000, ""type"": ""Net per month - B2B""}]",Data Engineering,16600,20000,Net per month - B2B
Full-time,Senior,B2B,Remote,577,Starszy Programista Baz Danych,P&P Solutions,"📍Lokalizacja: Hybrydowo / Zdalnie 🕒Wymiar pracy: Pełny etat, B2B📑Widełki: 125-150 zł/h netto na b2b PoszukujemyStarszego Programisty Baz Danychdo realizacji strategicznego projektu dlainstytucji publicznej w obszarze ochrony zdrowia. Projekt obejmuje działania związane z utrzymaniem, rozwojem i migracją danych w środowisku relacyjnych baz danych – ze szczególnym uwzględnieniem PostgreSQL. Dołączysz do zespołu ekspertów odpowiedzialnych za projektowanie, optymalizację i transformację danych w ramach kluczowych systemów informatycznych. To doskonała okazja do zaangażowania się w projekt o wysokim znaczeniu społecznym i dużej skali technologicznej. Minimum5 lat doświadczeniazawodowego w pracy z relacyjnymi bazami danych (np. PostgreSQL, MySQL, MSSQL, Oracle). Minimum4 lata doświadczeniaw programowaniu w proceduralnym języku bazodanowym (np. PL/SQL, PL/PgSQL). Bardzo dobra znajomość językaSQL. Doświadczenie z technologiąPostgreSQL– mile widziane pełne zrozumienie architektury i konfiguracji tej bazy. Praktyka wstrojenie zapytań SQL oraz procedur PL/PgSQL. Wiedza z zakresuzarządzania, konfiguracji i optymalizacji PostgreSQL. Doświadczenie wmigracji danych z systemów klasy enterprise. Znajomość procesów analizy i transformacji danych na potrzeby migracji. Udział w projektach realizowanych w obszarze ochrony zdrowia – zwłaszcza rejestry i część „biała” Projektowanie, implementacja i optymalizacja procedur oraz zapytań w relacyjnych bazach danych (PostgreSQL, PL/SQL, itp.); Udział w projektach migracji danych z systemów klasy enterprise – analiza, transformacja i integracja danych; Strojenie zapytań SQL oraz procedur w celu poprawy wydajności systemów bazodanowych; Współpraca z zespołami deweloperskimi oraz analitykami w zakresie integracji danych i wymagań systemowych; Utrzymanie, rozwój i konfiguracja istniejących struktur baz danych; Projektowanie i implementacja mechanizmów przetwarzania danych zgodnie z wymaganiami klienta; Tworzenie i utrzymywanie dokumentacji technicznej; Zapewnienie zgodności z wewnętrznymi standardami jakości i bezpieczeństwa danych (np. WCAG, RODO – jeśli dotyczy); Współpraca z zespołami odpowiedzialnymi za architekturę danych i DevOps w zakresie ciągłości działania oraz wdrożeń. Widełki do 150 zł/h netto na b2b. Możliwość pracy w 100% zdalnie. Możliwość udziału wprojekcie o znaczeniu publicznymi realnym wpływie na funkcjonowanie sektora ochrony zdrowia. Współpracę z doświadczonym zespołem ekspertów IT. Długoterminową współpracę i stabilność projektu (projekt publiczny).","[{""min"": 125, ""max"": 150, ""type"": ""Net per hour - B2B""}]",Database Administration,125,150,Net per hour - B2B
Full-time,Senior,B2B,Remote,579,Data Scientist,Britenet,"Projekt realizowany dla firmy z branży e-commerce. Oczekiwania: Minimum 3–5 lat doświadczenia na stanowisku Data Scientist Doświadczenie we wdrażaniu modeli uczenia maszynowego w środowisku produkcyjnym Praktyczne doświadczenie w pracy z dużymi zbiorami danych i modelami na dużą skalę (BigQuery) Doświadczenie w łączeniu danych z wielu źródeł oraz pracy z różnymi typami danych: tabelarycznymi, tekstowymi (NLP), obrazami i szeregami czasowymi Bardzo dobra znajomość języków Python i SQL Znajomość Google Cloud Platform (GCP) Znajomość narzędzi Apache Airflow i Apache Spark Doświadczenie w tworzeniu raportów i dashboardów w Looker Studio Dobra znajomość metod statystycznych oraz algorytmów uczenia maszynowego Praktyczne doświadczenie z algorytmami opartymi na drzewach decyzyjnych (np. XGBoost, LightGBM, CatBoost) Umiejętność przekładania wyzwań biznesowych na problemy z zakresu uczenia maszynowego Zdolność do samodzielnego definiowania, testowania i optymalizacji modeli ML Otwartość na proponowanie własnych rozwiązań i podejść w analizie danych Bardzo dobre umiejętności komunikacyjne – umiejętność wyjaśniania złożonych zagadnień technicznych osobom nietechnicznym Umiejętność pracy zespołowej, dzielenia się wiedzą i współpracy z innymi Zadania: Projektowanie, rozwijanie i wdrażanie modeli uczenia maszynowego w środowiskach produkcyjnych Analiza dużych i zróżnicowanych zbiorów danych (tabelaryczne, tekstowe, obrazy, szeregi czasowe) Integracja danych z wielu wewnętrznych i zewnętrznych źródeł w celu budowy solidnych pipeline’ów i cech dla modeli ML Przeprowadzanie analiz danych na dużą skalę z wykorzystaniem narzędzi takich jak BigQuery, z uwzględnieniem skalowalności i wydajności Wykorzystywanie zaawansowanych metod statystycznych i algorytmów uczenia maszynowego Ciągłe usprawnianie i optymalizacja istniejących modeli oraz rozwiązań opartych na danych poprzez testowanie i eksperymenty Współpraca z interesariuszami biznesowymi, product managerami i zespołami UX w celu definiowania przypadków użycia ML i dostosowania rozwiązań technicznych do celów biznesowych Przekładanie złożonych danych i wyników modelowania na klarowne wnioski i rekomendacje dla odbiorców nietechnicznych Tworzenie i utrzymywanie dashboardów oraz raportów w Looker Studio w celu monitorowania wydajności modeli i jakości danych Proponowanie nowych podejść, narzędzi i metodologii w celu rozwijania kompetencji data science w zespole","[{""min"": 90, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Science,90,160,Net per hour - B2B
Full-time,Senior,B2B,Remote,580,Senior Data Engineer,j-labs,"Dołącz do zespołu, który buduje platformę do zarządzania majątkiem opartym na ETF-ach – skalowalne, nowoczesne narzędzie mające realny wpływ na sposób inwestowania. Team jest odpowiedzialny za przetwarzanie dużej ilości danych potrzebnych do generowania różnego rodzaju raportów.​ Jedną z domen, którą zajmuje się zespół jest zarządzaniem ryzykiem.​ Szukamy osoby samodzielnej i zaangażowanej, która będzie: Rozwijać skalowalną infrastrukturę danych w chmurze, która stanowi podstawę działalności firmy opartej na danych, z wykorzystaniem najnowszych technologii. Tworzyć rozwiązania do przetwarzania danych opartych na AWS, integrujących dane z usług wewnętrznych i zewnętrznych. Budować jezioro danych finansowych, łączącej nowoczesne technologie z funkcjami wymaganymi przez przepisy regulacyjne. Ściśle współpracować z data scientistami, zespołami produktowymi i deweloperskimi przy wdrażaniu inteligentnych funkcji do naszego produktu. Dzielić się wiedzą ekspercką na temat najlepszych praktyk w zakresie danych wewnątrz firmy. Wymagania: Doświadczenie w projektowaniu i obsłudze potoków danych w środowisku AWS (min. 5 lat). Znajomość SQL . Doświadczenie w Pythonie, w tym znajomość frameworków takich jak DBT. Doświadczenie w pracy z usługami AWS , takimi jak S3, Athena i Glue. Znajomość narzędzi Infrastructure-as-Code, takich jak Terraform . Pasja do podejścia ""everything-as-code"" i pisanie dobrze zaprojektowanego, testowalnego i udokumentowanego kodu. Doświadczenie w pracy w metodykach zwinnych, np. Scrum. Zainteresowanie usługami finansowymi i rynkami. Biegłość w języku angielskim w mowie i piśmie- min. B2. Możliwa praca 100% zdalna, ale ze względu na potrzebę okazjonalnych spotkań zespołu w biurze, w pierwszej kolejności bierzemy pod uwagę osoby mieszkające w Warszawie lub Krakowie, gdzie mamy biura.","[{""min"": 190, ""max"": 210, ""type"": ""Net per hour - B2B""}]",Data Engineering,190,210,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Remote,581,Machine Learning Engineer (LLM),UNIVIO,"Jesteśmy polską firmą technologiczną z ponad 25-letnim doświadczeniem jako partner cyfrowej transformacji handlu. Realizujemy międzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocześnie luźną, niezobowiązującą atmosferę. Nasza organizacja opiera się na kulturze otwartości i dzielenia się wiedzą. Dowiedz się kogo szukamy i zaaplikuj, jeśli spełniamy Twoje oczekiwania 😉","[{""min"": 16800, ""max"": 20160, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 16500, ""type"": ""Gross per month - Permanent""}]",Data Science,16800,20160,Net per month - B2B
Full-time,Senior,B2B,Remote,583,Senior Data Engineer,Experis Manpower Group,"Join a dynamic and collaborative team working in a fast-paced Scrum environment, where agility and teamwork are key to delivering impactful solutions. Each day begins with a stand-up meeting alongside data & analytics engineers, the Scrum Master, and the Product Owner — aligning on sprint goals, resolving blockers, and sharing progress. Your Responsibilities: Design and enhance scalable data pipelines and infrastructure. Monitor system performance and optimize for cost-efficiency. Model data using DBT based on existing pipelines. Collaborate with functional analysts to refine business requirements. Participate in sprint ceremonies including refinements and retrospectives. Stay connected with your team and stakeholders through Jira and Slack. What We’re Looking For: Strong command ofSQL. Hands-on experience withDBTfor data transformation and pipeline automation. Enthusiasm forAWStechnologies. AWS Certified Associate accreditation. Proficiency inone or more of the following: Scala, Python, Go, Java, Shell scripting — or deep database expertise with a willingness to learn Python and Scala. Practical DevOps experience(CI/CD, system setup, monitoring). Excellent communication and analytical skills. Team-oriented approach and interest in pair programming. Nice to Have: Experience with Terraform or other Infrastructure as Code tools. Understanding of large-scale distributed systems. Familiarity with Domain-Driven Design. Knowledge of monitoring, logging, and security automation. What We Offer: B2B contract via Experis. Sports card. Life insurance. Private medical care. Fully remote work.","[{""min"": 160, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,190,Net per hour - B2B
Full-time,Senior,B2B,Remote,584,Engineering Manager (AI & Big Data),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition by Forbes as one of the top 10 AI consulting companies. As an Engineering Manager, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: AI optimization engine for airport operation management. This project involves creating an AI-driven optimization engine to streamline airport operations, using real-time data from sources like radar, air traffic, and passenger information. GenAI knowledge retrieval platform for the leading automotive company. This project involves the creation of an Azure-backed platform that leverages cutting-edge LLMs to deliver powerful insights from an enterprise-scale knowledge base. AI recommendation engine for the leading innovation company. This project involves the design of AWS-powered solutions to provide domain-specific recommendations by employing sophisticated LLMs. This role is ideal for a leader who combines technical expertise with strong leadership skills , ready to drive innovative projects in data science and big data. Discover our perks and benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage with top-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you to work remotely or from modern offices and coworking spaces. Accelerate your professional growth through career paths , knowledge-sharing initiatives, language classes, and sponsored training or conferences , including a partnership with Databricks , which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days off available for B2B contractors and individuals under contracts of mandate. Participate in team-building events and utilize the integration budget . Celebrate work anniversaries, birthdays, and milestones. Access medical and sports packages , eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you can boost your personal brand by speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. In this position, you will: Be responsible for project management and technical leadership for data science and big data projects. Maintaining top-quality and client-focused deliverables to meet and exceed their expectations towards delivered solutions. Manage and mentor a team of data scientists, ensuring they are engaged, motivated, productive, and are provided with regular feedback to support growth. Act as a trusted advisor, proposing effective approaches and data science architectures tailored to client needs. Building technical solutions that deliver measurable value. Identify opportunities to expand service offerings and project potential with clients. Take part in pre-sales activities, connecting technical solutions to client needs. Participate in the recruitment process of new talents. What you’ll need to succeed in this role: Bachelor’s or higher in Computer Science, Mathematics, Physics, or related field. Proven track record in managing technical teams and leading end-to-end projects. Experience working with corporate clients. Hands-on experience with Data Science applications (NLP, Computer Vision, Generative AI, Machine Learning, Predictive Modeling). Hands-on experience in ETL, data preparation, and data wrangling techniques. Strong critical thinking and problem-solving abilities. Excellent communication and presentation skills. Advanced English Skills – C1 proficiency level or higher. Proficiency in Python and cloud platforms (preferably Azure, AWS).","[{""min"": 24000, ""max"": 30000, ""type"": ""Net per month - B2B""}]",Data Science,24000,30000,Net per month - B2B
Full-time,Senior,B2B,Remote,585,ETL Developer – Informatica BDM/DEI,Calimala.ai,"Calimala.aiis seeking an experienced ETL Developer with expertise in Informatica BDM/DEI to join our innovative team in the telecom sector. In this capacity, you will be at the forefront of designing, developing, and optimizing scalable ETL pipelines that integrate data from diverse sources. This role requires a deep technical foundation in ETL development and a passion for turning complex data challenges into reliable, high-performance solutions. Responsibilities As an ETL Developer, you will work closely with data architects, analysts, and other team members to understand mapping specifications and implement efficient data workflows. Key responsibilities include: Design and develop complex ETL pipelines using Informatica BDM/DEI. Integrate data from various sources, including big data environments like Hive and Spark. Optimize mappings and workflows to ensure high performance and reliability. Collaborate with cross-functional teams to align data integration strategies with business needs. Document and maintain best practices in ETL and data governance. Requirements Candidates must bring at least five years of hands-on experience in ETL development and demonstrable expertise using Informatica BDM/DEI. A strong technical background in SQL, Hive, and Spark is essential along with proven experience in performance tuning and data integration. Additionally, familiarity with the telecom industry will serve as a significant advantage. What We Offer AtCalimala.ai, we provide a dynamic work environment where innovation meets real-world data challenges. In addition to a competitive salary we offer opportunities for professional growth and learning. Join our team to play a pivotal role in transforming data integration processes and driving business success.","[{""min"": 12000, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Engineering,12000,22000,Net per month - B2B
Full-time,Mid,B2B,Hybrid,589,Data Engineer,SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: projekt z branży automotive, dotyczy przetwarza danych z maszyn i części, aby wesprzeć klientów na całym świecie i pomóc im w przekształcaniu się w pełni operacyjne, oparte na danych organizacje, projektowanie, rozwój i utrzymanie rozwiązań przetwarzania danych, tworzenie i optymalizacja przepływów danych w środowisku Azure i Databricks, współpraca z interesariuszami z działów IT i biznesu w celu dostarczania wartościowych produktów danych, praca hybrydowa, 1 dzień w tygodniu z biura we Wrocławiu, stawka do 140 zł/h przy B2B. Ta oferta jest dla Ciebie, jeśli: posiadasz minimum 4-letnie doświadczenie w roli Data Engineer, bardzo dobrze znasz: Databricks, Workflows, Python, Spark, SQL, masz doświadczenie z DevOps - praca z pipelineami i repozytoriami, dobrze znasz: Azure Data Factory, Azure Key Vault i Power BI, znasz język angielski na poziomie min. B2, cechujesz się otwartością umysłu i zaangażowaniem. Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 22000, ""max"": 23500, ""type"": ""Net per month - B2B""}]",Data Engineering,22000,23500,Net per month - B2B
Part-time,Senior,B2B,Remote,590,Senior BI Consultant / Analityk Power BI,Ness Solution,"🎯 Poszukiwany Senior BI Consultant / Analityk Power BI Lokalizacja: 100% zdalnie opcjonalnie praca hybrydowa (Warszawa) Umowa B2B | 0,5 FTE Tech stack: Power BI, SQL, R (opcjonalnie), Tableau, Azure 💼 O roli Szukamy doświadczonego konsultanta BI, który połączy kompetencje analityczne, deweloperskie i konsultingowe, aby wspierać kluczowe inicjatywy raportowe oraz strategiczne projekty IT i transformacji operacyjnej w jednej z największych instytucji finansowych w Polsce. 🧰 Twoje zadania Tworzenie i rozwój zaawansowanych dashboardów w Power BI (DAX, M Query) Projektowanie warstw danych (Data Marts, KPI Engines) Współpraca z zespołem IT i raportingu zarządczego Automatyzacja procesów raportowych (ETL, API, data flows) Udział w definiowaniu metryk, KPI, SLA oraz tworzeniu raportów zarządczych (Opcjonalnie) wsparcie w analizach w języku R i skryptach w Pythonie / Linuxie ✅ Wymagania Min. 4–5 lat doświadczenia na stanowiskach analitycznych, BI lub konsultingowych Bardzo dobra znajomośćPower BI / Tableau Praktyczna znajomośćSQL– umiejętność pisania zapytań, łączenia tabel, optymalizacji Doświadczenie w pracy z danymi biznesowymi i przygotowywaniu raportów dla interesariuszy Komunikatywność i umiejętność pracy w środowisku biznesowym (kontakt z zespołem i zarządem) 💡 Mile widziane ZnajomośćR(np.dplyr,ggplot2,shiny) – projekt nie wymaga R, ale może się przydać ZnajomośćAzureDev,Python,Linux(bash, crontab) 💬 Oferujemy Współpraca B2B w wymiarze 0,5 etatu (ok. 20h tygodniowo) Możliwość elastycznego rozplanowania czasu (2–3 dni/tydzień lub podział godzinowy) Praca w zespole z doświadczonymi analitykami i architektami BI Projekt z dużym wpływem na decyzje zarządcze Jeśli oferta jest dla Ciebie interesująca, prześlij swoje CV!","[{""min"": 20160, ""max"": 26880, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,20160,26880,Net per month - B2B
Full-time,Senior,B2B,Remote,592,Senior Data Engineer,co.brick,"We're Hiring: Data Engineers & Data Architects 📍Location: Remote 💰Rate: Up to 170 PLN/h 🕒Engagement: Full-time 📅Start: July 📆Duration: Minimum 12 months (with possible extension) We're currently looking for experiencedData EngineersandData Architectsto join a long-term project for established UK-based client. About the role: You’ll be working on a project focused ondecommissioning legacy software and migrating data to modern systems. Required experience: ✔️ 7+ years in data-related roles ✔️ Strong skills in: •SnowflakeandDBT •Python •AWS(preferred) orAzure ✔️ Excellent communication skills (C1+ English level) ✔️ Open and collaborative mindset Nice to have: ➕ Experience withFivetran We’re looking for communicative, proactive individuals who thrive in remote environments and want to work with a solid, international team. Looking forward to hearing from you!","[{""min"": 160, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,160,170,Net per hour - B2B
Full-time,Senior,B2B,Remote,594,"Data Scientist (Gen AI, LLM)",emagine Polska,"Summary: The Senior Data Scientist role focuses on leveraging advanced data science techniques to drive innovations. The primary objective is to apply generative AI and machine learning methods to solve practical challenges in collaboration with both startup and established technology practices. Responsibilities: Utilizing machine learning techniques to solve real-world problems. Applying expertise in Generative AI and Large Language Models (LLMs). Implementing and optimizing natural language processing (NLP) or computer vision techniques. Developing scalable end-to-end machine learning solutions including data preprocessing and model evaluation. Working with cloud platforms and integrating MLOps practices. Analyzing large datasets for insights and model performance. Communicating technical concepts to non-technical stakeholders effectively. Collaborating on innovative solutions to complex data challenges. Key Requirements: 5+ years of experience as a Data Scientist. Strong proficiency in Python and machine learning libraries (scikit-learn, TensorFlow, PyTorch). Experience with Generative AI and LLMs. Knowledge of statistics and mathematics. Excellent problem-solving and critical-thinking skills. Fluent in English (written and spoken). Nice to Have: Programming skills in R, SQL, Java. Experience with big data tools (e.g., Hadoop, Spark, Kafka). Other Details: This position is part of a modern software house focusing on GenAI, cloud computing, and advanced machine learning algorithms. B2B contract! Fully remote with occasional visits in the client’s office.","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Science,180,200,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,595,Data Engineer,emagine Polska,"PROJECT INFORMATION: Industry: Construction Assignment type: B2B Start: September Work model: Hybrid model (2 days/week in office - Warsaw) Project length: 12 months + extensions Project language: English We are looking for a dedicatedData Engineer/BI Developer to join our Danish client'sBI scrum teamof six members, who develop and maintain our BI back-end solutions. The team you will join works in close collaboration with our highly innovative departments and companies, where you identify and build the foundation for their new BI reports. Reports covering everything from working environment, diversity, IOT machine data to the more financial. Our team is at the forefront of digitization and automation, focused on streamlining the way we work by processing and presenting data through Business Intelligence tools. The role involves collaborating with various departments to develop insightful BI reports that enhance decision-making and operational efficiency. Main Responsibilities Develop and maintain BI back-end solutions. Collaborate with various teams to identify and implement new BI reporting frameworks. Analyze data trends and provide actionable insights to colleagues. Facilitate the transition to cloud environments and integrate new data sources. Contribute to the development and execution of our digital strategies. Identify and implement new data sources and improve existing data frameworks. Work closely with cross-functional teams to define requirements for BI initiatives. Analyze complex data sets and translate findings into actionable insights. Contribute to cloud integration and Data Lakehouse projects. Key Requirements +3 years of experience in BI area Experience in Business Intelligence and data analysis. Strong understanding of Python and SQL at an advanced level. Strong Experience with Databricks Basic experience with DevOps practices Knowledge of cloud-based solutions, particularly Azure. Capacity to perform dimensional data modeling. Experience with structured and unstructured data sources (API, SQL). Adept at communicating complex ideas effectively. Good problem-solving skills and a proactive attitude toward technological advancements. Nice to Have Familiarity Data Factory. Knowledge of MS Dynamics and process automation. Familiarity with Power BI and data transformation tools.","[{""min"": 170, ""max"": 180, ""type"": ""Net per hour - B2B""}]",Data Engineering,170,180,Net per hour - B2B
Full-time,Senior,Permanent,Hybrid,597,Senior Big Data Engineer,Relativity,"Posting Type Hybrid Job Overview Here at Relativity we prioritize flexibility and work-life harmony. Our Hybrid work environment provides options tailored to your role and location, aiming to enhance engagement, connectivity, and productivity. Join us to experience a culture of collaboration and innovation, where connecting in-person adds value to our collective growth. Let's work together! Join our team as we innovate the future of data platform architecture, enabling massive scaling and data processing for ML and Gen AI projects. You'll be at the forefront of processing vast unstructured data, building high-throughput APIs, and supporting distributed compute frameworks for seamless model deployment. Ready to dive into the heart of cutting-edge tech? Job Description and Requirements Your role in action: Build our next-generation data platform tooling and services to support the ingestion and processing of billions of documents at scale. Improve and extend our Spark based distributed data processing pipeline. Improve and extend our Rust based distributed query engine used to request large amounts of document data. Create tools to automate and optimize processes across disciplines Actively participate in the on-call schedule to investigate and fix production issues related to our data processing pipeline or query engine. Participate in code reviews for projects written by your team Focus on quality through comprehensive unit and integration testing Your Skills: 4+ years of software development experience in writing performant, commercial-grade systems and applications Experience with monitoring and troubleshooting production environments • Proficiency in programming languages used in high volume data processing and applications like Java or Scala and Python Experience building data pipelines with distributed compute frameworks like Hadoop. Spark, or Dask Knowledge of Linux/Unix systems, Docker/Kubernetes and CI/CD including scripting in Python or other scripting languages to automate build and deployment processes Knowledge of professional software engineering practices & software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations Leverages best practices and past experiences to mentor and improve the productivity of the team We’d particularly love it if you have: Deep experience building and debugging distributed data pipelines Experience with columnar databases and storage formats like Delta Lake and Parquet Experience deploying and managing services on Kubernetes Experience building with Rust. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law. #LI-MM5 Relativity is committed to competitive, fair, and equitable compensation practices. This position is eligible for total compensation which includes a competitive base salary, an annual performance bonus, and long-term incentives. The expected salary range for this role is between following values: 181 000 and 271 000PLNThe final offered salary will be based on several factors, including but not limited to the candidate's depth of experience, skill set, qualifications, and internal pay equity. Hiring at the top end of the range would not be typical, to allow for future meaningful salary growth in this position.","[{""min"": 181000, ""max"": 271000, ""type"": ""Gross per year - Permanent""}]",Data Engineering,181000,271000,Gross per year - Permanent
Full-time,Manager / C-level,B2B,Hybrid,603,Lead Data Analyst,Experis Manpower Group,"We are looking for an experiencedData Analyst (Manager level)to lead analytical initiatives aimed at optimizing business performance and product growth. This role focuses on transforming raw data into actionable insights through advanced data analysis, visualization, and reporting. The ideal candidate combines strong technical proficiency with leadership capability to manage cross-functional collaboration and ensure the delivery of high-quality analytical outcomes. Lead data collection, cleaning, and organization efforts for structured data sets Optimize business and product growth scenarios through advanced data analysis Analyze large data sets to identify patterns, trends, and opportunities Build and maintain dashboards and reports to present key insights to technical and non-technical stakeholders Interpret data findings and provide actionable recommendations to leadership and product teams Collaborate closely with data scientists, data engineers, and cross-functional teams to ensure data integrity and alignment Oversee data-related initiatives, ensuring quality, timeliness, and business relevance Stay current with emerging tools, technologies, and methodologies in data analytics Mentor junior analysts and contribute to the development of data literacy across the organization 5+ years of experience in data analytics, with at least 2 years in a leadership or managerial role Proven ability to lead data-driven decision-making processes Strong proficiency inR, Python, SQL, SAS, and SAS Miner Experience working with structured data from relational databases, spreadsheets, and cloud sources Advanced skills in data visualization and reporting (e.g., Power BI, Tableau, or similar tools) Excellent communication skills with the ability to translate technical findings into business-friendly language Familiarity with data governance, data quality standards, and best practices Demonstrated ability to manage multiple projects and collaborate with cross-functional teams Bachelor’s or Master’s degree in Computer Science, Data Science, Statistics, or a related field Our offer: Work from Warsaw, Katowice or Gdansk office MultiSport card Private Healthcare Life insurance","[{""min"": 170, ""max"": 190, ""type"": ""Net per hour - B2B""}]",Data Analysis & BI,170,190,Net per hour - B2B
Full-time,Senior,B2B,Remote,604,Senior Oracle Database Administrator,Hirexa,"Job Title: Oracle DBA Location: Remote Employment Type: B2B About Hirexa Solutions: Hirexa Solutions is a leading player in the recruitment ecosystem across the United States, United Kingdom, Europe, and India. As the fastest-growing next-generation provider of technology talent, we empower our clients to become resourceful, achieve higher productivity, adopt agile structures, and effectively execute project deliverables. Envisioned and co-founded by veterans of the Information Technology industry, our mission is to make recruitment efficient, flawless, and cost-effective. Our unwavering commitment to strategic investments in intelligent technology underscores our passion for people and our dedication to helping organizations realize their true potential. Job Description Key Responsibilities: Design, install, configure, and maintain database systems (e.g., SQL Server, Oracle, MySQL, PostgreSQL). Monitor database performance and tune complex queries for optimal performance. Manage database backup, recovery, high availability (HA), and disaster recovery (DR) strategies. Implement security measures to protect sensitive data and ensure compliance with data governance policies. Perform database upgrades, patching, and migration activities. Collaborate with development and DevOps teams to support application deployment and database changes. Troubleshoot database-related issues and provide 24/7 support as needed. Create and maintain documentation, standards, and procedures. Automate routine tasks using scripts or tools (e.g., PowerShell, Bash, Python). Mentor junior DBAs and support database-related aspects of software development lifecycle (SDLC). Required Skills: 8 to 10 years of experience in a DBA role with enterprise-level databases. Expertise in at least one major RDBMS (e.g., Microsoft SQL Server, Oracle, PostgreSQL). Strong knowledge of SQL, T-SQL/PL-SQL, indexing, and query optimization. Experience with database monitoring and performance tuning tools. Proven experience with backup and recovery strategies and tools. Knowledge of cloud database platforms (e.g., AWS RDS, Azure SQL Database, Google Cloud SQL). Familiarity with replication, clustering, and HA/DR configurations. Strong scripting skills (e.g., PowerShell, Shell, Python) Position Overview: For one of our partners, we are seeking a Oracle DBA who will be responsible for MSSQL,Oracle DBA. The ideal candidate will possess the necessary skills and experience to contribute to the success of our partner organization. How to Apply: If you are interested in this opportunity, please submit your resume. We look forward to hearing from you!","[{""min"": 939, ""max"": 1024, ""type"": ""Net per day - B2B""}]",Database Administration,939,1024,Net per day - B2B
Full-time,Senior,B2B,Remote,605,AI/ML Principal Software Engineer,Sii,"Minimum of 7 years of experience building and deploying complex, production-grade software systems Proficient in backend technologies (e.g., Python, Java, Node.js) and frameworks (e.g., Django, Flask, Spring Boot) Skilled in frontend frameworks such as React or Angular Strong background in containerization (Docker) and orchestration (Kubernetes) Proven expertise with AWS and scalable cloud-based architectures Fluency in English, both spoken and written Residing in Poland required Join our client's team in the medical industry as an AI/ML Principal Software Engineer, and help build cutting-edge software solutions powered by machine learning for real-world applications. In this role, you will play a key part in designing and building scalable, high-performance systems that bring machine learning models into production environments. This is an exciting opportunity to lead critical initiatives at the intersection of software engineering and AI, while working remotely within a collaborative and forward-thinking team. Take ownership of designing and developing reliable, scalable software solutions that incorporate machine learning models Work closely with data scientists and ML engineers to transform experimental prototypes into fully operational, production-grade systems Contribute to key architectural decisions, infrastructure planning, and the selection and implementation of development tools Assist in building and maintaining cloud-native environments and solutions on AWS Apply industry best practices for automated testing, continuous integration and delivery (CI/CD), and system observability Develop and sustain backend infrastructures and user-facing applications with a strong focus on performance, security, and maintainability Start ASAP Praca w pełni zdalna Darmowe śniadanie Bez wymaganego dress code'u Darmowa kawa Szkolenia wewnętrzne Nowoczesne biuro Pakiet sportowy Budżet na szkolenia Międzynarodowe projekty Małe zespoły Prywatna opieka medyczna Since 2006 on the market, 7 500+ experts, PLN 2,1 billion revenue, 11 times Great Place to Work title – get to know Sii, the leading technology consulting, digital transformation, engineering, and business services vendor in Poland. We implement projects for over 200 clients from all over the world. Our mission is to identify and promote the best Workers – Power People. Learn more atsii.pl.","[{""min"": 24000, ""max"": 28000, ""type"": ""Net per month - B2B""}]",Data Science,24000,28000,Net per month - B2B
Full-time,Senior,B2B,Remote,606,Senior Data Engineer,Link Group,"Maintain and optimizeAzure SQL DatabaseandAzure SQL Managed Instances Design, implement, and manage data pipelines to support real-time and near real-time data exchange Analyze and improve complex SQL queries and performance bottlenecks Work closely with solution architects and product teams to propose architectural changes and data model improvements Monitor and troubleshoot data issues, proposing sustainable and scalable fixes Contribute to the overall data architecture strategy and ensure alignment with business needs Proven experience as aSenior Data EngineerorData Architect Strong expertise inAzure SQL DatabaseandAzure SQL Managed Instance Deep knowledge ofSQL performance tuningandquery optimization Experience withdata modeling,ETL/ELT pipelines, anddata integration High-level understanding ofdata architectureprinciples and patterns Hands-on experience withAzure Data Factory,Azure Data Lake, or other Azure data services Familiarity withCI/CD,Git, and agile workflows Experience withPower BI,Databricks, orSynapse Analytics Familiarity withsecurity and compliancestandards in the data domain Knowledge ofAI/MLintegration in data pipelines Previous work onmaturity or assessment platformsis a plus 100% remote work with flexible hours Opportunity to shape a high-impact platform used by global organizations Long-term project with potential for growth and ownership Competitive compensation based on experience","[{""min"": 120, ""max"": 140, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,140,Net per hour - B2B
Full-time,Senior,B2B,Hybrid,608,Senior Data Analyst,PeakData,"About Peak Data: Peak Data is a Swiss-based startup revolutionizing the pharmaceutical industry through innovative data solutions. Our mission is to empower pharmaceutical companies with actionable insights, driving better decision-making and improving patient outcomes. Join our dynamic team and be a part of a company that is at the forefront of data-driven healthcare innovation. Position Overview: We are seeking a highly skilled and motivated Data Analyst to join our Operations Department. The ideal candidate will have a strong background in data analysis, with a particular focus on the pharmaceutical industry. You will be responsible for analyzing complex data sets, developing insights, and providing recommendations to support our operational strategies. Proficiency in SQL, Python, and AWS is essential for this role. Key Responsibilities: • Collect, process, and analyse large datasets to identify trends, patterns, and insights related to pharmaceutical operations. • Develop and maintain dashboards and reports to monitor key performance indicators (KPIs) and operational metrics. • Collaborate with cross-functional teams to define data requirements and ensure data accuracy and integrity. • Utilize SQL to query databases and extract relevant data for analysis. • Apply Python for data manipulation, statistical analysis, and automation of data workflows. • Leverage AWS services to manage and analyse data in the cloud environment. • Provide actionable insights and recommendations to support decision-making processes within the operations department. • Stay updated with industry trends, best practices, and emerging technologies in data analysis and the pharmaceutical sector. Qualifications: • Bachelor’s degree in Data Science, Statistics, Computer Science, or a related field. A Master’s degree is a plus. • Proven experience (4y or more) as a Data Analyst, preferably within the pharmaceutical industry. • Strong proficiency in SQL for data querying and database management. • Advanced skills in Python for data analysis, statistical modelling, and automation. • At least 3y of experience with AWS services, including data storage, processing, and analytics. • Solid understanding of pharmaceutical industry operations, regulations, and data requirements. • Excellent analytical and problem-solving skills with the ability to interpret complex data sets. • Strong communication skills, with the ability to present findings and insights to both technical and non-technical stakeholders. • Detail-oriented, with a commitment to accuracy and data integrity. • Ability to work independently and collaboratively in a fast-paced startup environment. What We Offer: • Competitive salary and benefits package. • Opportunity to work with a passionate and innovative team in a growing startup. • Flexible working hours and remote work options. • Professional development opportunities and support for continuous learning. • A dynamic and inclusive work environment that values creativity and diversity.","[{""min"": 12808, ""max"": 19213, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,12808,19213,Net per month - B2B
Full-time,Senior,B2B,Remote,609,Big Data Engineer,ITDS,"Big Data Engineer Join us, and build data solutions that drive global innovation! Kraków - based opportunity with hybrid work model (2 days month in the office). As aBig Data Developer,you will be working for our client, a leading global financial institution, contributing to the design and development of cutting-edge data solutions for risk management and analytics. The client is undergoing a strategic digital transformation, focusing on scalable, cloud-based big data platforms that support advanced analytics and regulatory compliance. You will be part of a high-performing Agile team, collaborating closely with business stakeholders and technical teams to build and maintain robust distributed systems that process large volumes of data efficiently. Designing and developing distributed big data solutions using Spark Implementing microservices and APIs for data ingestion and analytics Managing cloud-native deployments primarily on GCP Writing and maintaining test automation frameworks using tools like JUnit, Cucumber, or Karate Collaborating with cross-functional teams to translate business requirements into technical specifications Developing and scheduling data workflows using Apache Airflow Maintaining and optimizing existing big data pipelines Utilizing DevOps tools such as Jenkins and Ansible for CI/CD automation Participating in Agile ceremonies and contributing to sprint planning and retrospectives Monitoring, troubleshooting, and improving data systems and services A degree in Computer Science, IT, or a related discipline Proven experience in designing and developing big data systems Hands-on experience with Spark and distributed computing SolidJava,Python, andGroovydevelopment skills Strong knowledge of the Spring ecosystem (Boot, Batch, Cloud) Familiarity with REST APIs, Web Services, and API Gateway technologies Practical experience in DevOps tooling like Jenkins and Ansible Proficiency in using RDBMS, especially PostgreSQL Hands-on experience with public cloud platforms, particularly GCP Excellent communication in English Experience with streaming technologies like Apache Beam or Flink Knowledge of OLAP solutions and data modeling Background in financial risk management or the banking industry Exposure to container technologies such as Docker and Kubernetes Familiarity with Traded Risk domain concepts Experience with RPC frameworks like gRPC Knowledge of data lakehouse tools like Dremio or Trino Hands-on experience with BI or UI development We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #7225 You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere.","[{""min"": 28000, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Engineering,28000,31500,Net per month - B2B
Full-time,Mid,Permanent or B2B,Remote,612,Senior Data Engineer,XTB,"Tworzymy XTB – globalną firmę inwestycyjną, oferującą innowacyjne rozwiązania technologiczne, które pozwalają naszym klientom skutecznie zarządzać swoimi finansami na wiele sposobów. Wszystko to w jednej intuicyjnej aplikacji XTB, z której korzysta już ponad milion użytkowników na całym świecie! Jesteśmy certyfikowaną firmąGreat Place to Work. Poszukujemy osoby, która dołączy do naszego zespołu Data Platform w roli Data Engineer. Głównym zadaniem zespołu jest utrzymanie i rozwój hurtowni danych on premise i jej docelowa migracja na środowisko chmurowe. Współpracujemy z zespołami produktowymi w celu dostarczenia danych potrzebnych do podejmowania kluczowych decyzji biznesowych. Pracujemy w frameworku Scrum. Do Twoich codziennych obowiązków będzie należało: Projektowanie i utrzymywanie hurtowni danych oraz migracja danych i procesów na środowisko chmure, Tworzenie i utrzymywanie modeli danych do wspierania kluczowych decyzji biznesowych, Integracja danych w celu wytworzenia wymaganych modeli danych, Wdrażanie kontroli jakości danych i procesów walidacji, aby zapewnić dokładność, spójność i kompletność, Współpraca z innymi zespołami, aby tworzyć zestawy danych spełniające potrzeby raportowania i rozwiązujące problemy systemowe, Projektowanie i wykonywanie testów wydajnościowych i integracyjnych, Raportowanie kluczowych wskaźników w firmie w narzędziu BI Wymagania: Co najmniej 4-letnie doświadczenie w pracy na stanowisku SQL Developer / Data Engineer Znajomość Python, SQL w tym T-SQL, pisanie złożonych procedur składowanych, optymalizacja wydajności, Umiejętność tworzenia procesów ETL (SSIS, Airflow), Doświadczenie zawodowe w eksploracji danych, analizie i modelowaniu złożonych zbiorów danych na dużą skalę; Doświadczenie w pracy z narzędziami do zarządzania kodem źródłowym, takimi jak GIT Dobra znajomość zasad standardów integracyjnych: REST, gRPC Umiejętność tworzenia rozwiązań w oparciu o serwisy w Snowflake Doświadczenie w tworzeniu, wdrażaniu i rozwiązywaniu problemów z aplikacjami danych na platformie Microsoft Azure Znajomość rozwiązań chmurowych (Azure, GCP), Silne umiejętności rozwiązywania problemów i dbałość o szczegóły. Umiejętność skutecznej komunikacji i współpracy w zespole. Chęć uczenia się i dostosowywania do nowych technologii i koncepcji. Rozumienie zasad Agile i Scrum (pracujemy w Scrumie) Dodatkowe atuty: Doświadczenie w budowaniu skalowalnych, działających w czasie rzeczywistym rozwiązań typu Data Lake, Doświadczenie ze strumieniowym przesyłaniem danych (Kafka), Doświadczenie w pracy z Kubernetes Znajomość koncepcji Data Mesh. Znajomość podejścia DevOps Oferujemy Realny wpływ na rozwój firmy i produktu Pracę w doświadczonym zespole, który chętnie dzieli się wiedzą Jasną wizję rozwoju dzięki regularnym feedbackom i klarownym ścieżkom karier Budżet szkoleniowy na interesujące Cię kursy i konferencje Dodatkowy dzień wolny z okazji Twoich urodzin Dodatkowy dzień wolny dla rodziców Sprzęt dopasowany do Twoich potrzeb Prywatną opiekę medyczną i ubezpieczenie grupowe Dostęp do platformy e-learningowej do nauki języka angielskiego oraz platformy benefitowej Dostęp do platformy wellbeingowej i możliwość skorzystania z warsztatów oraz prywatnych sesji terapeutycznych Pracę zdalną, z biura w Warszawie lub z coworku w Twoim mieście Regularne spotkania integracyjne","[{""min"": 15000, ""max"": 19000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15000,19000,Net per month - B2B
Full-time,Senior,B2B,Remote,613,Database Administrator,Experis Manpower Group,"We are looking for aDatabase Administratorwho will take part in planning and developing the database, while also proactively addressing and resolving user-related issues. Responsibilities: • Install, configure, and maintain database management systems (DBMS) • Monitor database performance and implement changes to improve efficiency • Ensure data integrity and security by implementing appropriate measures • Perform regular database backups and recovery operations • Collaborate with developers to design and optimize database structures • Troubleshoot database issues and provide technical support to users • Maintain documentation related to database configurations and procedures • Stay updated with the latest database technologies and best practices Requirements: • Bachelor's degree in Computer Science, Information Technology, or a related field • Proven experience as a Database Administrator or similar role • Proficiency in database management systems such as Oracle, SQL Server, or MySQL • Strong understanding of database design and data modelling • Knowledge of backup and recovery procedures • Excellent problem-solving skills and attention to detail • Strong communication and teamwork abilities • Experience with cloud-based database solutions • Familiarity with data warehousing and ETL processes • Knowledge of database security practices Our offer: • B2B via Experis • 100% remote work • MultiSport Plus • Group insurance • Medicover Premium • e-learning platform","[{""min"": 172, ""max"": 183, ""type"": ""Net per hour - B2B""}]",Database Administration,172,183,Net per hour - B2B
Full-time,Senior,B2B,Remote,614,Senior Data Engineer (2328),N-iX,"About us: N-iXis a software development service company that helps businesses across the globe develop successful software products. Founded in 2002 in Lviv,N-iXhas come a long way and increased its presence in eight countries Poland, Ukraine, Sweden, Bulgaria, Malta, the UK, the US, and Colombia. Today, we are a strong community of 2,000+ professionals and a reliable partner for global industry leaders and Fortune 500 companies About the position: Our customer is seeking to expand its Data Engineering team to stand up modern data platform for one of its portfolio companies in the financial services sector. The role requires expertise in Python, SQL (PostgreSQL, MySQL), Airflow, Snowflake, and AWS cloud experience. This project involves managing financial assets owned by the company. It utilizes machine-learning models that operate on data ingested from third-party APIs. The process includes ELT (extract, load, transform), data modeling in Snowflake using DBT, training ML models using AWS SageMaker, running predictions, and storing predictions back in Snowflake. As a Senior Data Engineer, you will design, build, and maintain scalable data pipelines and architectures for our cloud-based analytical platforms. You will collaborate closely with data scientists, analysts, and software engineering teams to deliver robust, high-quality data solutions that drive business decisions. Project involvement plans: Initially 5 months with the possibility of extension. Start in July 2025. Responsibilities: Design, implement, and maintain scalable data pipelines that support business analytics, reporting, and operational needs. Collaborate cross-functionally with analysts, engineers, and product teams to translate data requirements into efficient data models and pipelines. Ensure reliability and performance of data workflows by proactively monitoring, debugging, and optimizing data processes. Drive automation and testing practices in data workflows to maintain high code quality and deployment confidence. Contribute to architectural decisions involving cloud infrastructure, data warehousing strategies, and data governance policies. Required Skills and Qualifications: Key Skills: Python AWS Snowflake Airflow DBT data modeling PostgreSQL, MySQL (or similar) Technical Expertise: Programming Languages: Advanced proficiency in Python for data engineering, data wrangling, and pipeline development. Cloud Platforms: Hands-on experience working with AWS (S3, Glue, Redshift, Lambda, etc.). Data Warehousing: Proven expertise with Snowflake – schema design, performance tuning, data ingestion, and security. Workflow Orchestration: Production experience with Apache Airflow (Prefect, Dagster or similar), including authoring DAGs, scheduling workloads and monitoring pipeline execution. Data Modeling: Strong skills in DBT (Data Build Tool), including writing modular SQL transformations, building data models, and maintaining DBT projects. SQL Databases: Extensive experience with PostgreSQL, MySQL (or similar), including schema design, optimization, and complex query development. Additional Competencies: Version Control and CI/CD: Familiarity with Git-based workflows and continuous integration/deployment practices to ensure seamless code integration and deployment processes. Communication Skills: Ability to articulate complex technical concepts to technical and non-technical stakeholders alike. Tools: Git JIRA Confluence Must have: Extensive experience with Python for data analysis Declarative Data Modeling: Experience with modern tools like DBT for streamlined and efficient data modelling. Minimum 5 years of professional experience in production environments, emphasizing performance optimization and code quality. We offer: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits","[{""min"": 19209, ""max"": 24750, ""type"": ""Net per month - B2B""}]",Data Engineering,19209,24750,Net per month - B2B
Full-time,Senior,B2B,Remote,615,Senior Data Engineer,INFOPLUS TECHNOLOGIES,"Job Title: Senior Data Engineer 📍 Location: Poland (Remote) 💰 Rate: 1000 PLN/day 🕒 Seniority Level: Senior Required Skills & Experience: 5+ years of experience in data engineering roles Strong proficiency in Python for data engineering tasks Hands-on experience with AWS services such as: S3, Glue, Lambda, Step Functions, Redshift, Athena, CloudFormation Experience with CI/CD tools (e.g., GitLab CI, Jenkins, GitHub Actions) Proficiency with AWS CDK or other Infrastructure as Code frameworks Solid understanding of data warehousing , data lakes , and ETL best practices Familiar with version control (Git), unit testing, and agile delivery practices Excellent problem-solving skills and ability to work independently Strong communication skills with the ability to explain technical topics to stakeholders 5+ years of experience in data engineering roles Strong proficiency in Python for data engineering tasks Hands-on experience with AWS services such as: S3, Glue, Lambda, Step Functions, Redshift, Athena, CloudFormation Experience with CI/CD tools (e.g., GitLab CI, Jenkins, GitHub Actions) Proficiency with AWS CDK or other Infrastructure as Code frameworks Solid understanding of data warehousing , data lakes , and ETL best practices Familiar with version control (Git), unit testing, and agile delivery practices Excellent problem-solving skills and ability to work independently Strong communication skills with the ability to explain technical topics to stakeholders","[{""min"": 800, ""max"": 1000, ""type"": ""Net per day - B2B""}]",Data Engineering,800,1000,Net per day - B2B
Full-time,Senior,B2B,Remote,619,Data Platform Engineer - available ASAP,ITDS,"As aData Platform Engineer, you will be working for our client in the debt collection sector, helping to build and maintain a robust, cloud-native data platform. The role focuses heavily on data modelling, requiring an expert who can translate conceptual business needs into logical and physical data models, build data contracts, and implement scalable ELT pipelines using Azure Databricks. Your main responsibilities: Design and maintain logical and physical data models based on DDD (Domain-Driven Design) principles Translate conceptual models and business glossaries into technical data structures for the Data Warehouse Perform data mapping and create data contracts between the Data Platform and source systems Collaborate with source system owners to define data contract requirements Work on data ingestion processes from source systems using various methods: Direct database queries (bulk read/CDC), API communication, Event streaming Implement ELT processes across Bronze, Silver, and Gold layers in Azure Databricks Ensure alignment of data models with business and analytical requirements You're ideal for this role if you have: Strong experience in Data Modelling (logical & physical), preferably in DDD-based environments Proven ability to work with Data Governance inputs: glossaries, conceptual models, HLD/LLD documentation Experience preparing and maintaining data contracts Solid knowledge of data ingestion techniques and working with source systems Experience with Azure Databricks Ability to develop and maintain ELT pipelines in cloud-native environments Nice to have: Experience in writing clear technical documentation (e.g. data contracts, field definitions, extraction rules) Background in mapping source data to target DWH structures Ability to interpret and work with ERDs and relational models Knowledge of master data management practices Familiarity withdbdiagram.io Awareness of Data Quality, Data Lineage, and metadata management concepts Experience using tools like Azure Purview or other metadata management platforms We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participate in Social Events, training, and work in an international environment Access to attractive Medical Package Access to Multisport Program Access to Pluralsight Flexible hours & remote work Internal job number #6787 You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere.","[{""min"": 25200, ""max"": 31500, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,31500,Net per month - B2B
Full-time,Senior,B2B,Remote,620,Data Engineering Analyst,TechTorch,"About Us TechTorchis a fast-growing consultancy at the intersection of enterprise tech, AI, and private equity. We partner with top-tier PE funds and their portfolio companies to deliver AI-powered platforms, data accelerators, and digital transformation projects that generate measurable value — fast. Founded by former Bain consultants, CIOs, and enterprise tech leaders, we blend startup agility with big-firm experience. We’re not a typical startup — we were built to deliver. As aData Analyst, you’ll be a key contributor to building a modern Azure-native data platform for one of our private equity-backed clients. You’ll analyze current data sources and legacy reports, reverse-engineer business logic, and translate complex reporting requirements into structured data specifications. You’ll also support data profiling, validation, and the design of a Common Data Model (Bronze/Silver/Gold). Your work will directly influence dbt model development, data pipelines, and reporting layers — all serving as the organization’s single source of truth. SQL (advanced querying, troubleshooting) dbt, ETL, data modeling (dimensional/relational) Data profiling, validation, quality checks Azure Data Services (preferred), AWS/GCP also welcome Familiarity with Medallion architecture Bonus: Python (Pandas), Salesforce data Analyze raw and semi-structured datasets to understand key metrics and usage Reverse-engineer legacy SQL, stored procedures, reports Perform data profiling and assess quality issues Translate business needs into clear data specs, rules, and mappings Support Common Data Model design (Bronze/Silver/Gold) Define data validation, error handling, and quality checks Collaborate closely with Data Architects and engineers Support testing and debugging through validation reports and business logic guidance Engage with business stakeholders to clarify reporting needs 5+ years of experience in enterprise data analysis Strong SQL skills (querying, analysis, debugging) Deep experience in data profiling and validation Familiarity with relational and dimensional modeling Ability to understand and document transformation logic Experience working across technical and business teams Knowledge of modern data architectures (Medallion, warehouse design) Clear, structured communication skills English: fluent spoken and written Python (Pandas) for exploratory analysis Experience with Salesforce data Cloud experience (Azure preferred, AWS/GCP okay) Projects with high-impact PE-backed companies Work with ex-Bain, top-tier CIOs, and data leaders Real ownership and visibility in project delivery Remote-first culture built on speed, clarity, and results A fast-paced environment for high performers who want to grow Client First– Value and outcomes over slide decks We, Not Me– We move faster as a team Get Stuff Done– Execution over bureaucracy AI First– AI is embedded into what we build Own It– We take full responsibility Agile Mindset– We adapt fast and improve constantly","[{""min"": 16000, ""max"": 23500, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,16000,23500,Net per month - B2B
Full-time,Senior,B2B,Remote,621,Data Engineering Architect (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition byForbesas one of the top 10 AI consulting companies. As aData Engineering Architect, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company.This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies.This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. This role represents a gradual shift away from hands-on coding towards a more strategic focus on system design, business consultation, and creative problem-solving. It offers an opportunity to engage more deeply with architecture-level decisions, collaborate closely with clients, and contribute to building innovative data-driven solutions from a broader perspective. 🚀 Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Airflow, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. 🎯 What you'll need to succeed in this role: 5+ years of proven commercial experiencein implementing, developing, or maintaining Big Data systems. Strong programming skills inPythonorJava/Scala: writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Hands-on experience withBig Datatechnologies likeSpark,Airflow,Iceberg, CI/CD, Kafka. Proven expertise in implementing and deploying solutions in cloud environments (with a preference forAWS). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master’s or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. FluentEnglish(C1 level) is a must. 🎁 Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage withtop-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you towork remotelyor from modern offices and coworking spaces. Accelerate your professional growth throughcareer paths,knowledge-sharinginitiatives,languageclasses, and sponsoredtrainingorconferences, including a partnership withDatabricks, which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days offavailable for B2B contractors and individuals under contracts of mandate. Participate inteam-building eventsand utilize theintegration budget. Celebratework anniversaries, birthdays,andmilestones. Accessmedicalandsports packages, eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you canboostyourpersonal brandby speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website (career page) and social media (Facebook,LinkedIn,Instagram).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Architecture,21000,31920,Net per month - B2B
Full-time,Mid,B2B,Hybrid,626,Operation Center Engineer (DBA),emagine Polska,"🌍 Work model: hybrid work – 2 times a week from the office in Warsaw + business travels to Oslo from time to time. 📑 Assignment type: B2B - 125-145 zl/h. ⏳ Project length: Long term (24 months contract + extensions). 📕 Project language: English. ⚙️ Industry: finance. Are you a skilled IT consultant looking for your next challenge? We are on the lookout for a Frontline Senior Operation Center Engineer to support our dynamic client base. You will manage, optimize, and maintain complex database technologies while collaborating closely with a specialized team of professionals. Your problem-solving skills and technical knowledge will be crucial in our mission to deliver top-notch solutions and services. Responsibilities: Managing, optimizing, and scaling customer databases on-premises and in Microsoft Azure. Participating in onboarding new customer databases. Troubleshooting and resolving database and application-related incidents. Implementing change requests from customers. Proactively improving customer delivery. Taking ownership of problem tickets and implementing solutions. Key Requirements: Bachelor’s degree in Computer Science, Information Technology, or a related field (equivalent experience is acceptable). Strong analytical and problem-solving abilities. Independent worker with team collaboration skills. Experience with PostgreSQL and MySQL database engines. Proficiency in Atlassian products (Confluence, Jira). Competence in Linux, Windows, and Azure. Knowledge of Elastistack and ElastiCloud. Nice to Have: Experience with RavenDB and Redis database engines. Ability to troubleshoot LDAP/SSO/TLS issues. Basic understanding of virtualization technologies. Join us for a professional journey where innovation and technology meet expertise and creativity in the finance industry.","[{""min"": 125, ""max"": 145, ""type"": ""Net per hour - B2B""}]",Data Engineering,125,145,Net per hour - B2B
Full-time,Mid,Permanent or B2B,Hybrid,627,Data Architect / Data Warehouse Specialist,Power Media,"Nasz klient kompleksowo realizuje usługi w zakresie konsultingu, projektowania i zarządzania projektami. Specjalizuje się głównie w obszarach e-commerce, data management (Agile Data Warehouse, Data Governance) oraz Content Management. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozwój rozwiązań Data Platform w środowisku chmurowym. Kluczowym zadaniem będzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejsów w złożonym krajobrazie systemowym klienta. Projekt obejmuje budowę hurtowni danych oraz platform danych od podstaw. Zespółskłada się z 22 osób: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiązków: Rozwój i utrzymanie Data Warehouse oraz Data Platform przy użyciu narzędzi ETL i SQL, Analiza wymagań biznesowych i proponowanie rozwiązań technicznych, Współpraca z zespołem w celu zapewnienia dostępności rozwiązań biznesowych, Wykorzystanie swojej wiedzy i doświadczenia do tworzenia innowacyjnych rozwiązań. Główne wymagania: Bardzo dobra znajomość metod i technik projektowania oprogramowania, Ponad 10-letnie doświadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych, Znajomość ekosystemów Big Data, Znajomośćrozwiązań chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejętności analityczne (analiza i dokumentowanie wymagań biznesowych oraz specyfikacji technicznych) Doświadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomość SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomość rozwiązań ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomość języka angielskiego (min. B2+/C1)– codzienna praca w międzynarodowym środowisku, Gotowość do podróży służbowych. Mile widziane: Znajomośćnarzędzi Informatica, Znajomość Machine Learning oraz frameworków ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomość języków programowania (Java, Scala, C++, Python). Znajomość języka niemieckiego. Firma oferuje: Stabilne, długofalowe zatrudnienie w oparciu o B2B lub UoP, Możliwość pracy w większości zdalnej(praca z biura 1 raz w tygodniu, we wtorek) Płaska struktura, antykorporacyjne podejście do pracy i zespołu, Ciekawe, międzynarodowe projekty, Zgrany zespół chętnie uczestniczący w aktywnościach sportowo – rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team – building), Dodatkowe zajęcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajęć sportowych, Nowoczesne biuro ze strefą relaksu, Co roczny 5 dniowy wyjazd firmowy (cała firma), warsztaty kulinarne, wyjścia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywnościami Świetna atmosfera, partnerskie podejście, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa „miękko-techniczna”. CV w j. angielskim.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,20000,30000,Net per month - B2B
Full-time,Senior,Permanent,Hybrid,629,Senior Data Engineer,Harvey Nash Technology,"3+ years of strong Python development experience and solid engineering practices Expertise in microservices frameworks and event-driven architectures Advanced Spark skills (PySpark, Scala) with experience in scalable, maintainable pipelines Experience with Spark Streaming and Delta Lake Familiarity with Kubernetes and MongoDB Proven AWS cloud experience Design, build, and enhance data pipelines for streaming and batch processing Extend and support our AWS cloud data platform Develop features using Databricks pipelines, Unity Catalog, and Spark Streaming Lead and mentor the team, promoting best practices and process improvements Gather requirements from stakeholders and deliver high-quality solutions Partner with customers to ensure project success What You’ll Bring 3+ years of strong Python development experience and solid engineering practices Expertise in microservices frameworks and event-driven architectures Advanced Spark skills (PySpark, Scala) with experience in scalable, maintainable pipelines Experience with Spark Streaming and Delta Lake Familiarity with Kubernetes and MongoDB Proven AWS cloud experience Strong communication and a commitment to high ethical standards","[{""min"": 32000, ""max"": 40000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,32000,40000,Gross per month - Permanent
Full-time,Mid,B2B,Remote,633,Data Engineer,Ework Group,"💻 Ework Group- founded in 2000, listed on Nasdaq Stockholm, with around 13,000 independent professionals on assignment - we are the total talent solutions provider who partners with clients, in both the private and public sector, and professionals to create sustainable talent supply chains. With a focus on IT/OT, R&D, Engineering and Business Development, we deliver sustainable value through a holistic and independent approach to total talent management. By providing comprehensive talent solutions, combined with vast industry experience and excellence in execution, we form successful collaborations. We bridge clients and partners & professionals throughout the talent supply chain, for the benefit of individuals, organizations and society. 🔹 For our Client we are looking forData Engineer🔹 ✔️ VCE team is looking for a skilled Data Engineer. We process data coming from machines and factories in order to provide customized data products to our customers around the world. Our mission is to expose value of the data and support clients to become fully operational data driven company. At our company, data practitioners have the opportunity to work with various types of data coming through diverse channels with different frequency. Preferred hybrid work from the Wrocław office, but candidates from outside Wrocław will also be considered. ✔️ Competences and skills : Informatica Power Center Azure Databricks (Pyspark, Spark SQL, Unity Catalog, Jobs/Workflows) Advanced SQL Practical experience with at least one relational DBMS (SQL Server / Oracle / PostgreSQL) Azure DevOps (Repos, Pipelines, YAML) Azure Key Vault Azure Data Factory (optional) DBT (optional) ✔️ Soft skills: Open-minded Engaged and flexible Driver of topics - ready to find his/her way to develop or proceed with topics Good at collaboration with stakeholders from IT and business side Working in the past in the data mash environment (as a bonus) ✔️ We offer: B2B agreement Transparent working conditions with both Ework and the client Current support during our cooperation Possibility to work in an international environment Collaborative environment in Swedish organizational culture Private medical care Life insurance Multisport Teambuilding events","[{""min"": 120, ""max"": 128, ""type"": ""Net per hour - B2B""}]",Data Engineering,120,128,Net per hour - B2B
Full-time,Mid,B2B,Remote,636,Data Engineer,emagine Polska,"PROJECT INFORMATION: Industry: Banking Location: Remote Type of assignment: B2B RESPONSIBILITIES: As a Data Engineer, your core responsibility will be to manage data artifacts for integration. Develop Component Data Artifacts (CDAs). Break down Master Data Artifacts (MDAs) and Global Data Artifacts (GDAs) into CDAs for integration. Focus on data modeling, data reuse, and understanding data domains. Implement data normalization concepts. Efficiently query data and prepare it for integration into the data lake using Juniper pipelines. REQUIREMENTS: Strong technical understanding of Google Cloud Platform (GCP). Experience with Tableau or Looker. Experience with Apache Airflow or Hadoop. Proficiency in data modeling concepts. Experience with data normalization techniques. Knowledge of data integration methodologies. NICE TO HAVE: Understanding of Juniper pipelines. Knowledge of refinery data processes. OTHER DETAILS: The Data Engineer will collaborate closely with GCP engineers and platform teams. This role focuses on ensuring effective management of data modeling and reuse across various platforms.","[{""min"": 180, ""max"": 200, ""type"": ""Net per hour - B2B""}]",Data Engineering,180,200,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,637,Big Data Developer,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: 💻Hybrid work (2 days from the office) - Warszawa/Gdansk or Gdynia Responsibilities: Responsible for a successful design and implementation of a high-performing, flexible, robust, scalable and easily maintainable global reporting solution for the Bank Actively cooperating with Product Owner, Solution Architects, Analysts, Testers and Developers Working in an Agile team Requirements: 3+ years of experience with functional programming in Scala Experience with writing Spark-based applications in Scala Experience in containerized technologies including Docker and Kubernetes Deep understanding of Hadoop Technology Stack: Hive, Oozie, Kafka Strong communication skills and fluency in spoken and written English Experience within Agile ways of working Nice to have: Deep understanding of the principles of distributed systems Experience in data engineering and building ETL/ELT pipelines Experience with performance tuning of Hadoop/Spark solutions Experience in developing RESTful services and web applications with Bootstrap and ReactJS Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Senior,Permanent,Hybrid,638,Data Engineer,Vaillant Group Business Services,"What we achieve together We are looking for an experienced Data Engineer to join our innovative team at Vaillant. In this role, you will be instrumental in advancing our data infrastructure, driving analytics excellence, and leveraging cutting-edge technologies to transform data into actionable insights. If you are passionate about making a meaningful impact and eager to collaborate with a team that appreciates your expertise, we encourage you to apply. You will design, build, and maintain robust infrastructure and programs for efficient data extraction, transformation, loading, and serving. You will leverage Azure Cloud services, Spark technologies, and DataOps practices to handle large data volumes from diverse sources such as IoT, CRM, ERP, and PLM. You will play a pivotal role in supporting the design, development, maintenance, and automation of data products on our data platform, contributing significantly to data governance and security compliance. Ensuring data quality and accuracy is your priority, and you will implement validation and cleansing processes to achieve this. You will collaborate with a dynamic team of DevOps engineers, data scientists, and data engineers, and you will engage in advanced analytics of machine data, business process-related data, and customer data, enhancing Vaillant Group’s digital service portfolio. Your effective communication skills will shine as you work with cross-functional colleagues, sharing innovative ideas and fostering a collaborative environment. Agile methodologies are your preferred approach, allowing you to work seamlessly with the team and stakeholders. What makes us successful together Qualification: You hold a university degree, preferably in Computer Science or a related field, with a focus on Data Science, Data Mining, or Data Analytics. Experience: You bring at least 3 years of relevant experience in data analytics, particularly within a Big Data context. Know-how and skills: Your proficiency in Python (PySpark) and SQL is exceptional, and you are adept at working with cloud services and related components, ideally within the Microsoft Azure ecosystem and Databricks. Nice to have: Experience with additional data platforms and tools will be a plus. Personality: You thrive in a team setting, showcasing a high degree of initiative and motivation to excel in an agile and interdisciplinary environment. Language skills: Fluency in English is essential, enabling effective communication and collaboration across our global teams. What you can count on Hybrid work and environment that cares about your well-being: flexible working hours, hybrid work model and additional 2 days off per year for every employee. Package of additional benefits: private medical care, multi-sport card. Onboarding: our clearly structured onboarding process, including an Onboarding App, enables us to integrate new employees into Vaillant Group quickly and in a targeted manner. Constant learning opportunities for both technical and personal growth through a range of workshops, seminars, events, and e-learnings.","[{""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16000,20000,Gross per month - Permanent
Full-time,Senior,B2B,Remote,639,Senior Azure Data Engineer with Databricks,DCG,"As a recruitment company, DCG understands that every business is powered by experienced professionals. Our management style and partnership approach enable us to meet your needs and provide continuous support. Due to our ongoing growth and the large number of recruitment projects we undertake for our partners, we are currently looking for: Senior Azure Data Engineer with Databricks Responsibilities: Being responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems Building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies Evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards Driving creation of re-usable artifacts Establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation Working closely with analysts/data scientists to understand impact to the downstream data models Writing efficient and well-organized software to ship products in an iterative, continual release environment Contributing and promoting good software engineering practices across the team Communicating clearly and effectively to technical and non-technical audiences Defining data retention policies Monitoring performance and advising any necessary infrastructure changes Requirements: 3+ years’ experience with Azure Data Factory and Databricks 5+ years’ experience with data engineering or backend/fullstack software development Strong SQL skills Python scripting proficiency Experience with data transformation tools - Databricks and Spark Experience in structuring and modelling data in both relational and non-relational forms Experience with CI/CD tooling Working knowledge of Git English level: B2, C1 Nice to have: Experience with Azure Event Hubs, CosmosDB, Spark Streaming, Airflow Experience in Aviation Industry and Copilot Offer: Private medical care Co-financing for the sports card Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Senior,Permanent,Remote,641,Principal Software Engineer,dotLinkers,"Salary: up to 49 000 PLN/month Type of contract: UoP Working model: Remote Join our client, which makes software to help users organize data, discover the truth, and act on it. Their SaaS product manages large volumes of data and quickly identifies key issues during litigation and internal investigations. The AI-powered communication surveillance product proactively detects regulatory misconduct like insider trading, collusion, and other non-compliant behavior. They have over 300,000 users in 49 countries serving thousands of organizations globally. As a Principal Software Engineer you will serve as the key strategic and technical leader shaping the next generation of compute infrastructure. You’ll design scalable execution platforms supporting microservices, batch processing, streaming data pipelines, and long-running workflows—leveraging Azure technologies including AKS (Kubernetes), KEDA, Temporal, and Apache Spark. You’ll help define the broader platform architecture, influencing areas such as compute, storage, monitoring, and developer enablement. Working closely with engineering leaders and platform stakeholders, you’ll lead the evolution toward a scalable, cloud-native, and developer-friendly compute environment that supports a variety of workloads across the organization. Responsibilities: Develop and refine the technical roadmap for compute infrastructure, enabling scalability, flexibility, and reliability across multiple workload types. Design core cloud-native execution frameworks for workflows, stream processing, and high-volume batch processing. Lead the adoption of Kubernetes, KEDA, and Temporal to orchestrate compute workloads with strong observability and fault tolerance. Integrate advanced data processing tools like Apache Spark, Azure Stream Analytics, and event-driven architectures across product services. Guide multi-team transformations of legacy systems into scalable microservices, containerized workloads, or serverless solutions. Align compute architecture with business priorities in collaboration with product and platform teams, ensuring compliance and operational reliability. Mentor Staff and Lead Engineers, promoting best practices in scalable architecture, cloud infrastructure, and modern compute strategies. Participate in architectural reviews, contribute to design documentation, and support long-term technical planning. Advocate for platform quality, security, and a streamlined developer experience. Required Qualifications: 10+ years of experience in software engineering, infrastructure, or platform development with demonstrated leadership in architecture. Hands-on experience running large-scale, production-grade Kubernetes-based compute platforms. Strong knowledge of orchestration technologies like KEDA, Temporal, or similar workflow/job orchestration engines. Practical experience with both batch (e.g., Spark) and streaming (e.g., Kafka, Azure Event Hubs) data processing systems. Proficiency in multiple programming languages (Go, Python, C#, Rust) and infrastructure-as-code tools (Terraform, Pulumi). In-depth understanding of distributed systems, autoscaling strategies, and compute security standards. Proven ability to work cross-functionally with multiple engineering teams and contribute to broader platform strategy. Preferred Qualifications: Background in building internal developer platforms or compute services offered as products. Knowledge of Azure serverless technologies such as Azure Functions or Azure Container Apps. Familiarity with tools like Dapr, KEDA Scalers, and modern runtime technologies such as Wasm, Nomad, or OpenFaaS. Contributions to open-source projects within the cloud-native or CNCF ecosystems. Experience designing systems that are multi-region, multi-tenant, and support zero-downtime deployments. Leadership Expectations: Define the long-term vision for compute infrastructure, ensuring alignment with business and growth objectives. Provide architectural leadership and technical mentorship across engineering and product teams. Drive complex, multi-disciplinary initiatives covering compute, data, security, and reliability. Cultivate a culture of technical excellence, innovation, and continuous improvement. Mentor senior engineers and encourage platform thinking and service-oriented design. Core Skills: Visionary Architecture: Ability to craft and communicate future-proof compute strategies. Technical Authority: Expertise in modern compute technologies and the ability to resolve complex technical challenges. Cross-Functional Leadership: Comfortable working with technical leaders across infrastructure, platform, and product domains. Cloud-Native Expertise: Mastery of compute orchestration, workload management, and event-driven architectures on Azure. Balanced Innovation: Ability to combine cutting-edge technologies with pragmatic, reliable solutions. Benefit Highlights: Comprehensive health, dental, and vision plans Parental leave for primary and secondary caregivers Flexible work arrangements Two, week-long company breaks per year Unlimited time off Long-term incentive program Training investment program","[{""min"": 39000, ""max"": 49000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,39000,49000,Gross per month - Permanent
Full-time,Mid,B2B,Remote,643,Technical Application Support Engineer,SCALO,"W Scalo zajmujemy się dostarczaniem projektów software'owych i wspieraniem naszych partnerów w rozwijaniu ich biznesu. Tworzymy oprogramowanie, które umożliwia ludziom dokonywanie zmian, działanie w szybszym tempie oraz osiąganie lepszych rezultatów. Jesteśmy firmą, która wykorzystuje szerokie spektrum usług IT, żeby pomagać klientom. Obszary naszego działania to m.in.: doradztwo technologiczne, tworzenie oprogramowania, systemy wbudowane, rozwiązania chmurowe, zarządzanie danymi, dedykowane zespoły developerskie. Cześć! U nas znajdziesz to, czego szukasz - przekonaj się! W Scalo czeka na Ciebie: uczestnictwo w projekcie dot. branży Fintech. Wykorzystywany stos technologiczny: SQL, T-SQL, Neo4J, SOAP/XML, REST API, Tableau, SSRS, Power BI, Java, konfiguracja rozwiązań i produktów dla klientów, wsparciem drugiej linii (2nd level support) oraz modyfikacja skryptów, zapewnienie jakości przed przekazaniem klientowi: planowanie, definiowanie i realizacja testów, tworzenie dedykowanych raportów i dashboardów dla klientów, współpraca z zespołami developerskimi przy diagnozowaniu i rozwiązywaniu problemów, tworzenie i utrzymanie funkcjonalności opartych o bazę danych Neo4J z wykorzystaniem języka Cypher, komunikacja z klientami w celu diagnozowania problemów i proponowania rozwiązań, wsparcie po sprzedaży i po wdrożeniu – rozwiązywanie złożonych problemów, analiza i modyfikacja skryptów w języku Java (korekta błędów i tworzenie nowych), udział w rozwoju systemów w ramach zmian zgłaszanych przez obecnych klientów, stawka do 75 zł/h przy B2B w zależności od doświadczenia. Ta oferta jest dla Ciebie, jeśli: masz wykształcenie wyższe w zakresie informatyki, ekonomii lub matematyki, posiadasz minimum 2 lata doświadczenia zawodowego na podobnym stanowisku, posiadasz doświadczenie z Microsoft SQL, w tym czytania istniejącego kodu (T-SQL), masz doświadczenie z bazami danych Neo4J, posiadasz podstawową znajomość zagadnień związanych z sieciami komputerowymi, najlepiej z doświadczeniem w pracy z VPN, FTP i SFTP, znasz SOAP/XML i/lub REST/JSON, znasz narzędzia, np. Tableau, SSRS, Power BI, biegle komunikujesz się w języku angielskim (min. B2), mile widziana podstawowa znajomość Java. Co dla Ciebie mamy: długofalową współpracę - różnorodne projekty (działamy w ramach Software Services, Embedded Services, Data Services, Cloud Services), możliwość rozwoju kompetencji we współpracy z naszym Center of Excellence, kafeteryjny system benefitów – Motivizer, prywatną opiekę medyczną – Luxmed. Brzmi interesująco? Aplikuj – czekamy na Twoje CV!","[{""min"": 11760, ""max"": 12600, ""type"": ""Net per month - B2B""}]",Data Engineering,11760,12600,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,648,Senior Data Engineer with Snowflake (greenfield),N-iX,"#3359 Join an exciting journey to create a greenfield, cutting-edge Consumer Data Lake for a leading global organization based in Europe. This platform will unify, process, and leverage consumer data from various systems, unlocking advanced analytics, insights, and personalization opportunities. As a Senior Data Engineer, you will play a pivotal role in shaping and implementing the platform's architecture, focusing on hands-on technical execution and collaboration with cross-functional teams. Your work will transform consumer data into actionable insights and personalization on a global scale. Using advanced tools to tackle complex challenges, you’ll innovate within a collaborative environment alongside skilled architects, engineers, and leaders. Key Responsibilities: Hands-On Development : Build, maintain, and optimize data pipelines for ingestion, transformation, and activation. Create and implement scalable solutions to handle diverse data sources and high volumes of information. Data Modeling & Warehousing : Design and maintain efficient data models and schemas for a cloud-based data platform. Develop pipelines to ensure data accuracy, integrity, and accessibility for downstream analytics. Collaboration : Partner with Solution Architects to translate high-level designs into detailed implementation plans. Work closely with Technical Product Owners to align data solutions with business needs. Collaborate with global teams to integrate data from diverse platforms, ensuring scalability, security, and accuracy. Platform Development : Enable data readiness for advanced analytics, reporting, and segmentation. Implement robust frameworks to monitor data quality, accuracy, and performance. Testing & Quality Assurance : Implement robust security measures to protect sensitive consumer data at every stage of the pipeline Ensure compliance with data privacy regulations (e.g., GDPR, CCPA ..) and internal policies. Monitor and address potential vulnerabilities, ensuring the platform adheres to security best practices. Requirements: Over 4+ years of experience showcasing technical expertise and critical thinking in data engineering. Hands-on experience with DBT and strong Python programming skills. Proficiency in Snowflake and expertise in data modeling are essential. Demonstrated experience in building consumer data lakes and developing consumer analytics capabilities is required. In-depth understanding of privacy and security engineering within Snowflake , including concepts like RBAC, dynamic/tag-based data masking, row-level security/access policies, and secure views. Ability to design, implement, and promote advanced solution patterns and standards for solving complex challenges. Familiarity with multiple cloud platforms ( Azure or GCP preferred, with a focus on Azure). Practical experience with Big Data batch and streaming tools. Competence in SQL, NoSQL, relational database design (SAP HANA experience is a bonus), and efficient methods for data retrieval and preparation at scale. Proven ability to collect and process raw data at scale, including scripting, web scraping, API integration, and SQL querying. Experience working in global environments and collaborating with virtual teams. A Bachelor’s or Master’s degree in Data Science, Computer Science, Economics, or a related discipline. We offer*: Flexible working format - remote, office-based or flexible. A competitive salary and good compensation package. Personalized career growth. Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more). Active tech communities with regular knowledge sharing. Education reimbursement Memorable anniversary presents. Corporate events and team building. Other location-specific benefits. *not applicable for freelancers.","[{""min"": 22164, ""max"": 29553, ""type"": ""Net per month - B2B""}, {""min"": 17731, ""max"": 24566, ""type"": ""Gross per month - Permanent""}]",Data Engineering,22164,29553,Net per month - B2B
Full-time,Senior,B2B,Remote,652,Lead Data Engineer (Spark),Addepto,"Addepto is a leading consulting and technology company specializing in AI and Big Data, helping clients deliver innovative data projects. We partner with top-tier global enterprises and pioneering startups, including Rolls Royce, Continental, Porsche, ABB, and WGU. Our exclusive focus on AI and Big Data has earned us recognition byForbesas one of the top 10 AI consulting companies. As aLead Data Engineer, you will have the exciting opportunity to work with a team of technology experts on challenging projects across various industries, leveraging cutting-edge technologies. Here are some of the projects we are seeking talented individuals to join: Design and development of the platform for managing vehicle data for global automotive company.This project develops a shared platform for processing massive car data streams. It ingests terabytes of daily data, using both streaming and batch pipelines for near real-time insights. The platform transforms raw data for data analysis and Machine Learning, this empowers teams to build real-world applications like digital support and smart infotainment and unlocks data-driven solutions for car maintenance and anomaly detection across the organization. Design and development of a universal data platform for global aerospace companies.This Azure and Databricks powered initiative combines diverse enterprise and public data sources. The data platform is at the early stages of the development, covering design of architecture and processes as well as giving freedom for technology selection. This role represents a gradual shift away from hands-on coding towards a more strategic focus on system design, business consultation, and creative problem-solving. It offers an opportunity to engage more deeply with architecture-level decisions, collaborate closely with clients, and contribute to building innovative data-driven solutions from a broader perspective. 🚀 Your main responsibilities: Design and develop scalable data management architectures, infrastructure, and platform solutions for streaming and batch processing using Big Data technologies like Apache Spark, Hadoop, Iceberg. Design and implement data management and data governance processes and best practices. Contribute to the development of CI/CD and MLOps processes. Develop applications to aggregate, process, and analyze data from diverse sources. Collaborate with the Data Science team on data analysis and Machine Learning projects, including text/image analysis and predictive model building. Develop and organize data transformations using DBT and Apache Airflow. Translate business requirements into technical solutions and ensure optimal performance and quality. 🎯 What you'll need to succeed in this role: 5+ years of proven commercial experiencein implementing, developing, or maintaining Big Data systems. Strong programming skills inPythonorJava/Scala: writing a clean code, OOP design. Experience in designing and implementing data governance and data management processes. Familiarity withBig Datatechnologies likeSpark, Cloudera,Airflow, NiFi,Docker,Kubernetes,Iceberg, Trino or Hudi. Proven expertise in implementing and deploying solutions in cloud environments (with a preference forAWS). Excellent understanding of dimensional data and data modeling techniques. Excellent communication skills and consulting experience with direct interaction with clients. Ability to work independently and take ownership of project deliverables. Master’s or Ph.D. in Computer Science, Data Science, Mathematics, Physics, or a related field. FluentEnglish(C1 level) is a must. 🎁 Discover our perks & benefits: Work in a supportive team of passionate enthusiasts of AI & Big Data. Engage withtop-tier global enterprises and cutting-edge startups on international projects. Enjoy flexible work arrangements, allowing you towork remotelyor from modern offices and coworking spaces. Accelerate your professional growth throughcareer paths,knowledge-sharinginitiatives,languageclasses, and sponsoredtrainingorconferences, including a partnership withDatabricks, which offers industry-leading training materials and certifications. Choose from various employment options: B2B, employment contracts, or contracts of mandate. Make use of 20 fully paid days offavailable for B2B contractors and individuals under contracts of mandate. Participate inteam-building eventsand utilize theintegration budget. Celebratework anniversaries, birthdays,andmilestones. Accessmedicalandsports packages, eye care, and well-being support services, including psychotherapy and coaching. Get full work equipment for optimal productivity, including a laptop and other necessary devices. With our backing, you canboostyourpersonal brandby speaking at conferences, writing for our blog, or participating in meetups. Experience a smooth onboarding with a dedicated buddy, and start your journey in our friendly, supportive, and autonomous culture. Are you interested in Addepto and would like to join us? Get in touch! We are looking forward to receiving your application. Would you like to know more about us? Visit our website (career page) and social media (Facebook,LinkedIn,Instagram).","[{""min"": 21000, ""max"": 31920, ""type"": ""Net per month - B2B""}]",Data Engineering,21000,31920,Net per month - B2B
Full-time,Senior,B2B,Remote,653,Platform Integration Engineer (Salesforce & Qualtrics),ITDS,"As aPlatform Developer, you will be working for our client, a global leader in healthcare and agriculture. The client is dedicated to revolutionizing the digital experience across global markets by integrating state-of-the-art SaaS platforms into their customer and internal operations. Your contributions will directly support strategic initiatives, streamline business processes, and enable data-driven decision-making at a global scale. Working closely with IT, business stakeholders, and external vendors, you will architect scalable solutions that elevate both user engagement and data management capabilities. This is an opportunity to make a tangible impact in a fast-paced, mission-driven environment that’s transforming the future of crop science through digital platforms. Develop and distribute surveys using multiple channels including WhatsApp, SMS, and email Design and implement automated workflows for survey logic and data processes Support integration between Salesforce and Qualtrics based on business events Manage Salesforce workflows, rules, and data objects related to survey responses Collaborate with Salesforce development teams to address evolving integration needs Integrate survey platforms with Google Cloud to support data lakes and warehouse strategies Work with data teams to design ingestion strategies and perform gap analysis Configure AI-powered survey analytics and insights for reporting Partner with cross-functional teams to gather requirements and deliver technical solutions 5–7 years of experience in SaaS development and platform integration Proven expertise in Qualtrics platform administration and workflow configuration Strong knowledge of Salesforce configuration, workflows, and data architecture Basic knowledge of Salesforce Service Cloud Experience with integration tools, APIs, and cloud platforms like Google Cloud Solid understanding of data modeling, ingestion strategies, and architecture Proficiency in Apex, Lightning Components, SOQL, and Salesforce permissions management Knowledge of XML, HTML, CSS, SOAP/REST Ability to translate business requirements into scalable technical solutions Familiarity with AI analysis tools within survey platforms Strong collaboration and communication skills in cross-functional environments Experience working in Agile development frameworks We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious and driven people. The offer includes: Stable and long-term cooperation with very good conditions Enhance your skills and develop your expertise in the financial industry Work on the most strategic projects available in the market Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years Participation in Social Events, training, and work in an international environment Access to an attractive Medical Package Access to Multisport Program #GETREADY Internal job ID #7440 📌 You can report violations in accordance with ITDS’s Whistleblower Procedure availablehere.","[{""min"": 25200, ""max"": 29400, ""type"": ""Net per month - B2B""}]",Data Engineering,25200,29400,Net per month - B2B
Full-time,Senior,B2B,Hybrid,654,Data Engineer (Cybersecurity),Antal Sp. z o.o.,"Data Engineer – B2B Contract (Hybrid – Kraków or Warsaw) Location: Hybrid – 6 days per month on-site (preferably Kraków or Warsaw) Contract type: B2B Workload: Full-time We are looking for an experienced Data Engineer to join a team focused on data processing and analytics in the cybersecurity domain. This role involves designing and building robust data architecture and scalable data pipelines that power reporting, data analytics, and future machine learning models. Design, build, and test multi-layered data architecture and efficient, scalable data pipelines. Design, build, and test multi-layered data architecture and efficient, scalable data pipelines. Perform deep analysis of large, complex datasets to extract insights and generate statistical metrics for business reporting and data estate assessment. Perform deep analysis of large, complex datasets to extract insights and generate statistical metrics for business reporting and data estate assessment. Develop systems and software for data acquisition, aggregation, and refinement. Develop systems and software for data acquisition, aggregation, and refinement. Integrate data across various sources, systems, and platforms. Integrate data across various sources, systems, and platforms. Optimize query performance and overall data processing efficiency. Optimize query performance and overall data processing efficiency. Work on data ingestion, sourcing, aggregation, API integration, and feature engineering. Work on data ingestion, sourcing, aggregation, API integration, and feature engineering. Collaborate with stakeholders to align deliverables with business needs and better understand the data context. Collaborate with stakeholders to align deliverables with business needs and better understand the data context. Write clean, reusable, and maintainable code using the team's DevOps practices. Write clean, reusable, and maintainable code using the team's DevOps practices. Follow Agile methodologies, including test-driven development (TDD). Follow Agile methodologies, including test-driven development (TDD). Continuously grow technical and domain-specific skills in collaboration with the team. Continuously grow technical and domain-specific skills in collaboration with the team. At least 12 months of hands-on experience with Spark, PySpark, and/or Databricks (or a comparable modern data platform). At least 12 months of hands-on experience with Spark, PySpark, and/or Databricks (or a comparable modern data platform). Strong hands-on experience with Python and SQL in end-to-end data engineering workflows. Strong hands-on experience with Python and SQL in end-to-end data engineering workflows. Experience building and maintaining data pipelines and ETL workflows across disparate datasets. Experience building and maintaining data pipelines and ETL workflows across disparate datasets. Working knowledge of Azure DevOps, scripting (Azure CLI), Git/version control, and CI/CD processes. Working knowledge of Azure DevOps, scripting (Azure CLI), Git/version control, and CI/CD processes. Practical experience with Databricks. Practical experience with Databricks. Ability to communicate complex technical topics clearly and effectively to diverse audiences. Ability to communicate complex technical topics clearly and effectively to diverse audiences. A proactive, eager-to-learn attitude, especially regarding cybersecurity. A proactive, eager-to-learn attitude, especially regarding cybersecurity. Hands-on experience working within Agile teams. Hands-on experience working within Agile teams. B2B contract and support of the Contractor Care Team Private Medical Care Cafeteria system Life insurance Zapraszamy do odwiedzenia naszej strony www.antal.pl","[{""min"": 30000, ""max"": 33000, ""type"": ""Net per month - B2B""}]",Data Engineering,30000,33000,Net per month - B2B
Full-time,Mid,B2B,Remote,656,Senior BI Developer with Snowflake,Holisticon Connect,"Holisticon Connectis a division within NEXER GROUP - a custom software development company. We started in Poland in 2017 and are now a team of over 140 people. We have the opportunity to work with world-renowned brands from Scandinavia, the UK, and Western Europe. Our goal is to grow stronger, in competence rather than in numbers. If you like what we do, check out our offer, maybe we will have the pleasure of meeting you! 😊 Design, build, and develop data warehouses based on Snowflake; Create and optimize ETL/ELT processes using Matillion; Integrate data from various sources (databases, APIs, files); Develop reports and dashboards in Power BI for internal clients; Maintain, monitor, and further develop existing BI solutions; Collaborate with project teams and business stakeholders to gather and analyze requirements; Participate in data migration from legacy systems (e.g., Oracle, SSIS) to Snowflake; Ensure high-quality technical and business documentation; Implement best practices for data management, security, and version control; Actively participate in Agile team meetings (e.g., daily stand-ups, sprint planning, retrospectives); At least 4 years' experiencein a BI Developer role; Advanced and proven experience as a Snowflake developer,responsible for building ETL processes and creating tables and views within the Snowflake environment; Solid skills inPower BI; Eager to work as afull-stack BI Developer(both backend and frontend); StrongEnglish skills (min. C1 level, daily communication with international clients); Proactive and creative- skills to drive improvements and engage with both technical and non-technical stakeholders; Strong documentation skills and a knack for business analysis. Experience withMatillion; Experience withOracle(we’re migrating to Snowflake, but legacy knowledge is a plus); Experience withSSAS/SSIS/SSRS; Familiarity withVisual Studio; Understanding ofversion control concepts(branching, merging, pushing; Git integrated with Matillion). Background in procurement, supply chain, or business data analysis related to orders and internal corporate stakeholders; Background inManaged Servicesdelivery models - you know how to take end-to-end ownership of BI solutions, ensuring their reliability, scalability, and alignment with client needs throughout the entire lifecycle; Experience working inAgileteams. By joining us, you gain the following: Opportunity to work on exciting, international projects in cutting-edge industries like Automotive, Biotech, IoT; Possibility to develop in cloud technologies; Becoming part of a team that believes that the next step to a promising future is to put your heart into it and make it happen; Respect for your private lifeso you don't have to work overtime or on weekends; Team Events budget to socialize outside of work; Company Events to celebrate smaller and bigger successes (Summer Party, Programmer's Day, and trips abroad – so far we've been in Cape Town, Are, and Barcelona). Fully remotework or in our office in Wrocław; B2B Contract: 135 – 150 PLN net/hour + VAT Free benefits such asLuxmed,Multisport, andlife insurance in Nationale Nederlanden; Attractivereferral system(9,5k for senior, 6k for mid, 2,5k for junior); Personal Training Budgetwith additional paid hours; Passion Day -an extra day off for your hobby to spend as you please; Flexible working hourswith no micro-management approach. Our core hours are 9-15, the rest of the working time is up to you; We provide high-quality work equipment +2 additional monitorsand accessories.","[{""min"": 135, ""max"": 155, ""type"": ""Net per hour - B2B""}]",Data Engineering,135,155,Net per hour - B2B
Full-time,Senior,Permanent,Remote,657,Staff Data Engineer,Dropbox,"Dropbox is a special place where we are all seeking to fulfill our mission to design a more enlightened way of working. We’re looking for innovative talent to join us on our journey. The words shared by our founders at the start of Dropbox still ring true today. Wouldn’t it be great if our working environment—and the tools we use—were designed with people’s actual needs in mind? Imagine if every minute at work were well spent—if we could focus and spend our time on the things that matter. This is possible, and Dropbox is connecting the dots. The nearly 3,000 Dropboxers around the world have helped make Dropbox a living workspace - the place where people come together and their ideas come to life. Our 700+ million global users have been some of our best salespeople, and they have helped us acquire customers with incredible efficiency. As a result, we reached a billion dollar revenue run rate faster than any software-as-a-service company in history. Dropbox is making the dream of a fulfilling and seamless work life a reality. We hope you’ll join us on the journey. Dropbox is rebuilding the foundation of its monetization and financial data systems - and at the center of that is our revenue and growth data platform. We’re looking for aStaff Data Engineerto lead the design and delivery of this critical foundation, enabling insights and systems that drive ARR tracking, financial reporting, and product monetization. Architect the next-generation data platform for ARR, revenue attribution, and growth analytics - setting vision, driving alignment, and delivering at scale. Own and evolve core data models and systems used across Finance, Product, and Analytics teams, ensuring accuracy, trust, and accessibility. Lead platform modernization, including the adoption of scalable lakehouse architectures (Databricks, Spark, Delta), CI/CD for data, and observability frameworks. Drive adoption of scalable data practices across Dropbox through reusable tooling, process improvements, and cross-team collaboration. Partner with stakeholders (e.g., Finance, Product, Data Science, and Infrastructure) to understand data needs and deliver solutions that drive real business outcomes. Mentor and grow junior engineers, and cultivate a high-performing, innovation-driven team culture. BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience. 10+ years building large-scale data systems, with a demonstrated track record of technical leadership, including ownership of architectural direction and cross-team platform work. Proven ability to set architectural direction, lead platform evolution, and influence technical strategy across teams. Deep hands-on expertise with Spark, Spark SQL, and Databricks, along with experience orchestrating data pipelines using Apache Airflow, and writing performant, maintainable Python and SQL code. Track record of implementing data quality, testing, and observability systems at scale. Experience supporting monetization, financial, or product growth analytics with trusted and governed data models. Familiarity with cloud platforms (AWS, GCP, or Azure) and lakehouse paradigms (e.g., Delta Lake, Iceberg). Experience leading data migrations or platform transformations (e.g., from on-prem to cloud, Hadoop to Databricks). Familiarity with tools for data contracts, lineage, and governance (e.g., dbt, Monte Carlo, Great Expectations). Understanding of data privacy and compliance frameworks, including GDPR, SOX, and audit-readiness. Dropbox applies increased tax deductible costs to remuneration earned by certain qualifying employees (to the extent an employee will be involved in the creation of the software as an “author”) for the transfer of copyrights, in accordance with the relevant provisions of the Personal Income Tax Act. Poland Annual Pay Range 249 900 zł—338 100 zł The range listed above is the expected annual base salary/OTE (On-Target Earnings) for this role, subject to change. Please note, OTE are for sales roles only. Salary/OTE is just one component of Dropbox’s total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs). Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to: Competitive medical, dental and vision coverage* Retirement savings through a defined contribution pension or savings plan** Flexible PTO/Paid Time Off policy in addition to statutory holidays, allowing you time to unplug, unwind, and refresh Income Protection Plans: Life and disability insurance* Business Travel Protection: Travel medical and accident insurance* Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more Parental benefits including: Parental Leave, Fertility Benefits, Adoptions and Surrogacy support, and Lactation support Mental health and wellness benefits Additional benefits details are available upon request. Where group plans are not available, allowances may be provided Benefit, amount, and type are dependent on geographical location, based upon applicable law or company policy Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. A big part of that effort is our support for members and allies of internal groups like Asians at Dropbox, BlackDropboxers, enABLE, TODOS (Latinx), Pridebox (LGBTQ), Vets at Dropbox, and Women at Dropbox.","[{""min"": 20825, ""max"": 28175, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20825,28175,Gross per month - Permanent
Full-time,Senior,Permanent,Hybrid,658,Sr Data Engineer - Product Supply Analytics,Bayer Sp. z o.o.,"For Digital Hub Warsaw, we are looking for: Senior Data Engineer - Product Supply Analytics The PS Data & Analytics team at Bayer Consumer Health focuses on driving digital transformation and innovation by creating best-in-class analytical solutions that enable data-driven decision making and performance optimization for Bayer Consumer Health’s Supply Chain organization. You will be part of the data & analytics organization and will be responsible for building data products in the area of product supply and supply chain. You partner with business stakeholders, data architects, data scientists, analytics leads as well as other engineers. You will build data pipelines, data models and provision data for front end developers and data scientists. You will also make sure that proper development processes are followed within the team, enhance implementation frameworks and guide other team members in building scalable, secure and well performing data products. If you are interested in joining a young and dynamic team driving the digital transformation of Bayer Consumer Health, we would like to hear from you. Key Tasks & Responsibilities: Integrate data from different sources (e.g. supply chain planning data, financial data, quality data, distributor and transportation data, market data (sell-in, sell-out)) to develop globally harmonized data models & KPIs. Continuously enhance implementation frameworks based on the needs of the analytics products in your responsibility. Ensure that product supply data products adhere to the analytics data architecture guidance and deliver fit-for purpose & scalable analytical solutions. Guide other data engineers in your team and ensure that all engineers apply same design principles. Ensure that data is well-managed to build stable, reusable and quality assured data assets. Collaborate with other IT functions (enabling functions data asset teams, analytics teams, platform product managers & integration architects) to ensure the aforementioned activities are executed effectively. Together with the assigned data architect, ensure that cost and time estimations are accurate, quality of delivery is assured, and deliverables are properly handed over to the operations team Qualifications & Competencies (education, skills, experience): Bachelor/Master’s degree in Computer Science, Engineering, or a related field. 5+ years of working experience in the field of Data & Analytics, preferably in the area of product supply and the CPG industry Excellent data engineering & technology knowledge (Azure Data Lake Gen2, Azure Synapse, Databricks, Snowflake, potentially also legacy stack SAP Hana, SQL, Python as well as data management knowhow (data cataloguing, data quality management) Knowledge of CI/CD processes and tools (GitHub, Azure DevOps Pipelines) Profound data content knowledge (Supply Chain, Logistics, Quality Management) and Product Supply process knowhow Experience in Agile methodologies (Scrum, Kanban) Strong problem solving and analytical skills, combined with impeccable business judgment. Excellent interpersonal and communication skills, active listening, consulting, challenging, presentation skills. Fluent in English, both written & spoken, intercultural awareness and willingness to travel What do We offer: A flexible, hybrid work model Great workplace in a new modern office in Warsaw Career development, 360° Feedback & Mentoring programme Wide access to professional development tools, trainings, & conferences Company Bonus & Reward Structure VIP Medical Care Package (including Dental & Mental health) Holiday allowance (“Wczasy pod gruszą”) Life & Travel Insurance Pension plan Co-financed sport card - FitProfit Meals Subsidy in Office Additional days off Budget for Home Office Setup & Maintenance Access to Company Game Room equipped with table tennis, soccer table, Sony PlayStation 5 and Xbox Series X consoles setup with premium game passes, and massage chairs Tailored-made support in relocation to Warsaw when needed Please send your CV in English You feel you do not meet all criteria we are looking for? That doesn’t mean you aren’t the right fit for the role. Apply with confidence, we value potential over perfection WORK LOCATION: WARSAW AL.JEROZOLIMSKIE 158","[{""min"": 20000, ""max"": 28500, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,20000,28500,Gross per month - Permanent
Full-time,Senior,B2B,Remote,662,Senior Data Engineer (SSAS / BI),P&P Solutions,"📍 Warszawa lub Katowice / hybrydowo lub 100% zdalnie 💰 130–160 PLN/h netto B2B Dołącz do rozbudowanego HUB-u danych w sektorze bankowym , który wspiera kluczowe decyzje zarządcze i tworzy nowoczesne rozwiązania BI dla jednej z największych instytucji finansowych. Szukamy doświadczonego Senior Data Engineer’a , który specjalizuje się w SSAS, OLAP i Power BI i wniesie realną wartość do istniejącego zespołu. O projekcie Zasilisz zespół Reporting Enablement Product , który odpowiada za rozwój i utrzymanie data martu MI (Management Information) – centralnego źródła danych dla zarządu i kadry menedżerskiej. To międzynarodowy, multidyscyplinarny zespół (10 osób: data engineers, analitycy biznesowi, PO, Scrum Master), pracujący w modelu Agile. Dołączysz jako specjalista odpowiedzialny za rozwój modeli SSAS OLAP i tabular , optymalizację wydajności oraz wsparcie rozwiązań raportowych i analitycznych. Wymagania (must have) Min. 7 lat doświadczenia w pracy z SSAS (OLAP, tabular) Bardzo dobra znajomość SQL, MDX i DAX Doświadczenie w data warehousing, modelowaniu danych, procesach ETL Znajomość narzędzi wizualizacji danych: Power BI Umiejętność optymalizacji wydajności i analizy danych Komunikatywność i umiejętność pracy w zespole Angielski na poziomie min. B2 Praktyka w pracy w metodyce Agile/Scrum Tech stack w projekcie SSAS (SQL Server Analysis Services) SQL, MDX, DAX OLAP, tabular models, Power BI ETL, Data Warehousing, Performance tuning, Data security Obowiązki Projektowanie, wdrażanie i zarządzanie kostkami OLAP oraz modelami tabularnymi Optymalizacja zapytań i monitorowanie wydajności systemu Współpraca z zespołami danych w celu projektowania rozwiązań BI Rozwiązywanie problemów z modelami danych i SSAS Implementacja polityk bezpieczeństwa i procedur backupu Bieżące śledzenie trendów w technologiach SSAS i BI Dlaczego warto? ✔ Stabilny, rozbudowany projekt o dużym wpływie biznesowym ✔ Realny rozwój kompetencji w obszarze BI i danych korporacyjnych ✔ Praca z doświadczonym zespołem i nowoczesnymi narzędziami ✔ Możliwość pracy hybrydowej lub zdalnej (dla topowych kandydatów) Oferujemy Stawka do 160 zł/h netto na b2b Przelew w dogodnej formie Krótki 14-dniowy termin płatności faktury Bogaty pakiet usług prywatnej opieki medycznej Dostęp do platformy kafeteryjnej MyBenefit (umożliwiającej zamawianie kart Multisport, kart przedpłaconych do Ikea, Zalando, Notino i wielu innych)","[{""min"": 130, ""max"": 160, ""type"": ""Net per hour - B2B""}]",Data Engineering,130,160,Net per hour - B2B
Full-time,Mid,B2B,Remote,663,Data Engineer with Blockchain,DCG,"Responsibilities: Design, implement, and maintain scalable data pipelines using Azure Databricks, Spark, and PySpark Work with Delta Lake to manage large-scale data storage and optimize performance Develop robust data integration solutions using Azure Data Factory and Azure Functions Build and maintain structured and semi-structured data models, leveraging formats such as Parquet, Avro, and JSON Ensure efficient and secure data processing through proper performance tuning and code optimization Collaborate with development and analytics teams to support business data needs Apply version control best practices using Git and follow coding standards in Python and SQL Requirements: Strong hands-on experience with Azure Databricks, Spark, and PySpark Proficiency in building and tuning data pipelines with Delta Lake Solid understanding of data modeling and performance optimization techniques Practical experience with Azure Data Factory, Azure Functions, and Git Competence in working with data formats such as Parquet, Avro, and JSON Strong programming skills in Python and SQL Ability to work effectively in a fast-paced, enterprise-level environment Strong communication skills and fluency in spoken and written English (C1) Nice to have: Understanding of blockchain-related concepts and data structures Offer: Private medical care Co-financing for the sports card Training & learning opportunities Constant support of dedicated consultant Employee referral program","[{""min"": 150, ""max"": 170, ""type"": ""Net per hour - B2B""}]",Data Engineering,150,170,Net per hour - B2B
Full-time,Mid,Permanent,Hybrid,665,Senior Data Engineer,SIX,"Senior Data Engineer Warsaw | working from home up to 40% | Reference 7218 Are you passionate about the stock market and cutting-edge technology? At SIX, we operate one of the most advanced, innovative, and stable stock exchanges in the world — and we’re looking for a Senior Data Engineer to join our Data Management team. If you have a strong background in Python, SQL, databases, and cloud-based analytics, this is your opportunity to design and deliver high-impact data products and pipelines that power critical financial services. design, develop, and maintain our cloud-based data platform (Azure) for the Swiss Stock Exchange, aligned with our architecture standards lead the migration of legacy data solutions to modern cloud-based infrastructure conceptualize and implement transformation projects in the DWH and Big Data ecosystems collaborate with and support team members in building data-centric solutions, including direct involvement in solution design work cross-functionally with developers, testers, product owners, and business stakeholders in an agile setup (SCRUM & SAFe) 4+ years of experience building data pipelines on public cloud platforms such as Microsoft Azure, or with Hadoop (e.g., Hortonworks, Cloudera) proficiency in relational databases and SQL/NoSQL datastores; hands-on experience with data modeling, Spark, and preferably Databricks familiarity with agile methodologies (SCRUM, Kanban) and collaboration tools like Jira and Confluence a proactive, open mindset with a strong drive to learn and adapt to new technologies; excellent teamwork and communication skills strong customer focus with the ability to work independently and collaborate across diverse teams and cultures; fluency in English (German is a plus) sharing the costs of sports activities private medical care & life insurance sharing the costs of foreign language classes sharing the costs of professional training & courses remote work opportunities &flexible working time integration events &charity initiatives fruits and popcorn in the office video games at work, no dress code & leisure zone extra social benefits & holiday funds (Christmas/Easter gifts) meal and transportation allowance employee referral program Employee Assistance Program Day for U (Day for Medical Checkup) My Benefit Cafeteria Udemy for Business days for remote work from abroad If you have any questions, please call Gabriela Swiatek at +48 22 104 67 70. For this vacancy we only accept direct applicationsin English. Diversity is important to us. Therefore, we are looking to receiving applications regardless of any personal background.","[{""min"": 19000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,19000,27000,Gross per month - Permanent
Full-time,Mid,B2B,Remote,666,Data Engineer (pharma),7N,"Work Mode: Remote from the territory of Poland Expectations 3+ years of experience indata engineering,1+ year of experience inPythonwith data frameworks likePandas Proficiency inSQL,Python, andPandasfor data manipulation Experience indbtfor transformations Strong understanding ofdata modelling,ETL processes, anddata warehousing concepts Ability to set up and manage automated data pipelines usingGitLab CI/CD Experience withSnowflake's architecture, including warehouses, schemas, and security Ability to write efficientPythonscripts for data processing, leveragingPandasfor data wrangling and analysis Experience documenting data workflows Ability to collaborate effectively in cross-functional, international teams Sc., B.Eng. (or higher) in Computer Science, Data Engineering or related fields Excellentverbal and written communication skills (in English and Polish) Ongoing support from adedicatedagent, taking care of your project continuity, client contact, necessary formalities, work comfort and development, Consultant Development Program– advice on growth planning based on the latest trends and market needs in IT, including consultations withagents and growth mentors, Access to7N Learning & Development– a development and educational platform with webinars, a library of articles and industry reports, and regular invitations to one-time and recurring development events – technical, business, and lifestyle, Spectacular integration events, both for you (e.g.,annual Kick-Off trip, Christmas parties, or Summer Olympics sports events) and for your loved ones (e.g., family picnics, movie premieres), Professional developmentnot only during the project – you can get involved in knowledge transfer to others within the7N Servicesoffering directed at 7N clients, Relationships and access to the knowledge ofthe most experienced IT expertsin the market – the average professional tenure of our consultants in Poland is over 10 years, A complete benefits package, including funding for medical care, life insurance, sports cards for you and your loved ones, as well as discounts in stores in Poland and abroad.","[{""min"": 130, ""max"": 145, ""type"": ""Net per hour - B2B""}]",Data Engineering,130,145,Net per hour - B2B
Full-time,Mid,B2B,Hybrid,667,IPC Developer,ITDS,"Drive Innovation in Data Warehousing: IPC Developer wanted! Łódź based opportunity with remote work model (2 days in the office/month). As an IPC Developer , you will be working for our client, a leading player in the online banking sector, on the development and optimization of data warehouse solutions. This includes designing and implementing ETL processes, enhancing data architecture, and supporting the business intelligence environment to ensure effective data reporting and analysis. The project involves using cutting-edge technologies like Informatica Power Center and Oracle-based systems to support complex financial systems. You’ll be part of a dynamic IT team that ensures data integrity, performance, and scalability of large-scale data systems. Your main responsibilities: Design and implement ETL processes using Informatica Power Center Develop and maintain data warehouses and reporting data marts Participate in the implementation and integration of Informatica tools Design, implement and develop BI-class analytical environments Create and maintain Oracle databases and applications Define and document technical specifications and administrative documentation Test and validate software solutions created by others Prepare software installation packages Support the software release and handover to production teams You're ideal for this role if you have: Strong knowledge of Informatica Power Center for ETL process development Solid understanding of RDBMS Oracle 9i/10g and database design Excellent command of SQL and good knowledge of PL/SQL Understanding of information systems engineering and development methodologies At least 6 months of experience designing and implementing ETL solutions At least 1 year of experience with Oracle-based systems in information environments Practical experience designing data warehouses for large-scale institutions Ability to read and write technical documentation in English Strong analytical thinking and problem-solving skills Team-oriented mindset with attention to quality and detail Nice to have: Theoretical knowledge and practical experience with Big Data, Python, and Spark Familiarity with Business Intelligence (BI) concepts Ability to work using Agile methodologies (Scrum, Kanban) Knowledge of tools such as SQL Developer, SVN, GitHub, JIRA, and Confluence We offer you: ITDS Business Consultants is involved in many various, innovative and professional IT projects for international companies in the financial industry in Europe. We offer an environment for professional, ambitious, and driven people. The offer includes: Stable and long-term cooperation with very good conditions. Enhance your skills and develop your expertise in various industries. Work on the most strategic projects available in the market. Define your career roadmap and develop yourself in the best and fastest possible way by delivering strategic projects for different clients of ITDS over several years. Participate in Social Events, training, and work in an international environment. Access to attractive Medical Package. Access to Multisport Program. Access to Pluralsight. Flexible hours & remote work. You can report violations in accordance with ITDS’s Whistleblower Procedure available here . Ref. number 7154","[{""min"": 16800, ""max"": 22000, ""type"": ""Net per month - B2B""}]",Data Engineering,16800,22000,Net per month - B2B
Full-time,Senior,B2B,Hybrid,668,Ekspert ds. Hurtowni Danych,ITDS,"Dołącz do nas i zarządzaj danymi, które napędzają biznes! Lokalizacja: Gdańsk z możliwością pracy hybrydowej (8 dni/miesiąc w biurze) JakoStarszy Ekspert ds. Hurtowni Danych, będziesz pracować dla naszego klienta, dużej instytucji finansowej świadczącej usługi bankowe na rynku krajowym, zorientowanej na rozwój nowoczesnych rozwiązań raportowo-analitycznych. Dołączysz do zespołu odpowiedzialnego za utrzymanie i rozwój hurtowni danych oraz systemów wspierających procesy raportowania wewnętrznego i zewnętrznego. Twoja praca będzie mieć bezpośredni wpływ na jakość danych, efektywność procesów analitycznych oraz zgodność z wymaganiami regulatorów. Twoje główne obowiązki: Analizowanie wymagań biznesowych dotyczących systemów raportowo-analitycznych Opracowywanie specyfikacji biznesowych i technicznych na potrzeby rozwoju hurtowni danych Tworzenie i optymalizowanie procesów ETL w istniejących rozwiązaniach Przygotowywanie i wdrażanie raportów przy użyciu narzędzi BI, w tym MS Power BI Przeprowadzanie testów funkcjonalnych i integracyjnych wdrażanych rozwiązań Utrzymywanie serwisów zgodnie z wymaganiami SLA Rozwiązywanie incydentów i zgłoszeń użytkowników końcowych Współpracowanie z zespołami audytu oraz przedstawicielami instytucji nadzorczych Idealnie pasujesz do tej roli, jeśli: Masz co najmniej 4 lata doświadczenia w obszarze systemów raportowo-analitycznych Znasz systemy BI, w szczególności MS Power BI Znasz bazy danych Oracle i potrafisz efektywnie korzystać z ich zasobów Potrafisz pisać i optymalizować zapytania SQL Swobodnie posługujesz się pakietem MS Office Myślisz analitycznie i cechujesz się dużą dokładnością Potrafisz działać pod presją czasu i dotrzymywać terminów Posiadasz dobrą organizację pracy własnej Znasz język angielski na poziomie co najmniej B2 Dodatkowym atutem będzie, jeśli masz: Doświadczenie w pracy z narzędziami ETL, w szczególności Informatica Doświadczenie w prowadzeniu projektów lub koordynowaniu zadań zespołu Znajomość procesów raportowania regulacyjnego w instytucjach finansowych Znajomość zagadnień związanych z hurtowniami danych w sektorze bankowym Doświadczenie we współpracy z audytorami wewnętrznymi i zewnętrznymi","[{""min"": 900, ""max"": 1100, ""type"": ""Net per day - B2B""}]",Data Engineering,900,1100,Net per day - B2B
Full-time,Mid,Permanent or B2B,Hybrid,9,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for aData Engineerto join us to provide value to our customers in line with the Volue mission. In Volue Insight we enable our customers to make data driven decisions – spanning from creating pricing models for their energy products, when to buy energy, to invest in renewable power plants or power-consuming industry. In the fuels team, we provide actuals and forecasts for prices, production levels, flows for the gas markets and conventional power plant operators. We build and maintain data pipelines, collect and process data from external sources, craft mathematical models, analyze time series, train machine learning models, build web applications, and enjoy working together. What you will be doing to make a difference: Design, build and maintain flexible and scalable end-to-end data pipelines for forecasting and prediction models. Contribute to our continuous push towards highly scalable and automated data pipelines and forecasting models where performance is monitored, quality is continuously evaluated, and experimentation is easy. Take part in the entire lifecycle of our models, from initial concept to deployment and ongoing maintenance, ensuring reliability and performance. Implement automated tests, participate in peer code reviews, and embrace continuous integration practices to ensure robust, maintainable code. What you need to succeed: A Bachelor’s or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Reasons to join Volue team and what we offer: Large degree of influence in shaping and developing the role further Great colleagues in one of Europe’s most exciting green tech companies with innovative and international work environment Flexible working hours and competitive compensation package, which includes a Multisport card, group life insurance, private healthcare, English classes, memorable offsite events, outstanding referral programme and access to various sports groups. In Volue, we cherish each employee’s competence, ideas and personality. Let your skills and talent be a part of our team – and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,12000,20000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,46,Data Privacy and Compliance Specialist,OChK,"Data Privacy and Compliance Specialist Miejsce pracy: Warszawa / hybrydowo Poziom stanowiska: Intermediate 10.000 - 13.000 brutto UoP lub umowa B2B Twój zakres obowiązków: udział w negocjacjach i opiniowaniu umów z klientami oraz dostawcami, w zakresie zapisów dotyczących ochrony danych i bezpieczeństwa informacji, wsparcie w ocenie i monitorowaniu bezpieczeństwa łańcucha dostaw, ze szczególnym uwzględnieniem aspektów związanych z przetwarzaniem danych osobowych, udział w kontrolach zgodności, audytach wewnętrznych i zewnętrznych oraz wdrażanie działań korygujących i zapobiegawczych, przygotowywanie, aktualizacja i rozwój dokumentacji z zakresu bezpieczeństwa informacji (m.in. polityki, procedury, klauzule, rejestry), wsparcie zespołów biznesowych w analizie ryzyka oraz ocenie i zapewnianiu zgodności planowanych działań z obowiązującymi regulacjami, weryfikacja zgodności przetwarzania danych osobowych w procesach biznesowych i systemach IT, prowadzenie szkoleń i działań edukacyjnych dla pracowników w obszarze bezpieczeństwa informacji, w tym ochrony danych, monitorowanie zmian w przepisach prawa i regulacjach oraz inicjowanie i wdrażanie niezbędnych działań dostosowawczych. Nasze wymagania: wykształcenie wyższe (preferowane: prawnicze, cyberbezpieczeństwo), doskonała znajomość przepisów oraz norm i standardów dotyczących bezpieczeństwa informacji oraz ochrony danych osobowych, z uwzględnieniem zagadnień dotyczących chmury obliczeniowej oraz sztucznej inteligencji, znajomość języka angielskiego na poziomie minimum B2, co najmniej 3-letnie doświadczenie na stanowisku związanym z bezpieczeństwem informacji/ochroną danych osobowych, certyfikaty poświadczające wiedzę z zakresu bezpieczeństwa informacji oraz ochrony danych osobowych – mile widziane. W OChK: pracujemy zadaniowo w trybie hybrydowym (nowoczesne biuro przy ul. Grzybowskiej), działamy w zwinnym środowisku, z wykorzystaniem aplikacji zwiększających efektywność (m. in. Google Workspace, Slack, GitHub, Jira), inwestujemy w Twój rozwój poprzez finansowanie szkoleń i certów, a od pierwszego dnia pracy udostępniamy platformy edukacyjne Google i Microsoft, oferujemy prywatne ubezpieczenie medyczne, preferencyjne warunki ubezpieczenia grupowego oraz kartę Multisport organizujemy i współfinansujemy naukę języka angielskiego, udostępniamy program poleceń, dzięki któremu pracownicy zyskują dodatkowe bonusy za skuteczną rekomendację kandydatów do pracy, cenimy proaktywność i inicjatywę własną, dlatego wspieramy autonomię w podejmowaniu decyzji, budujemy kulturę organizacyjną na wartościach takich jak profesjonalizm, współodpowiedzialność i wzajemny szacunek, przykładamy dużą wagę do efektywnego onboardingu, podczas którego w luźnej atmosferze i ze wsparciem Twojego CloudBuddiego poznajesz zespół, firmę i swoje obowiązki, stawiamy na integrację zespołów podczas różnorodnych inicjatyw, zarówno firmowych jak i oddolnych, które pomagają nam lepiej się poznawać oraz budować i utrzymywać dobrą atmosferę współpracy.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 13000, ""type"": ""Gross per month - Permanent""}]",Unclassified,10000,13000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,70,Senior Data Engineer,N-iX,"We are seeking a Senior Data Engineer specializing in Databricks to join our global team. You will be instrumental in setting up and maintaining our Databricks platform, building robust data pipelines, and collaborating closely with our solution architects and data scientists. Your expertise will directly support our mission to leverage data and AI effectively within a cutting-edge automotive claims management environment. Key Responsibilities: Design, build, and maintain robust data pipelines within Databricks. Collaborate closely with international teams, including data scientists and architects, to develop scalable data solutions. Debug complex issues in data pipelines and proactively enhance system performance and reliability. Set up Databricks environments on cloud platforms (Azure/AWS). Automate processes using CI/CD practices and infrastructure tools such as Terraform. Create and maintain detailed documentation, including workflows and operational checklists. Develop integration and unit tests to ensure data quality and reliability. Migrate legacy data systems to Databricks, ensuring minimal disruption. Participate actively in defining data governance and management strategies. What We Expect from You (Requirements): 5+ years of proven experience as a Data Engineer. Advanced proficiency in Python for developing production-grade data pipelines. Extensive hands-on experience with Databricks platform. Strong knowledge of Apache Spark for big data processing. Familiarity with cloud environments, specifically Azure or AWS. Proficiency with SQL and experience managing relational databases (MS SQL preferred). Practical experience with Airflow or similar data orchestration tools. Strong understanding of CI/CD pipelines and experience with tools like GitLab. Solid skills in debugging complex data pipeline issues. Proficiency in structured documentation practices. B2 level or higher proficiency in English. Strong collaboration skills, ability to adapt, and eagerness to learn in an international team environment. Nice to have: Experience with Docker and Kubernetes. Familiarity with Elasticsearch or other vector databases. Understanding of DBT (data build tool). Ability to travel abroad twice a year for on-site workshops. Why Join Us Work on impactful projects with cross-functional teams. Opportunity to grow your BI and analytics career in a data-driven organization. Flexible working hours and remote work options. Competitive compensation and benefits. Opportunity to work on presales We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 29553, ""max"": 30661, ""type"": ""Net per month - B2B""}, {""min"": 24381, ""max"": 25489, ""type"": ""Gross per month - Permanent""}]",Data Engineering,24381,25489,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,93,Senior Data Engineer,N-iX,"#3204 Join our team to work on enhancing a robust data pipeline that powers our SaaS product, ensuring seamless contextualization, validation, and ingestion of customer data. Collabd engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Client was established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client’s platform rates with product teams to unlock new user experiences by leveraging data insights. Engage with domain experts to analyze real-world empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renowned Scandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities : Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements : Programming: Minimum of 3-4 years as a data engineer, or in a relevant field Python Proficiency: Advanced experience in Python, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights Cloud: Familiarity with cloud platforms (preferably Azure) Data Platforms: Experience with Databricks, Snowflake, or similar data platforms Database Skills: Knowledge of relational databases, with proficiency in SQL. Big Data: Experience using Apache Spark Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability Version Control: Experience with Gitlab or equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have : Experience with Docker and Kubernetes Experience with document and graph databases Ability to travel abroad twice a year for an on-site workshops","[{""min"": 18359, ""max"": 30993, ""type"": ""Net per month - B2B""}, {""min"": 14776, ""max"": 26228, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14776,26228,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Hybrid,117,Data Scientist,Link Group,"Poszukujemy doświadczonego Data Scientista do zespołu inżynieryjnego międzynarodowej firmy technologicznej specjalizującej się w rozwiązaniach automatyzujących procesy finansowe i księgowe. Firma od ponad 20 lat rozwija własną platformę SaaS wykorzystywaną globalnie przez działy finansowe dużych organizacji. Projekt skupia się na budowaniu nowoczesnych, skalowalnych rozwiązań AI/ML wspierających procesy księgowe i finansowe, z wykorzystaniem Python, TensorFlow lub PyTorch, Google Cloud Platform oraz MLOps. Na co dzień Data Scientist w tym zespole odpowiada za projektowanie, trenowanie i wdrażanie modeli uczenia maszynowego, budowanie pipelines danych, integrację mikroserwisów w środowisku chmurowym oraz rozwój usług AI w bliskiej współpracy z product ownerami, innymi inżynierami i zespołami cloudowymi. Ważny jest tu nie tylko mocny background techniczny, ale także umiejętność szukania rozwiązań w otwartych, nieoczywistych problemach i dzielenia się wiedzą wewnątrz zespołu. Wymagania: Bardzo dobra znajomość Python, SQL, Spark Doświadczenie w budowie i wdrażaniu modeli ML (klasyfikacja, klasteryzacja, prognozowanie) Znajomość TensorFlow i/lub PyTorch Praktyka w pracy z GCP lub inną dużą chmurą (AWS, Azure) Doświadczenie z MLOps, pipelines, kontrolą wersji (Git) Wykształcenie wyższe kierunkowe (Informatyka, Statystyka, Data Science) Bardzo dobra znajomość języka angielskiego – komunikacja wewnętrzna w firmie odbywa się w tym języku Mile widziane: Doświadczenie z GCP (BigQuery, Vertex AI) Znajomość specyfiki usług finansowych lub rozwiązań dla księgowości Wiedza z zakresu generative AI vs AI agents Informacje organizacyjne: 📍 Lokalizacja: Kraków – model hybrydowy (2 dni w tygodniu z biura) 💬 Wymagana bardzo dobra znajomość języka angielskiego 📑 Forma współpracy: Umowa o pracę na czas nieokreślony lub B2B (w zależności od preferencji) 🗓 Start: możliwie jak najszybciej 💼 Proces rekrutacyjny: rozmowy techniczne oraz spotkanie z przedstawicielem firmy","[{""min"": 21000, ""max"": 25000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Science,21000,25000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,124,Senior Data Engineer (Azure/Fabric),Onwelo,"🟠 Poznaj Onwelo: Onwelo to nowoczesna polska spółka technologiczna, która specjalizuje się w budowaniu innowacyjnych rozwiązań IT dla organizacji z szeregu sektorów na całym świecie. Główne obszary działalności Onwelo to: tworzenie oprogramowania, jego rozwój oraz utrzymanie, a także mocne wsparcie kompetencyjne. W krótkim czasie firma wdrożyła ponad 300 projektów w Europie i w USA, a także otworzyła biura w siedmiu miastach Polski oraz oddziały w Stanach Zjednoczonych, Niemczech i w Szwajcarii. 🚀 O projekcie: Do naszego zespołu Data & Analytics poszukujemy doświadczonego Azure/Fabric Data Engineera, który będzie wspierał naszych klientów w planowaniu, budowie i wdrażaniu nowoczesnych rozwiązań danych w środowisku Microsoft Azure i Fabric. Będziesz pracować w zespole z ekspertami od analizy danych, chmury i architektury, w środowisku międzynarodowym i projektach o dużej skali. . 🎯 Z nami będziesz: Projektować i wdrażać rozwiązania oparte na Microsoft Fabric – nowoczesnej platformie analitycznej łączącej dane, raportowanie i orkiestrację w jednym środowisku Planować i przeprowadzać orkiestrację danych w środowisku Microsoft Azure oraz Fabric Budować, rozwijać i wdrażać nowoczesną hurtownię danych w oparciu o Databricks, Data Vault 2.0, Python i PySpark Tworzyć i optymalizować potoki danych oraz procesy ETL/ELT zasilające hurtownie danych Projektować modele danych wspierające analitykę biznesową i raportowanie w Power BI Wdrażać rozwiązania z wykorzystaniem Microsoft Fabric, Azure Data Factory, Synapse, Data Lake, Azure SQL Przeprowadzać analizę danych i projektować modele danych wspierające cele biznesowe Wspierać innych członków zespołu – technicznie i merytorycznie Monitorować jakość i efektywność przepływów danych oraz optymalizować je pod kątem kosztów i wydajności 😎 Czekamy na Ciebie, jeśli: Masz minimum 5-letnie doświadczenie jako Data Engineer – w projektach związanych z integracją danych, modelowaniem i budową hurtowni Masz praktyczne doświadczenie z Microsoft Fabric lub chcesz rozwijać się w tym obszarze i szybko się uczysz Pracujesz z usługami chmurowymi Azure, w tym: Azure Data Factory, Azure Databricks, Azure SQL, Data Lake Znasz SQL na poziomie eksperckim Biegle posługujesz się językami Python i/lub PySpark Rozumiesz architekturę nowoczesnych hurtowni danych (np. Data Vault 2.0 ) Masz wyższe wykształcenie techniczne (np. informatyka, matematyka, inżynieria danych) Komunikujesz się po angielsku na poziomie min. B2 (część projektów i zespołów jest międzynarodowa) 🤝 Dowiedz się, jak skorzystasz, będąc w Onwelo: Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Potrzebujesz pracować zdalnie? Jesteśmy otwarci! Zaoszczędzisz czas na dojazdach – pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych i zewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15000,18000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,136,Backend Engineer (Data & AI),Appliscale,"About the role Our client is an early-stage, venture-backed startup transforming the $800B franchising industry. Their innovative AI platform helps leading franchise brands automate operations, leverage data insights, and scale faster and smarter. We're hiring a product-focused Backend Software Engineer eager to build impactful solutions, work closely with AI technology, and thrive in a fast-paced startup environment. This is an opportunity to contribute directly to product development, seeing your code and ideas quickly in users' hands. Responsibilities Please note, availability to attend daily afternoon/evening meetings is a specific requirement for this role as most of the team is located in the US Develop scalable backend systems, data pipelines, and APIs using Python, TypeScript, and AWS infrastructure Collaborate cross-functionally with product, ML, and infrastructure teams to integrate and deploy AI features in a multi-tenant SaaS environment Required qualifications Minimum of 2 years full-time commercial backend software development experience, ideally with Python and TypeScript Bachelor's or higher degree in Computer Science, Software Engineering, or a related field Comfortable building and managing services in AWS environments (EC2, Lambda, ECS, Airflow) Experienced using AutoML frameworks, time-series DBs Product-minded engineer who enjoys collaborating closely with product and business teams Startup-oriented: thrives in ambiguity, eager to learn quickly, iterate fast, and build impactful solutions Excellent communication skills and high fluency in English, it’s our daily business language Nice to have AI-curious: experience deploying ML models into production is a strong plus but not required","[{""min"": 14000, ""max"": 20000, ""type"": ""Net per month - B2B""}, {""min"": 10000, ""max"": 16000, ""type"": ""Gross per month - Permanent""}]",Data Science,10000,16000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,138,Senior Data Science/AI Engineer,N-iX,"Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management.Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact.Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company’s R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 21700, ""max"": 29000, ""type"": ""Net per month - B2B""}, {""min"": 17500, ""max"": 23950, ""type"": ""Gross per month - Permanent""}]",Data Science,17500,23950,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,144,Senior Data Engineer with Databricks,Crestt,"Cześć! Poszukujemy osoby na stanowisko Senior Data Engineer , która dołączy do zespołu naszego Klienta zajmującego się projektowaniem i rozwijaniem nowoczesnych rozwiązań w obszarze Data Lakehouse, Business Intelligence oraz zaawansowanej analityki w chmurze. Współpracując w międzynarodowym środowisku, będziesz miał(a) okazję pracować z najnowszymi technologiami w tej dziedzinie. Lokalizacja : praca głównie zdalna (1x w miesiącu spotkanie w biurze w Warszawie) Widełki: B2B 160-200 pln netto+vat/h UoP 26-30 tys. brutto/mies. Wymagania: Biegłość w SQL oraz Pythonie (min. 5 lat doświadczenia) Co najmniej 2-letnie doświadczenie w pracy z Databricks Doświadczenie w pracy w środowisku chmurowym (preferowany Azure) Minimum 5-letnie doświadczenie w projektowaniu oraz implementacji rozwiązań klasy BI, ETL/ELT, Data Warehouse, Data Lake, Big Data oraz OLAP Praktyczna znajomość zarówno relacyjnych, jak i nierelacyjnych Doświadczenie z narzędziami typu Apache Airflow, dbt, Apache Kafka, Flink, Azure Data Factory, Hadoop/CDP Znajomość zagadnień związanych z zarządzaniem danymi, jakością danych oraz przetwarzaniem wsadowym i strumieniowym Umiejętność stosowania wzorców architektonicznych w obszarze danych (Data Mesh, Data Vault, Modelowanie wymiarowe, Medallion Architecture, Lambda/Kappa Architectures) Praktyczna znajomość systemów kontroli wersji (Bitbucket, GitHub, GitLab) Wysoko rozwinięte umiejętności komunikacyjne, otwartość na bezpośredni kontakt z Klientem końcowym Certyfikaty z Databricks lub Azure będą dodatkowym atutem Zakres obowiązków: Projektowanie i wdrażanie nowych rozwiązań oraz wprowadzanie usprawnień w istniejących platformach danych Udział w rozwoju platform danych i procesów ETL/ELT, optymalizacja przetwarzania dużych zbiorów danych zgodnie z najlepszymi praktykami inżynierii danych Standaryzacja i usprawnianie procesów technicznych – implementacja standardów kodowania, testowania i zarządzania dokumentacją Dbanie o jakość kodu i zgodność z przyjętymi standardami – przeprowadzanie regularnych code review Aktywna współpraca z innymi ekspertami technologicznymi, w celu doskonalenia procesów oraz identyfikacji nowych wyzwań technologicznych Mentoring i wsparcie zespołu w zakresie projektowania rozwiązań, optymalizacji procesów i wdrażania najlepszych praktyk Klient oferuje: Udział w międzynarodowych projektach opartych na najnowocześniejszych technologiach chmurowych Pokrycie kosztów certyfikacji (Microsoft, AWS, Databricks) 60 płatnych godzin rocznie na naukę i rozwój Możliwość wyboru między pracą zdalną a spotkaniami w biurze Indywidualnie dopasowane benefity: prywatna opieka medyczna, dofinansowanie karty sportowej, kursy językowe, premie roczne i medialne oraz bonus za polecenie nowego pracownika (do 15 000 PLN)","[{""min"": 25600, ""max"": 32000, ""type"": ""Net per month - B2B""}, {""min"": 26000, ""max"": 30000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,26000,30000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,165,👉 Data Platform Architect,Xebia sp. z o.o.,"🟣You will be: 🟣Your profile: extensive experience working with Azure cloud provider, 🟣Recruitment Process: CVreview –HRcall –Technical Interview–ClientInterview (with Live-coding) –Hiring ManagerInterview –Decision 🎁Benefits 🎁 ✍Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 29800, ""max"": 36800, ""type"": ""Net per month - B2B""}, {""min"": 23900, ""max"": 29700, ""type"": ""Gross per month - Permanent""}]",Data Architecture,23900,29700,Gross per month - Permanent
Full-time,Manager / C-level,Permanent or B2B,Remote,179,Data Engineering Team Leader,Profitroom,"We are currently looking for an experiencedData Engineering Team Leaderto join our Data Team and help us make our company even more data-driven. The results of your work will directly impact product development, the way we support our customers, and influence our high-level business strategy. If you are ready to take initiative and believe in data-driven decision-making – this role is perfect for you! Serve as a technical and team leader in the Data Engineering team: Lead team development and foster soft people management Support career paths and mentor team members Act as a Delivery Manager for data-related projects: Define, prioritize, and ensure timely delivery of engineering tasks Take ownership of major technical decisions and engineering excellence: Establish and enforce standards for testing, code review, CI/CD, documentation, and monitoring Oversee the architecture of our data platform: Maintain and evolve our Lakehouse infrastructure (preferably Databricks-based) Introduce new tools and technologies aligned with business and technical goals Supervise the logical structure and modeling of data: Ensure semantic and architectural consistency of datasets Leverage approaches such as the Kimball model or Medallion architecture Contribute to coding: Develop and optimize data pipelines using Dagster, Python, and PySpark Collaborate cross-functionally with data analysts, PMs, and other business stakeholders Drive and mature data governance practices, including cataloguing and lineage Minimum of 3+ years of experience as a Data Engineer or in a similar role related to data Experience in leading technical teams or managing data projects = at least 1 year (a big advantage) or willingness to grow into it Strong knowledge of Python, PySpark (nice to have), orchestration tools like Dagster or Airflow and SQL Experience working with cloud data platforms (Databricks experience is a plus) and Lake/Lakehouse architectures Ability to define and uphold high engineering standards and processes Proficiency in data modeling (Kimball, Medallion) Excellent communication skills (English at B2+ level) Strong ownership and self-organization Nice to have: Experience with Databricks and Delta Lake Familiarity with tools such as DataHub, Terraform, Docker Background in implementing data governance, lineage, and quality frameworks Python, PySpark, SQL, Databricks, GCP (BigQuery), Dagster, Airflow, Delta Lake, Docker, Terraform, DataHub Enjoy Work-Life Balance: Embrace a fully remote and flexible work environment. Explore the World: Avail annual 'Work with Us, Travel with Us' vouchers. Grow Your Skills: Access to English language classes along with a dedicated team development fund. Stay Healthy: Benefit from co-financed life and medical insurance, access sports facilities and receive professional mental health support whenever needed. Take Time Off: Get 26 days off with a Contract of Employment and 24 days off break with B2B contracts. Share hospitality: Take 2 extra days off (annually) for CSR activities. Join Celebrations: Participate in company retreats, events, and wedding & baby packs, benefit from our employee referral program. Transparent Culture: Experience a flat hierarchy and open communication channels for transparency. Contract Enhancements: earn between 25 500 PLN to 30 000 PLN on a B2B contract or between 21 000 to 25 000 PLN gross for Contract of Employment. About Us: We're a global leader in hospitality software, founded in Poznań, Poland in 2008. We’ve grown to serve over 3,500 customers across five continents, helping hotels and resorts maximize their revenue and guest satisfaction.","[{""min"": 25500, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 21000, ""max"": 25000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,21000,25000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,194,👉 Senior AWS Data Engineer (Future Opening),Xebia sp. z o.o.,"🟣 You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. 🟣 Your profile: 3+ years’ experience with AWS (Glue, Lambda, Redshift, RDS, S3), 5+ years’ experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools – Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, working knowledge of Git, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ 🟣 Nice to have: experience with Amazon EMR and Apache Hadoop, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification, 🟣 Recruitment Process: CV review – HR call – Interview (with Live-coding) – Client Interview (with Live-coding) – Hiring Manager Interview – Decision 🎁 Benefits 🎁 ✍ Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺 We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️ We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16600,25900,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,259,Senior Data Engineer (Microsoft),Onwelo,"🟠 Poznaj Onwelo: Onwelo to nowoczesna polska spółka technologiczna, która specjalizuje się w budowaniu innowacyjnych rozwiązań IT dla organizacji z szeregu sektorów na całym świecie. Główne obszary działalności Onwelo to: tworzenie oprogramowania, jego rozwój oraz utrzymanie, a także mocne wsparcie kompetencyjne. W krótkim czasie firma wdrożyła ponad 300 projektów w Europie i w USA, a także otworzyła biura w siedmiu miastach Polski oraz oddziały w Stanach Zjednoczonych, Niemczech i w Szwajcarii. 🚀 O projekcie: Szukamy doświadczonego Microsoft Data Engineera , który poprowadzi techniczne aspekty projektów i będzie kluczowym łącznikiem między IT a biznesem. Pracując z nami, będziesz mieć realny wpływ na sposób, w jaki przetwarzane są dane kluczowe dla biznesu i klientów. Jeśli masz doświadczenie w hurtowniach danych i lubisz wyzwania związane z presales oraz definiowaniem wymagań , dołącz do nas i współtwórz rozwiązania. 🎯 Z nami będziesz: Projektować, rozwijać i optymalizować hurtownie danych opartych na technologii Microsoft SQL Server Implementować i utrzymywać procesy ETL przy użyciu SSIS Tworzyć raporty i analizy z wykorzystaniem SSRS oraz modele wielowymiarowe i tabelaryczne w SSAS Integrować dane z różnych źródeł, w tym Oracle Optymalizować wydajności zapytań SQL oraz procesów przetwarzania danych Aktywnie współpracować z klientami w celu zbierania i definiowania wymagań biznesowych oraz ich przekładania na rozwiązania techniczne Prowadzić działania presales – przygotowywanie ofert, udział w spotkaniach z klientami, doradztwo w zakresie architektury danych Koordynować i nadzorować pracę zespołu developerskiego, pełnić rolę lidera technicznego 😎 Czekamy na Ciebie, jeśli: Masz m in. 5 lat doświadczenia w pracy z hurtowniami danych oraz rozwiązaniami Microsoft BI Bardzo dobra znasz SQL Server , w tym mechanizmów przechowywania i przetwarzania danych Posiadasz Doświadczenie w pracy z SSIS, SSRS, SSAS oraz umiejętność efektywnego wykorzystywania tych narzędzi. Pracujesz również z innymi źródłami danych, w szczególności Azure , Oracle Potrafisz prowadzić projekty i współpracować z biznesem – umiejętność definiowania wymagań, rekomendowania rozwiązań oraz prezentowania wyników. Masz doświadczenie w działaniach presales , tworzeniu ofert i doradztwie technologicznym. Możesz pochwalić się umiejętnością zarządzania zespołem oraz mentoringu młodszych członków zespołu. Znasz języka angielskiego na poziomie min. B2 🤝 Dowiedz się, jak skorzystasz, będąc w Onwelo: Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Potrzebujesz pracować zdalnie? Jesteśmy otwarci! Zaoszczędzisz czas na dojazdach – pracuj w naszym atrakcyjnie zlokalizowanym biurze w jednym z 7 miast w Polsce Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych i zewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 18000, ""max"": 27000, ""type"": ""Net per month - B2B""}, {""min"": 15000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,15000,18000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,262,Tableau/BI Developer (in the EU),Andersen,"Andersen is seeking aTableau/BI Developerin the EUfor a major logistics project with a global UK-based transport leader. The role involves supporting data-related tasks for a mobile app focused on trip planning, ticket booking, and real-time transport information.","[{""min"": 10300, ""max"": 19400, ""type"": ""Gross per month - Permanent""}, {""min"": 10300, ""max"": 19400, ""type"": ""Net per month - B2B""}]",Data Engineering,10300,19400,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,285,👉 Senior GenAI Engineer (Future Opening),Xebia sp. z o.o.,"🟣About project: This role focuses on developing and deploying AI applications using tools such as Azure AI Studio, vector databases, and Retrieval-Augmented Generation (RAG) frameworks. You will collaborate closely with senior team members to deliver robust, high-quality solutions that drive innovation. 🟣You will be: designing, developing, and implementing AI solutions using Python, with a focus on LLMs, vision models, and generative AI technologies, building, testing, and optimizing RAG applications, leveraging effective prompt engineering techniques to improve model performance, integrating AI models with Azure AI Studio, SharePoint, and Azure Blob Storage for efficient deployment and data handling, utilizing vector databases and Agentic frameworks (e.g., LlamaIndex) to enhance the functionality and intelligence of AI systems, implementing event-driven architectures using tools like Event Hub and Kafka for real-time data processing and scalability, collaborating with the AI Lead and team members to troubleshoot issues, test models, and ensure successful deployment, writing clean, efficient, and well-documented code adhering to best practices and version control standards, staying current with emerging AI tools, frameworks, and technologies to continuously improve development processes and outcomes. 🟣Your profile: Bachelor’s degree in computer science, Data Science, Engineering, or a related field, 4+ years of hands-on experience in AI/ML development, with an emphasis on generative AI and related technologies, strong experience with Python, REST APIs, Git proven expertise in developing and deploying LLMs, vision models, vector databases, and RAG applications, strong proficiency in Azure AI Studio, Azure Blob Storage, Event Hub, Kafka, familiarity with Agentic frameworks and tools like LlamaIndex for advanced AI development, ability to thrive in a collaborative team environment while managing multiple tasks effectively, good verbal and written communication skills in English (min. B2). 🟣Nice to have: exposure to cloud fundamentals (Azure preferred) and containerization tools like Docker, experience with CI/CD pipelines for AI model deployment, understanding RESTful services and API integration. 🟣Recruitment Process: CVreview –HRcall –Interview(with Live-coding) –ClientInterview (with Live-coding) –Hiring ManagerInterview –Decision 🎁Benefits 🎁 ✍Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 23500, ""max"": 33500, ""type"": ""Net per month - B2B""}, {""min"": 18000, ""max"": 27000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18000,27000,Gross per month - Permanent
Full-time,Mid,Mandate,Remote,314,IT Support & Systems Administrator (Salesforce CRM),BookingHost Sp. z o.o.,"Do zespołu IT poszukujemy osoby skrupulatnej, bystrej, komunikatywnej i samodzielnej, która wesprze nas w codziennej obsłudze i rozwoju systemów informatycznych, w tym rozwiązywaniu zgłoszeń naszych pracowników (ticketów). Administracji systemów informatycznych - Salesforce CRM i szeregiem innych aplikacji - zarządzaniem użytkownikami, konfiguracją kont, zakresem ich uprawnień itp.); Udziale w planowaniu procesów biznesowych w firmie, przygotowaniem pod nie wymagań technicznych oraz ich realizacji; Wdrażaniu nowych funkcjonalności m.in. automatyzacji z użyciem Salesforce Flows, Make; Integracji nowych narzędzi; Sprawowaniu pieczy nad porządkiem w modelu danych; Generowaniu raportów i analizie danych; Bieżącym wsparciu użytkowników. Dobra znajomość administracji CRM Salesforce (ewentualnie innego systemu CRM / ERP); Doświadczenie w administracji innych systemów i/lub wsparciu technicznym; Sprawne posługiwanie się arkuszami Google Sheets / Excel; Samodzielność i chęć uczenia się, także z wykorzystaniem dostępnych w sieci materiałów; Umiejętność pracy w dynamicznym środowisku, wielozadaniowość, szybkość adaptacji do zmian; Dobra organizacja pracy, umiejętność trafnej oceny priorytetów; Swoboda w komunikacji, wysoka kultura osobista; Dobra znajomość języka angielskiego (czytanie dokumentacji, aktywne uczestnictwo w spotkaniach). architekta rozwiązań Salesforce i/lub w programowaniu w APEX lub Java; z narzędziami takimi jak Front, Calendly, Make, Zapier, Google Workspace, Slack, wirtualna centralka, systemy ticketowe (np. Jira, ServiceNow); w nadzorowaniu projektów prowadzonych z zewnętrznymi podmiotami jak agencje deweloperskie; W integracji Salesforce z innymi systemami / aplikacjami. W pełni zdalną pracę, swobodną atmosfera w zespole, elastyczne godziny; Płatny urlop przy umowie B2B; Wsparcie w rozwoju umiejętności administrowania systemów; Realny wpływ na codzienną pracę firmy i rozwój struktury IT; Pracę w szybko rosnącej i dynamicznej firmie, aspirującej do pozycji lidera na rynku wynajmu mieszkań; Dostępne opcje Karty MultiSport i MultiLife. 📝Proces rekrutacji Proces rekrutacji składa się z dwóch etapów – oba odbywają się zdalnie: Rozmowa telefoniczna(ok. 15–30 minut) – krótkie poznanie się, omówienie doświadczenia i oczekiwań. Spotkanie online( ok. 60 minut) – rozmowa techniczna z członkiem zespołu IT, podczas której sprawdzimy wiedzę praktyczną i dopasowanie do roli.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 9000, ""type"": ""Gross per month - Mandate""}]",Database Administration,7000,9000,Gross per month - Mandate
Full-time,Mid,Permanent or B2B,Remote,318,Middle Data Analyst,N-iX,"#3683 We are seeking afor aMiddle Data Analystin Polandwith strong expertise in Tableau and a passion for turning data into actionable insights. In this role, you will lead the development of advanced visual analytics, collaborate with stakeholders to understand business needs, and help shape data-driven decision-making across the organization. Responsibilities: Develop and maintain dashboards and reports using Tableau to support key business functions Translate complex business questions into analytical solutions Work closely with stakeholders to gather requirements, understand KPIs, and deliver meaningful insights Use SQL to extract, transform, and analyze data from various sources Present findings in a clear, concise, and impactful way to both technical and non-technical audiences Continuously improve reporting performance and usability through iteration and feedback Requirements: 4+ years of experience in analytics, business intelligence, or data visualization roles Python skills for data processing (at least 1 year of experience) Advanced Tableau skills (dashboard development, calculated fields, LOD expressions, performance optimization) Strong SQL proficiency for data querying and preparation Proven ability to derive insights from data and explain them effectively Solid understanding of data modeling, joins, and ETL principles Strong analytical thinking and attention to detail Experience working with cross-functional teams and translating business requirements into data deliverables Must-Have: Expert-level Tableau development experience (at least 2+ years of experience) Hands-on experience creating scalable, interactive dashboards for enterprise use Advanced SQL skills applied to analytical/reporting contexts Ability to work independently and proactively in a data-driven environment Nice-to-Have: Experience with additional BI tools (Power BI, Looker, etc.) Knowledge of modern data warehouses (Snowflake, BigQuery) Familiarity with dbt or Python for data manipulation Exposure to A/B testing, experimentation frameworks, or product analytics Understanding of data visualization best practices and UX principles We offer: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits","[{""min"": 12560, ""max"": 16254, ""type"": ""Net per month - B2B""}, {""min"": 9974, ""max"": 12929, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,9974,12929,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,322,Data Analyst,Remodevs,"Please note - hybrid from Warsaw (3 days from the office, 2 remotely). About us: We help businesses use AI and digital tools to work better and grow faster, especially in private capital markets. Our Core Platform improves workflows and gives useful insights with AI. Olympus Software is a fast, smart cloud system that grows with your needs. The Pantheon Suite offers flexible tools to manage and improve business performance. With over 10 years of experience, we know how to turn technology into real business value. About the Role We are seeking a Data Analyst to join our PaaS (Platform-as-a-Service) Customer Delivery team in Warsaw. In this role, you will be responsible for transforming data into actionable insights to support decision-making across product, operations, and strategy for customer-facing platform implementations. You will work closely with stakeholders across implementation projects to develop dashboards, conduct ad-hoc analyses, and ensure the integrity of platform usage and performance metrics. This role is ideal for someone who is curious, business-minded, and eager to make an impact through data. Key Responsibilities Partner with cross-functional teams to define key metrics and build dashboards and reports that provide visibility into business performance. Conduct deep-dive analyses to answer business questions, uncover trends, and identify opportunities for growth and optimization. Design and maintain scalable data models and SQL queries to support reporting and analytical needs. Collaborate with data engineers to ensure data availability, quality, and consistency across systems. Communicate findings and recommendations clearly to technical and non-technical audiences. Develop documentation and contribute to data literacy across the organization. Qualifications Required 2–4 years of experience in a data analyst or business intelligence role. Strong SQL skills and experience working with large datasets in a cloud data warehouse environment. Proficiency with BI tools such as Looker, Tableau, Power BI, or similar. Strong analytical thinking and attention to detail. Excellent communication and data storytelling skills. Preferred Experience working with dbt or similar modeling tools. Familiarity with A/B testing design and analysis. Some experience with Python, R, or another scripting language for data analysis. Exposure to product analytics platforms (e.g., Mixpanel, Amplitude).","[{""min"": 16623, ""max"": 18470, ""type"": ""Net per month - B2B""}, {""min"": 16623, ""max"": 18470, ""type"": ""Gross per month - Permanent""}]",Data Analysis & BI,16623,18470,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,369,Data Engineer,Tooploox,"We areTooploox 💎,an AI software development companyoffering custom AI solutions and services. We help innovative companies and startups design and build digital products with generative AI, mobile, and web technologies. Our team, consisting of nearly 200 experts including our R&D team of over 40 engineers, many with PhDs, has pioneered AI solutions across industries like healthcare, fashion, and e-commerce. We’ve published over 15 research papers in top conferences like NeurIPS and ICML. We're on the lookout for aData Engineer📊 to take on a pivotal role in our team.You'll be at the heart of working with data, focusing on scalable batch and streaming data pipelines. If you're someone who loves to merge traditional software development with innovative AI technologies, this role is tailor-made for you. Design, develop, and maintain scalable batch and streaming data pipelines. Work withPythonto transform, process, and integrate data. Handle a mix of structured and unstructured data, including work withNoSQL and vector databases. Optimize performance acrossbig data workflows, including tuningHive and Sparkjobs. BS/BA in Data Engineering/Computer Science+ 2 years of experience or related field or 5 years of relevant experience. Extensive expertise withApache Spark (especially PySpark), Hadoop, and Apache Hivewith a proven track record of optimizing large-scale data systems. Strong programming skills inPython. Comprehensive understanding of database concepts, including experience withNoSQL databases (e.g., MongoDB, Redis)and ideally vector databases. Proven hands-on experience with stream processing, preferably usingApache Flink. In-depth knowledge of distributed computing, data warehousing, and performance optimization techniques. Exceptional problem-solving and communication skills, with experience working in cross-functional teams. Fluency in Polish and English. Experience with LLMs, prompt engineering, or machine learning workflows (we use this in conjunction with vector DBs). Experience in Java or Scala - useful for deeper Spark optimization or contributing to broader engineering projects. Familiarity with Spring Boot for building and deploying data applications. 🏖️26 days of annual service break. 🤒An additional pool of 14 days per yearpaid at80% of your standard rate. 🇬🇧English lessonsonce a week or more frequently, depending on your needs. 📚Access to a curated libraryof books and e-books, regularly updated based on employee suggestions, plus recurringknowledge-sharing initiatives. 🏡Flexible hoursand the option to work100% remotelyor from one of our offices inWrocław or Warsaw. 💻Top-quality equipment– we provide MacBooks, new monitors, noise-cancelling headphones, and any additional gear you may need to work comfortably. 🏥Group insurancewith Warta andprivate medical carewith Enel-Med for just 1 PLN. 🧠Mental health support– we offer access to a psychologist with fully anonymous consultations if needed. 🏋️‍♂️Multisport card(we cover most of the cost – your contribution is currently no more than 45 PLN, or less depending on the selected package), access togyms in our Wrocław and Warsaw offices, andsports initiativeslike the annualBike 2 Work Challenge. 🍕🎮🕺🏻 We hostteam lunches, webinars, game nights, and social events. We enjoy the occasional barbecue, dance party, time on the terrace, foosball, or PlayStation session.","[{""min"": 18000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16000,20000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,373,Snowflake Data Engineer,Onwelo Sp. z o.o.,"Jesteśmy nowoczesną polską firmą technologiczną, która dostarcza wsparcie eksperckie organizacjom na całym świecie. Tworzymy, rozwijamy i utrzymujemy zaawansowane rozwiązania IT, oferując przy tym solidne zaplecze kompetencyjne. W ciągu kilku lat zrealizowaliśmy ponad 300 projektów w Europie i USA, dynamicznie rozbudowując zespół do kilkuset specjalistów i otwierając sześć biur w Polsce oraz oddziały w USA, Niemczech i Szwajcarii. Dołączysz do zespołu realizującego projekt dla globalnej organizacji z sektoralife science i healthcare, specjalizującej się w rozwiązaniach laboratoryjnych i biotechnologicznych. Klient prowadzi działalność na wielu rynkach i obsługuje tysiące jednostek operacyjnych na całym świecie.Celem projektu jest budowa i rozwój skalowalnej platformy danych w środowisku chmurowym, która wspiera analitykę biznesową, planowanie operacyjne oraz zaawansowane modele predykcyjne.Zespół Onwelo wspiera klientam.in. w rozwoju hurtowni danych, projektowaniu potoków ETL, modelowaniu danych i zapewnieniu jakości danych w środowisku enterprise.Szukamy osoby, która wniesie swoje doświadczenie i wesprze zespół w rozwoju architektury danych, zapewniając wydajność, jakość i bezpieczeństwo danych. Projektować, budować i rozwijać hurtownie danych oparte na platformieSnowflake Tworzyć oraz optymalizowaćpotoki danych (ETL/ELT)w środowiskach chmurowych Wdrażać i zarządzać komponentami Snowflake: Snowpipe, Streams, Tasks, Secure Views Projektować i rozwijaćmodele danychwspierające analitykę biznesową Współpracować z zespołami Data Science i BI w zakresie zasilania modeli i dashboardów Wspierać automatyzację procesów danych poprzez integrację z narzędziami CI/CD (GitLab, Jenkins) Posiadasz minimum 3-letnie doświadczenie jakoData Engineer– z naciskiem na Snowflake Znasz platformęSnowflake: strukturę danych, architekturę, optymalizację zapytań, zarządzanie schematami i dostępem Biegle posługujesz sięSQL(w tym: CTE, window functions, UDF, optymalizacja zapytań) Masz doświadczenie z procesamiETL/ELT, również z wykorzystaniem danych półstrukturalnych (JSON, XML, Parquet) Znasz zasady projektowania nowoczesnych modeli danych (np. Kimball, Data Vault) Maszwyższe wykształcenie techniczne(np. informatyka, matematyka, inżynieria danych) Komunikujesz siępo angielsku na poziomie min. B2 Wybierzesz wygodną dla Ciebie formę zatrudnienia Otrzymasz możliwość korzystania z elastycznych godzin pracy Praca w trybie hybrydowym lub zdalnym Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Integracje firmowe pozwolą Ci na bliższe poznanie zespołu Zabezpieczysz swoją przyszłość, korzystając z dodatkowego ubezpieczenia na życie Z prywatną opieką medyczną zadbasz o zdrowie swoje i swoich bliskich Karta MultiSport Plus umożliwi Ci prowadzenie aktywnego trybu życia","[{""min"": 16800, ""max"": 23100, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 17000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14000,17000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,375,Data Scientist,Onwelo Sp. z o.o.,"Onweloto nowoczesna polska spółka technologiczna, specjalizująca się w budowie innowacyjnych rozwiązań IT dla organizacji z różnych sektorów na całym świecie. Firma oferuje kompleksowe usługi z zakresu tworzenia, rozwoju i utrzymania oprogramowania, oraz silne wsparcie kompetencyjne. Do naszego zespołuData & Analyticsposzukujemy Data Scientista, który będzie pracował nad budową i rozwojem modeli analitycznych oraz uczenia maszynowego dla klientów z różnych branż – zarówno z Polski, jak i z rynków zagranicznych. Będziesz współpracować z zespołami analitycznymi i biznesowymi, wspierać podejmowanie decyzji na podstawie danych i tworzyć rozwiązania, które realnie wpływają na działalność naszych klientów. Przeprowadzać eksploracyjnąanalizę danych (EDA) Poszukiwaćzależności, wzorców i insightów w danych biznesowych Budowaćmodele klasyfikacyjne, regresyjne i klasteryzacyjne Przeprowadzaćfeature engineering i przygotowywać dane do modelowania Współpracować z zespołami analitycznymi, technologicznymi i biznesowymi Wizualizować wyniki analiz i przygotowywać raporty oraz prezentacje Maszminimum 2-letnie doświadczeniew pracy jakoData Scientistlub na podobnym stanowisku Bardzo dobrze znaszPythona i pracowałeś z bibliotekami: pandas, scikit-learn, numpy, matplotlib, seaborn Swobodnie pracujesz zdanymi tabelarycznymii znasztechniki EDA Potrafisz wyciągać trafne wnioski z danych i prezentować je w przystępny sposób ZnaszSQLi masz doświadczenie z dużymi zbiorami danych Dodatkowo docenimy, jeśli: Korzystałeś znarzędzi BI i znasz metody interpretacji modeli Masz doświadczenie w automatyzacji procesów analitycznych Znasz system kontroli wersjiGITi technologie konteneryzacji (Docker) Pracowałeś z rozwiązaniami w chmurze obliczeniowej(Azure, AWS lub GCP) Wybierzesz wygodną dla Ciebie formę zatrudnienia Potrzebujesz pracować zdalnie? Jesteśmy otwarci! Rozwiniesz swoje umiejętności, współpracując z doświadczonymi ekspertami Będziesz pracować z wykorzystaniem nowych technologii Uzyskasz dostęp do szkoleń wewnętrznych i zewnętrznych Weźmiesz udział w ciekawych projektach dla polskich i międzynarodowych klientów Będzie na Ciebie czekać przyjazne i komfortowe środowisko pracy Wydarzenia firmowe pozwolą Ci na bliższe poznanie zespołu","[{""min"": 15750, ""max"": 21000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 15000, ""type"": ""Gross per month - Permanent""}]",Data Science,12000,15000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Hybrid,384,DATA Architect,Power Media,"Nasz klient to firma, która odważnie podchodzi do wyzwań technologicznych i nie boi się szukać nieszablonowych rozwiązań. Innowacyjność łączy tu się z solidną wiedzą techniczną oraz doskonałą znajomością realiów branży. Specjalizują się w projektach z pogranicza eCommerce i Business Intelligence, oferując kompleksowe rozwiązania oparte na sprawdzonych technologiach. Co więcej – umiejętnie łączą te dwa światy, tworząc narzędzia, które zapewniają pełen wgląd w dane i realne wsparcie dla biznesu. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozwój rozwiązań Data Platform w środowisku chmurowym. Kluczowym zadaniem będzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejsów w złożonym krajobrazie systemowym klienta. Projekt obejmuje budowę hurtowni danych oraz platform danych od podstaw. Zespółskłada się z 22 osób: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiązków: Rozwój i utrzymanie Data Warehouse oraz Data Platform przy użyciu narzędzi ETL i SQL, Analiza wymagań biznesowych i proponowanie rozwiązań technicznych, Współpraca z zespołem w celu zapewnienia dostępności rozwiązań biznesowych, Wykorzystanie swojej wiedzy i doświadczenia do tworzenia innowacyjnych rozwiązań. Główne wymagania: Bardzo dobra znajomość metod i technik projektowania oprogramowania, Ponad 10-letnie doświadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych, Znajomość ekosystemów Big Data, Znajomośćrozwiązań chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejętności analityczne (analiza i dokumentowanie wymagań biznesowych oraz specyfikacji technicznych) Doświadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomość SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomość rozwiązań ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomość języka angielskiego (min. B2+/C1)– codzienna praca w międzynarodowym środowisku, Gotowość do podróży służbowych. Mile widziane: Znajomośćnarzędzi Informatica, Znajomość Machine Learning oraz frameworków ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomość języków programowania (Java, Scala, C++, Python). Znajomość języka niemieckiego. Firma oferuje: Stabilne, długofalowe zatrudnienie w oparciu o B2B lub UoP, Możliwość pracy w większości zdalnej(praca z biura 1 raz w tygodniu, we wtorek) Płaska struktura, anty korporacyjne podejście do pracy i zespołu, Ciekawe, międzynarodowe projekty, Zgrany zespół chętnie uczestniczący w aktywnościach sportowo – rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team – building), Dodatkowe zajęcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajęć sportowych, Nowoczesne biuro ze strefą relaksu, Co roczny 5 dniowy wyjazd firmowy (cała firma), warsztaty kulinarne, wyjścia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywnościami Świetna atmosfera, partnerskie podejście, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa „miękko-techniczna”. Gorąca prośba o CV w j. angielskim : )","[{""min"": 18000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,14000,18000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,402,Data Engineer,dotLinkers,"Type of contract: contract of employment (UoP) / B2B Salary ranges: up to 24 000 PLN a month Working model: 100% remote Join our client, one of the leading logistics and transport solutions providers. About the role: Looking for a skilled Lead Data Engineer to drive the design and implementation of robust data systems, while effectively connecting technical teams with business goals. This role involves hands-on development, strategic planning, and cross-team collaboration. Responsibilities: Designing scalable data systems and tools to support analytics, modeling, and decision-making Building advanced data pipelines and platforms on Azure Collaborating closely with technical and business teams Establishing best practices and reusable standards in data engineering Engaging with international projects and external partners Requirements: 5+ years in IT, data engineering, or information systems Expertise in Azure, Spark (Scala/PySpark), Databricks, Kafka, Event Hubs, Python, Java, SQL/NoSQL Experience with DevOps tools and practices (CI/CD, Terraform, Kubernetes) Skilled in streaming data and big data environments Strong leadership and communication skills (English and Polish) Experience working with distributed teams The offer: Flexible remote work with occasional office visits Benefits include private healthcare, insurance, and a sports card High-end equipment Development budget for learning and growth","[{""min"": 20000, ""max"": 24000, ""type"": ""Net per month - B2B""}, {""min"": 20000, ""max"": 24000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,20000,24000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,415,Tester Data&BI,summ-it,"Uprzejmie informujemy, że z uwagi na sezon urlopowy czas odpowiedzi w procesie rekrutacji może się wydłużyć. Jeśli chcesz nauczyć się nowych technologii, a następnie pracować w projektach dla marek znanych na całym świecie, a jednak wciąż w ramach niekorporacyjnej struktury, to idealne miejsce dla Ciebie! W summ-it stwarzamy przestrzeń na zgłaszanie swoich pomysłów i dbamy o Twój rozwój. Nasi pracownicy biorą udział w konferencjach branżowych i wydarzeniach dotyczących szeroko pojętej tematyki IT, a także udoskonalają swoje umiejętności dzięki różnego rodzaju szkoleniom i warsztatom. Pracujemy z bazami danych o łącznej wielkości liczonej w PB, i optymalizujemy systemy walcząc o każdą ms. Obecnie zarządzamy ponad 10 000 systemów baz dla naszych klientów. Zatrudnienie na podstawie: umowy o pracę, umowy B2B, umowy zlecenie Pracę w atmosferze koleżeństwa i zaufania – w ankiecie satysfakcji ponad 92% pracowników zgadza się z tym stwierdzeniem Odpowiednie wsparcie i współpracę w swoim zespole – w ankiecie satysfakcji 100% pracowników zgadza się z tym stwierdzeniem Elastyczne godziny pracy oraz pracę w modelu hybrydowym Pracę z biura w centrum Poznania Dostęp do najnowszych technologii IT Możliwość rozwoju w międzynarodowej firmie Szkolenia zewnętrzne i wewnętrzne: Miękka środa, Bezpieczne czwartki, Science Friday Spotkania firmowe: Summer Party, Winter Party, AllHands, Talk to Your Boss, Lokalne środy, summ‑itowe śniadania Programy doceniania pracowników: summ-it heores i nagrody za przyznane kudosy Program poleceń pracowniczych Możliwość dołączenia do benefitów (opieka medyczna, karta Multisport, ubezpieczenie grupowe) Pracę w zrównoważonym zespole – 3 generacji: X, Y, Z Przeprowadzanie testów w obszarze danych (Azure Databricks, Azure Data Factory, Azure Synapse, Azure Analysis Services, Power BI, MDS) zgodnie z opisem w User stories Rejestrowanie wyników testów w Azure DevOps zgodnie z procesem Analiza wyniku testów i informacja zwrotna dla developer’ów i SME Opracowywanie scenariuszy testowych (unit, integration, regression) Analiza testów, identyfikacja i wdrażanie automatyzacji testów oraz usprawnień w procesach QA Proponowanie usprawnień w zakresie testów Identyfikacja wzorców na powtarzające się błędy Min. 3 lata doświadczenia w testowaniu danych Znajomość Azure Databricks (notebook, job, cluster Spark) Znajomość Azure Data Factory (pipeline’s datasets, linked services, monitor) Znajmość Azure Synapse (Dedicated pool) Znajomość modeli tabularnych (Analysis Services) i serwisu Power BI (data lineage, model semantyczny, połączenia) Biegłość w SQL (weryfikacja jakości danych, przygotowywanie zapytań testowych) Zrozumienie procesów ETL Znajomość narzędzi do automatyzacji testów i CI/CD (w szczególności Azure DevOps, TestPlans) Znajomość języka angielskiego na poziomie min. B2 Dziękujemy za zainteresowanie naszą ofertą i nadesłanie aplikacji. Uprzejmie informujemy, że skontaktujemy się z wybranymi kandydatami.","[{""min"": 10000, ""max"": 13000, ""type"": ""Net per month - B2B""}, {""min"": 7000, ""max"": 10000, ""type"": ""Gross per month - Permanent""}]",Unclassified,7000,10000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,426,Data Engineer,Tesco Technology,"Tesco Technology is multi-functional and specialist team that drives operational excellence of services improves scale for our systems and processes globally and creates business leading capabilities. We are an agile team of an industry-leading team of engineers. We create the future continuous integration and delivery tools for Colleague and Customer & Loyalty areas, solving problems, and developing new features through quality, scalable, performant, and maintainable technical solutions. The solutions that we are responsible for will have a global reach, impacting hundreds of thousands of Tesco colleagues worldwide. We operate in a DevOps philosophy. We take responsibility for the software through its entire lifecycle. We practice continuous integration, delivery, and support of our code through to production and beyond. As Tech Hub we cooperate within the group of Tesco Technology Hubs located in the UK, Poland, Hungary, and India. We always welcome conversations about flexible working, so feel free to talk to us during your application about how we can support you.We value connecting, collaborating, and innovating with our colleagues in person. At Tesco Technology, we work in a hybrid model. This role requires you to be based in or near Kraków, as we currently meet in the office three days a week. The Data Engineering department at Tesco Technology is at the forefront of data processing within the retail and technology industry. This vital department handles a range of responsibilities, including: Analyzing order and delivery data to optimize logistics processes and enhance delivery efficiency. Managing critical data related to customer orders, suppliers, and products to ensure the seamless flow of our fulfillment operations. Upholding data integrity and security during the processing of order and delivery-related information. As we continue to expand, we are actively seeking a skilled Data Engineer to join our team of analytics experts. In this role, you will take charge of expanding and refining our data and data pipeline architecture. Additionally, you will be instrumental in optimizing data flow and collection to cater to the needs of cross-functional teams. Our ideal candidate is an experienced data pipeline builder and data enthusiast who relishes the opportunity to optimize data systems and construct them from the ground up. As a Data Engineer, you will collaborate closely with software developers, database architects, data analysts, and data scientists on various data-driven initiatives. You will play a crucial role in ensuring that the optimal data delivery architecture remains consistent across all ongoing projects. This role calls for a high level of self-direction and the ability to effectively support the data requirements of multiple teams, systems, and products. If you are enthusiastic about the prospect of optimizing, and possibly even redesigning, our company's data architecture to support our next generation of products and data initiatives, we encourage you to apply and be part of our dynamic team shaping the future of our data operations! Responsibilities Create and maintain optimal data pipeline architecture Assemble large complex data sets that meet functional / non-functional business requirements. Identify design and implement internal process improvements: automating manual processes optimising data delivery re-designing infrastructure for greater scalability etc. Build the infrastructure required for optimal extraction transformation and loading of data from a wide variety of data sources Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition operational efficiency and other key business performance metrics. Work with stakeholders including the Executive Product Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure Create data tools for analytics and data scientist team members that assist them in building and optimising our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. Mandatory skills: Data Processing: Apache Spark - Scala or Python Data Storage: Apache HDFS or respective cloud alternative Resource Manager: Apache Yarn or respective cloud alternative Lakehouse: Apache Hive/Kyuubi or alternative Workflow Scheduler: Airflow or alternative Nice to have skills: Functional programming Apache Kafka Kubernetes Stream processing CI/CD Unsure if you fit all the criteria? Apply and give us the chance to evaluate your potential – you could be the perfect fit! We value flexibility at Tesco; therefore, this position is also available for candidates who are interested in working part time – about 120 hours a month or more. Please let us know what would work for you. Hybrid work We know life looks a little different for each of us. That’s why at Tesco, we always welcome chats about different flexible working options. Some people are at the start of their careers, some want the freedom to do the things they love. Others are going through life-changing moments like becoming a carer, adapting to parenthood, or something else. So, talk to us throughout your application about how we can support.This role requires you to be based in or near Kraków, as you will spend 60% (3 days) of your week collaborating with colleagues at our office locations or local sites and the rest remotely. Benefits Tesco is a diverse and exciting employer, dedicated to being #aplacetogeton, providing career-defining opportunities to all of our colleagues. If you choose to join our business, we will provide you with (for all): MacBook as your tool for work Learning opportunities - certified technical training and learning platforms like Udemy, Pluralsight and O’reily Referral Bonus Sports activities with a personal trainer in the office Benefits for colleagues on employment of contract only: Additional 4 days of paid leave to support your well-being and family life Up to 20% yearly salary bonus – based on both individual and business performance Private healthcare (LuxMed) Cafeteria & Multisport Supporting those, who are not yet eligible for full holiday entitlement, by expanding their pool from 20 to 25 days Relocation Help IP Tax Deductible Costs If that sounds exciting, then we'd love to hear from you. Tesco is committed to celebrating diversity and everyone is welcome at Tesco. As a Disability Confident Employer, we’re committed to providing a fully inclusive and accessible recruitment process, allowing candidates the opportunity to thrive and inform us of any reasonable adjustments they may require.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 18500, ""max"": 26000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,18500,26000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,432,Senior Data Science/AI Engineer,N-iX,"(3767) About client: Since 2002, the client has been a market leader in automotive claims management, processing over 18 million vehicle claims annually with a global team of more than 1,000 employees. Operating in over 30 countries, the company specializes in digital solutions that optimize vehicle damage processing for insurance companies, car dealerships, repair shops, leasing firms, and automotive manufacturers. By leveraging automation, advanced technologies, and industry expertise, the client continuously enhances efficiency and accuracy in claims handling. An in-house research and development team drives innovation, tailoring solutions to local market needs while advancing digital transformation in the industry. At the core of this evolution is a strong development team, building scalable, high-performance software solutions that integrate data-driven processes with human expertise to reshape automotive claims management.Your Role: As a Senior AI Engineer, you will play a key role in shaping the future of digital automotive claims management by developing advanced AI and machine learning models. Working within a diverse and collaborative team of 30 data scientists, you will explore innovative research approaches and build intelligent systems that enhance the accuracy and efficiency of insurance-related processes. You will leverage state-of-the-art techniques in computer vision, large language models (LLMs), and multimodal AI to create impactful solutions tailored to the specific needs of the insurance and automotive industries.Your responsibilities will include developing foundation models and intelligent systems that are capable of processing complex data inputs, improving claim automation, and supporting digital transformation. This is an opportunity to apply your deep technical expertise in a forward-thinking, international environment where data-driven innovation meets real-world impact. Key Responsibilities: Collaborate within a cross-functional team of 30+ data scientists and AI/ML experts to drive research and innovation in the image processing & recognition field Develop and deploy cutting-edge machine learning and AI models, including computer vision, LLMs, and multimodal solutions. Design and train proprietary foundation models tailored to automotive and insurance industry needs. Translate complex technical concepts into scalable digital solutions that enhance claims processing efficiency. Continuously evaluate and integrate emerging AI technologies to ensure high performance and accuracy. Contribute to the company’s R&D efforts to advance AI innovation and digital transformation. Requirements: Qualifications: Degree in mathematics, computer science, data science, engineering, physics, or a related field. 5+ years in data science with proven experience in ML, DL, and data analysis using Python. Proficient in applying data science methods, building predictive models, and deploying AI solutions. Strong analytical mindset, proactive attitude, and ability to communicate complex topics clearly to both technical and non-technical stakeholders. Experience within the insurance or automotive industries. Excellent English communication skills (written and verbal); German is a plus. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 22164, ""max"": 25859, ""type"": ""Net per month - B2B""}, {""min"": 18470, ""max"": 21056, ""type"": ""Gross per month - Permanent""}]",Data Science,18470,21056,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,436,Middle/Senior Data Engineer,N-iX,"#3682 We are seeking aMiddle/Senior Data Engineerwith proven expertise inAWS, Snowflake, and dbtto design and build scalable data pipelines and modern data infrastructure. You'll play a key role in shaping the data ecosystem, ensuring data availability, quality, and performance across business units. 4+ years of experience in Data Engineering roles. Experience with theAWScloud platform. Proven experience withSnowflakein production environments. Hands-on experience building data pipelines usingdbt. Pythonskills for data processing and orchestration. Deep understanding of data modeling and ELT best practices. Experience with CI/CD and version control systems (e.g., Git). Strong communication and collaboration skills. Strong experience withSnowflake(e.g., performance tuning, storage layers, cost management) Production-level proficiency withdbt(modular development, testing, deployment).. Experience developingPythondata pipelines. Proficiency in SQL (analytical queries, performance optimization). Experience with orchestration tools like Airflow, Prefect, or Dagster. Familiarity with cloud platforms (e.g., GCP, or Azure). Knowledge of data governance, lineage, and catalog tools. Experience in working in Agile teams and CI/CD deployment pipelines. Exposure to BI tools like Tableau or Power BI. We offer*: Flexible working format - remote, office-based or flexible A competitive salary and good compensation package Personalized career growth Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more) Active tech communities with regular knowledge sharing Education reimbursement Memorable anniversary presents Corporate events and team buildings Other location-specific benefits *not applicable for freelancers","[{""min"": 110, ""max"": 195, ""type"": ""Net per hour - B2B""}, {""min"": 81, ""max"": 162, ""type"": ""Gross per hour - Permanent""}]",Data Engineering,81,162,Gross per hour - Permanent
Full-time,Senior,Permanent or B2B,Remote,437,👉 Data & AI Enterprise Architect,Xebia sp. z o.o.,"🟣You will be: leading and inspiring cross-functional architecture efforts across our client’s organization, filling an internal strategic role designed to bring added value to the client by aligning data architecture with business innovation and long-term growth, acting as a thought leader and enabler, supporting dedicated architects and teams across multiple domains, including: AI & ML Projects (40–60%), B2B & B2C E-commerce Platforms (20%), DataOps & Data Engineering (20%). 🟣Your profile: proven experience as an Enterprise or Lead Data Architect in complex, enterprise-scale environments, deep expertise in Microsoft Azure and Databricks, strong understanding of data architecture principles, data governance, and modern data platforms, ability to work across business and technical domains, with a focus on value creation and business impact, willingness to occasionally work on site in Amsterdam, very good command of English (min. C1). 🟣Nice to have: familiarity with technologies used across the client’s ecosystem, such as: Salesforce, Event Hubs / Kafka, Contentful or other headless CMS platforms, experience in customer-facing roles, pre-sales, or innovation consulting.in. Work from the European Union region and a work permit are required. 🟣Recruitment Process: CVreview –HRcall –Technical Interview–ClientInterview (with Live-coding) –Hiring ManagerInterview –Decision 🎁Benefits 🎁 ✍Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 43500, ""type"": ""Net per month - B2B""}, {""min"": 26850, ""max"": 35500, ""type"": ""Gross per month - Permanent""}]",Data Architecture,26850,35500,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,467,👉 GenAI Lead,Xebia sp. z o.o.,"🟣About project: GenAI Lead will take ownership of designing, developing, and deploying advanced AI solutions, specializing in machine learning, deep learning, and generative AI technologies. This role demands a strategic leader with deep technical expertise to architect scalable GenAI solutions, guide a team of AI professionals, and deliver transformative business outcomes through innovative AI applications. 🟣You will be: designing and implementing scalable Generative AI (GenAI) systems using Large Language Models (LLMs), vision models, and vector databases, leading the design, development, and deployment of AI-driven solutions, including machine learning models, deep learning frameworks, and generative AI applications, overseeing seamless integration of AI solutions with Azure AI Studio, SharePoint, and Power BI for deployment, reporting, and visualization, collaborating with cross-functional teams to shape AI strategies and deliver high-impact solutions, providing technical leadership in implemention of Agentic frameworks and tools like LlamaIndex to advance AI workflows, mentoring and empowering a team of AI developers, fostering a culture of innovation, collaboration, and technical excellence, staying at the forefront of AI advancements, proposing creative and practical solutions to address complex challenges, enforcing best practices in code quality, model optimization, and solution scalability to ensure robust production-ready systems. 🟣Your profile: Bachelor’s or Master’s degree in computer science, Data Science, Engineering, or a related field (PhD preferred), 8+ years of hands-on experience in AI/ML development, including at least 3 years in leadership or solution architect capacity, demonstrated success in delivering machine learning, deep learning, and generative AI projects in production environments, exceptional programming proficiency in Python and experience with frameworks such as TensorFlow, PyTorch, or equivalent. in-depth expertise in LLMs, vision models, vector databases, and RAG applications, strong command of Azure AI Studio, SharePoint, and basic Power BI for integration and reporting purposes, proven experience with Agentic frameworks and tools like LlamaIndex for intelligent system development, outstanding problem-solving abilities, with a knack for translating business needs into technical solutions, superior communication and leadership skills to manage teams, align stakeholders, and drive project success, excellent verbal and written communication skills in English (min. C1). 🟣Nice to have: experience with cloud platforms beyond Azure (e.g., AWS, GCP), knowledge of DevOps practices (e.g., CI/CD pipelines, Kubernetes), familiarity with AI ethics, governance, and compliance standards, exposure to advanced visualization tools or BI platforms. 🟣Recruitment Process: CVreview –HRcall –Interview(with Live-coding) –ClientInterview (with Live-coding) –Hiring ManagerInterview –Decision 🎁Benefits 🎁 ✍Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 33500, ""max"": 40000, ""type"": ""Net per month - B2B""}, {""min"": 27000, ""max"": 32500, ""type"": ""Gross per month - Permanent""}]",Unclassified,27000,32500,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,476,Data Analyst,Vasco Electronics,"Miejsce pracy: Kraków Tryb pracy: Hybrydowy, 3 dni z biura, 2 dni zdalnie Etat: Full time Rodzaj umowy: Umowa o Pracę, Umowa Zlecenie, B2B Wynagrodzenie: UoP (brutto): 9900 - 12000 PLN UZ (brutto): 59 - 71 PLN/h B2B (netto/h FV): 73 - 93 PLN/h Zakres obowiązków Kompleksowa analiza danych (marketingowych, sprzedażowych, finansowych, magazynowych) z wykorzystaniemGoogle BigQueryiSQL Łączenie, agregowanie, porównywanie i weryfikowanie danych z różnych źródeł Projektowanie, tworzenie i utrzymywanie interaktywnych dashboardów oraz raportów wPower BI Tłumaczenie wymagań biznesowych na specyfikacje techniczne, przygotowywanie analiz i prezentowanie rekomendacji dla interesariuszy Ścisła współpraca zzespołem Data Engineeróworaz dbałość o jakość i spójność dokumentacji Wykorzystywanie LLMów do automatyzacji swojej pracy Nasze oczekiwania 3 - 5 lat doświadczenia Zaawansowana, praktyczna znajomośćSQLoraz środowiska bazodanowegoGoogle BigQuerylub innych baz danych Praca z wersjonowaniem kodu (Dataform, dbt lub podobne) Biegłość w wizualizacji danych i budowaniu raportów wPower BIlub podobne Doświadczenie w samodzielnym prowadzeniu analiz, od ekstrakcji danych po prezentację wniosków Wysoko rozwinięte umiejętności komunikacyjne i zdolność do efektywnej współpracy z odbiorcami biznesowymi i technicznymi Proaktywne podejście, umiejętność jasnego formułowania rekomendacji i otwartość na kulturę feedbacku Język angielski na poziomie B2 Mile widziane BigQuery Power BI Dataform Python Oferujemy Środowisko oparte na wartościach i przyjazną, nieformalną atmosferę – bez nadęcia, z fajnymi ludźmi i dobrą kawą Duży wpływ na kształt pracy zespołu Budżet do wykorzystania na platformie Worksmile, która oferuje dostęp do takich benefitów jak m.in. Multisport, Allianz, Luxmed, PZU oraz wiele innych Elastyczny czas pracy Inicjatywy rozwojowe Dofinansowanie okularów korekcyjnych 800 zł Częste integracje Atrakcje w biurze tj. PS5 + VR2, biuro przyjazne zwierzętom, przekąski i owoce w biurze Parking przy biurze","[{""min"": 9900, ""max"": 12000, ""type"": ""Gross per month - Permanent""}, {""min"": 73, ""max"": 93, ""type"": ""Net per month - B2B""}]",Data Analysis & BI,73,93,Net per month - B2B
Full-time,Senior,Permanent or B2B,Remote,487,👉 Senior GCP Data Engineer,Xebia sp. z o.o.,"🟣 You will be: developing and maintaining data pipelines to ensure seamless data flow from the Loyalty system to the data lake and data warehouse, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, ensuring data integrity, consistency, and availability across all data systems, integrating data from various sources, including transactional databases, third-party APIs, and external data sources, into the data lake, implementing ETL processes to transform and load data into the data warehouse for analytics and reporting, working closely with cross-functional teams including Engineering, Business Analytics, Data Science and Product Management to understand data requirements and deliver solutions, collaborating with data engineers to ensure data engineering best practices are integrated into the development process, optimizing data storage and retrieval to improve performance and scalability, monitoring and troubleshooting data pipelines to ensure high reliability and efficiency, implementing and enforcing data governance policies to ensure data security, privacy, and compliance, developing documentation and standards for data processes and procedures. 🟣 Your profile: 7+ years in a data engineering role, with hands-on experience in building data processing pipelines, experience in leading the design and implementing of data pipelines and data products, proficiency with GCP services, for large-scale data processing and optimization, extensive experience with Apache Airflow, including DAG creation, triggers, and workflow optimization, knowledge of data partitioning, batch configuration, and performance tuning for terabyte-scale processing, strong Python proficiency, with expertise in modern data libraries and frameworks (e.g., Databricks, Snowflake, Spark, SQL), hands-on experience with ETL tools and processes, practical experience with dbt for data transformation, deep understanding of relational and NoSQL databases, data modelling, and data warehousing concepts, excellent command of oral and written English, Bachelor’s or Master’s degree in Computer Science, Information Systems, or a related field. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ 🟣 Nice to have: experience with ecommerce systems and their data integration, knowledge of data visualization tools (e.g., Tableau, Looker), understanding of machine learning and data analytics, certification in cloud platforms (AWS Certified Data Analytics, Google Professional Data Engineer, etc.). 🟣 Recruitment Process: CV review – HR call – Interview (with Live-coding) – Client Interview (with Live-coding) – Hiring Manager Interview – Decision 🎁 Benefits 🎁 ✍ Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺 We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️ We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 21500, ""max"": 33000, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16600,25900,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,503,👉 Senior Azure Data Engineer,Xebia sp. z o.o.,"🟣 You will be: responsible for at-scale infrastructure design, build and deployment with a focus on distributed systems, building and maintaining architecture patterns for data processing, workflow definitions, and system to system integrations using Big Data and Cloud technologies, evaluating and translating technical design to workable technical solutions/code and technical specifications at par with industry standards, driving creation of re-usable artifacts, establishing scalable, efficient, automated processes for data analysis, data model development, validation, and implementation, working closely with analysts/data scientists to understand impact to the downstream data models, writing efficient and well-organized software to ship products in an iterative, continual release environment, contributing and promoting good software engineering practices across the team, communicating clearly and effectively to technical and non-technical audiences, defining data retention policies, monitoring performance and advising any necessary infrastructure changes. 🟣 Your profile: ready to start immediately , 3+ years’ experience with Azure (Data Factory, SQL, Data Lake, Power BI, Devops, Delta Lake, CosmosDB), 5+ years’ experience with data engineering or backend/fullstack software development, strong SQL skills, Python scripting proficiency, experience with data transformation tools – Databricks and Spark, data manipulation libraries (such as Pandas, NumPy, PySpark), experience in structuring and modelling data in both relational and non-relational forms, ability to elaborate and propose relational/non-relational approach, normalization / denormalization and data warehousing concepts (star, Snowflake schemas), designing for transactional and analytical operations, experience with CI/CD tooling (GitHub, Azure DevOps, Harness etc), working knowledge of Git, Databricks will be benefit, good verbal and written communication skills in English. Work from the European Union region and a work permit are required. Candidates must have an active VAT status in the EU VIES registry: https: //ec.europa.eu/taxation_customs/vies/ Please note that we are currently looking to expand our talent pool for future opportunities within the IT industry. While we may not have an immediate project for you at the moment, we are proactively recruiting to ensure that we have the right expertise when new projects arise. We will contact you when a potential project matching your skills and experience becomes available. Thank you for your interest in joining our team. 🟣 Nice to have: ﻿ experience with Azure Event Hubs, Azure Blob Storage, Azure Synapse, Spark Streaming, experience with data modelling tools, preferably DBT, experience with Enterprise Data Warehouse solutions, preferably Snowflake, familiarity with ETL tools (such as Informatica, Talend, Datastage, Stitch, Fivetran etc.), experience in containerization and orchestration (Docker, Kubernetes etc.), cloud (Azure, AWS, GCP) certification. 🟣 Recruitment Process: CV review – HR call – Technical Interview (with live-coding elements) – Client Interview (live-coding)– Hiring Manager call – Decision 🎁 Benefits 🎁 ✍ Development: development budgets of up to 6,800 PLN, we fund certifications e.g.: AWS, Azure, access to Udemy, O'Reilly (formerly Safari Books Online) and more, events and technology conferences, technology Guilds, internal training, Xebia Upskill. 🩺 We take care of your health: private medical healthcare, multiSport card - we subsidise a MultiSport card, mental Health Support. 🤸‍♂️ We are flexible: B2B or employment contract, contract for an indefinite period.","[{""min"": 22300, ""max"": 33700, ""type"": ""Net per month - B2B""}, {""min"": 16600, ""max"": 25900, ""type"": ""Gross per month - Permanent""}]",Data Engineering,16600,25900,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,515,Middle Data Engineer (Databricks),N-iX,"#3821 Join our team to work on enhancing a robust data pipeline that powers ourSaaS product,ensuring seamless contextualization, validation, and ingestion of customer data. Collaborate withproduct teamsto unlock new user experiences by leveragingdata insights.Engage with domain experts to analyze real-world engineering data and build data quality solutions that inspire customer confidence. Additionally, identify opportunities to develop self-service tools that streamline data onboarding and make it more accessible for our users. Our Clientwas established with the mission to fundamentally transform the execution of capital projects and operations. Designed by industry experts for industry experts, Client’s platform empowers users to digitally search, visualize, navigate, and collaborate on assets. Drawing on 30 years of software expertise and 180 years of industrial legacy as part of the renownedScandinavian business group, Client plays an active role in advancing the global energy transition. The company operates from Norway, the UK, and the U.S. Key Responsibilities: Design, build, and maintain data pipelines using Python Collaborate with an international team to develop scalable data solutions Conduct in-depth analysis and debugging of system bugs (Tier 2) Develop and maintain smart documentation for process consistency, including the creation and refinement of checklists and workflows Set up and configure new tenants, collaborating closely with team members to ensure smooth onboarding Write integration tests to ensure the quality and reliability of data services Work with Gitlab to manage code and collaborate with team members Utilize Databricks for data processing and management Requirements: Programming: Minimum of3-4 yearsas data engineer, or in a relevant field. Python Proficiency: Advanced experience inPython, particularly in delivering production-grade data pipelines and troubleshooting code-based bugs. Data Skills: Structured approach to data insights. Cloud: Familiarity with cloud platforms (preferablyAzure). Data Platforms: Experience withDatabricks, Snowflake, or similar data platforms. Database Skills: Knowledge of relational databases, with proficiency inSQL. Big Data: Experience using Apache Spark. Documentation: Experience in creating and maintaining structured documentation. Testing: Proficiency in utilizing testing frameworks to ensure code reliability and maintainability. Version Control: Experience withGitlabor equivalent tools. English Proficiency: B2 level or higher. Interpersonal Skills: Strong collaboration abilities, experience in an international team environment, willing to learn new skills and tools, adaptive and exploring mindset Nice to have: Experience withDockerandKubernetes. Experience with document and graph databases. Ability to travel abroad twice a year for an on-site workshops.","[{""min"": 18101, ""max"": 21536, ""type"": ""Net per month - B2B""}, {""min"": 14591, ""max"": 17547, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14591,17547,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,517,Data Scientist,NeuroSYS,"Nasz klient toglobalna firma farmaceutyczna, która rozbudowuje swoje systemy zarządzania produkcji opierając się na analityce danych z użyciem Data Science i Machine Learning. Obecnie poszukujemy doświadczonego Data Scientista, który pomoże namwykorzystać zgromadzone dane do optymalizacji procesów produkcyjnych. W ramach projektu klient pracuje nad przygotowaniem zestawu narzędzi analitycznych wykorzystujących Power BI jako interfejs do wizualizacji danych oraz bazujący na algorytmach Machine Learning do analizy trendów, odkrywania nieoczywistych wzorców w celu usprawnienia procesu produkcyjnego i wykrywania wczesnych oznak zużycia lub nieprawidłowego działania elementów maszyn (Predictive Maintenance). Twoje zadania: Wykorzystywanie technologiiOCR do ekstrakcji danychz dokumentów papierowych, Ścisła współpraca z ekspertami branżowymi w celu zrozumienia procesów i danych produkcyjnych Analiza potrzeb i wymagań biznesowych w zakresie analizy danych i oczekiwanych rezultatów, Ocena wykonalności i szacowanie czasochłonności dla zgłaszanych potrzeb, Dobór, implementacja i usprawnianiemodeli analitycznych, Przetwarzanie i eksploracja danych pochodzących zsystemu typu historian, Regularne raportowanie postępów prac i prezentacja wyników interesariuszom. Wymagane umiejętności: Doświadczenie wMachine Learning i Data Science: min.3 lata doświadczeniaw komercyjnych projektach wykorzystujących ML, umiejętność trenowania i oceny modeli ML, doświadczenie z technikami uczenia nadzorowanego i nienadzorowanego. Umiejętności programistyczne: znajomość narzędziOCR(AWS Textract, Tesseract), biegła znajomość językaPythonoraz popularnych bibliotekML(m.in. aiohttp, scikit-learn, TensorFlow, PyTorch, XGBoost), doświadczenie w przetwarzaniu i analizie dużych zbiorów danych (Big Data), znajomość narzędzi doprzetwarzania danych, takich jak Pandas, NumPy czy SQL, doświadczenie w pracyz bazami danychSQL i noSQL, praktyczna znajomość GIT oraz GitFlow, podstawowa znajomość Power BI będzie dodatkowym atutem. Umiejętności analityczne i komunikacyjne: zdolność do analizy wymagań biznesowych i przekładania ich na techniczne rozwiązania, umiejętność prezentacji wyników oraz klarownego wyjaśniania złożonych koncepcji technicznych osobom nietechnicznym, swoboda komunikacji wjęzyku angielskimw mowie i piśmie na poziomie C1. Mile widziane: Znajomość koncepcji i metod Predictive Maintenance i Przemysłu 4.0. (np. prognozowanie stanu maszyn, analiza drgań, wykrywanie awarii), zrozumienie pojęć takich jak IoT, SCADA, DCS, IIoT, Znajomość systemów typu historian (np. AVEVA Historian, OSIsoft PI) lub protokołów przemysłowych (OPC, Modbus), Doświadczenie z technologiami chmurowymi (np. Azure, AWS, Google Cloud), zwłaszcza w kontekście ML Ops, Znajomość technik DevOps i CI/CD (np. Docker, Kubernetes) używanych w środowisku ML, Doświadczenie w analizie danych procesowych i integracji z systemami przemysłowymi, Doświadczenie w pracy z danymi produkcyjnymi i procesowymi (np. dane z PLC, SCADA, Historian), Doświadczenie w tworzeniu dashboardów Power BI i ich integracji ze źródłami danych. Oferujemy: Atrakcyjną, pełną wyzwań pracę w zgranym zespole pasjonatów IT i luźnej atmosferze, Udział w innowacyjnych projektach realizowanych dla globalnego klienta, Dowolną formę zatrudnienia, elastyczne godziny pracy, Prywatną opiekę medyczną, Możliwość pracy zdalnej, jak i w biurze; ze sporadycznymi, obowiązkowymi spotkaniami w biurze, w którym czekają świeże owoce, przekąski i pyszna kawa non stop!","[{""min"": 15000, ""max"": 21800, ""type"": ""Net per month - B2B""}, {""min"": 11200, ""max"": 16200, ""type"": ""Gross per month - Permanent""}]",Data Science,11200,16200,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Hybrid,522,Senior Data Engineer,Remodevs,"Please note it's now remote role but later turns into hybrid - so only candidates from Warsaw and surroundings are required. About: We are seeking a highly motivated and self-driven data engineer for our growing data team -who is able to work and deliver independently and as a team. In this role, you will play a crucial part in designing, building and maintaining our ETL infrastructure and data pipelines. Major Responsibilities: ● Design, develop, and deploy Python scripts and ETL processes with Prefect and Airflow to prepare data for analysis. ● Model dimensional and denormalized schemas for optimal performance reporting and discovery. ● Design AI-friendly DB schemas and ontologies. ● Architect cloud ops solutions for data topologies. ● Transform and migrate data with Python, DBT, and Pandas. ● Work with event-based/streaming technologies for real-time ETL. ● Ingest and transform structured, semi-structured, and unstructured data. ● Optimize ETL jobs for performance and scalability to handle big data workloads. ● Monitor and troubleshoot ETL jobs to identify and resolve issues or bottlenecks. ● Implement best practices for data management, security, and governance with Prefect, DBT, and Pandas. ● Write SQL queries, program stored procedures, and reverse engineer existing data pipelines. ● Perform code reviews to ensure fit to requirements, optimal execution pattern,s and adherence to established standards. ● Assist with automated release management and CI/CD processes. ● Validate and cleanse data and handle error conditions gracefully. Skills ● 3+ years of Python development experience, including Pandas ● 5+ years writing complex SQL queries with RDBMSes. ● 5+ years of Experience with developing and deploying ETL pipelines using Airflow, Prefect, or similar tools. ● Experience with cloud-based data warehouses in environments such as RDS, Redshift, or Snowflake. ● Experience with data warehouse design: OLTP, OLAP, Dimensions, and Facts. ● Experience with Cloud-based data architectures, messaging, and analytics. Pluses: Experience with ● Docker ● Kubernetes ● CI/CD automation ● AWS lambdas/step functions ● Data partitioning ● Databricks ● Pyspark ● Cloud certifications","[{""min"": 24012, ""max"": 25859, ""type"": ""Net per month - B2B""}, {""min"": 24012, ""max"": 25859, ""type"": ""Gross per month - Permanent""}]",Data Engineering,24012,25859,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,545,Data Engineer,Volue Sp. z o.o.,"Volue is a market leader in technologies and services that power the green transition. Around 800 employees work with more than 2 200 customers on energy, power grid, water & infrastructure projects. Our mission is to provide innovative services critical to society, unlocking a cleaner, better and more profitable future. Working towards this goal, Volue has become a leading technology supplier and enabler of the green transition, helping energy companies simplify and optimize everyday operations. We are now looking for aData Engineerwho can help us to provide solutions for customers for making informed decisions in volatile short-term markets. You will design, implement, and maintain data pipelines and storages, become a data manager of our model inputs, and create insightful and powerful analysis and visualization for day-ahead, intraday and balancing data. In our day-to-day work, we include pair programming, joint learning sessions, and recurring hacking days to explore new ideas. We have very much an agile and digital way of working with fast feedback loops and embracing a culture of learning and personal growth. What you will be doing to make a difference? Thrive in an empowered, self-driven team where you take ownership across the entire data lifecycle: Work together with in-house analysts to understand the domain and the data in question. Design, build and maintain flexible and scalable end-to-end data pipelines together with software engineers. Monitor quality and reliability of data and implement required tooling in coordination with data scientists. Visualize data in a meaningful way for in-house analysis and together with product manager and UX designer, for customer facing dashboards. What do you need to succeed in the role? A Bachelor’s or Master's degree in a relevant field, such as Computer Science or Data Engineering. At least 3 years of hands-on experience in programming and data processing using Python; expertise in general statistics is a strong advantage. A good understanding of database systems. Be familiar with some of the following concepts: REST APIs, gRPC, CI/CD, Docker, Cloud services, Prometheus/Grafana, Airflow. A strong sense of clean code, test-driven development, and data quality. An open mind, curiosity, and excellent communication skills that foster collaboration and innovation. A passion for teamwork, valuing constructive feedback, and a belief that success is best achieved together. Why will you love being part of our team? Supportive Onboarding: Begin your journey with a thorough introduction and a steep learning curve. Room to Grow: Shape and develop your role with a large degree of influence. Mission-Driven Culture: Join one of Europe’s most exciting green tech companies and contribute to building a more sustainable future. Inclusive Environment: Work in an innovative, international, and supportive atmosphere. Competitive Benefits: Enjoy salaries that reflect your professional experience, flexible working hours, and a hybrid work model that fits your lifestyle. Team Spirit: Collaborate with talented, inspiring colleagues who believe in succeeding together. Attractive Perks: Benefit from our referral program and other employee-focused initiatives. We are looking to hire for Volue office in Gdańsk but will be ready to consider other locations for the right candidate. In Volue, we cherish each employee’s competence, ideas and personality. Let your skills and talent be a part of our team – and let us leave our mark on the world together! Company Culture In Volue, we believe that in order to be a successful company, we need to bring everyone to the table. We look at diversity as a competitive advantage. A diverse workforce enables better decision-making and creates more value. By inclusion, we refer to the sense of belonging and being part of a community at work. We want the people of Volue to feel welcome, valued and not least encouraged to bring their whole, unique selves to work. Volue is about people. From staff to client, people are at the center of all our operations, and we always strive for a flat structure where everyone feels included, appreciated and recognized for their individual efforts. Or as we call it ""ONE Volue"". We hire talented individuals, regardless of gender, race, ethnicity, ancestry, age, disability, sexual orientation, gender identity or expression, cultural background or religious beliefs.","[{""min"": 14000, ""max"": 22000, ""type"": ""Net per month - B2B""}, {""min"": 12000, ""max"": 20000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,12000,20000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,549,Senior Data Engineer,N-iX,"#3071 We are seeking a proactive Senior Data Engineer to join our vibrant team. As a Senior Data Engineer, you will play a critical role in designing, developing, and maintaining sophisticated data pipelines, Ontology Objects, and Foundry Functions within Palantir Foundry. The ideal candidate will possess a robust background in cloud technologies, data architecture, and a passion for solving complex data challenges. Key Responsibilities: Collaborate with cross-functional teams to understand data requirements, and design, implement and maintain scalable data pipelines in Palantir Foundry, ensuring end-to-end data integrity and optimizing workflows. Gather and translate data requirements into robust and efficient solutions, leveraging your expertise in cloud-based data engineering. Create data models, schemas, and flow diagrams to guide development. Develop, implement, optimize and maintain efficient and reliable data pipelines and ETL/ELT processes to collect, process, and integrate data to ensure timely and accurate data delivery to various business applications, while implementing data governance and security best practices to safeguard sensitive information. Monitor data pipeline performance, identify bottlenecks, and implement improvements to optimize data processing speed and reduce latency. Troubleshoot and resolve issues related to data pipelines, ensuring continuous data availability and reliability to support data-driven decision-making processes. Stay current with emerging technologies and industry trends, incorporating innovative solutions into data engineering practices, and effectively document and communicate technical solutions and processes. Tools and skills you will use in this role: Palantir Foundry Python PySpark SQL TypeScript Required: 5+ years of experience in data engineering, preferably within the pharmaceutical or life sciences industry; Strong proficiency in Python and PySpark; Proficiency with big data technologies (e.g., Apache Hadoop, Spark, Kafka, BigQuery, etc.); Hands-on experience with cloud services (e.g., AWS Glue, Azure Data Factory, Google Cloud Dataflow); Expertise in data modeling, data warehousing, and ETL/ELT concepts; Hands-on experience with database systems (e.g., PostgreSQL, MySQL, NoSQL, etc.); Proficiency in containerization technologies (e.g., Docker, Kubernetes); Effective problem-solving and analytical skills, coupled with excellent communication and collaboration abilities; Strong communication and teamwork abilities; Understanding of data security and privacy best practices; Strong mathematical, statistical, and algorithmic skills. Nice to have: Certification in Cloud platforms, or related areas; Experience with search engine Apache Lucene, Webservice Rest API; Familiarity with Veeva CRM, Reltio, SAP, and/or Palantir Foundry; Knowledge of pharmaceutical industry regulations, such as data privacy laws, is advantageous; Previous experience working with JavaScript and TypeScript.","[{""min"": 18470, ""max"": 19579, ""type"": ""Net per month - B2B""}, {""min"": 14776, ""max"": 15515, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14776,15515,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,564,BI Developer / SQL Developer,Beesafe,"About Us: Join our trailblazing team as we expand our digital horizons. From our beginnings as visionary InsurTech to becoming a key player in the Polish digital insurance market, we are part of the esteemed Vienna Insurance Group. We're redefining the rules in the insurance industry with our innovative approach. Our hybrid working model supports both the collaborative energy of office work and the flexibility of remote work. About the Role: We're seeking passionate BI/SQL Developer to join our team. You'll be at the heart of developing and implementing high-quality application software, using state-of-the-art tools and technologies. This is your chance to make a significant impact on one of the most exciting and unique products in the Polish digital insurance market. Why it is worth to work with us? You’ll be contributing to the reporting solution and data model design of our Data Platform with close cooperation with our Data Engineers You’ll gather business requirements and work closely with business stakeholders You’ll deliver end-to-end business intelligence/reporting solutions using SQL/PowerBI What you need to start the adventure with us: +1 year of commercial experience in data extraction, ETL, and report development Proficiency in SQL Understanding of Relational Database Management System and Business Intelligence concepts Business and collaboration skills, and responsive to service needs and operational demands Domain knowledge gained across the insurance or financial sector Nice to have: Experience with BI tools (Power BI would be a plus) Experience with cloud solutions (we use Azure) Familiarity with code version control systems such as GIT Understanding of the principles of Agile and Scrum (we work in Scrum) Enthusiastic approach to coffee breaks (we love informal discussions with a cup of favorite coffee or tea) Why Join Us? Be part of a dynamic team driving digital innovation in the insurance and eCommerce sectors Opportunity to work in a collaborative and forward-thinking environment Contract options: B2B cooperation Engage in meaningful work that directly impacts business success Join a company that values work-life balance and fosters a positive team culture Comprehensive onboarding, including a dedicated Buddy program Remote work flexibility with hybrid office visits Flexible working hours Access to the latest tools and cloud-native solutions A comprehensive benefits package, including health insurance and MultiSport card Employee discounts on insurance products Referral program and sports club memberships Sounds interesting? Join us and help shape the future! 🚀","[{""min"": 9000, ""max"": 12000, ""type"": ""Net per month - B2B""}, {""min"": 9000, ""max"": 12000, ""type"": ""Gross per month - Permanent""}]",Database Administration,9000,12000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,569,Azure Data Engineer z kompetencjami Devopsowymi,UNIVIO,"Jesteśmy polską firmą technologiczną z ponad 25-letnim doświadczeniem jako partner cyfrowej transformacji handlu. Realizujemy międzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocześnie luźną, niezobowiązującą atmosferę. Nasza organizacja opiera się na kulturze otwartości i dzielenia się wiedzą. Dowiedz się kogo szukamy i zaaplikuj, jeśli spełniamy Twoje oczekiwania 😉","[{""min"": 16800, ""max"": 23520, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 17300, ""type"": ""Gross per month - Permanent""}]",Data Engineering,13600,17300,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,581,Machine Learning Engineer (LLM),UNIVIO,"Jesteśmy polską firmą technologiczną z ponad 25-letnim doświadczeniem jako partner cyfrowej transformacji handlu. Realizujemy międzynarodowe projekty dla topowych marek. Znajdziesz u nas stabilne zatrudnienie, a jednocześnie luźną, niezobowiązującą atmosferę. Nasza organizacja opiera się na kulturze otwartości i dzielenia się wiedzą. Dowiedz się kogo szukamy i zaaplikuj, jeśli spełniamy Twoje oczekiwania 😉","[{""min"": 16800, ""max"": 20160, ""type"": ""Net per month - B2B""}, {""min"": 13600, ""max"": 16500, ""type"": ""Gross per month - Permanent""}]",Data Science,13600,16500,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Remote,612,Senior Data Engineer,XTB,"Tworzymy XTB – globalną firmę inwestycyjną, oferującą innowacyjne rozwiązania technologiczne, które pozwalają naszym klientom skutecznie zarządzać swoimi finansami na wiele sposobów. Wszystko to w jednej intuicyjnej aplikacji XTB, z której korzysta już ponad milion użytkowników na całym świecie! Jesteśmy certyfikowaną firmąGreat Place to Work. Poszukujemy osoby, która dołączy do naszego zespołu Data Platform w roli Data Engineer. Głównym zadaniem zespołu jest utrzymanie i rozwój hurtowni danych on premise i jej docelowa migracja na środowisko chmurowe. Współpracujemy z zespołami produktowymi w celu dostarczenia danych potrzebnych do podejmowania kluczowych decyzji biznesowych. Pracujemy w frameworku Scrum. Do Twoich codziennych obowiązków będzie należało: Projektowanie i utrzymywanie hurtowni danych oraz migracja danych i procesów na środowisko chmure, Tworzenie i utrzymywanie modeli danych do wspierania kluczowych decyzji biznesowych, Integracja danych w celu wytworzenia wymaganych modeli danych, Wdrażanie kontroli jakości danych i procesów walidacji, aby zapewnić dokładność, spójność i kompletność, Współpraca z innymi zespołami, aby tworzyć zestawy danych spełniające potrzeby raportowania i rozwiązujące problemy systemowe, Projektowanie i wykonywanie testów wydajnościowych i integracyjnych, Raportowanie kluczowych wskaźników w firmie w narzędziu BI Wymagania: Co najmniej 4-letnie doświadczenie w pracy na stanowisku SQL Developer / Data Engineer Znajomość Python, SQL w tym T-SQL, pisanie złożonych procedur składowanych, optymalizacja wydajności, Umiejętność tworzenia procesów ETL (SSIS, Airflow), Doświadczenie zawodowe w eksploracji danych, analizie i modelowaniu złożonych zbiorów danych na dużą skalę; Doświadczenie w pracy z narzędziami do zarządzania kodem źródłowym, takimi jak GIT Dobra znajomość zasad standardów integracyjnych: REST, gRPC Umiejętność tworzenia rozwiązań w oparciu o serwisy w Snowflake Doświadczenie w tworzeniu, wdrażaniu i rozwiązywaniu problemów z aplikacjami danych na platformie Microsoft Azure Znajomość rozwiązań chmurowych (Azure, GCP), Silne umiejętności rozwiązywania problemów i dbałość o szczegóły. Umiejętność skutecznej komunikacji i współpracy w zespole. Chęć uczenia się i dostosowywania do nowych technologii i koncepcji. Rozumienie zasad Agile i Scrum (pracujemy w Scrumie) Dodatkowe atuty: Doświadczenie w budowaniu skalowalnych, działających w czasie rzeczywistym rozwiązań typu Data Lake, Doświadczenie ze strumieniowym przesyłaniem danych (Kafka), Doświadczenie w pracy z Kubernetes Znajomość koncepcji Data Mesh. Znajomość podejścia DevOps Oferujemy Realny wpływ na rozwój firmy i produktu Pracę w doświadczonym zespole, który chętnie dzieli się wiedzą Jasną wizję rozwoju dzięki regularnym feedbackom i klarownym ścieżkom karier Budżet szkoleniowy na interesujące Cię kursy i konferencje Dodatkowy dzień wolny z okazji Twoich urodzin Dodatkowy dzień wolny dla rodziców Sprzęt dopasowany do Twoich potrzeb Prywatną opiekę medyczną i ubezpieczenie grupowe Dostęp do platformy e-learningowej do nauki języka angielskiego oraz platformy benefitowej Dostęp do platformy wellbeingowej i możliwość skorzystania z warsztatów oraz prywatnych sesji terapeutycznych Pracę zdalną, z biura w Warszawie lub z coworku w Twoim mieście Regularne spotkania integracyjne","[{""min"": 15000, ""max"": 19000, ""type"": ""Net per month - B2B""}, {""min"": 14000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Engineering,14000,18000,Gross per month - Permanent
Full-time,Mid,Permanent or B2B,Hybrid,627,Data Architect / Data Warehouse Specialist,Power Media,"Nasz klient kompleksowo realizuje usługi w zakresie konsultingu, projektowania i zarządzania projektami. Specjalizuje się głównie w obszarach e-commerce, data management (Agile Data Warehouse, Data Governance) oraz Content Management. Stanowisko: Data Architect / Data Warehouse Specialist. Lokalizacja biura: Katowice. Model pracy: hybrydowy (praca z biura 1 raz w tygodniu, we wtorek). O projekcie: Przygotowanie, utrzymanie oraz rozwój rozwiązań Data Platform w środowisku chmurowym. Kluczowym zadaniem będzie budowa warstwy integracji danych w ramach migracji systemu legacy do nowej instalacji w chmurze oraz przygotowanie interfejsów w złożonym krajobrazie systemowym klienta. Projekt obejmuje budowę hurtowni danych oraz platform danych od podstaw. Zespółskłada się z 22 osób: 4x PM, 2x Leader, 6xArchitect, 10x Senior Developer/DWH Specialist. Zakres obowiązków: Rozwój i utrzymanie Data Warehouse oraz Data Platform przy użyciu narzędzi ETL i SQL, Analiza wymagań biznesowych i proponowanie rozwiązań technicznych, Współpraca z zespołem w celu zapewnienia dostępności rozwiązań biznesowych, Wykorzystanie swojej wiedzy i doświadczenia do tworzenia innowacyjnych rozwiązań. Główne wymagania: Bardzo dobra znajomość metod i technik projektowania oprogramowania, Ponad 10-letnie doświadczenie w projektowaniu i utrzymaniu BI/DWH, rozwoju ETL i modelowaniu danych, Znajomość ekosystemów Big Data, Znajomośćrozwiązań chmurowych Azure/AWS/Google Cloud Platform/SnowFlake, Dobre umiejętności analityczne (analiza i dokumentowanie wymagań biznesowych oraz specyfikacji technicznych) Doświadczenie w pracy z bazami danych: Oracle, MySQL, Teradata, MS SQL, Znajomość SQL, PL/SQL, T-SQL, optymalizacji SQL, Unix-Shellscripts, Data Modeling, Znajomość rozwiązań ETL w chmurze (Informatica, Talend, Azure Data Factory), Bardzo dobra znajomość języka angielskiego (min. B2+/C1)– codzienna praca w międzynarodowym środowisku, Gotowość do podróży służbowych. Mile widziane: Znajomośćnarzędzi Informatica, Znajomość Machine Learning oraz frameworków ML (TensorFlow, PyTorch, Caffe, MxNet), Znajomość języków programowania (Java, Scala, C++, Python). Znajomość języka niemieckiego. Firma oferuje: Stabilne, długofalowe zatrudnienie w oparciu o B2B lub UoP, Możliwość pracy w większości zdalnej(praca z biura 1 raz w tygodniu, we wtorek) Płaska struktura, antykorporacyjne podejście do pracy i zespołu, Ciekawe, międzynarodowe projekty, Zgrany zespół chętnie uczestniczący w aktywnościach sportowo – rekreacyjnych (lokalne akcje, wyjazdy integracyjne, warsztaty team – building), Dodatkowe zajęcia z j. angielskiego, Dofinansowanie do ubezpieczenia i zajęć sportowych, Nowoczesne biuro ze strefą relaksu, Co roczny 5 dniowy wyjazd firmowy (cała firma), warsztaty kulinarne, wyjścia firmowe, cykliczne wjazdy weekendowe zamiennie z lokalnymi sportowymi aktywnościami Świetna atmosfera, partnerskie podejście, Prosty i sprawny proces rekrutacji -> tylko 1 etap: Rozmowa „miękko-techniczna”. CV w j. angielskim.","[{""min"": 20000, ""max"": 30000, ""type"": ""Net per month - B2B""}, {""min"": 16000, ""max"": 18000, ""type"": ""Gross per month - Permanent""}]",Data Architecture,16000,18000,Gross per month - Permanent
Full-time,Senior,Permanent or B2B,Remote,648,Senior Data Engineer with Snowflake (greenfield),N-iX,"#3359 Join an exciting journey to create a greenfield, cutting-edge Consumer Data Lake for a leading global organization based in Europe. This platform will unify, process, and leverage consumer data from various systems, unlocking advanced analytics, insights, and personalization opportunities. As a Senior Data Engineer, you will play a pivotal role in shaping and implementing the platform's architecture, focusing on hands-on technical execution and collaboration with cross-functional teams. Your work will transform consumer data into actionable insights and personalization on a global scale. Using advanced tools to tackle complex challenges, you’ll innovate within a collaborative environment alongside skilled architects, engineers, and leaders. Key Responsibilities: Hands-On Development : Build, maintain, and optimize data pipelines for ingestion, transformation, and activation. Create and implement scalable solutions to handle diverse data sources and high volumes of information. Data Modeling & Warehousing : Design and maintain efficient data models and schemas for a cloud-based data platform. Develop pipelines to ensure data accuracy, integrity, and accessibility for downstream analytics. Collaboration : Partner with Solution Architects to translate high-level designs into detailed implementation plans. Work closely with Technical Product Owners to align data solutions with business needs. Collaborate with global teams to integrate data from diverse platforms, ensuring scalability, security, and accuracy. Platform Development : Enable data readiness for advanced analytics, reporting, and segmentation. Implement robust frameworks to monitor data quality, accuracy, and performance. Testing & Quality Assurance : Implement robust security measures to protect sensitive consumer data at every stage of the pipeline Ensure compliance with data privacy regulations (e.g., GDPR, CCPA ..) and internal policies. Monitor and address potential vulnerabilities, ensuring the platform adheres to security best practices. Requirements: Over 4+ years of experience showcasing technical expertise and critical thinking in data engineering. Hands-on experience with DBT and strong Python programming skills. Proficiency in Snowflake and expertise in data modeling are essential. Demonstrated experience in building consumer data lakes and developing consumer analytics capabilities is required. In-depth understanding of privacy and security engineering within Snowflake , including concepts like RBAC, dynamic/tag-based data masking, row-level security/access policies, and secure views. Ability to design, implement, and promote advanced solution patterns and standards for solving complex challenges. Familiarity with multiple cloud platforms ( Azure or GCP preferred, with a focus on Azure). Practical experience with Big Data batch and streaming tools. Competence in SQL, NoSQL, relational database design (SAP HANA experience is a bonus), and efficient methods for data retrieval and preparation at scale. Proven ability to collect and process raw data at scale, including scripting, web scraping, API integration, and SQL querying. Experience working in global environments and collaborating with virtual teams. A Bachelor’s or Master’s degree in Data Science, Computer Science, Economics, or a related discipline. We offer*: Flexible working format - remote, office-based or flexible. A competitive salary and good compensation package. Personalized career growth. Professional development tools (mentorship program, tech talks and trainings, centers of excellence, and more). Active tech communities with regular knowledge sharing. Education reimbursement Memorable anniversary presents. Corporate events and team building. Other location-specific benefits. *not applicable for freelancers.","[{""min"": 22164, ""max"": 29553, ""type"": ""Net per month - B2B""}, {""min"": 17731, ""max"": 24566, ""type"": ""Gross per month - Permanent""}]",Data Engineering,17731,24566,Gross per month - Permanent
